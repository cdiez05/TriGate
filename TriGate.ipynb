{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TriGate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrag.config import Config\n",
    "\n",
    "\n",
    "def set_config(dataset_name: str, split='train'):\n",
    "    # Initialize a note indicating this is a zero-shot setup\n",
    "    save_note = 'zero-shot'\n",
    "    \n",
    "    # Create a dictionary to store configuration details including the dataset name and split\n",
    "    config_dict = {'save_note': save_note, 'dataset_name': dataset_name, 'split': split}\n",
    "\n",
    "    # Create a configuration object using an external YAML file and the provided config_dict\n",
    "    config = Config('carlos_config.yaml', config_dict)\n",
    "    \n",
    "    \n",
    "    # Return the configuration object\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects2/dsml/cdiezmar/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def get_load_generator_model(config, hidden_states=True, float16=True):\n",
    "    # Set the cache directory path for Hugging Face models\n",
    "    cache_path = './hf_cache/'\n",
    "\n",
    "    # Set environment variables for Hugging Face to use the cache directory\n",
    "    os.environ['HF_HOME'] = cache_path\n",
    "    os.environ['TRANSFORMERS_CACHE'] = cache_path\n",
    "\n",
    "    # Extract the model name from the configuration\n",
    "    name_model = config['generator_model']\n",
    "\n",
    "    # Define the folder where the model is located\n",
    "    folder_path = '/cs/student/projects2/dsml/cdiezmar/models/'\n",
    "    model_path = folder_path + name_model\n",
    "\n",
    "    # Load model configuration with or without hidden states based on the input argument\n",
    "    if hidden_states:\n",
    "        model_config = AutoConfig.from_pretrained(model_path, cache_dir=cache_path)\n",
    "        model_config.output_hidden_states = True  # Enable output of hidden states\n",
    "\n",
    "    # Load the model with the specified precision (float16 or default precision)\n",
    "    if float16:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, config=model_config, cache_dir=cache_path, torch_dtype=torch.float16)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, config=model_config, cache_dir=cache_path)\n",
    "\n",
    "    # Load the tokenizer for the model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=cache_path)\n",
    "\n",
    "    # Return the loaded model and tokenizer\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "def prepare_data(one_hidden_states, zero_hidden_states):\n",
    "    # Create labels for time-sensitive (1) and non-time-sensitive (0) data\n",
    "    one_labels = torch.ones(one_hidden_states.size(0), dtype=torch.long)  # Time-sensitive = 1\n",
    "    zero_labels = torch.zeros(zero_hidden_states.size(0), dtype=torch.long)  # Non-time-sensitive = 0\n",
    "\n",
    "    # Concatenate the time-sensitive and non-time-sensitive hidden states into one dataset\n",
    "    all_data = torch.cat((one_hidden_states, zero_hidden_states), dim=0)\n",
    "    \n",
    "    # Concatenate the corresponding labels\n",
    "    all_labels = torch.cat((one_labels, zero_labels), dim=0)\n",
    "\n",
    "    # Create a dataset by pairing the hidden states with their corresponding labels\n",
    "    dataset = list(zip(all_data, all_labels))\n",
    "    \n",
    "    # Shuffle the dataset to randomize the order of samples\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    # Split the dataset into training (80%) and validation (20%) sets\n",
    "    split_idx = int(0.8 * len(dataset))\n",
    "    train_data, val_data = dataset[:split_idx], dataset[split_idx:]\n",
    "\n",
    "    # Unzip the inputs and labels for the training and validation datasets\n",
    "    train_inputs, train_labels = zip(*train_data)\n",
    "    val_inputs, val_labels = zip(*val_data)\n",
    "\n",
    "    # Convert the training and validation inputs into tensors\n",
    "    train_inputs = torch.stack(train_inputs)\n",
    "    train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "    val_inputs = torch.stack(val_inputs)\n",
    "    val_labels = torch.tensor(val_labels, dtype=torch.long)\n",
    "\n",
    "    # Return the prepared training and validation inputs and labels\n",
    "    return train_inputs, train_labels, val_inputs, val_labels\n",
    "\n",
    "def prepare_data_intent(new_hidden_states):\n",
    "    # Assume half of the dataset is intent-aware (1) and the other half is not (0)\n",
    "    half_size = new_hidden_states.size(0) // 2\n",
    "\n",
    "    # Split the dataset into two halves: one for intent-aware (1) and the other for non-intent-aware (0)\n",
    "    one_hidden_states = new_hidden_states[:half_size]  # Intent-aware\n",
    "    zero_hidden_states = new_hidden_states[half_size:]  # Non-intent-aware\n",
    "\n",
    "    # Create labels: 1 for intent-aware, 0 for non-intent-aware\n",
    "    one_labels = torch.ones(one_hidden_states.size(0), dtype=torch.long)  # Intent-aware = 1\n",
    "    zero_labels = torch.zeros(zero_hidden_states.size(0), dtype=torch.long)  # Non-intent-aware = 0\n",
    "\n",
    "    # Concatenate the hidden states and corresponding labels to form the complete dataset\n",
    "    all_data = torch.cat((one_hidden_states, zero_hidden_states), dim=0)\n",
    "    all_labels = torch.cat((one_labels, zero_labels), dim=0)\n",
    "\n",
    "    # Pair the data and labels, and shuffle the dataset\n",
    "    dataset = list(zip(all_data, all_labels))\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    # Split the dataset into training (80%) and validation (20%) sets\n",
    "    split_idx = int(0.8 * len(dataset))\n",
    "    train_data, val_data = dataset[:split_idx], dataset[split_idx:]\n",
    "\n",
    "    # Unzip the training and validation data to separate inputs and labels\n",
    "    train_inputs, train_labels = zip(*train_data)\n",
    "    val_inputs, val_labels = zip(*val_data)\n",
    "\n",
    "    # Convert the inputs and labels into PyTorch tensors\n",
    "    train_inputs = torch.stack(train_inputs)\n",
    "    train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "    val_inputs = torch.stack(val_inputs)\n",
    "    val_labels = torch.tensor(val_labels, dtype=torch.long)\n",
    "\n",
    "    # Return the prepared training and validation data\n",
    "    return train_inputs, train_labels, val_inputs, val_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "### GPU\n",
    "\n",
    "def extract_hidden_states(config, template, batch_size=6, device='cpu'):\n",
    "    # Prepare the dataset and load the model and tokenizer\n",
    "    all_split = get_dataset(config)  # Retrieve the dataset based on the provided config\n",
    "    split = config['split'][0]  # Use the specified dataset split (e.g., train, test)\n",
    "    dataset = all_split[split]  # Access the split dataset (e.g., questions in this case)\n",
    "    \n",
    "    # Load the generator model with hidden state outputs and float16 precision\n",
    "    model, tokenizer = get_load_generator_model(config=config, hidden_states=True, float16=True)\n",
    "    \n",
    "    # Set pad token if it is not already set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Move model to the specified device (CPU or GPU)\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    hidden_states_list = []  # Initialize a list to store hidden states\n",
    "\n",
    "    # Process the dataset in batches\n",
    "    for i in tqdm(range(0, len(dataset.question), batch_size)):\n",
    "        # Get a batch of questions\n",
    "        batch_questions = dataset.question[i:i+batch_size]\n",
    "        \n",
    "        # Use the template to create input prompts based on the questions\n",
    "        input_prompts = [template.get_string(question=q) for q in batch_questions]\n",
    "        \n",
    "        # Tokenize the input prompts, returning them as tensors with padding and truncation\n",
    "        inputs = tokenizer(input_prompts, return_tensors='pt', padding=True, truncation=True)\n",
    "        \n",
    "        # Move the inputs to the specified device (CPU or GPU)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Disable gradient calculations as we're in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            # Forward pass through the model to get outputs including hidden states\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Extract the last hidden state of the last token in each sequence and move to CPU\n",
    "        batch_hidden_states = outputs.hidden_states[-1][:, -1, :].cpu().numpy()\n",
    "        \n",
    "        # Add the extracted hidden states to the list\n",
    "        hidden_states_list.extend(batch_hidden_states)\n",
    "\n",
    "    # Return the complete list of hidden states\n",
    "    return hidden_states_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrag.utils import get_dataset\n",
    "def load_datasets(config_taqa, config_triviaqa, template_taqa, template_triviaqa):\n",
    "    # Extract hidden states from TAQA dataset using the provided config and template\n",
    "    taqa_hidden_states = extract_hidden_states(config=config_taqa, template=template_taqa)\n",
    "    \n",
    "    # Extract hidden states from TriviaQA dataset using the provided config and template\n",
    "    triviaqa_hidden_states = extract_hidden_states(config=config_triviaqa, template=template_triviaqa)\n",
    "    \n",
    "    # Return the hidden states for both datasets\n",
    "    return taqa_hidden_states, triviaqa_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines - ZeroShot SKR VanillaFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrag.config import Config\n",
    "from flashrag.pipeline import SequentialPipeline\n",
    "from flashrag.utils import get_dataset\n",
    "\n",
    "def zero_shot(dataset_name: str, split: str):\n",
    "    # Set the save note to indicate a zero-shot task\n",
    "    save_note = 'zero-shot'\n",
    "\n",
    "    # Create a configuration dictionary with the dataset name and split (train, test, etc.)\n",
    "    config_dict = {'save_note': save_note, 'dataset_name': dataset_name, 'split': split}\n",
    "\n",
    "    # Load the configuration using a YAML file and the provided config_dict\n",
    "    config = Config('carlos_config.yaml', config_dict)\n",
    "\n",
    "    # Retrieve the dataset using the configuration and extract the specific split (e.g., test data)\n",
    "    all_split = get_dataset(config)\n",
    "    test_data = all_split[split]\n",
    "\n",
    "    # Import the PromptTemplate to create a question-answer prompt structure\n",
    "    from flashrag.prompt import PromptTemplate\n",
    "    \n",
    "    # Set up a system prompt and user prompt for zero-shot question answering\n",
    "    template = PromptTemplate(\n",
    "        config=config,\n",
    "        system_prompt=\"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt=\"Question: {question}\"\n",
    "    )\n",
    "\n",
    "    # Set up the pipeline for running the zero-shot inference\n",
    "    pipeline = SequentialPipeline(config, template)\n",
    "\n",
    "    # Run the pipeline on the test data, performing evaluation (`do_eval=True`)\n",
    "    result = pipeline.naive_run(test_data, do_eval=True)\n",
    "\n",
    "    # Return the result, configuration, and template\n",
    "    return result, config, template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrag.judger import SKRJudger\n",
    "from flashrag.config import Config\n",
    "from flashrag.utils import get_dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def skr_judger_fun(dataset):\n",
    "    # Set up the configuration for the SKR Judger model\n",
    "    judger_name = 'skr'  # Name of the judger\n",
    "    model_path = '/cs/student/projects2/dsml/cdiezmar/models/skr_embeddings'  # Path to the SKR model embeddings\n",
    "    training_data_path = '/cs/student/projects2/dsml/cdiezmar/dataset/skr_dataset/skr_training.json'  # Path to the SKR training data\n",
    "    dataset_name = dataset  # Name of the dataset being processed\n",
    "    split = 'train'  # Specify the split (train, test, etc.)\n",
    "\n",
    "    # Create a configuration dictionary for the SKR judger\n",
    "    config_dict = {\n",
    "        'judger_name': judger_name,\n",
    "        'judger_model_path': model_path,\n",
    "        'judger_training_data_path': training_data_path,\n",
    "        'judger_topk': 3,  # Number of top results to consider\n",
    "        'save_note': 'skr',  # Note to indicate this is related to SKR\n",
    "        'dataset_name': dataset_name,  # Dataset name from the input argument\n",
    "        'split': split  # Data split (train/test)\n",
    "    }\n",
    "\n",
    "    # Load the configuration from a YAML file using the configuration dictionary\n",
    "    config = Config('carlos_config.yaml', config_dict)\n",
    "\n",
    "    # Retrieve the dataset based on the configuration and extract the specified split (train)\n",
    "    all_split = get_dataset(config)\n",
    "    test_data = all_split[split]\n",
    "\n",
    "    # Initialize the SKRJudger with the loaded configuration\n",
    "    skr = SKRJudger(config)\n",
    "\n",
    "    # Perform the judgment task on the test data using the SKRJudger\n",
    "    inverted_result = skr.judge(test_data)\n",
    "\n",
    "    # Invert the result (flip True/False) to return the expected outcome\n",
    "    result = [not item for item in inverted_result]\n",
    "\n",
    "    # Return the processed results\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrag.config import Config\n",
    "from flashrag.pipeline import SequentialPipeline\n",
    "from flashrag.utils import get_dataset\n",
    "import numpy as np\n",
    "\n",
    "def vanillaprompt_filter(dataset_name: str, split: str):\n",
    "    # Set the save note to indicate a zero-shot task\n",
    "    save_note = 'zero-shot'\n",
    "\n",
    "    # Create a configuration dictionary with the dataset name and split\n",
    "    config_dict = {'save_note': save_note, 'dataset_name': dataset_name, 'split': split}\n",
    "\n",
    "    # Load the configuration using a YAML file and the provided config_dict\n",
    "    config = Config('carlos_config.yaml', config_dict)\n",
    "\n",
    "    # Retrieve the dataset using the configuration and extract the specified split (e.g., test data)\n",
    "    all_split = get_dataset(config)\n",
    "    test_data = all_split[split]\n",
    "\n",
    "    # Import the PromptTemplate to create a prompt structure for the filter\n",
    "    from flashrag.prompt import PromptTemplate\n",
    "    \n",
    "    # Set up a prompt template that asks if the question requires retrieving an answer\n",
    "    template = PromptTemplate(\n",
    "        config=config,\n",
    "        system_prompt=\"Given a question, determine whether you need to retrieve the answer. Answer [Yes] or [No]\",\n",
    "        user_prompt=\"Question: {question}\"\n",
    "    )\n",
    "\n",
    "    # Define a function that processes the model's output and converts it to 1 or 0\n",
    "    pred_process_fun = lambda x: 1 if \"Yes\" in x.split(\"\\n\")[0] else 0\n",
    "\n",
    "    # Set up the pipeline with the configuration and prompt template\n",
    "    pipeline = SequentialPipeline(config, template)\n",
    "\n",
    "    # Get the raw results from the pipeline\n",
    "    raw_result = pipeline.naive_output(test_data)\n",
    "\n",
    "    # Apply the prediction processing function to convert raw results into binary (0 or 1)\n",
    "    result = [pred_process_fun(answer) for answer in raw_result]\n",
    "\n",
    "    # Convert the result list to a NumPy array\n",
    "    result_array = np.array(result)\n",
    "\n",
    "    # Define the folder path and file name to save the result as a .npy file\n",
    "    folder_path = '/cs/student/projects2/dsml/cdiezmar/vanilla_results/'\n",
    "    file_type = '.npy'\n",
    "    file_path = folder_path + dataset_name + file_type\n",
    "\n",
    "    # Save the result array to the specified file\n",
    "    np.save(file_path, result_array)\n",
    "\n",
    "    # Return the result array\n",
    "    return result_array\n",
    "\n",
    "# result = vanillaprompt_filter('retrievalqa_24000', 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks - classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def train_mlp_classifier_cuda(train_data, train_labels, val_data, val_labels, input_size, num_classes, epochs=500, lr=1e-4):\n",
    "    # Determine if a CUDA-capable GPU is available; otherwise, use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Move training and validation data to the specified device (GPU or CPU)\n",
    "    train_data, train_labels = train_data.to(device), train_labels.to(device)\n",
    "    val_data, val_labels = val_data.to(device), val_labels.to(device)\n",
    "\n",
    "    # Initialize the MLP classifier model and move it to the device\n",
    "    model = MLPClassifier(input_size, num_classes).to(device)\n",
    "\n",
    "    # Define the loss function (CrossEntropyLoss for multi-class classification)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Use Adam optimizer with the specified learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop for the specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        outputs = model(train_data)  # Forward pass: compute model outputs for the training data\n",
    "        loss = criterion(outputs, train_labels)  # Compute the loss\n",
    "        loss.backward()  # Backpropagate the loss to compute gradients\n",
    "        optimizer.step()  # Update model parameters using the optimizer\n",
    "\n",
    "        # Validation phase (no gradient calculation)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient tracking during validation\n",
    "            val_outputs = model(val_data)  # Forward pass on the validation data\n",
    "            val_loss = criterion(val_outputs, val_labels)  # Compute validation loss\n",
    "\n",
    "        # Print training and validation loss for each epoch\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "        c_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x.unsqueeze(1), (h_0, c_0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def train_lstm_classifier_cuda(train_data, train_labels, val_data, val_labels, input_size, hidden_size, num_layers, num_classes, epochs=500, lr=1e-4):\n",
    "    # Check if a CUDA-capable GPU is available, otherwise use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Move training and validation data (and labels) to the selected device (GPU or CPU)\n",
    "    train_data, train_labels = train_data.to(device), train_labels.to(device)\n",
    "    val_data, val_labels = val_data.to(device), val_labels.to(device)\n",
    "    \n",
    "    # Initialize the LSTM classifier and move it to the selected device\n",
    "    model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "    # Define the loss function as CrossEntropyLoss for multi-class classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Use Adam optimizer with the specified learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop for the specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear the gradients from the previous step\n",
    "        outputs = model(train_data)  # Forward pass: compute the LSTM's predictions for the training data\n",
    "        loss = criterion(outputs, train_labels)  # Compute the loss\n",
    "        loss.backward()  # Backpropagate the loss to compute the gradients\n",
    "        optimizer.step()  # Update the model's weights using the optimizer\n",
    "\n",
    "        # Validation phase (no gradient calculation)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient tracking during validation\n",
    "            val_outputs = model(val_data)  # Forward pass on the validation data\n",
    "            val_loss = criterion(val_outputs, val_labels)  # Compute validation loss\n",
    "\n",
    "        # Print training and validation loss for each epoch\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n",
    "    \n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x.unsqueeze(1), h_0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def train_gru_classifier_cuda(train_data, train_labels, val_data, val_labels, input_size, hidden_size, num_layers, num_classes, epochs=500, lr=1e-4):\n",
    "    # Determine if CUDA is available, otherwise use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Move the training and validation data (and labels) to the chosen device (GPU or CPU)\n",
    "    train_data, train_labels = train_data.to(device), train_labels.to(device)\n",
    "    val_data, val_labels = val_data.to(device), val_labels.to(device)\n",
    "    \n",
    "    # Initialize the GRU classifier and move it to the chosen device\n",
    "    model = GRUClassifier(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "    # Define the loss function as CrossEntropyLoss for multi-class classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Use the Adam optimizer with the specified learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop for the specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear the gradients from the previous step\n",
    "        outputs = model(train_data)  # Forward pass: compute the GRU's predictions for the training data\n",
    "        loss = criterion(outputs, train_labels)  # Compute the loss\n",
    "        loss.backward()  # Backpropagate the loss to compute the gradients\n",
    "        optimizer.step()  # Update the model's parameters using the optimizer\n",
    "\n",
    "        # Validation phase (no gradient calculation)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient tracking during validation\n",
    "            val_outputs = model(val_data)  # Forward pass on the validation data\n",
    "            val_loss = criterion(val_outputs, val_labels)  # Compute the validation loss\n",
    "\n",
    "        # Print the training and validation loss for each epoch\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x.unsqueeze(1), h_0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def train_rnn_classifier_cuda(train_data, train_labels, val_data, val_labels, input_size, hidden_size, num_layers, num_classes, epochs=500, lr=1e-4):\n",
    "    # Check if CUDA is available, else use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Move training and validation data and labels to the selected device (GPU or CPU)\n",
    "    train_data, train_labels = train_data.to(device), train_labels.to(device)\n",
    "    val_data, val_labels = val_data.to(device), val_labels.to(device)\n",
    "    \n",
    "    # Initialize the RNN classifier and move it to the chosen device\n",
    "    model = RNNClassifier(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "    # Define the loss function as CrossEntropyLoss for multi-class classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Use Adam optimizer with the specified learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop for the specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear any previously accumulated gradients\n",
    "        outputs = model(train_data)  # Forward pass: compute the RNN's predictions for the training data\n",
    "        loss = criterion(outputs, train_labels)  # Compute the loss using the predictions and true labels\n",
    "        loss.backward()  # Backpropagate the loss to compute gradients\n",
    "        optimizer.step()  # Update the model's parameters using the optimizer\n",
    "\n",
    "        # Validation phase (no gradient calculation)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient calculation for validation\n",
    "            val_outputs = model(val_data)  # Forward pass on the validation data\n",
    "            val_loss = criterion(val_outputs, val_labels)  # Compute validation loss\n",
    "\n",
    "        # Print the training and validation loss for each epoch\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n",
    "\n",
    "    # Return the trained model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define Bidirectional RNN Classifier\n",
    "class BidirectionalRNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BidirectionalRNNClassifier, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(num_layers * 2, x.size(0), hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x.unsqueeze(1), h_0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def train_bidir_rnn_classifier_cuda(train_data, train_labels, val_data, val_labels, input_size, hidden_size, num_layers, num_classes, epochs=500, lr=1e-4):\n",
    "    # Check if CUDA is available, otherwise use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Move training and validation data and labels to the chosen device (GPU or CPU)\n",
    "    train_data, train_labels = train_data.to(device), train_labels.to(device)\n",
    "    val_data, val_labels = val_data.to(device), val_labels.to(device)\n",
    "    \n",
    "    # Initialize the Bidirectional RNN classifier and move it to the chosen device\n",
    "    model = BidirectionalRNNClassifier(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "    # Define the loss function as CrossEntropyLoss for multi-class classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Use Adam optimizer with the specified learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop for the specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear any previously accumulated gradients\n",
    "        outputs = model(train_data)  # Forward pass: compute the model's predictions for the training data\n",
    "        loss = criterion(outputs, train_labels)  # Compute the loss using predictions and true labels\n",
    "        loss.backward()  # Backpropagate the loss to compute gradients\n",
    "        optimizer.step()  # Update the model's parameters using the optimizer\n",
    "\n",
    "        # Validation phase (no gradient calculation)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient calculation for validation\n",
    "            val_outputs = model(val_data)  # Forward pass on the validation data\n",
    "            val_loss = criterion(val_outputs, val_labels)  # Compute the validation loss\n",
    "\n",
    "        # Print the training and validation loss for each epoch\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judgers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "def judger(dataset_to_evaluate, mlp_models):\n",
    "    # Define the path to the hidden states folder and the extension for the files\n",
    "    folder_path = '/cs/student/projects2/dsml/cdiezmar/hidden_states/'\n",
    "    extension = '.pkl'\n",
    "    file_path = folder_path + dataset_to_evaluate + extension\n",
    "\n",
    "    # Load the hidden states for the dataset to evaluate from the pickle file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        hidden_states = pickle.load(file)\n",
    "        print('Length of the dataset: ', len(hidden_states))\n",
    "        # Convert hidden states to a tensor of type float32\n",
    "        hidden_states_tensor = torch.tensor(hidden_states, dtype=torch.float32)\n",
    "    \n",
    "    # Define the folder where the MLP models are stored and the file extension for models\n",
    "    model_folder = \"/cs/student/projects2/dsml/cdiezmar/mlp_models/\"\n",
    "    extension = \".pth\"\n",
    "\n",
    "    # Initialize an array to accumulate predictions from all classifiers\n",
    "    sum_predictions = np.zeros(len(hidden_states_tensor))\n",
    "\n",
    "    # Iterate through the list of MLP model names\n",
    "    for classifier in mlp_models:\n",
    "        # Load each MLP classifier model from its file\n",
    "        classifier_name = classifier\n",
    "        model_path = model_folder + classifier_name + extension\n",
    "        \n",
    "        # Determine the input size based on the hidden states tensor and set the number of output classes\n",
    "        input_size = hidden_states_tensor.size(1)\n",
    "        num_classes = 2\n",
    "        \n",
    "        # Initialize the MLPClassifier model and load the pre-trained weights\n",
    "        mlp_model = MLPClassifier(input_size, num_classes)\n",
    "        mlp_model.load_state_dict(torch.load(model_path))\n",
    "        mlp_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        # Make predictions with the MLP model, without computing gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = mlp_model(hidden_states_tensor)\n",
    "            _, predicted_labels = torch.max(outputs, 1)  # Get the predicted class (with the highest score)\n",
    "        \n",
    "        # Accumulate predictions (binary: 0 or 1) from the current classifier\n",
    "        sum_predictions += predicted_labels.cpu().numpy()\n",
    "\n",
    "    # Determine the final result based on accumulated predictions (result is True if prediction is non-zero)\n",
    "    result = (sum_predictions == 0)\n",
    "\n",
    "    # Return the negation of the result (logical NOT), indicating final classification\n",
    "    return np.logical_not(result)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "def judger_lstm(dataset_to_evaluate, lstm_models):\n",
    "    # Define the hidden size and number of layers for the LSTM model\n",
    "    hidden_size = 128\n",
    "    num_layers = 4  # Number of layers in the LSTM model\n",
    "\n",
    "    # Define the folder path where hidden states are stored and the extension for the files\n",
    "    folder_path = '/cs/student/projects2/dsml/cdiezmar/hidden_states/'\n",
    "    extension = '.pkl'\n",
    "    file_path = folder_path + dataset_to_evaluate + extension\n",
    "\n",
    "    # Load the hidden states from the specified pickle file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        hidden_states = pickle.load(file)\n",
    "        print('Length of the dataset: ', len(hidden_states))\n",
    "        # Convert the hidden states to a PyTorch tensor of type float32\n",
    "        hidden_states_tensor = torch.tensor(hidden_states, dtype=torch.float32)\n",
    "    \n",
    "    # Define the folder where the LSTM models are stored and the file extension for the models\n",
    "    model_folder = \"/cs/student/projects2/dsml/cdiezmar/lstm_models/\"\n",
    "    extension = \".pth\"\n",
    "\n",
    "    # Initialize an array to accumulate predictions from all classifiers\n",
    "    sum_predictions = np.zeros(len(hidden_states_tensor))\n",
    "\n",
    "    # Iterate through the list of LSTM model names\n",
    "    for classifier in lstm_models:\n",
    "        # Load each LSTM classifier model from the specified path\n",
    "        classifier_name = classifier\n",
    "        model_path = model_folder + classifier_name + extension\n",
    "        \n",
    "        # Determine the input size based on the hidden states tensor\n",
    "        input_size = hidden_states_tensor.size(1)\n",
    "        num_classes = 2  # Assuming binary classification (two classes)\n",
    "\n",
    "        # Initialize the LSTM classifier with the specified parameters\n",
    "        lstm_model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "        \n",
    "        # Load the pre-trained weights of the LSTM model\n",
    "        lstm_model.load_state_dict(torch.load(model_path))\n",
    "        lstm_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        # Make predictions with the LSTM model, without computing gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = lstm_model(hidden_states_tensor)\n",
    "            _, predicted_labels = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "        \n",
    "        # Accumulate the predictions from the current classifier\n",
    "        sum_predictions += predicted_labels.cpu().numpy()\n",
    "\n",
    "    # Determine the final result based on the accumulated predictions\n",
    "    # If sum_predictions is zero, return False, otherwise return True\n",
    "    result = (sum_predictions == 0)\n",
    "    \n",
    "    # Return the logical NOT of the result (flip the classification)\n",
    "    return np.logical_not(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "def judger_gru(dataset_to_evaluate, gru_models):\n",
    "    # Define the hidden size and number of layers for the GRU model\n",
    "    hidden_size = 128\n",
    "    num_layers = 4  # Number of GRU layers\n",
    "\n",
    "    # Define the folder path where hidden states are stored and the extension for the files\n",
    "    folder_path = '/cs/student/projects2/dsml/cdiezmar/hidden_states/'\n",
    "    extension = '.pkl'\n",
    "    file_path = folder_path + dataset_to_evaluate + extension\n",
    "\n",
    "    # Load the hidden states for the dataset from the pickle file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        hidden_states = pickle.load(file)\n",
    "        print('Length of the dataset: ', len(hidden_states))\n",
    "        # Convert hidden states to a PyTorch tensor of type float32\n",
    "        hidden_states_tensor = torch.tensor(hidden_states, dtype=torch.float32)\n",
    "    \n",
    "    # Define the folder where the GRU models are stored and the file extension for models\n",
    "    model_folder = \"/cs/student/projects2/dsml/cdiezmar/gru_models/\"\n",
    "    extension = \".pth\"\n",
    "\n",
    "    # Initialize an array to accumulate predictions from all classifiers\n",
    "    sum_predictions = np.zeros(len(hidden_states_tensor))\n",
    "\n",
    "    # Iterate through the list of GRU model names\n",
    "    for classifier in gru_models:\n",
    "        # Load each GRU classifier model from its file\n",
    "        classifier_name = classifier\n",
    "        model_path = model_folder + classifier_name + extension\n",
    "        \n",
    "        # Determine the input size based on the hidden states tensor\n",
    "        input_size = hidden_states_tensor.size(1)\n",
    "        num_classes = 2  # Assuming binary classification\n",
    "\n",
    "        # Initialize the GRUClassifier with the input size, hidden size, number of layers, and number of classes\n",
    "        gru_model = GRUClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "        \n",
    "        # Load the pre-trained weights of the GRU model\n",
    "        gru_model.load_state_dict(torch.load(model_path))\n",
    "        gru_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        # Make predictions with the GRU model, without computing gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = gru_model(hidden_states_tensor)\n",
    "            _, predicted_labels = torch.max(outputs, 1)  # Get the predicted class (with the highest score)\n",
    "        \n",
    "        # Accumulate predictions (binary: 0 or 1) from the current classifier\n",
    "        sum_predictions += predicted_labels.cpu().numpy()\n",
    "\n",
    "    # Determine the final result based on accumulated predictions (True if sum_predictions is zero)\n",
    "    result = (sum_predictions == 0)\n",
    "\n",
    "    # Return the negation of the result (logical NOT), indicating final classification\n",
    "    return np.logical_not(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "def judger_rnn(dataset_to_evaluate, rnn_models):\n",
    "    # Define the hidden size and number of layers for the RNN model\n",
    "    hidden_size = 128\n",
    "    num_layers = 4  # Number of RNN layers\n",
    "\n",
    "    # Define the folder path where hidden states are stored and the extension for the files\n",
    "    folder_path = '/cs/student/projects2/dsml/cdiezmar/hidden_states/'\n",
    "    extension = '.pkl'\n",
    "    file_path = folder_path + dataset_to_evaluate + extension\n",
    "\n",
    "    # Load the hidden states for the dataset from the pickle file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        hidden_states = pickle.load(file)\n",
    "        print('Length of the dataset: ', len(hidden_states))\n",
    "        # Convert the hidden states to a PyTorch tensor of type float32\n",
    "        hidden_states_tensor = torch.tensor(hidden_states, dtype=torch.float32)\n",
    "    \n",
    "    # Define the folder where the RNN models are stored and the file extension for models\n",
    "    model_folder = \"/cs/student/projects2/dsml/cdiezmar/rnn_models/\"\n",
    "    extension = \".pth\"\n",
    "\n",
    "    # Initialize an array to accumulate predictions from all classifiers\n",
    "    sum_predictions = np.zeros(len(hidden_states_tensor))\n",
    "\n",
    "    # Iterate through the list of RNN model names\n",
    "    for classifier in rnn_models:\n",
    "        # Load each RNN classifier model from its file\n",
    "        classifier_name = classifier\n",
    "        model_path = model_folder + classifier_name + extension\n",
    "        \n",
    "        # Determine the input size based on the hidden states tensor\n",
    "        input_size = hidden_states_tensor.size(1)\n",
    "        num_classes = 2  # Assuming binary classification\n",
    "\n",
    "        # Initialize the RNNClassifier with the input size, hidden size, number of layers, and number of classes\n",
    "        rnn_model = RNNClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "        \n",
    "        # Load the pre-trained weights of the RNN model\n",
    "        rnn_model.load_state_dict(torch.load(model_path))\n",
    "        rnn_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        # Make predictions with the RNN model, without computing gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = rnn_model(hidden_states_tensor)\n",
    "            _, predicted_labels = torch.max(outputs, 1)  # Get the predicted class (with the highest score)\n",
    "        \n",
    "        # Accumulate predictions (binary: 0 or 1) from the current classifier\n",
    "        sum_predictions += predicted_labels.cpu().numpy()\n",
    "\n",
    "    # Determine the final result based on accumulated predictions (True if sum_predictions is zero)\n",
    "    result = (sum_predictions == 0)\n",
    "\n",
    "    # Return the negation of the result (logical NOT), indicating the final classification\n",
    "    return np.logical_not(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "def judger_bidir_rnn(dataset_to_evaluate, rnn_models):\n",
    "    # Define the hidden size and number of layers for the bidirectional RNN model\n",
    "    hidden_size = 128\n",
    "    num_layers = 4  # Number of layers in the bidirectional RNN\n",
    "\n",
    "    # Define the folder path where hidden states are stored and the extension for the files\n",
    "    folder_path = '/cs/student/projects2/dsml/cdiezmar/hidden_states/'\n",
    "    extension = '.pkl'\n",
    "    file_path = folder_path + dataset_to_evaluate + extension\n",
    "\n",
    "    # Load the hidden states for the dataset from the pickle file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        hidden_states = pickle.load(file)\n",
    "        print('Length of the dataset: ', len(hidden_states))\n",
    "        # Convert the hidden states to a PyTorch tensor of type float32\n",
    "        hidden_states_tensor = torch.tensor(hidden_states, dtype=torch.float32)\n",
    "    \n",
    "    # Define the folder where the bidirectional RNN models are stored and the file extension for models\n",
    "    model_folder = \"/cs/student/projects2/dsml/cdiezmar/bidir_rnn_models/\"\n",
    "    extension = \".pth\"\n",
    "\n",
    "    # Initialize an array to accumulate predictions from all classifiers\n",
    "    sum_predictions = np.zeros(len(hidden_states_tensor))\n",
    "\n",
    "    # Iterate through the list of bidirectional RNN model names\n",
    "    for classifier in rnn_models:\n",
    "        # Load each bidirectional RNN classifier model from its file\n",
    "        classifier_name = classifier\n",
    "        model_path = model_folder + classifier_name + extension\n",
    "        \n",
    "        # Determine the input size based on the hidden states tensor\n",
    "        input_size = hidden_states_tensor.size(1)\n",
    "        num_classes = 2  # Assuming binary classification\n",
    "\n",
    "        # Initialize the BidirectionalRNNClassifier with the input size, hidden size, number of layers, and number of classes\n",
    "        bidir_rnn_model = BidirectionalRNNClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "        \n",
    "        # Load the pre-trained weights of the bidirectional RNN model\n",
    "        bidir_rnn_model.load_state_dict(torch.load(model_path))\n",
    "        bidir_rnn_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        # Make predictions with the bidirectional RNN model, without computing gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = bidir_rnn_model(hidden_states_tensor)\n",
    "            _, predicted_labels = torch.max(outputs, 1)  # Get the predicted class (with the highest score)\n",
    "        \n",
    "        # Accumulate predictions (binary: 0 or 1) from the current classifier\n",
    "        sum_predictions += predicted_labels.cpu().numpy()\n",
    "\n",
    "    # Determine the final result based on accumulated predictions (True if sum_predictions is zero)\n",
    "    result = (sum_predictions == 0)\n",
    "\n",
    "    # Return the negation of the result (logical NOT), indicating the final classification\n",
    "    return np.logical_not(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "def judger_with_vanilla_filter_lstm(dataset_to_evaluate, lstm_models):\n",
    "    # Define the hidden size and number of layers for the LSTM model\n",
    "    hidden_size = 128\n",
    "    num_layers = 4  # Number of LSTM layers\n",
    "\n",
    "    # Define the folder path where hidden states are stored and the extension for the files\n",
    "    folder_path = '/cs/student/projects2/dsml/cdiezmar/hidden_states/'\n",
    "    extension = '.pkl'\n",
    "    file_path = folder_path + dataset_to_evaluate + extension\n",
    "\n",
    "    # Load the hidden states for the dataset to evaluate from the pickle file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        hidden_states = pickle.load(file)\n",
    "        print('Length of the dataset: ', len(hidden_states))\n",
    "        # Convert the hidden states to a PyTorch tensor of type float32\n",
    "        hidden_states_tensor = torch.tensor(hidden_states, dtype=torch.float32)\n",
    "    \n",
    "    # Define the folder where the LSTM models are stored and the file extension for models\n",
    "    model_folder = \"/cs/student/projects2/dsml/cdiezmar/lstm_models/\"\n",
    "    extension = \".pth\"\n",
    "\n",
    "    # Initialize an array to accumulate predictions from all classifiers\n",
    "    sum_predictions = np.zeros(len(hidden_states_tensor))\n",
    "\n",
    "    # Iterate through the list of LSTM model names\n",
    "    for classifier in lstm_models:\n",
    "        # Load each LSTM classifier model from its file\n",
    "        classifier_name = classifier\n",
    "        model_path = model_folder + classifier_name + extension\n",
    "        \n",
    "        # Determine the input size based on the hidden states tensor\n",
    "        input_size = hidden_states_tensor.size(1)\n",
    "        num_classes = 2  # Assuming binary classification (two classes)\n",
    "\n",
    "        # Initialize the LSTMClassifier with the input size, hidden size, number of layers, and number of classes\n",
    "        lstm_model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "        \n",
    "        # Load the pre-trained weights of the LSTM model\n",
    "        lstm_model.load_state_dict(torch.load(model_path))\n",
    "        lstm_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        # Make predictions with the LSTM model, without computing gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = lstm_model(hidden_states_tensor)\n",
    "            _, predicted_labels = torch.max(outputs, 1)  # Get the predicted class (with the highest score)\n",
    "        \n",
    "        # Accumulate the predictions (binary: 0 or 1) from the current classifier\n",
    "        sum_predictions += predicted_labels.cpu().numpy()\n",
    "\n",
    "    # Load the vanilla filtering results (precomputed) to further refine the predictions\n",
    "    folder_path_vanilla = '/cs/student/projects2/dsml/cdiezmar/vanilla_results/'\n",
    "    extension_vanilla = '.npy'\n",
    "    file_path_vanilla = folder_path_vanilla + dataset_to_evaluate + extension_vanilla\n",
    "\n",
    "    # Load the vanilla filter predictions\n",
    "    predicted_labels_vanilla = np.load(file_path_vanilla)\n",
    "\n",
    "    # Add the vanilla filter predictions to the sum of predictions\n",
    "    sum_predictions += predicted_labels_vanilla\n",
    "\n",
    "    # Determine the final result based on accumulated predictions (True if sum_predictions is zero)\n",
    "    result = (sum_predictions == 0)\n",
    "\n",
    "    # Return the negation of the result (logical NOT), indicating the final classification\n",
    "    return np.logical_not(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "def judger_with_vanilla_filter(dataset_to_evaluate, lstm_models):\n",
    "    # Define the folder path where hidden states are stored\n",
    "    folder_path = '/cs/student/projects2/dsml/cdiezmar/hidden_states/'\n",
    "    extension = '.pkl'\n",
    "    file_path = folder_path + dataset_to_evaluate + extension\n",
    "\n",
    "    # Load the hidden states from the pickle file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        hidden_states = pickle.load(file)\n",
    "        print('Length of the dataset: ', len(hidden_states))\n",
    "        # Convert the hidden states to a PyTorch tensor of type float32\n",
    "        hidden_states_tensor = torch.tensor(hidden_states, dtype=torch.float32)\n",
    "    \n",
    "    # Define the folder where the MLP models are stored and the file extension for models\n",
    "    model_folder = \"/cs/student/projects2/dsml/cdiezmar/mlp_models/\"\n",
    "    extension = \".pth\"\n",
    "\n",
    "    # Initialize an array to accumulate predictions from all classifiers\n",
    "    sum_predictions = np.zeros(len(hidden_states_tensor))\n",
    "\n",
    "    # Iterate through the list of LSTM model names (even though they seem to be MLP models)\n",
    "    for classifier in lstm_models:\n",
    "        # Load each MLP classifier model from its file\n",
    "        classifier_name = classifier\n",
    "        model_path = model_folder + classifier_name + extension\n",
    "        \n",
    "        # Determine the input size based on the hidden states tensor\n",
    "        input_size = hidden_states_tensor.size(1)\n",
    "        num_classes = 2  # Assuming binary classification\n",
    "\n",
    "        # Initialize the MLPClassifier with the input size and number of classes\n",
    "        lstm_model = MLPClassifier(input_size, num_classes)\n",
    "        \n",
    "        # Load the pre-trained weights of the MLP model\n",
    "        lstm_model.load_state_dict(torch.load(model_path))\n",
    "        lstm_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        # Make predictions with the MLP model, without computing gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = lstm_model(hidden_states_tensor)\n",
    "            _, predicted_labels = torch.max(outputs, 1)  # Get the predicted class (with the highest score)\n",
    "        \n",
    "        # Accumulate the predictions (binary: 0 or 1) from the current classifier\n",
    "        sum_predictions += predicted_labels.cpu().numpy()\n",
    "\n",
    "    # Load the vanilla filter results (precomputed) to further refine the predictions\n",
    "    folder_path_vanilla = '/cs/student/projects2/dsml/cdiezmar/vanilla_results/'\n",
    "    extension_vanilla = '.npy'\n",
    "    file_path_vanilla = folder_path_vanilla + dataset_to_evaluate + extension_vanilla\n",
    "\n",
    "    # Load the vanilla filter predictions\n",
    "    predicted_labels_vanilla = np.load(file_path_vanilla)\n",
    "\n",
    "    # Add the vanilla filter predictions to the sum of predictions\n",
    "    sum_predictions += predicted_labels_vanilla\n",
    "\n",
    "    # Determine the final result based on accumulated predictions (True if sum_predictions is zero)\n",
    "    result = (sum_predictions == 0)\n",
    "\n",
    "    # Return the negation of the result (logical NOT), indicating the final classification\n",
    "    return np.logical_not(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "def judger_skr(dataset_to_evaluate, mlp_models):\n",
    "    # Define the folder path where hidden states are stored and the extension for the files\n",
    "    folder_path = '/cs/student/projects2/dsml/cdiezmar/hidden_states/'\n",
    "    extension = '.pkl'\n",
    "    file_path = folder_path + dataset_to_evaluate + extension\n",
    "\n",
    "    # Load the hidden states for the dataset from the pickle file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        hidden_states = pickle.load(file)\n",
    "        print('Length of the dataset: ', len(hidden_states))\n",
    "        # Convert the hidden states to a PyTorch tensor of type float32\n",
    "        hidden_states_tensor = torch.tensor(hidden_states, dtype=torch.float32)\n",
    "    \n",
    "    # Define the folder where the MLP models are stored and the file extension for models\n",
    "    model_folder = \"/cs/student/projects2/dsml/cdiezmar/mlp_models/\"\n",
    "    extension = \".pth\"\n",
    "\n",
    "    # Initialize an array to accumulate predictions from all classifiers\n",
    "    sum_predictions = np.zeros(len(hidden_states_tensor))\n",
    "\n",
    "    # Iterate through the list of MLP model names\n",
    "    for classifier in mlp_models:\n",
    "        # Load each MLP classifier model from its file\n",
    "        classifier_name = classifier\n",
    "        model_path = model_folder + classifier_name + extension\n",
    "        \n",
    "        # Determine the input size based on the hidden states tensor\n",
    "        input_size = hidden_states_tensor.size(1)\n",
    "        num_classes = 2  # Assuming binary classification\n",
    "\n",
    "        # Initialize the MLPClassifier with the input size and number of classes\n",
    "        mlp_model = MLPClassifier(input_size, num_classes)\n",
    "        \n",
    "        # Load the pre-trained weights of the MLP model\n",
    "        mlp_model.load_state_dict(torch.load(model_path))\n",
    "        mlp_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        # Make predictions with the MLP model, without computing gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = mlp_model(hidden_states_tensor)\n",
    "            _, predicted_labels = torch.max(outputs, 1)  # Get the predicted class (with the highest score)\n",
    "        \n",
    "        # Accumulate the predictions (binary: 0 or 1) from the current classifier\n",
    "        sum_predictions += predicted_labels.cpu().numpy()\n",
    "\n",
    "    # Get predictions from the skr_judger_fun for further refinement\n",
    "    skr_predictions = skr_judger_fun(dataset_to_evaluate)\n",
    "    print(skr_predictions)\n",
    "\n",
    "    # Perform a logical OR operation between the skr_judger_fun result and the sum of predictions\n",
    "    result = np.logical_or(skr_predictions, sum_predictions == 1)\n",
    "\n",
    "    # Return the negation of the result (logical NOT), indicating the final classification\n",
    "    return np.logical_not(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judger_TriGate_with_skr(dataset_to_evaluate, mlp_models):\n",
    "\n",
    "    # dataset_to_evaluate = 'small_dataset'\n",
    "\n",
    "    folder_path = '/cs/student/projects2/dsml/cdiezmar/hidden_states/'\n",
    "    extension = '.pkl'\n",
    "    file_path = folder_path + dataset_to_evaluate + extension\n",
    "\n",
    "    with open(file_path, 'rb') as file:\n",
    "        hidden_states = pickle.load(file)\n",
    "        print('Length small_dataset: ', len(hidden_states))\n",
    "        hidden_states_tensor= torch.tensor(hidden_states, dtype=torch.float32)\n",
    "    \n",
    "    model_folder = \"/cs/student/projects2/dsml/cdiezmar/mlp_models/\"\n",
    "    extension = \".pth\"\n",
    "    sum_predictions = np.zeros(len(hidden_states_tensor))\n",
    "\n",
    "    for classifier in mlp_models:\n",
    "        classifier_name = classifier\n",
    "        model_path = model_folder + classifier_name + extension\n",
    "        input_size = hidden_states_tensor.size(1)\n",
    "        num_classes = 2\n",
    "        mlp_model = MLPClassifier(input_size, num_classes)\n",
    "        mlp_model.load_state_dict(torch.load(model_path))\n",
    "        mlp_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for prediction\n",
    "            outputs = mlp_model(hidden_states_tensor)\n",
    "            _, predicted_labels = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "        sum_predictions += predicted_labels.cpu().numpy()\n",
    "\n",
    "    result = np.logical_or(skr_judger_fun(dataset_to_evaluate), sum_predictions==0)\n",
    "    print(skr_judger_fun(dataset_to_evaluate))\n",
    "    return np.logical_not(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process knowledge-aware data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 52970 non-knowledge-intensive instructions\n",
      "{\"question\":\"Question: Write a text based on \\\"rangers show some iron to tie the series\\\"\\nText: along with being talented , self-assured and highly paid , these experienced rangers are also considerate .\\n\\nQuestion: Write a text based on \\\"union wo n't dismantle blockage of gm canada headquarters\\\"\\nText: canadian auto workers officials friday refused to end a blockade of general motors canada headquarters despite an offer to potentially bring new car production to a complex where a truck plant is slated for closure .\\n\\nQuestion: Write a text based on \\\"six azerbaijan opposition parliamentary candidates declare hunger strike\\\"\\nText:\",\"output\":\"[No Retrieval]six opposition candidates in the upcoming parliamentary elections in azerbaijan declared a hunger strike friday to pressure the government into ensuring fair polls .[Utility:5]\",\"input\":\"\",\"id\":\"flan_v2_18667\",\"dataset_name\":\"flan_v2\"}\n",
      "\n",
      "{\"question\":\"You will be given a sentence. Check whether the sentence is grammatically correct and is meaningful. If the sentence is grammatically correct, then answer with '1', otherwise answer with '0'.\\n\\nQ: You are the only person that I can rely on.\\n\\nA: 0\\n****\\nQ: Fruit at once hit the roof from the tree.\\n\\nA: 1\\n****\\nQ: Students who fail the final exam or who do not do the reading will be executed.\\n\\nA:\",\"output\":\"[No Retrieval]1\\n****\\n[Utility:5]\",\"input\":\"\",\"id\":\"flan_v2_87754\",\"dataset_name\":\"flan_v2\"}\n",
      "\n",
      "{\"question\":\"Q:Is there a negative or positive tone to this product review? === Title: At the bottom of the River, a review by Dylan Review: At the Bottom of the river isn't, in my perspective, a very good book. I gave it one star. I gave it one star because there isn't really a plot, main character (at least with a name), and it is boring. The book is sort of written in a mix between the first person and third person perspectives. The book is hard to understand, especially with 2 two page long sentences! The book kind of seems like it is someone thinking, with no real reason. It skips from one place to another in one chapter. It is hard to get into the story, and since there is no plot, there is little suspense. All in all, it isn't a very good book, and I don't recommend it. Answer:\\nA:\",\"output\":\"[No Retrieval]Negative[Utility:5]\",\"input\":\"\",\"id\":\"flan_v2_47789\",\"dataset_name\":\"flan_v2\"}\n",
      "\n",
      "{\"question\":\"Question: Fertility Clinics Vary on Embryo Disposal Techniques\\n\\nBy DAVID B. CARUSO    PHILADELPHIA (AP) -- The nation's fertility clinics vary widely when it comes to how they perform one of the most delicate aspects of their jobs: disposing of unused frozen human embryos that were created to help infertile women become pregnant...\\n\\nWhich topic is this article about?\\nOPTIONS:\\n- World\\n- Sports\\n- Business\\n- Science\\/Tech\\nAnswer: Science\\/Tech\\n\\nQuestion: Canada #39;s Inflation Rate Unexpectedly Picks Up to 2.4 (Update1)\\n\\nCanada #39;s inflation rate unexpectedly accelerated in November to a 2.4 percent rate, reflecting a jump in prices for new cars, gasoline and houses from a year earlier.\\n\\nWhich topic is this article about?\\nOPTIONS:\\n- World\\n- Sports\\n- Business\\n- Science\\/Tech\\nAnswer: Business\\n\\nQuestion: Paris bomb at Indonesia Embassy wounds 10\\n\\nPARIS A bomb rocked a quiet, elegant neighborhood of apartment buildings in the west of the city, slightly wounding at least 10 people, including 4 workers at the Indonesian Embassy, where a rigged package exploded below its crimson and white national flag \\n\\nWhich topic is this article about?\\nOPTIONS:\\n- World\\n- Sports\\n- Business\\n- Science\\/Tech\\nAnswer: World\\n\\nQuestion: UN Urged to Ignore Bush Plea for Human Cloning Ban\\n\\n LONDON (Reuters) - Britain's national academy of science  urged the United Nations on Monday to ignore a call by  President Bush to ban all forms of human cloning.\\n\\nWhich topic is this article about?\\nOPTIONS:\\n- World\\n- Sports\\n- Business\\n- Science\\/Tech\",\"output\":\"[No Retrieval]Answer: World[Utility:5]\",\"input\":\"\",\"id\":\"flan_v2_10803\",\"dataset_name\":\"flan_v2\"}\n",
      "\n",
      "{\"question\":\"Teacher:In this task, you need to replace a letter in the sentence with another given letter.\\nTeacher: Now, understand the problem? Solve this instance: Sentence: 'a weird yellow sign sitting on the side of a dirt road'. Replace the letter 'n' with 'v' in the sentence.\\nStudent:\",\"output\":\"[No Retrieval]a weird yellow sigv sittivg ov the side of a dirt road[Utility:5]\",\"input\":\"\",\"id\":\"flan_v2_76462\",\"dataset_name\":\"flan_v2\"}\n",
      "\n",
      "dataset/knowledge-aware\n",
      "['Question: Write a text based on \"rangers show some iron to tie the series\"\\nText: along with being talented , self-assured and highly paid , these experienced rangers are also considerate .\\n\\nQuestion: Write a text based on \"union wo n\\'t dismantle blockage of gm canada headquarters\"\\nText: canadian auto workers officials friday refused to end a blockade of general motors canada headquarters despite an offer to potentially bring new car production to a complex where a truck plant is slated for closure .\\n\\nQuestion: Write a text based on \"six azerbaijan opposition parliamentary candidates declare hunger strike\"\\nText:', \"You will be given a sentence. Check whether the sentence is grammatically correct and is meaningful. If the sentence is grammatically correct, then answer with '1', otherwise answer with '0'.\\n\\nQ: You are the only person that I can rely on.\\n\\nA: 0\\n****\\nQ: Fruit at once hit the roof from the tree.\\n\\nA: 1\\n****\\nQ: Students who fail the final exam or who do not do the reading will be executed.\\n\\nA:\", \"Q:Is there a negative or positive tone to this product review? === Title: At the bottom of the River, a review by Dylan Review: At the Bottom of the river isn't, in my perspective, a very good book. I gave it one star. I gave it one star because there isn't really a plot, main character (at least with a name), and it is boring. The book is sort of written in a mix between the first person and third person perspectives. The book is hard to understand, especially with 2 two page long sentences! The book kind of seems like it is someone thinking, with no real reason. It skips from one place to another in one chapter. It is hard to get into the story, and since there is no plot, there is little suspense. All in all, it isn't a very good book, and I don't recommend it. Answer:\\nA:\", \"Question: Fertility Clinics Vary on Embryo Disposal Techniques\\n\\nBy DAVID B. CARUSO    PHILADELPHIA (AP) -- The nation's fertility clinics vary widely when it comes to how they perform one of the most delicate aspects of their jobs: disposing of unused frozen human embryos that were created to help infertile women become pregnant...\\n\\nWhich topic is this article about?\\nOPTIONS:\\n- World\\n- Sports\\n- Business\\n- Science/Tech\\nAnswer: Science/Tech\\n\\nQuestion: Canada #39;s Inflation Rate Unexpectedly Picks Up to 2.4 (Update1)\\n\\nCanada #39;s inflation rate unexpectedly accelerated in November to a 2.4 percent rate, reflecting a jump in prices for new cars, gasoline and houses from a year earlier.\\n\\nWhich topic is this article about?\\nOPTIONS:\\n- World\\n- Sports\\n- Business\\n- Science/Tech\\nAnswer: Business\\n\\nQuestion: Paris bomb at Indonesia Embassy wounds 10\\n\\nPARIS A bomb rocked a quiet, elegant neighborhood of apartment buildings in the west of the city, slightly wounding at least 10 people, including 4 workers at the Indonesian Embassy, where a rigged package exploded below its crimson and white national flag \\n\\nWhich topic is this article about?\\nOPTIONS:\\n- World\\n- Sports\\n- Business\\n- Science/Tech\\nAnswer: World\\n\\nQuestion: UN Urged to Ignore Bush Plea for Human Cloning Ban\\n\\n LONDON (Reuters) - Britain's national academy of science  urged the United Nations on Monday to ignore a call by  President Bush to ban all forms of human cloning.\\n\\nWhich topic is this article about?\\nOPTIONS:\\n- World\\n- Sports\\n- Business\\n- Science/Tech\", \"Teacher:In this task, you need to replace a letter in the sentence with another given letter.\\nTeacher: Now, understand the problem? Solve this instance: Sentence: 'a weird yellow sign sitting on the side of a dirt road'. Replace the letter 'n' with 'v' in the sentence.\\nStudent:\", 'Q:Title: BtlFan Review: Imagine having a tooth drilled without novocaine. Then add the sound of a massive auto wreck in which the brake shoes of all of the cars were worn to the point where bare metal was scraping against bare metal. Then imagine putting Tobasco sauce in your eyes and grating your knee with a cheese grater. Is the review positive or negative?\\nA:', 'Title: GREAT SHOW.....WHY IS PRICE HIGHER THAN OTHER CBS SHOWS ? Review: The series was a wonderful show and I would give it a five star. I agree it did not get a fair shake, just like SHARK of a few years ago. Same network. Now it appears that the boys at CBS are just as unfair to the fans regarding the DVD release. I guess CBS feels that it is only right to make the fans of this show suffer more by paying more for this series than other CBS releases on DVD. They are charging 30% for this release and for fewer episodes. I see that Blue Bloods and Hawaii ZERO (sorry Five O - I was thinking of quality) are priced at $ 38 while Defenders is $ 50. I am certain when it is all said and done the same network know nothings who said it was a ratings decision will somehow justify their lack of intelligence by saying \"you see...the DVDs did not sell either\". They will not mention that they overpriced the product to get that result. Is this product review negative?\\nOptions: 1. Yes; 2. No;\\nAnswer:', 'Fill in the response. A 2 person conversation:\\n + What is the main artistic work of  ⁇ lvaro Zardoni?\\n + Álvaro Zardoni Álvaro Zardoni (born January 4, 1964) is a Mexican sculptor and architect who has been a member of the Salón de la Plástica Mexicana since 2006.\\n + Did he formally train in any school or university?\\n + Although he studied painting and drawing in the 1970s and 1980s, he is a self-taught sculptor who began showing his work regularly in 2000.\\n + Are there any other interesting aspects about this article?\\n + Since then, he has had over thirty individual exhibitions, twenty private showings and his work has appeared in over 100 collective exhibitions.\\n + What are some of his paitings?\\n + He specializes in small bronze sculptures which focus on the human face, which is almost always male, expressing something emotional and/or psychological.\\n + What are some of his other pieces of art?\\n + ', \"Given the task definition and input, reply with output. In this task, you're given the middle and ending of a three-part story. Your job is to complete the short story by writing a probable beginning of the story. Generated sentences must be short, have fewer than 10 words, and be simple as if narrating to a child. Avoid using any irrelevant extra information when creating the beginning, and use names (e.g. Jack, Barbara,...) instead of pronouns (e.g. he / she) wherever possible.\\n\\nMiddle: Bobby's neighbor ask him to mows his grass. Ending: Many people appreciate the mowing that Bobby does.\", 'Title: DOA Review: We received the neat receipt scanner for Christmas to be used with our business. I spent 2 hours installing the software: updating the already out of date software, reinstalling because the first install and following update did not work and finally installing it cleanly from their website. This was on top of several restarts some of which interrupted the install process that was still ongoing. I then spent almost another hour trying to log in to their \"cloud\" using the email I gave them. I never gave a password but it still showed my email address as registered so I reset my password then it asked me a security question that was blank that I had not given an answer to because the setup had failed earlier. Once all the other issues were finally worked out the supposed document scanner would do nothing but return blank scans of the page. Returning for a refund... Is the review positive or negative?\\nAnswer:']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from flashrag.utils import get_dataset\n",
    "\n",
    "# Load the Self-RAG training set\n",
    "cache_path = './hf_cache/'\n",
    "ds = load_dataset(\"selfrag/selfrag_train_data\", cache_dir = cache_path)\n",
    "\n",
    "# Convert dataset to pandas dataframe for easier manipulation\n",
    "df = ds['train'].to_pandas()\n",
    "\n",
    "# Define a function to check if the only tags inside square brackets are 'No Retrieval' and 'Utility:5', which means the end of each output.\n",
    "def is_only_no_retrieval(output):\n",
    "    matches = re.findall(r'\\[([^\\]]+)\\]', output)\n",
    "    return all(match.strip() == 'No Retrieval' or match.strip() == 'Utility:5' for match in matches)\n",
    "\n",
    "# Apply the function to filter the dataset\n",
    "non_knowledge_df = df[df['output'].apply(is_only_no_retrieval)]\n",
    "\n",
    "# Print the number of filtered non-knowledge-intensive instructions\n",
    "print(f\"Filtered {len(non_knowledge_df)} non-knowledge-intensive instructions\")\n",
    "\n",
    "# Rename 'instruction' to 'question' to match the required format\n",
    "non_knowledge_df = non_knowledge_df.head(20000)\n",
    "non_knowledge_df = non_knowledge_df.rename(columns={'instruction': 'question'})\n",
    "\n",
    "# Convert to JSONL\n",
    "jsonl_file_path = '/cs/student/projects2/dsml/cdiezmar/dataset/knowledge-aware/train.jsonl'\n",
    "jsonl_data = non_knowledge_df.to_json(orient='records', lines=True)\n",
    "\n",
    "# Save to a file\n",
    "with open(jsonl_file_path, 'w') as file:\n",
    "    file.write(jsonl_data)\n",
    "\n",
    "# Verify the structure of the JSONL file\n",
    "with open(jsonl_file_path, 'r') as file:\n",
    "    for i in range(5):  # Print the first 5 lines\n",
    "        print(file.readline())\n",
    "\n",
    "# Load the dataset using flashrag.utils\n",
    "config_know = set_config(dataset_name='knowledge-aware')\n",
    "all_split = get_dataset(config_know)\n",
    "train_data = all_split['train']\n",
    "\n",
    "# Print the question field to verify\n",
    "print(config_know['dataset_path'])\n",
    "print(train_data.question[:10])  # Print first 10 questions to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden-states computation & saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/retrievalqa_2400\n",
      "[\"What is the name of the train station that services Amtrak and Sound Transit's Sounder commuter train for the city of Seattle?\", \"Which brand of foodstuff still bears the slogan 'Original and Best'?\", 'Which year saw the assassination of US President James Garfield?', 'In what city was Klaus-Degenhard Schmidt born?', 'What name is given to a woman divorced, separated, or living away from her spouse?']\n",
      "2400\n"
     ]
    }
   ],
   "source": [
    "from flashrag.utils import get_dataset\n",
    "config_small = set_config(dataset_name='retrievalqa_2400')\n",
    "all_split = get_dataset(config_small)\n",
    "train_data = all_split['train']\n",
    "print(config_small['dataset_path'])\n",
    "print(train_data.question[:5])\n",
    "print(len(train_data.question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `question` in template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [05:13<00:00, 44.74s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/1200 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 1200/1200 [00:50<00:00, 23.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from flashrag.prompt import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(\n",
    "        config = config_small,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "import torch\n",
    "\n",
    "\n",
    "# Ensure you are using a GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "hidden_states = extract_hidden_states(config_small, template, batch_size=2, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_states saved to /cs/student/projects2/dsml/cdiezmar/hidden_states/retrievalqa_2400.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "file_path = '/cs/student/projects2/dsml/cdiezmar/hidden_states/retrievalqa_2400.pkl'\n",
    "\n",
    "# Open the file in write-binary mode and save the hidden_states object\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(hidden_states, file)\n",
    "\n",
    "print(f'hidden_states saved to {file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `question` in template\n",
      "Find `question` in template\n",
      "Length triviaqa:  78785\n",
      "Length taqa:  10148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178256/3219143386.py:30: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  triviaqa_hidden_states_tensor= torch.tensor(triviaqa_hidden_states, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5629320740699768, Val Loss: 0.37159672379493713\n",
      "Epoch 2, Loss: 0.3804071247577667, Val Loss: 0.3906501233577728\n",
      "Epoch 3, Loss: 0.40479347109794617, Val Loss: 0.41813352704048157\n",
      "Epoch 4, Loss: 0.4348546266555786, Val Loss: 0.42239144444465637\n",
      "Epoch 5, Loss: 0.4395950436592102, Val Loss: 0.4058825373649597\n",
      "Epoch 6, Loss: 0.4220767915248871, Val Loss: 0.3758879005908966\n",
      "Epoch 7, Loss: 0.3900403082370758, Val Loss: 0.3413049578666687\n",
      "Epoch 8, Loss: 0.35277417302131653, Val Loss: 0.31317025423049927\n",
      "Epoch 9, Loss: 0.3217155337333679, Val Loss: 0.30286088585853577\n",
      "Epoch 10, Loss: 0.30869120359420776, Val Loss: 0.31228020787239075\n",
      "Epoch 11, Loss: 0.3160477578639984, Val Loss: 0.32347798347473145\n",
      "Epoch 12, Loss: 0.3261188268661499, Val Loss: 0.3163106441497803\n",
      "Epoch 13, Loss: 0.31875354051589966, Val Loss: 0.2935589551925659\n",
      "Epoch 14, Loss: 0.2964992821216583, Val Loss: 0.2711319625377655\n",
      "Epoch 15, Loss: 0.2749754786491394, Val Loss: 0.258657842874527\n",
      "Epoch 16, Loss: 0.2635396122932434, Val Loss: 0.25522711873054504\n",
      "Epoch 17, Loss: 0.26104044914245605, Val Loss: 0.255291223526001\n",
      "Epoch 18, Loss: 0.2617398798465729, Val Loss: 0.25406163930892944\n",
      "Epoch 19, Loss: 0.26074257493019104, Val Loss: 0.24931667745113373\n",
      "Epoch 20, Loss: 0.25581124424934387, Val Loss: 0.24121689796447754\n",
      "Epoch 21, Loss: 0.24716851115226746, Val Loss: 0.2315831482410431\n",
      "Epoch 22, Loss: 0.23675204813480377, Val Loss: 0.2229907214641571\n",
      "Epoch 23, Loss: 0.2272806316614151, Val Loss: 0.2175758183002472\n",
      "Epoch 24, Loss: 0.22102853655815125, Val Loss: 0.21572335064411163\n",
      "Epoch 25, Loss: 0.2184867113828659, Val Loss: 0.21554747223854065\n",
      "Epoch 26, Loss: 0.2178315967321396, Val Loss: 0.2141462117433548\n",
      "Epoch 27, Loss: 0.21618174016475677, Val Loss: 0.20991702377796173\n",
      "Epoch 28, Loss: 0.21191920340061188, Val Loss: 0.20364218950271606\n",
      "Epoch 29, Loss: 0.2057821899652481, Val Loss: 0.19739343225955963\n",
      "Epoch 30, Loss: 0.19978177547454834, Val Loss: 0.19273528456687927\n",
      "Epoch 31, Loss: 0.19541597366333008, Val Loss: 0.18989156186580658\n",
      "Epoch 32, Loss: 0.19284355640411377, Val Loss: 0.18808165192604065\n",
      "Epoch 33, Loss: 0.19122788310050964, Val Loss: 0.18629783391952515\n",
      "Epoch 34, Loss: 0.1895228922367096, Val Loss: 0.18389523029327393\n",
      "Epoch 35, Loss: 0.1870705932378769, Val Loss: 0.1808021068572998\n",
      "Epoch 36, Loss: 0.1838122308254242, Val Loss: 0.17741574347019196\n",
      "Epoch 37, Loss: 0.18017859756946564, Val Loss: 0.17431333661079407\n",
      "Epoch 38, Loss: 0.17679038643836975, Val Loss: 0.17191310226917267\n",
      "Epoch 39, Loss: 0.17410868406295776, Val Loss: 0.17023637890815735\n",
      "Epoch 40, Loss: 0.17218928039073944, Val Loss: 0.16890810430049896\n",
      "Epoch 41, Loss: 0.17067919671535492, Val Loss: 0.16741503775119781\n",
      "Epoch 42, Loss: 0.16907502710819244, Val Loss: 0.16544903814792633\n",
      "Epoch 43, Loss: 0.16706717014312744, Val Loss: 0.1630772203207016\n",
      "Epoch 44, Loss: 0.164712056517601, Val Loss: 0.1606294959783554\n",
      "Epoch 45, Loss: 0.16232241690158844, Val Loss: 0.15843594074249268\n",
      "Epoch 46, Loss: 0.16020764410495758, Val Loss: 0.1566317230463028\n",
      "Epoch 47, Loss: 0.15848101675510406, Val Loss: 0.15513500571250916\n",
      "Epoch 48, Loss: 0.15704108774662018, Val Loss: 0.15375478565692902\n",
      "Epoch 49, Loss: 0.15568220615386963, Val Loss: 0.15232539176940918\n",
      "Epoch 50, Loss: 0.15423165261745453, Val Loss: 0.15078938007354736\n",
      "Epoch 51, Loss: 0.15263330936431885, Val Loss: 0.1492019146680832\n",
      "Epoch 52, Loss: 0.15095122158527374, Val Loss: 0.1476757824420929\n",
      "Epoch 53, Loss: 0.14931149780750275, Val Loss: 0.14630332589149475\n",
      "Epoch 54, Loss: 0.14782144129276276, Val Loss: 0.14510059356689453\n",
      "Epoch 55, Loss: 0.14651019871234894, Val Loss: 0.14400193095207214\n",
      "Epoch 56, Loss: 0.1453217715024948, Val Loss: 0.14290501177310944\n",
      "Epoch 57, Loss: 0.14415936172008514, Val Loss: 0.14173507690429688\n",
      "Epoch 58, Loss: 0.14294908940792084, Val Loss: 0.1404842585325241\n",
      "Epoch 59, Loss: 0.14168080687522888, Val Loss: 0.13920462131500244\n",
      "Epoch 60, Loss: 0.14040103554725647, Val Loss: 0.13796614110469818\n",
      "Epoch 61, Loss: 0.1391724795103073, Val Loss: 0.13681362569332123\n",
      "Epoch 62, Loss: 0.13803207874298096, Val Loss: 0.13574853539466858\n",
      "Epoch 63, Loss: 0.1369737684726715, Val Loss: 0.1347401738166809\n",
      "Epoch 64, Loss: 0.13596084713935852, Val Loss: 0.13375131785869598\n",
      "Epoch 65, Loss: 0.13495273888111115, Val Loss: 0.13276098668575287\n",
      "Epoch 66, Loss: 0.13392795622348785, Val Loss: 0.13177186250686646\n",
      "Epoch 67, Loss: 0.13289159536361694, Val Loss: 0.13080289959907532\n",
      "Epoch 68, Loss: 0.1318667232990265, Val Loss: 0.1298731118440628\n",
      "Epoch 69, Loss: 0.13087774813175201, Val Loss: 0.12898819148540497\n",
      "Epoch 70, Loss: 0.12993541359901428, Val Loss: 0.12813681364059448\n",
      "Epoch 71, Loss: 0.1290324628353119, Val Loss: 0.12729789316654205\n",
      "Epoch 72, Loss: 0.1281505823135376, Val Loss: 0.12645310163497925\n",
      "Epoch 73, Loss: 0.12727230787277222, Val Loss: 0.12559638917446136\n",
      "Epoch 74, Loss: 0.12639141082763672, Val Loss: 0.12473532557487488\n",
      "Epoch 75, Loss: 0.12551352381706238, Val Loss: 0.12388435751199722\n",
      "Epoch 76, Loss: 0.12465061247348785, Val Loss: 0.12305575609207153\n",
      "Epoch 77, Loss: 0.12381205707788467, Val Loss: 0.12225411087274551\n",
      "Epoch 78, Loss: 0.12299935519695282, Val Loss: 0.12147592008113861\n",
      "Epoch 79, Loss: 0.12220671027898788, Val Loss: 0.12071409821510315\n",
      "Epoch 80, Loss: 0.12142537534236908, Val Loss: 0.11996278911828995\n",
      "Epoch 81, Loss: 0.12064918875694275, Val Loss: 0.11922071129083633\n",
      "Epoch 82, Loss: 0.11987730860710144, Val Loss: 0.11849010735750198\n",
      "Epoch 83, Loss: 0.11911328136920929, Val Loss: 0.11777421087026596\n",
      "Epoch 84, Loss: 0.11836221069097519, Val Loss: 0.11707419157028198\n",
      "Epoch 85, Loss: 0.11762718856334686, Val Loss: 0.11638770997524261\n",
      "Epoch 86, Loss: 0.11690758168697357, Val Loss: 0.11571020632982254\n",
      "Epoch 87, Loss: 0.11620001494884491, Val Loss: 0.11503712832927704\n",
      "Epoch 88, Loss: 0.11550057679414749, Val Loss: 0.11436626315116882\n",
      "Epoch 89, Loss: 0.1148071214556694, Val Loss: 0.11369843035936356\n",
      "Epoch 90, Loss: 0.11411988735198975, Val Loss: 0.11303654313087463\n",
      "Epoch 91, Loss: 0.11344090104103088, Val Loss: 0.11238383501768112\n",
      "Epoch 92, Loss: 0.11277230083942413, Val Loss: 0.11174215376377106\n",
      "Epoch 93, Loss: 0.1121148094534874, Val Loss: 0.11111161857843399\n",
      "Epoch 94, Loss: 0.11146750301122665, Val Loss: 0.110491082072258\n",
      "Epoch 95, Loss: 0.11082860827445984, Val Loss: 0.10987928509712219\n",
      "Epoch 96, Loss: 0.11019647121429443, Val Loss: 0.10927550494670868\n",
      "Epoch 97, Loss: 0.10957053303718567, Val Loss: 0.10867968946695328\n",
      "Epoch 98, Loss: 0.108951136469841, Val Loss: 0.10809216648340225\n",
      "Epoch 99, Loss: 0.10833923518657684, Val Loss: 0.107512928545475\n",
      "Epoch 100, Loss: 0.1077355220913887, Val Loss: 0.10694123804569244\n",
      "Epoch 101, Loss: 0.10714000463485718, Val Loss: 0.10637596994638443\n",
      "Epoch 102, Loss: 0.10655196011066437, Val Loss: 0.10581586509943008\n",
      "Epoch 103, Loss: 0.10597055405378342, Val Loss: 0.1052602231502533\n",
      "Epoch 104, Loss: 0.10539516061544418, Val Loss: 0.10470902919769287\n",
      "Epoch 105, Loss: 0.10482554137706757, Val Loss: 0.10416281968355179\n",
      "Epoch 106, Loss: 0.10426200181245804, Val Loss: 0.10362236946821213\n",
      "Epoch 107, Loss: 0.10370492935180664, Val Loss: 0.1030883714556694\n",
      "Epoch 108, Loss: 0.10315443575382233, Val Loss: 0.10256099700927734\n",
      "Epoch 109, Loss: 0.10261037200689316, Val Loss: 0.10204014927148819\n",
      "Epoch 110, Loss: 0.1020723506808281, Val Loss: 0.10152558982372284\n",
      "Epoch 111, Loss: 0.10153993964195251, Val Loss: 0.1010170504450798\n",
      "Epoch 112, Loss: 0.10101284831762314, Val Loss: 0.10051435977220535\n",
      "Epoch 113, Loss: 0.10049110651016235, Val Loss: 0.10001743584871292\n",
      "Epoch 114, Loss: 0.09997479617595673, Val Loss: 0.09952609986066818\n",
      "Epoch 115, Loss: 0.09946396946907043, Val Loss: 0.09904000163078308\n",
      "Epoch 116, Loss: 0.09895866364240646, Val Loss: 0.09855884313583374\n",
      "Epoch 117, Loss: 0.09845869243144989, Val Loss: 0.09808219224214554\n",
      "Epoch 118, Loss: 0.097963847219944, Val Loss: 0.09760979562997818\n",
      "Epoch 119, Loss: 0.09747391194105148, Val Loss: 0.0971415564417839\n",
      "Epoch 120, Loss: 0.09698870778083801, Val Loss: 0.09667760878801346\n",
      "Epoch 121, Loss: 0.09650827944278717, Val Loss: 0.09621809422969818\n",
      "Epoch 122, Loss: 0.09603264927864075, Val Loss: 0.0957631841301918\n",
      "Epoch 123, Loss: 0.09556180238723755, Val Loss: 0.09531299769878387\n",
      "Epoch 124, Loss: 0.0950956642627716, Val Loss: 0.09486747533082962\n",
      "Epoch 125, Loss: 0.09463415294885635, Val Loss: 0.09442664682865143\n",
      "Epoch 126, Loss: 0.09417703002691269, Val Loss: 0.09399037063121796\n",
      "Epoch 127, Loss: 0.09372430294752121, Val Loss: 0.09355852752923965\n",
      "Epoch 128, Loss: 0.09327587485313416, Val Loss: 0.09313105791807175\n",
      "Epoch 129, Loss: 0.09283170849084854, Val Loss: 0.0927077904343605\n",
      "Epoch 130, Loss: 0.09239178895950317, Val Loss: 0.09228863567113876\n",
      "Epoch 131, Loss: 0.09195604175329208, Val Loss: 0.091873399913311\n",
      "Epoch 132, Loss: 0.09152446687221527, Val Loss: 0.0914619043469429\n",
      "Epoch 133, Loss: 0.0910969078540802, Val Loss: 0.09105407446622849\n",
      "Epoch 134, Loss: 0.09067334234714508, Val Loss: 0.09064983576536179\n",
      "Epoch 135, Loss: 0.09025365114212036, Val Loss: 0.09024918079376221\n",
      "Epoch 136, Loss: 0.08983780443668365, Val Loss: 0.08985211700201035\n",
      "Epoch 137, Loss: 0.08942580223083496, Val Loss: 0.08945868164300919\n",
      "Epoch 138, Loss: 0.08901755511760712, Val Loss: 0.08906890451908112\n",
      "Epoch 139, Loss: 0.08861306309700012, Val Loss: 0.08868278563022614\n",
      "Epoch 140, Loss: 0.08821219950914383, Val Loss: 0.08830028772354126\n",
      "Epoch 141, Loss: 0.08781502395868301, Val Loss: 0.08792134374380112\n",
      "Epoch 142, Loss: 0.08742136508226395, Val Loss: 0.08754594624042511\n",
      "Epoch 143, Loss: 0.08703123033046722, Val Loss: 0.0871739536523819\n",
      "Epoch 144, Loss: 0.08664457499980927, Val Loss: 0.08680535852909088\n",
      "Epoch 145, Loss: 0.0862613245844841, Val Loss: 0.08644004911184311\n",
      "Epoch 146, Loss: 0.08588149398565292, Val Loss: 0.08607795834541321\n",
      "Epoch 147, Loss: 0.08550503849983215, Val Loss: 0.08571898192167282\n",
      "Epoch 148, Loss: 0.08513183891773224, Val Loss: 0.08536306768655777\n",
      "Epoch 149, Loss: 0.08476192504167557, Val Loss: 0.08501021564006805\n",
      "Epoch 150, Loss: 0.08439521491527557, Val Loss: 0.08466034382581711\n",
      "Epoch 151, Loss: 0.08403168618679047, Val Loss: 0.08431345224380493\n",
      "Epoch 152, Loss: 0.08367127180099487, Val Loss: 0.08396954834461212\n",
      "Epoch 153, Loss: 0.08331397920846939, Val Loss: 0.08362862467765808\n",
      "Epoch 154, Loss: 0.08295975625514984, Val Loss: 0.08329062908887863\n",
      "Epoch 155, Loss: 0.08260858058929443, Val Loss: 0.08295558393001556\n",
      "Epoch 156, Loss: 0.0822603702545166, Val Loss: 0.0826234295964241\n",
      "Epoch 157, Loss: 0.08191514760255814, Val Loss: 0.08229412883520126\n",
      "Epoch 158, Loss: 0.08157279342412949, Val Loss: 0.08196765184402466\n",
      "Epoch 159, Loss: 0.08123333007097244, Val Loss: 0.08164394646883011\n",
      "Epoch 160, Loss: 0.08089675009250641, Val Loss: 0.08132295310497284\n",
      "Epoch 161, Loss: 0.08056296408176422, Val Loss: 0.08100463449954987\n",
      "Epoch 162, Loss: 0.08023198693990707, Val Loss: 0.0806889533996582\n",
      "Epoch 163, Loss: 0.079903744161129, Val Loss: 0.08037585765123367\n",
      "Epoch 164, Loss: 0.0795782282948494, Val Loss: 0.08006531000137329\n",
      "Epoch 165, Loss: 0.07925540953874588, Val Loss: 0.07975731790065765\n",
      "Epoch 166, Loss: 0.07893521338701248, Val Loss: 0.07945185154676437\n",
      "Epoch 167, Loss: 0.07861767709255219, Val Loss: 0.07914883643388748\n",
      "Epoch 168, Loss: 0.078302763402462, Val Loss: 0.07884833216667175\n",
      "Epoch 169, Loss: 0.07799036800861359, Val Loss: 0.07855025678873062\n",
      "Epoch 170, Loss: 0.0776805430650711, Val Loss: 0.07825463265180588\n",
      "Epoch 171, Loss: 0.077373206615448, Val Loss: 0.07796139270067215\n",
      "Epoch 172, Loss: 0.07706835120916367, Val Loss: 0.07767056673765182\n",
      "Epoch 173, Loss: 0.07676596194505692, Val Loss: 0.07738208770751953\n",
      "Epoch 174, Loss: 0.07646600902080536, Val Loss: 0.07709592580795288\n",
      "Epoch 175, Loss: 0.07616843283176422, Val Loss: 0.07681205868721008\n",
      "Epoch 176, Loss: 0.07587326318025589, Val Loss: 0.07653041929006577\n",
      "Epoch 177, Loss: 0.07558043301105499, Val Loss: 0.07625102996826172\n",
      "Epoch 178, Loss: 0.07528993487358093, Val Loss: 0.07597386091947556\n",
      "Epoch 179, Loss: 0.07500170916318893, Val Loss: 0.0756988376379013\n",
      "Epoch 180, Loss: 0.07471577823162079, Val Loss: 0.07542596757411957\n",
      "Epoch 181, Loss: 0.07443208992481232, Val Loss: 0.07515524327754974\n",
      "Epoch 182, Loss: 0.07415062934160233, Val Loss: 0.07488662004470825\n",
      "Epoch 183, Loss: 0.07387139648199081, Val Loss: 0.07462010532617569\n",
      "Epoch 184, Loss: 0.07359430193901062, Val Loss: 0.07435566186904907\n",
      "Epoch 185, Loss: 0.07331939041614532, Val Loss: 0.0740932822227478\n",
      "Epoch 186, Loss: 0.07304660230875015, Val Loss: 0.0738329216837883\n",
      "Epoch 187, Loss: 0.07277590036392212, Val Loss: 0.07357460260391235\n",
      "Epoch 188, Loss: 0.07250731438398361, Val Loss: 0.0733182355761528\n",
      "Epoch 189, Loss: 0.07224078476428986, Val Loss: 0.07306385785341263\n",
      "Epoch 190, Loss: 0.07197628170251846, Val Loss: 0.07281141728162766\n",
      "Epoch 191, Loss: 0.07171379029750824, Val Loss: 0.07256090641021729\n",
      "Epoch 192, Loss: 0.07145334780216217, Val Loss: 0.07231230288743973\n",
      "Epoch 193, Loss: 0.0711948573589325, Val Loss: 0.0720655620098114\n",
      "Epoch 194, Loss: 0.07093833386898041, Val Loss: 0.0718206986784935\n",
      "Epoch 195, Loss: 0.07068374752998352, Val Loss: 0.07157764583826065\n",
      "Epoch 196, Loss: 0.07043108344078064, Val Loss: 0.07133644074201584\n",
      "Epoch 197, Loss: 0.07018030434846878, Val Loss: 0.07109703123569489\n",
      "Epoch 198, Loss: 0.06993142515420914, Val Loss: 0.07085942476987839\n",
      "Epoch 199, Loss: 0.06968439370393753, Val Loss: 0.07062359154224396\n",
      "Epoch 200, Loss: 0.06943920999765396, Val Loss: 0.07038947194814682\n",
      "Epoch 201, Loss: 0.06919586658477783, Val Loss: 0.07015713304281235\n",
      "Epoch 202, Loss: 0.06895428895950317, Val Loss: 0.06992649286985397\n",
      "Epoch 203, Loss: 0.06871454417705536, Val Loss: 0.0696975365281105\n",
      "Epoch 204, Loss: 0.06847651302814484, Val Loss: 0.06947029381990433\n",
      "Epoch 205, Loss: 0.06824027746915817, Val Loss: 0.06924469769001007\n",
      "Epoch 206, Loss: 0.06800577044487, Val Loss: 0.06902076303958893\n",
      "Epoch 207, Loss: 0.06777293235063553, Val Loss: 0.06879845261573792\n",
      "Epoch 208, Loss: 0.06754184514284134, Val Loss: 0.06857773661613464\n",
      "Epoch 209, Loss: 0.06731244176626205, Val Loss: 0.0683586373925209\n",
      "Epoch 210, Loss: 0.0670846700668335, Val Loss: 0.06814111769199371\n",
      "Epoch 211, Loss: 0.06685856729745865, Val Loss: 0.06792515516281128\n",
      "Epoch 212, Loss: 0.06663408875465393, Val Loss: 0.0677107498049736\n",
      "Epoch 213, Loss: 0.06641123443841934, Val Loss: 0.06749789416790009\n",
      "Epoch 214, Loss: 0.06618998199701309, Val Loss: 0.06728653609752655\n",
      "Epoch 215, Loss: 0.0659702941775322, Val Loss: 0.0670766830444336\n",
      "Epoch 216, Loss: 0.06575218588113785, Val Loss: 0.06686833500862122\n",
      "Epoch 217, Loss: 0.06553561985492706, Val Loss: 0.06666147708892822\n",
      "Epoch 218, Loss: 0.06532061845064163, Val Loss: 0.06645605713129044\n",
      "Epoch 219, Loss: 0.06510712951421738, Val Loss: 0.06625210493803024\n",
      "Epoch 220, Loss: 0.0648951381444931, Val Loss: 0.06604957580566406\n",
      "Epoch 221, Loss: 0.06468465924263, Val Loss: 0.0658484622836113\n",
      "Epoch 222, Loss: 0.06447562575340271, Val Loss: 0.06564876437187195\n",
      "Epoch 223, Loss: 0.0642680898308754, Val Loss: 0.06545047461986542\n",
      "Epoch 224, Loss: 0.06406199187040329, Val Loss: 0.06525354087352753\n",
      "Epoch 225, Loss: 0.06385733932256699, Val Loss: 0.06505796313285828\n",
      "Epoch 226, Loss: 0.06365408003330231, Val Loss: 0.06486374139785767\n",
      "Epoch 227, Loss: 0.06345223635435104, Val Loss: 0.06467088311910629\n",
      "Epoch 228, Loss: 0.06325183063745499, Val Loss: 0.06447932869195938\n",
      "Epoch 229, Loss: 0.06305276602506638, Val Loss: 0.06428910046815872\n",
      "Epoch 230, Loss: 0.06285508722066879, Val Loss: 0.06410018354654312\n",
      "Epoch 231, Loss: 0.06265873461961746, Val Loss: 0.06391254812479019\n",
      "Epoch 232, Loss: 0.062463775277137756, Val Loss: 0.06372617185115814\n",
      "Epoch 233, Loss: 0.062270112335681915, Val Loss: 0.06354107707738876\n",
      "Epoch 234, Loss: 0.06207777559757233, Val Loss: 0.06335724145174026\n",
      "Epoch 235, Loss: 0.06188676133751869, Val Loss: 0.06317463517189026\n",
      "Epoch 236, Loss: 0.06169701740145683, Val Loss: 0.06299326568841934\n",
      "Epoch 237, Loss: 0.06150854378938675, Val Loss: 0.06281311810016632\n",
      "Epoch 238, Loss: 0.061321359127759933, Val Loss: 0.06263417750597\n",
      "Epoch 239, Loss: 0.06113544479012489, Val Loss: 0.0624564066529274\n",
      "Epoch 240, Loss: 0.06095074117183685, Val Loss: 0.06227985769510269\n",
      "Epoch 241, Loss: 0.06076726317405701, Val Loss: 0.06210446357727051\n",
      "Epoch 242, Loss: 0.06058504804968834, Val Loss: 0.061930228024721146\n",
      "Epoch 243, Loss: 0.06040399149060249, Val Loss: 0.0617571584880352\n",
      "Epoch 244, Loss: 0.06022418662905693, Val Loss: 0.061585232615470886\n",
      "Epoch 245, Loss: 0.06004554033279419, Val Loss: 0.06141442805528641\n",
      "Epoch 246, Loss: 0.05986808240413666, Val Loss: 0.06124473363161087\n",
      "Epoch 247, Loss: 0.059691790491342545, Val Loss: 0.06107616052031517\n",
      "Epoch 248, Loss: 0.05951663479208946, Val Loss: 0.06090870127081871\n",
      "Epoch 249, Loss: 0.0593426451086998, Val Loss: 0.06074230372905731\n",
      "Epoch 250, Loss: 0.05916977673768997, Val Loss: 0.06057700887322426\n",
      "Epoch 251, Loss: 0.05899804085493088, Val Loss: 0.060412775725126266\n",
      "Epoch 252, Loss: 0.05882740393280983, Val Loss: 0.06024959683418274\n",
      "Epoch 253, Loss: 0.058657873421907425, Val Loss: 0.06008746474981308\n",
      "Epoch 254, Loss: 0.058489445596933365, Val Loss: 0.05992639437317848\n",
      "Epoch 255, Loss: 0.05832208693027496, Val Loss: 0.059766337275505066\n",
      "Epoch 256, Loss: 0.05815580114722252, Val Loss: 0.05960731580853462\n",
      "Epoch 257, Loss: 0.05799059942364693, Val Loss: 0.059449292719364166\n",
      "Epoch 258, Loss: 0.057826414704322815, Val Loss: 0.059292297810316086\n",
      "Epoch 259, Loss: 0.05766329914331436, Val Loss: 0.059136275202035904\n",
      "Epoch 260, Loss: 0.05750121921300888, Val Loss: 0.05898125469684601\n",
      "Epoch 261, Loss: 0.057340141385793686, Val Loss: 0.05882721766829491\n",
      "Epoch 262, Loss: 0.05718010663986206, Val Loss: 0.058674126863479614\n",
      "Epoch 263, Loss: 0.05702107027173042, Val Loss: 0.058521997183561325\n",
      "Epoch 264, Loss: 0.056863028556108475, Val Loss: 0.05837084352970123\n",
      "Epoch 265, Loss: 0.056705962866544724, Val Loss: 0.05822060629725456\n",
      "Epoch 266, Loss: 0.05654989555478096, Val Loss: 0.058071319013834\n",
      "Epoch 267, Loss: 0.0563947856426239, Val Loss: 0.057922955602407455\n",
      "Epoch 268, Loss: 0.05624064430594444, Val Loss: 0.05777550861239433\n",
      "Epoch 269, Loss: 0.0560874305665493, Val Loss: 0.057628970593214035\n",
      "Epoch 270, Loss: 0.05593520775437355, Val Loss: 0.05748334527015686\n",
      "Epoch 271, Loss: 0.055783893913030624, Val Loss: 0.057338591665029526\n",
      "Epoch 272, Loss: 0.05563350021839142, Val Loss: 0.05719473585486412\n",
      "Epoch 273, Loss: 0.055484045296907425, Val Loss: 0.057051777839660645\n",
      "Epoch 274, Loss: 0.05533549189567566, Val Loss: 0.05690968036651611\n",
      "Epoch 275, Loss: 0.05518786236643791, Val Loss: 0.056768447160720825\n",
      "Epoch 276, Loss: 0.055041100829839706, Val Loss: 0.05662805959582329\n",
      "Epoch 277, Loss: 0.05489524453878403, Val Loss: 0.0564885139465332\n",
      "Epoch 278, Loss: 0.05475024878978729, Val Loss: 0.05634982883930206\n",
      "Epoch 279, Loss: 0.05460614338517189, Val Loss: 0.05621197074651718\n",
      "Epoch 280, Loss: 0.05446290597319603, Val Loss: 0.05607494339346886\n",
      "Epoch 281, Loss: 0.05432051047682762, Val Loss: 0.0559387244284153\n",
      "Epoch 282, Loss: 0.05417896434664726, Val Loss: 0.0558033362030983\n",
      "Epoch 283, Loss: 0.054038286209106445, Val Loss: 0.05566873773932457\n",
      "Epoch 284, Loss: 0.05389842763543129, Val Loss: 0.05553495138883591\n",
      "Epoch 285, Loss: 0.0537593811750412, Val Loss: 0.05540195107460022\n",
      "Epoch 286, Loss: 0.05362118035554886, Val Loss: 0.055269744247198105\n",
      "Epoch 287, Loss: 0.05348377674818039, Val Loss: 0.05513828620314598\n",
      "Epoch 288, Loss: 0.05334717407822609, Val Loss: 0.055007629096508026\n",
      "Epoch 289, Loss: 0.05321139097213745, Val Loss: 0.054877735674381256\n",
      "Epoch 290, Loss: 0.05307638645172119, Val Loss: 0.05474858731031418\n",
      "Epoch 291, Loss: 0.05294216796755791, Val Loss: 0.054620202630758286\n",
      "Epoch 292, Loss: 0.0528087243437767, Val Loss: 0.05449256673455238\n",
      "Epoch 293, Loss: 0.052676066756248474, Val Loss: 0.05436566472053528\n",
      "Epoch 294, Loss: 0.05254416912794113, Val Loss: 0.05423949286341667\n",
      "Epoch 295, Loss: 0.052413031458854675, Val Loss: 0.054114051163196564\n",
      "Epoch 296, Loss: 0.052282627671957016, Val Loss: 0.05398932844400406\n",
      "Epoch 297, Loss: 0.05215301364660263, Val Loss: 0.053865332156419754\n",
      "Epoch 298, Loss: 0.052024103701114655, Val Loss: 0.053742047399282455\n",
      "Epoch 299, Loss: 0.05189594253897667, Val Loss: 0.05361945554614067\n",
      "Epoch 300, Loss: 0.05176851153373718, Val Loss: 0.053497571498155594\n",
      "Epoch 301, Loss: 0.0516417995095253, Val Loss: 0.053376369178295135\n",
      "Epoch 302, Loss: 0.05151579529047012, Val Loss: 0.05325585603713989\n",
      "Epoch 303, Loss: 0.05139049515128136, Val Loss: 0.053136032074689865\n",
      "Epoch 304, Loss: 0.0512659028172493, Val Loss: 0.05301686376333237\n",
      "Epoch 305, Loss: 0.05114203318953514, Val Loss: 0.052898380905389786\n",
      "Epoch 306, Loss: 0.05101881921291351, Val Loss: 0.052780572324991226\n",
      "Epoch 307, Loss: 0.0508963018655777, Val Loss: 0.05266339331865311\n",
      "Epoch 308, Loss: 0.05077448859810829, Val Loss: 0.05254688858985901\n",
      "Epoch 309, Loss: 0.050653330981731415, Val Loss: 0.05243103951215744\n",
      "Epoch 310, Loss: 0.05053284391760826, Val Loss: 0.052315812557935715\n",
      "Epoch 311, Loss: 0.050413016229867935, Val Loss: 0.05220123380422592\n",
      "Epoch 312, Loss: 0.05029385909438133, Val Loss: 0.05208728462457657\n",
      "Epoch 313, Loss: 0.05017535388469696, Val Loss: 0.05197396129369736\n",
      "Epoch 314, Loss: 0.05005748197436333, Val Loss: 0.05186126381158829\n",
      "Epoch 315, Loss: 0.049940258264541626, Val Loss: 0.05174919217824936\n",
      "Epoch 316, Loss: 0.04982367902994156, Val Loss: 0.05163772031664848\n",
      "Epoch 317, Loss: 0.04970772564411163, Val Loss: 0.05152686685323715\n",
      "Epoch 318, Loss: 0.04959239810705185, Val Loss: 0.05141660198569298\n",
      "Epoch 319, Loss: 0.049477700144052505, Val Loss: 0.051306940615177155\n",
      "Epoch 320, Loss: 0.04936360940337181, Val Loss: 0.05119788646697998\n",
      "Epoch 321, Loss: 0.04925014078617096, Val Loss: 0.05108940228819847\n",
      "Epoch 322, Loss: 0.049137260764837265, Val Loss: 0.05098149925470352\n",
      "Epoch 323, Loss: 0.04902500659227371, Val Loss: 0.05087418854236603\n",
      "Epoch 324, Loss: 0.048913322389125824, Val Loss: 0.0507674440741539\n",
      "Epoch 325, Loss: 0.048802245408296585, Val Loss: 0.05066128820180893\n",
      "Epoch 326, Loss: 0.0486917570233345, Val Loss: 0.05055566877126694\n",
      "Epoch 327, Loss: 0.04858185723423958, Val Loss: 0.050450630486011505\n",
      "Epoch 328, Loss: 0.048472531139850616, Val Loss: 0.05034615099430084\n",
      "Epoch 329, Loss: 0.04836376756429672, Val Loss: 0.050242211669683456\n",
      "Epoch 330, Loss: 0.048255570232868195, Val Loss: 0.05013882741332054\n",
      "Epoch 331, Loss: 0.04814794287085533, Val Loss: 0.050035979598760605\n",
      "Epoch 332, Loss: 0.04804089292883873, Val Loss: 0.049933675676584244\n",
      "Epoch 333, Loss: 0.04793437570333481, Val Loss: 0.049831900745630264\n",
      "Epoch 334, Loss: 0.04782842472195625, Val Loss: 0.04973067715764046\n",
      "Epoch 335, Loss: 0.04772301763296127, Val Loss: 0.04962996765971184\n",
      "Epoch 336, Loss: 0.04761815071105957, Val Loss: 0.049529772251844406\n",
      "Epoch 337, Loss: 0.04751381650567055, Val Loss: 0.04943011701107025\n",
      "Epoch 338, Loss: 0.0474100261926651, Val Loss: 0.04933095723390579\n",
      "Epoch 339, Loss: 0.04730675369501114, Val Loss: 0.04923233762383461\n",
      "Epoch 340, Loss: 0.047204017639160156, Val Loss: 0.04913419112563133\n",
      "Epoch 341, Loss: 0.04710180312395096, Val Loss: 0.04903656244277954\n",
      "Epoch 342, Loss: 0.047000110149383545, Val Loss: 0.048939432948827744\n",
      "Epoch 343, Loss: 0.04689892381429672, Val Loss: 0.04884280636906624\n",
      "Epoch 344, Loss: 0.04679824039340019, Val Loss: 0.04874666407704353\n",
      "Epoch 345, Loss: 0.04669807478785515, Val Loss: 0.04865100234746933\n",
      "Epoch 346, Loss: 0.046598393470048904, Val Loss: 0.04855583980679512\n",
      "Epoch 347, Loss: 0.046499237418174744, Val Loss: 0.04846116155385971\n",
      "Epoch 348, Loss: 0.046400558203458786, Val Loss: 0.04836695268750191\n",
      "Epoch 349, Loss: 0.046302370727062225, Val Loss: 0.04827320575714111\n",
      "Epoch 350, Loss: 0.04620467498898506, Val Loss: 0.04817994311451912\n",
      "Epoch 351, Loss: 0.0461074523627758, Val Loss: 0.04808714613318443\n",
      "Epoch 352, Loss: 0.04601071774959564, Val Loss: 0.047994811087846756\n",
      "Epoch 353, Loss: 0.04591445252299309, Val Loss: 0.047902945429086685\n",
      "Epoch 354, Loss: 0.04581866413354874, Val Loss: 0.04781151935458183\n",
      "Epoch 355, Loss: 0.04572334885597229, Val Loss: 0.04772054776549339\n",
      "Epoch 356, Loss: 0.04562849551439285, Val Loss: 0.04763004928827286\n",
      "Epoch 357, Loss: 0.04553410783410072, Val Loss: 0.04753997176885605\n",
      "Epoch 358, Loss: 0.04544016346335411, Val Loss: 0.04745034873485565\n",
      "Epoch 359, Loss: 0.04534668102860451, Val Loss: 0.047361165285110474\n",
      "Epoch 360, Loss: 0.04525364190340042, Val Loss: 0.047272417694330215\n",
      "Epoch 361, Loss: 0.045161064714193344, Val Loss: 0.04718409851193428\n",
      "Epoch 362, Loss: 0.045068927109241486, Val Loss: 0.047096215188503265\n",
      "Epoch 363, Loss: 0.044977229088544846, Val Loss: 0.047008756548166275\n",
      "Epoch 364, Loss: 0.044885970652103424, Val Loss: 0.046921730041503906\n",
      "Epoch 365, Loss: 0.04479515552520752, Val Loss: 0.04683511331677437\n",
      "Epoch 366, Loss: 0.044704753905534744, Val Loss: 0.046748921275138855\n",
      "Epoch 367, Loss: 0.04461478069424629, Val Loss: 0.046663135290145874\n",
      "Epoch 368, Loss: 0.044525258243083954, Val Loss: 0.046577781438827515\n",
      "Epoch 369, Loss: 0.04443613067269325, Val Loss: 0.04649282246828079\n",
      "Epoch 370, Loss: 0.04434744641184807, Val Loss: 0.046408265829086304\n",
      "Epoch 371, Loss: 0.04425915330648422, Val Loss: 0.04632412642240524\n",
      "Epoch 372, Loss: 0.0441712848842144, Val Loss: 0.04624037444591522\n",
      "Epoch 373, Loss: 0.044083837419748306, Val Loss: 0.046157028526067734\n",
      "Epoch 374, Loss: 0.04399677738547325, Val Loss: 0.04607406258583069\n",
      "Epoch 375, Loss: 0.04391014575958252, Val Loss: 0.04599151387810707\n",
      "Epoch 376, Loss: 0.043823882937431335, Val Loss: 0.0459093376994133\n",
      "Epoch 377, Loss: 0.043738044798374176, Val Loss: 0.04582754522562027\n",
      "Epoch 378, Loss: 0.04365259408950806, Val Loss: 0.04574614390730858\n",
      "Epoch 379, Loss: 0.043567534536123276, Val Loss: 0.045665111392736435\n",
      "Epoch 380, Loss: 0.043482840061187744, Val Loss: 0.04558447375893593\n",
      "Epoch 381, Loss: 0.04339856281876564, Val Loss: 0.045504216104745865\n",
      "Epoch 382, Loss: 0.04331466555595398, Val Loss: 0.04542430490255356\n",
      "Epoch 383, Loss: 0.04323115199804306, Val Loss: 0.04534478858113289\n",
      "Epoch 384, Loss: 0.04314802214503288, Val Loss: 0.04526562616229057\n",
      "Epoch 385, Loss: 0.04306524991989136, Val Loss: 0.0451868437230587\n",
      "Epoch 386, Loss: 0.042982857674360275, Val Loss: 0.04510841891169548\n",
      "Epoch 387, Loss: 0.04290083795785904, Val Loss: 0.04503035172820091\n",
      "Epoch 388, Loss: 0.04281919449567795, Val Loss: 0.0449526347219944\n",
      "Epoch 389, Loss: 0.04273790493607521, Val Loss: 0.044875290244817734\n",
      "Epoch 390, Loss: 0.04265698418021202, Val Loss: 0.04479827731847763\n",
      "Epoch 391, Loss: 0.04257642850279808, Val Loss: 0.04472162202000618\n",
      "Epoch 392, Loss: 0.04249623045325279, Val Loss: 0.044645313173532486\n",
      "Epoch 393, Loss: 0.042416367679834366, Val Loss: 0.044569361954927444\n",
      "Epoch 394, Loss: 0.042336881160736084, Val Loss: 0.04449375718832016\n",
      "Epoch 395, Loss: 0.04225774109363556, Val Loss: 0.04441847652196884\n",
      "Epoch 396, Loss: 0.04217894747853279, Val Loss: 0.04434354230761528\n",
      "Epoch 397, Loss: 0.042100485414266586, Val Loss: 0.044268932193517685\n",
      "Epoch 398, Loss: 0.04202238470315933, Val Loss: 0.04419467970728874\n",
      "Epoch 399, Loss: 0.04194461554288864, Val Loss: 0.044120751321315765\n",
      "Epoch 400, Loss: 0.04186718538403511, Val Loss: 0.044047143310308456\n",
      "Epoch 401, Loss: 0.04179009050130844, Val Loss: 0.043973859399557114\n",
      "Epoch 402, Loss: 0.04171333089470863, Val Loss: 0.04390091449022293\n",
      "Epoch 403, Loss: 0.041636910289525986, Val Loss: 0.04382828623056412\n",
      "Epoch 404, Loss: 0.041560810059309006, Val Loss: 0.04375598579645157\n",
      "Epoch 405, Loss: 0.041485052555799484, Val Loss: 0.04368399456143379\n",
      "Epoch 406, Loss: 0.04140959680080414, Val Loss: 0.043612319976091385\n",
      "Epoch 407, Loss: 0.04133448004722595, Val Loss: 0.043540969491004944\n",
      "Epoch 408, Loss: 0.04125967249274254, Val Loss: 0.043469931930303574\n",
      "Epoch 409, Loss: 0.041185200214385986, Val Loss: 0.04339919239282608\n",
      "Epoch 410, Loss: 0.04111103713512421, Val Loss: 0.04332877695560455\n",
      "Epoch 411, Loss: 0.041037172079086304, Val Loss: 0.0432586595416069\n",
      "Epoch 412, Loss: 0.04096364229917526, Val Loss: 0.04318884387612343\n",
      "Epoch 413, Loss: 0.0408904030919075, Val Loss: 0.04311933368444443\n",
      "Epoch 414, Loss: 0.04081748053431511, Val Loss: 0.0430501252412796\n",
      "Epoch 415, Loss: 0.04074486345052719, Val Loss: 0.04298120737075806\n",
      "Epoch 416, Loss: 0.04067256301641464, Val Loss: 0.042912598699331284\n",
      "Epoch 417, Loss: 0.040600549429655075, Val Loss: 0.0428442656993866\n",
      "Epoch 418, Loss: 0.04052882641553879, Val Loss: 0.04277624562382698\n",
      "Epoch 419, Loss: 0.04045742005109787, Val Loss: 0.04270850494503975\n",
      "Epoch 420, Loss: 0.04038631543517113, Val Loss: 0.042641058564186096\n",
      "Epoch 421, Loss: 0.040315475314855576, Val Loss: 0.04257388785481453\n",
      "Epoch 422, Loss: 0.0402449294924736, Val Loss: 0.04250701889395714\n",
      "Epoch 423, Loss: 0.040174707770347595, Val Loss: 0.042440421879291534\n",
      "Epoch 424, Loss: 0.04010475054383278, Val Loss: 0.042374104261398315\n",
      "Epoch 425, Loss: 0.04003509134054184, Val Loss: 0.04230807349085808\n",
      "Epoch 426, Loss: 0.039965707808732986, Val Loss: 0.042242325842380524\n",
      "Epoch 427, Loss: 0.03989660367369652, Val Loss: 0.04217683896422386\n",
      "Epoch 428, Loss: 0.03982779011130333, Val Loss: 0.042111631482839584\n",
      "Epoch 429, Loss: 0.03975925222039223, Val Loss: 0.04204670339822769\n",
      "Epoch 430, Loss: 0.03969098627567291, Val Loss: 0.04198204353451729\n",
      "Epoch 431, Loss: 0.039622996002435684, Val Loss: 0.04191765561699867\n",
      "Epoch 432, Loss: 0.03955528512597084, Val Loss: 0.04185352474451065\n",
      "Epoch 433, Loss: 0.039487846195697784, Val Loss: 0.041789665818214417\n",
      "Epoch 434, Loss: 0.039420682936906815, Val Loss: 0.04172607511281967\n",
      "Epoch 435, Loss: 0.03935376927256584, Val Loss: 0.041662752628326416\n",
      "Epoch 436, Loss: 0.03928713873028755, Val Loss: 0.04159967973828316\n",
      "Epoch 437, Loss: 0.03922075033187866, Val Loss: 0.041536856442689896\n",
      "Epoch 438, Loss: 0.03915465250611305, Val Loss: 0.04147431254386902\n",
      "Epoch 439, Loss: 0.039088811725378036, Val Loss: 0.04141201451420784\n",
      "Epoch 440, Loss: 0.039023227989673615, Val Loss: 0.04134996980428696\n",
      "Epoch 441, Loss: 0.03895788639783859, Val Loss: 0.04128818213939667\n",
      "Epoch 442, Loss: 0.038892827928066254, Val Loss: 0.04122663661837578\n",
      "Epoch 443, Loss: 0.03882800042629242, Val Loss: 0.041165344417095184\n",
      "Epoch 444, Loss: 0.03876343369483948, Val Loss: 0.04110430181026459\n",
      "Epoch 445, Loss: 0.038699131458997726, Val Loss: 0.04104350879788399\n",
      "Epoch 446, Loss: 0.038635075092315674, Val Loss: 0.040982965379953384\n",
      "Epoch 447, Loss: 0.038571249693632126, Val Loss: 0.040922652930021286\n",
      "Epoch 448, Loss: 0.038507699966430664, Val Loss: 0.04086260125041008\n",
      "Epoch 449, Loss: 0.03844437748193741, Val Loss: 0.040802765637636185\n",
      "Epoch 450, Loss: 0.03838129714131355, Val Loss: 0.040743183344602585\n",
      "Epoch 451, Loss: 0.03831847012042999, Val Loss: 0.040683843195438385\n",
      "Epoch 452, Loss: 0.038255881518125534, Val Loss: 0.04062473401427269\n",
      "Epoch 453, Loss: 0.03819352760910988, Val Loss: 0.0405658520758152\n",
      "Epoch 454, Loss: 0.03813141584396362, Val Loss: 0.04050721973180771\n",
      "Epoch 455, Loss: 0.03806954622268677, Val Loss: 0.04044879972934723\n",
      "Epoch 456, Loss: 0.038007888942956924, Val Loss: 0.04039062187075615\n",
      "Epoch 457, Loss: 0.03794650360941887, Val Loss: 0.040332671254873276\n",
      "Epoch 458, Loss: 0.037885311990976334, Val Loss: 0.04027494788169861\n",
      "Epoch 459, Loss: 0.0378243662416935, Val Loss: 0.04021746665239334\n",
      "Epoch 460, Loss: 0.03776365891098976, Val Loss: 0.04016019031405449\n",
      "Epoch 461, Loss: 0.03770316764712334, Val Loss: 0.04010314121842384\n",
      "Epoch 462, Loss: 0.037642911076545715, Val Loss: 0.040046319365501404\n",
      "Epoch 463, Loss: 0.037582878023386, Val Loss: 0.03998972102999687\n",
      "Epoch 464, Loss: 0.03752307966351509, Val Loss: 0.03993334248661995\n",
      "Epoch 465, Loss: 0.037463489919900894, Val Loss: 0.03987717255949974\n",
      "Epoch 466, Loss: 0.03740411996841431, Val Loss: 0.03982121869921684\n",
      "Epoch 467, Loss: 0.03734497353434563, Val Loss: 0.03976549580693245\n",
      "Epoch 468, Loss: 0.03728604316711426, Val Loss: 0.03970998153090477\n",
      "Epoch 469, Loss: 0.03722734749317169, Val Loss: 0.0396546833217144\n",
      "Epoch 470, Loss: 0.03716884180903435, Val Loss: 0.03959958627820015\n",
      "Epoch 471, Loss: 0.037110570818185806, Val Loss: 0.039544712752103806\n",
      "Epoch 472, Loss: 0.03705250471830368, Val Loss: 0.03949004039168358\n",
      "Epoch 473, Loss: 0.03699466586112976, Val Loss: 0.039435576647520065\n",
      "Epoch 474, Loss: 0.03693701699376106, Val Loss: 0.039381321519613266\n",
      "Epoch 475, Loss: 0.03687957674264908, Val Loss: 0.03932728245854378\n",
      "Epoch 476, Loss: 0.036822360008955, Val Loss: 0.03927343711256981\n",
      "Epoch 477, Loss: 0.03676534444093704, Val Loss: 0.039219796657562256\n",
      "Epoch 478, Loss: 0.0367085337638855, Val Loss: 0.03916635364294052\n",
      "Epoch 479, Loss: 0.03665192797780037, Val Loss: 0.0391131192445755\n",
      "Epoch 480, Loss: 0.03659553825855255, Val Loss: 0.039060089737176895\n",
      "Epoch 481, Loss: 0.03653934225440025, Val Loss: 0.03900725767016411\n",
      "Epoch 482, Loss: 0.036483339965343475, Val Loss: 0.03895462304353714\n",
      "Epoch 483, Loss: 0.03642754256725311, Val Loss: 0.03890218585729599\n",
      "Epoch 484, Loss: 0.036371950060129166, Val Loss: 0.03884993493556976\n",
      "Epoch 485, Loss: 0.036316558718681335, Val Loss: 0.03879788890480995\n",
      "Epoch 486, Loss: 0.03626134991645813, Val Loss: 0.038746025413274765\n",
      "Epoch 487, Loss: 0.03620634600520134, Val Loss: 0.03869437053799629\n",
      "Epoch 488, Loss: 0.036151546984910965, Val Loss: 0.03864288702607155\n",
      "Epoch 489, Loss: 0.03609692305326462, Val Loss: 0.03859161213040352\n",
      "Epoch 490, Loss: 0.03604249656200409, Val Loss: 0.03854052349925041\n",
      "Epoch 491, Loss: 0.03598826006054878, Val Loss: 0.038489606231451035\n",
      "Epoch 492, Loss: 0.035934217274188995, Val Loss: 0.03843890130519867\n",
      "Epoch 493, Loss: 0.03588036447763443, Val Loss: 0.03838835656642914\n",
      "Epoch 494, Loss: 0.03582669049501419, Val Loss: 0.03833801671862602\n",
      "Epoch 495, Loss: 0.03577321767807007, Val Loss: 0.03828785568475723\n",
      "Epoch 496, Loss: 0.03571992367506027, Val Loss: 0.03823787346482277\n",
      "Epoch 497, Loss: 0.0356668159365654, Val Loss: 0.03818807005882263\n",
      "Epoch 498, Loss: 0.03561389446258545, Val Loss: 0.038138456642627716\n",
      "Epoch 499, Loss: 0.03556114062666893, Val Loss: 0.03808901086449623\n",
      "Epoch 500, Loss: 0.035508595407009125, Val Loss: 0.038039762526750565\n"
     ]
    }
   ],
   "source": [
    "from flashrag.prompt import PromptTemplate\n",
    "import pickle\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "config_taqa = set_config(dataset_name='taqa', split = split)\n",
    "template_taqa= PromptTemplate(\n",
    "        config = config_taqa,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "config_triviaqa= set_config(dataset_name='triviaqa', split = split)\n",
    "template_triviaqa= PromptTemplate(\n",
    "        config = config_triviaqa,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "\n",
    "\n",
    "loading_hidden = True\n",
    "if not loading_hidden:\n",
    "    taqa_hidden_states, triviaqa_hidden_states = load_datasets(config_taqa, config_triviaqa, template_taqa, template_triviaqa)\n",
    "else:\n",
    "    file_path_triviaqa = '/cs/student/projects2/dsml/cdiezmar/hidden_states/triviaqa.pkl'\n",
    "    file_path_taqa = '/cs/student/projects2/dsml/cdiezmar/hidden_states/taqa.pkl'\n",
    "\n",
    "    with open(file_path_triviaqa, 'rb') as file_triviaqa:\n",
    "        triviaqa_hidden_states = pickle.load(file_triviaqa)\n",
    "        print('Length triviaqa: ', len(triviaqa_hidden_states))\n",
    "        triviaqa_hidden_states_tensor= torch.tensor(triviaqa_hidden_states, dtype=torch.float32)\n",
    "    with open(file_path_taqa, 'rb') as file_taqa:\n",
    "        taqa_hidden_states = pickle.load(file_taqa)\n",
    "        print('Length taqa: ', len(taqa_hidden_states))\n",
    "        taqa_hidden_states_tensor= torch.tensor(taqa_hidden_states, dtype=torch.float32)\n",
    "train_inputs, train_labels, val_inputs, val_labels = prepare_data(taqa_hidden_states_tensor, triviaqa_hidden_states_tensor)\n",
    "\n",
    "input_size = train_inputs.size(1)\n",
    "num_classes = 2  # Time-sensitive or not\n",
    "epochs = 500\n",
    "lr = 1e-4\n",
    "\n",
    "model = train_mlp_classifier_cuda(train_inputs, train_labels, val_inputs, val_labels, input_size, num_classes, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `question` in template\n",
      "Find `question` in template\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length triviaqa:  78785\n",
      "Length taqa:  10148\n",
      "Epoch 1, Loss: 0.6717779040336609, Val Loss: 0.6706259250640869\n",
      "Epoch 2, Loss: 0.6709496974945068, Val Loss: 0.6698081493377686\n",
      "Epoch 3, Loss: 0.6701421141624451, Val Loss: 0.6690090298652649\n",
      "Epoch 4, Loss: 0.6693528890609741, Val Loss: 0.6682175397872925\n",
      "Epoch 5, Loss: 0.6685718297958374, Val Loss: 0.6674311757087708\n",
      "Epoch 6, Loss: 0.6677964329719543, Val Loss: 0.6666488647460938\n",
      "Epoch 7, Loss: 0.6670251488685608, Val Loss: 0.6658666133880615\n",
      "Epoch 8, Loss: 0.6662543416023254, Val Loss: 0.6650797724723816\n",
      "Epoch 9, Loss: 0.6654788255691528, Val Loss: 0.6642841696739197\n",
      "Epoch 10, Loss: 0.6646950840950012, Val Loss: 0.6634777784347534\n",
      "Epoch 11, Loss: 0.6638999581336975, Val Loss: 0.6626630425453186\n",
      "Epoch 12, Loss: 0.66309654712677, Val Loss: 0.6618430614471436\n",
      "Epoch 13, Loss: 0.6622875332832336, Val Loss: 0.6610212922096252\n",
      "Epoch 14, Loss: 0.6614768505096436, Val Loss: 0.6601990461349487\n",
      "Epoch 15, Loss: 0.6606661081314087, Val Loss: 0.6593765616416931\n",
      "Epoch 16, Loss: 0.6598547101020813, Val Loss: 0.6585497856140137\n",
      "Epoch 17, Loss: 0.6590401530265808, Val Loss: 0.6577170491218567\n",
      "Epoch 18, Loss: 0.6582196950912476, Val Loss: 0.6568773984909058\n",
      "Epoch 19, Loss: 0.6573923826217651, Val Loss: 0.6560293436050415\n",
      "Epoch 20, Loss: 0.6565566658973694, Val Loss: 0.6551722884178162\n",
      "Epoch 21, Loss: 0.6557120680809021, Val Loss: 0.6543066501617432\n",
      "Epoch 22, Loss: 0.6548599004745483, Val Loss: 0.6534304022789001\n",
      "Epoch 23, Loss: 0.6539976596832275, Val Loss: 0.6525402665138245\n",
      "Epoch 24, Loss: 0.6531152129173279, Val Loss: 0.6516333818435669\n",
      "Epoch 25, Loss: 0.6522271037101746, Val Loss: 0.6507038474082947\n",
      "Epoch 26, Loss: 0.6513105034828186, Val Loss: 0.6497472524642944\n",
      "Epoch 27, Loss: 0.6503667235374451, Val Loss: 0.6487591862678528\n",
      "Epoch 28, Loss: 0.6493911743164062, Val Loss: 0.6477371454238892\n",
      "Epoch 29, Loss: 0.6483842134475708, Val Loss: 0.646680474281311\n",
      "Epoch 30, Loss: 0.6473419070243835, Val Loss: 0.6455907821655273\n",
      "Epoch 31, Loss: 0.646266520023346, Val Loss: 0.6444697976112366\n",
      "Epoch 32, Loss: 0.6451604962348938, Val Loss: 0.6433190107345581\n",
      "Epoch 33, Loss: 0.6440245509147644, Val Loss: 0.6421352028846741\n",
      "Epoch 34, Loss: 0.6428567171096802, Val Loss: 0.6409146785736084\n",
      "Epoch 35, Loss: 0.6416524052619934, Val Loss: 0.6396538019180298\n",
      "Epoch 36, Loss: 0.6404085755348206, Val Loss: 0.6383464932441711\n",
      "Epoch 37, Loss: 0.6391198635101318, Val Loss: 0.6369910836219788\n",
      "Epoch 38, Loss: 0.6377840638160706, Val Loss: 0.63558429479599\n",
      "Epoch 39, Loss: 0.6363980770111084, Val Loss: 0.6341267824172974\n",
      "Epoch 40, Loss: 0.6349613666534424, Val Loss: 0.6326161026954651\n",
      "Epoch 41, Loss: 0.6334722638130188, Val Loss: 0.6310442686080933\n",
      "Epoch 42, Loss: 0.6319228410720825, Val Loss: 0.6294029355049133\n",
      "Epoch 43, Loss: 0.6303040981292725, Val Loss: 0.6276819109916687\n",
      "Epoch 44, Loss: 0.6286078691482544, Val Loss: 0.6258736252784729\n",
      "Epoch 45, Loss: 0.6268258094787598, Val Loss: 0.6239624619483948\n",
      "Epoch 46, Loss: 0.6249410510063171, Val Loss: 0.6219374537467957\n",
      "Epoch 47, Loss: 0.6229429244995117, Val Loss: 0.6198019981384277\n",
      "Epoch 48, Loss: 0.6208374500274658, Val Loss: 0.6175726056098938\n",
      "Epoch 49, Loss: 0.6186412572860718, Val Loss: 0.6152780055999756\n",
      "Epoch 50, Loss: 0.6163815855979919, Val Loss: 0.6129277348518372\n",
      "Epoch 51, Loss: 0.6140664219856262, Val Loss: 0.6105080246925354\n",
      "Epoch 52, Loss: 0.611682653427124, Val Loss: 0.6079950928688049\n",
      "Epoch 53, Loss: 0.6092056035995483, Val Loss: 0.6053633093833923\n",
      "Epoch 54, Loss: 0.6066098809242249, Val Loss: 0.6026067733764648\n",
      "Epoch 55, Loss: 0.6038907766342163, Val Loss: 0.5997167229652405\n",
      "Epoch 56, Loss: 0.6010403037071228, Val Loss: 0.5967051982879639\n",
      "Epoch 57, Loss: 0.5980702042579651, Val Loss: 0.5935573577880859\n",
      "Epoch 58, Loss: 0.5949671268463135, Val Loss: 0.590280294418335\n",
      "Epoch 59, Loss: 0.591737687587738, Val Loss: 0.586868166923523\n",
      "Epoch 60, Loss: 0.5883755683898926, Val Loss: 0.5833122730255127\n",
      "Epoch 61, Loss: 0.5848708748817444, Val Loss: 0.5795851945877075\n",
      "Epoch 62, Loss: 0.5811970233917236, Val Loss: 0.5756688714027405\n",
      "Epoch 63, Loss: 0.5773367881774902, Val Loss: 0.571550726890564\n",
      "Epoch 64, Loss: 0.5732779502868652, Val Loss: 0.5672422051429749\n",
      "Epoch 65, Loss: 0.5690295100212097, Val Loss: 0.5627762079238892\n",
      "Epoch 66, Loss: 0.5646252632141113, Val Loss: 0.558142900466919\n",
      "Epoch 67, Loss: 0.5600541830062866, Val Loss: 0.5533146262168884\n",
      "Epoch 68, Loss: 0.5552896857261658, Val Loss: 0.5482625365257263\n",
      "Epoch 69, Loss: 0.5503061413764954, Val Loss: 0.5429718494415283\n",
      "Epoch 70, Loss: 0.545089602470398, Val Loss: 0.5374312400817871\n",
      "Epoch 71, Loss: 0.539629340171814, Val Loss: 0.5316013097763062\n",
      "Epoch 72, Loss: 0.5338858366012573, Val Loss: 0.525445818901062\n",
      "Epoch 73, Loss: 0.5278238654136658, Val Loss: 0.5189876556396484\n",
      "Epoch 74, Loss: 0.5214663147926331, Val Loss: 0.5122780203819275\n",
      "Epoch 75, Loss: 0.5148645639419556, Val Loss: 0.5053097605705261\n",
      "Epoch 76, Loss: 0.5080059170722961, Val Loss: 0.4980454444885254\n",
      "Epoch 77, Loss: 0.5008500814437866, Val Loss: 0.4905126690864563\n",
      "Epoch 78, Loss: 0.49342814087867737, Val Loss: 0.48283517360687256\n",
      "Epoch 79, Loss: 0.4858674705028534, Val Loss: 0.47513440251350403\n",
      "Epoch 80, Loss: 0.47829654812812805, Val Loss: 0.46749311685562134\n",
      "Epoch 81, Loss: 0.4707862436771393, Val Loss: 0.45990651845932007\n",
      "Epoch 82, Loss: 0.46332502365112305, Val Loss: 0.45232605934143066\n",
      "Epoch 83, Loss: 0.455864816904068, Val Loss: 0.44470545649528503\n",
      "Epoch 84, Loss: 0.4483608603477478, Val Loss: 0.4370425045490265\n",
      "Epoch 85, Loss: 0.4408179819583893, Val Loss: 0.4293242394924164\n",
      "Epoch 86, Loss: 0.43322357535362244, Val Loss: 0.4215462803840637\n",
      "Epoch 87, Loss: 0.42555302381515503, Val Loss: 0.41371944546699524\n",
      "Epoch 88, Loss: 0.41783174872398376, Val Loss: 0.4059906005859375\n",
      "Epoch 89, Loss: 0.41022351384162903, Val Loss: 0.39810630679130554\n",
      "Epoch 90, Loss: 0.40248361229896545, Val Loss: 0.39004307985305786\n",
      "Epoch 91, Loss: 0.3945762813091278, Val Loss: 0.38195744156837463\n",
      "Epoch 92, Loss: 0.3866507411003113, Val Loss: 0.37397265434265137\n",
      "Epoch 93, Loss: 0.37881961464881897, Val Loss: 0.366123765707016\n",
      "Epoch 94, Loss: 0.37109389901161194, Val Loss: 0.3584352433681488\n",
      "Epoch 95, Loss: 0.3635146915912628, Val Loss: 0.35104379057884216\n",
      "Epoch 96, Loss: 0.3562580943107605, Val Loss: 0.3440966010093689\n",
      "Epoch 97, Loss: 0.34944355487823486, Val Loss: 0.33725500106811523\n",
      "Epoch 98, Loss: 0.3427107334136963, Val Loss: 0.33050015568733215\n",
      "Epoch 99, Loss: 0.33603885769844055, Val Loss: 0.32405704259872437\n",
      "Epoch 100, Loss: 0.32968994975090027, Val Loss: 0.31806638836860657\n",
      "Epoch 101, Loss: 0.3238101005554199, Val Loss: 0.3122851550579071\n",
      "Epoch 102, Loss: 0.3181420862674713, Val Loss: 0.3065551221370697\n",
      "Epoch 103, Loss: 0.31254851818084717, Val Loss: 0.3010218143463135\n",
      "Epoch 104, Loss: 0.3071743845939636, Val Loss: 0.29593130946159363\n",
      "Epoch 105, Loss: 0.302249014377594, Val Loss: 0.2912593185901642\n",
      "Epoch 106, Loss: 0.29773545265197754, Val Loss: 0.28684017062187195\n",
      "Epoch 107, Loss: 0.29343003034591675, Val Loss: 0.28258323669433594\n",
      "Epoch 108, Loss: 0.28925779461860657, Val Loss: 0.27847567200660706\n",
      "Epoch 109, Loss: 0.2852294445037842, Val Loss: 0.2745656371116638\n",
      "Epoch 110, Loss: 0.2813921570777893, Val Loss: 0.2709001302719116\n",
      "Epoch 111, Loss: 0.27779945731163025, Val Loss: 0.26744765043258667\n",
      "Epoch 112, Loss: 0.27442023158073425, Val Loss: 0.26413464546203613\n",
      "Epoch 113, Loss: 0.27115947008132935, Val Loss: 0.2608458399772644\n",
      "Epoch 114, Loss: 0.26791203022003174, Val Loss: 0.25752609968185425\n",
      "Epoch 115, Loss: 0.264587938785553, Val Loss: 0.2540789544582367\n",
      "Epoch 116, Loss: 0.26113253831863403, Val Loss: 0.25039106607437134\n",
      "Epoch 117, Loss: 0.25741875171661377, Val Loss: 0.24641819298267365\n",
      "Epoch 118, Loss: 0.25336045026779175, Val Loss: 0.24223798513412476\n",
      "Epoch 119, Loss: 0.24903878569602966, Val Loss: 0.23829086124897003\n",
      "Epoch 120, Loss: 0.2448834925889969, Val Loss: 0.23482918739318848\n",
      "Epoch 121, Loss: 0.24135059118270874, Val Loss: 0.23130792379379272\n",
      "Epoch 122, Loss: 0.23772108554840088, Val Loss: 0.22730638086795807\n",
      "Epoch 123, Loss: 0.2336127609014511, Val Loss: 0.22302104532718658\n",
      "Epoch 124, Loss: 0.22912272810935974, Val Loss: 0.21861454844474792\n",
      "Epoch 125, Loss: 0.2244730144739151, Val Loss: 0.21425725519657135\n",
      "Epoch 126, Loss: 0.21989136934280396, Val Loss: 0.2103913426399231\n",
      "Epoch 127, Loss: 0.21581587195396423, Val Loss: 0.20678217709064484\n",
      "Epoch 128, Loss: 0.21205797791481018, Val Loss: 0.20297221839427948\n",
      "Epoch 129, Loss: 0.20808623731136322, Val Loss: 0.19888590276241302\n",
      "Epoch 130, Loss: 0.20379206538200378, Val Loss: 0.1944015473127365\n",
      "Epoch 131, Loss: 0.19910000264644623, Val Loss: 0.18993131816387177\n",
      "Epoch 132, Loss: 0.19441819190979004, Val Loss: 0.18649078905582428\n",
      "Epoch 133, Loss: 0.1907494217157364, Val Loss: 0.18394646048545837\n",
      "Epoch 134, Loss: 0.18811199069023132, Val Loss: 0.18131795525550842\n",
      "Epoch 135, Loss: 0.18540209531784058, Val Loss: 0.17831003665924072\n",
      "Epoch 136, Loss: 0.1823127567768097, Val Loss: 0.17517565190792084\n",
      "Epoch 137, Loss: 0.17898201942443848, Val Loss: 0.17203500866889954\n",
      "Epoch 138, Loss: 0.1756870597600937, Val Loss: 0.16890953481197357\n",
      "Epoch 139, Loss: 0.1724056750535965, Val Loss: 0.16568787395954132\n",
      "Epoch 140, Loss: 0.16908134520053864, Val Loss: 0.16250188648700714\n",
      "Epoch 141, Loss: 0.1658216267824173, Val Loss: 0.15947391092777252\n",
      "Epoch 142, Loss: 0.16267991065979004, Val Loss: 0.15649832785129547\n",
      "Epoch 143, Loss: 0.15962158143520355, Val Loss: 0.15367722511291504\n",
      "Epoch 144, Loss: 0.156696617603302, Val Loss: 0.15096957981586456\n",
      "Epoch 145, Loss: 0.1539229452610016, Val Loss: 0.1483595222234726\n",
      "Epoch 146, Loss: 0.15122045576572418, Val Loss: 0.14576011896133423\n",
      "Epoch 147, Loss: 0.1484958678483963, Val Loss: 0.14316269755363464\n",
      "Epoch 148, Loss: 0.14581336081027985, Val Loss: 0.140725240111351\n",
      "Epoch 149, Loss: 0.1432608962059021, Val Loss: 0.1382790207862854\n",
      "Epoch 150, Loss: 0.14073510468006134, Val Loss: 0.13576245307922363\n",
      "Epoch 151, Loss: 0.1381089687347412, Val Loss: 0.13320152461528778\n",
      "Epoch 152, Loss: 0.13545112311840057, Val Loss: 0.13082937896251678\n",
      "Epoch 153, Loss: 0.1329956352710724, Val Loss: 0.12878069281578064\n",
      "Epoch 154, Loss: 0.13086168467998505, Val Loss: 0.12682253122329712\n",
      "Epoch 155, Loss: 0.1288917362689972, Val Loss: 0.12495244294404984\n",
      "Epoch 156, Loss: 0.12694081664085388, Val Loss: 0.12314195185899734\n",
      "Epoch 157, Loss: 0.12508496642112732, Val Loss: 0.12145165354013443\n",
      "Epoch 158, Loss: 0.1233411654829979, Val Loss: 0.11977500468492508\n",
      "Epoch 159, Loss: 0.12162256240844727, Val Loss: 0.11808130890130997\n",
      "Epoch 160, Loss: 0.11989538371562958, Val Loss: 0.11643128097057343\n",
      "Epoch 161, Loss: 0.11818325519561768, Val Loss: 0.11476855725049973\n",
      "Epoch 162, Loss: 0.11649688333272934, Val Loss: 0.1131925880908966\n",
      "Epoch 163, Loss: 0.11487780511379242, Val Loss: 0.11169156432151794\n",
      "Epoch 164, Loss: 0.11335550248622894, Val Loss: 0.1103307455778122\n",
      "Epoch 165, Loss: 0.11196643859148026, Val Loss: 0.10909296572208405\n",
      "Epoch 166, Loss: 0.11070080101490021, Val Loss: 0.10791468620300293\n",
      "Epoch 167, Loss: 0.10951076447963715, Val Loss: 0.1067918986082077\n",
      "Epoch 168, Loss: 0.1083533987402916, Val Loss: 0.10566876828670502\n",
      "Epoch 169, Loss: 0.10721439123153687, Val Loss: 0.10458331555128098\n",
      "Epoch 170, Loss: 0.10609563440084457, Val Loss: 0.1035253182053566\n",
      "Epoch 171, Loss: 0.1050029918551445, Val Loss: 0.10248874127864838\n",
      "Epoch 172, Loss: 0.10393934696912766, Val Loss: 0.10148079693317413\n",
      "Epoch 173, Loss: 0.1029069647192955, Val Loss: 0.10051068663597107\n",
      "Epoch 174, Loss: 0.10190658271312714, Val Loss: 0.09956509619951248\n",
      "Epoch 175, Loss: 0.10094062983989716, Val Loss: 0.09866516292095184\n",
      "Epoch 176, Loss: 0.10001111030578613, Val Loss: 0.09780280292034149\n",
      "Epoch 177, Loss: 0.09911935031414032, Val Loss: 0.09695734083652496\n",
      "Epoch 178, Loss: 0.09826444834470749, Val Loss: 0.0961737334728241\n",
      "Epoch 179, Loss: 0.09744440019130707, Val Loss: 0.09538552165031433\n",
      "Epoch 180, Loss: 0.09665362536907196, Val Loss: 0.09466244280338287\n",
      "Epoch 181, Loss: 0.09588601440191269, Val Loss: 0.09390408545732498\n",
      "Epoch 182, Loss: 0.09513815492391586, Val Loss: 0.09320389479398727\n",
      "Epoch 183, Loss: 0.09440480172634125, Val Loss: 0.09249309450387955\n",
      "Epoch 184, Loss: 0.09368791431188583, Val Loss: 0.09180516749620438\n",
      "Epoch 185, Loss: 0.09298903495073318, Val Loss: 0.09114880859851837\n",
      "Epoch 186, Loss: 0.09230633825063705, Val Loss: 0.09048512578010559\n",
      "Epoch 187, Loss: 0.09164354205131531, Val Loss: 0.08987578749656677\n",
      "Epoch 188, Loss: 0.09099868685007095, Val Loss: 0.08923449367284775\n",
      "Epoch 189, Loss: 0.09037096053361893, Val Loss: 0.08867001533508301\n",
      "Epoch 190, Loss: 0.08976206183433533, Val Loss: 0.08805736154317856\n",
      "Epoch 191, Loss: 0.0891699269413948, Val Loss: 0.08752108365297318\n",
      "Epoch 192, Loss: 0.08859216421842575, Val Loss: 0.08694501966238022\n",
      "Epoch 193, Loss: 0.08802475035190582, Val Loss: 0.0864291563630104\n",
      "Epoch 194, Loss: 0.08747062087059021, Val Loss: 0.0858844667673111\n",
      "Epoch 195, Loss: 0.08692597597837448, Val Loss: 0.08538978546857834\n",
      "Epoch 196, Loss: 0.08639213442802429, Val Loss: 0.08485296368598938\n",
      "Epoch 197, Loss: 0.08586719632148743, Val Loss: 0.08439373970031738\n",
      "Epoch 198, Loss: 0.08535383641719818, Val Loss: 0.08385153859853745\n",
      "Epoch 199, Loss: 0.08484997600317001, Val Loss: 0.08344628661870956\n",
      "Epoch 200, Loss: 0.084357850253582, Val Loss: 0.08290126174688339\n",
      "Epoch 201, Loss: 0.08387677371501923, Val Loss: 0.08254245668649673\n",
      "Epoch 202, Loss: 0.08340447396039963, Val Loss: 0.0819973275065422\n",
      "Epoch 203, Loss: 0.0829412117600441, Val Loss: 0.0816478431224823\n",
      "Epoch 204, Loss: 0.08248558640480042, Val Loss: 0.08113562315702438\n",
      "Epoch 205, Loss: 0.08203732222318649, Val Loss: 0.0807829424738884\n",
      "Epoch 206, Loss: 0.08159668743610382, Val Loss: 0.08032583445310593\n",
      "Epoch 207, Loss: 0.0811627209186554, Val Loss: 0.07991362363100052\n",
      "Epoch 208, Loss: 0.08073840290307999, Val Loss: 0.07953620702028275\n",
      "Epoch 209, Loss: 0.08032238483428955, Val Loss: 0.07909343391656876\n",
      "Epoch 210, Loss: 0.07991426438093185, Val Loss: 0.0787738561630249\n",
      "Epoch 211, Loss: 0.07951200008392334, Val Loss: 0.07830943912267685\n",
      "Epoch 212, Loss: 0.07911394536495209, Val Loss: 0.07800205796957016\n",
      "Epoch 213, Loss: 0.07872235774993896, Val Loss: 0.0775790885090828\n",
      "Epoch 214, Loss: 0.07833661139011383, Val Loss: 0.0772680789232254\n",
      "Epoch 215, Loss: 0.07795816659927368, Val Loss: 0.07687527686357498\n",
      "Epoch 216, Loss: 0.0775841549038887, Val Loss: 0.07653456181287766\n",
      "Epoch 217, Loss: 0.07721865177154541, Val Loss: 0.07620754837989807\n",
      "Epoch 218, Loss: 0.07685738801956177, Val Loss: 0.07585164904594421\n",
      "Epoch 219, Loss: 0.07650161534547806, Val Loss: 0.07554223388433456\n",
      "Epoch 220, Loss: 0.0761503055691719, Val Loss: 0.0751814991235733\n",
      "Epoch 221, Loss: 0.0758032277226448, Val Loss: 0.07489762455224991\n",
      "Epoch 222, Loss: 0.07545863091945648, Val Loss: 0.07454512268304825\n",
      "Epoch 223, Loss: 0.075119249522686, Val Loss: 0.07425641268491745\n",
      "Epoch 224, Loss: 0.07478263229131699, Val Loss: 0.07391577959060669\n",
      "Epoch 225, Loss: 0.07445123791694641, Val Loss: 0.0736367478966713\n",
      "Epoch 226, Loss: 0.07412301748991013, Val Loss: 0.07331443578004837\n",
      "Epoch 227, Loss: 0.0737992599606514, Val Loss: 0.07303179800510406\n",
      "Epoch 228, Loss: 0.07347860187292099, Val Loss: 0.07271189987659454\n",
      "Epoch 229, Loss: 0.0731620192527771, Val Loss: 0.07245481759309769\n",
      "Epoch 230, Loss: 0.07284875959157944, Val Loss: 0.07213142514228821\n",
      "Epoch 231, Loss: 0.0725392997264862, Val Loss: 0.07189380377531052\n",
      "Epoch 232, Loss: 0.0722338929772377, Val Loss: 0.07154795527458191\n",
      "Epoch 233, Loss: 0.0719328373670578, Val Loss: 0.07136756926774979\n",
      "Epoch 234, Loss: 0.07163840532302856, Val Loss: 0.07099004089832306\n",
      "Epoch 235, Loss: 0.07135072350502014, Val Loss: 0.07085201144218445\n",
      "Epoch 236, Loss: 0.07105797529220581, Val Loss: 0.07044653594493866\n",
      "Epoch 237, Loss: 0.07075630873441696, Val Loss: 0.07024693489074707\n",
      "Epoch 238, Loss: 0.07045093178749084, Val Loss: 0.06996162235736847\n",
      "Epoch 239, Loss: 0.07015879452228546, Val Loss: 0.06966719776391983\n",
      "Epoch 240, Loss: 0.06988120824098587, Val Loss: 0.06948886811733246\n",
      "Epoch 241, Loss: 0.06960347294807434, Val Loss: 0.06914311647415161\n",
      "Epoch 242, Loss: 0.06931865960359573, Val Loss: 0.06893863528966904\n",
      "Epoch 243, Loss: 0.06902993470430374, Val Loss: 0.06866499781608582\n",
      "Epoch 244, Loss: 0.06874881684780121, Val Loss: 0.06839221715927124\n",
      "Epoch 245, Loss: 0.06847596913576126, Val Loss: 0.06819318234920502\n",
      "Epoch 246, Loss: 0.06820610910654068, Val Loss: 0.06788099557161331\n",
      "Epoch 247, Loss: 0.06793436408042908, Val Loss: 0.06767755001783371\n",
      "Epoch 248, Loss: 0.06766059249639511, Val Loss: 0.06740961223840714\n",
      "Epoch 249, Loss: 0.06738999485969543, Val Loss: 0.06716015934944153\n",
      "Epoch 250, Loss: 0.06712425500154495, Val Loss: 0.06695719063282013\n",
      "Epoch 251, Loss: 0.06686262786388397, Val Loss: 0.06667166203260422\n",
      "Epoch 252, Loss: 0.06659995019435883, Val Loss: 0.06648358702659607\n",
      "Epoch 253, Loss: 0.06633511930704117, Val Loss: 0.06621377915143967\n",
      "Epoch 254, Loss: 0.06606944650411606, Val Loss: 0.06598670035600662\n",
      "Epoch 255, Loss: 0.06580739468336105, Val Loss: 0.06577010452747345\n",
      "Epoch 256, Loss: 0.06554903835058212, Val Loss: 0.06550859659910202\n",
      "Epoch 257, Loss: 0.0652911588549614, Val Loss: 0.06531038880348206\n",
      "Epoch 258, Loss: 0.06503346562385559, Val Loss: 0.06504546105861664\n",
      "Epoch 259, Loss: 0.06477499008178711, Val Loss: 0.0648333877325058\n",
      "Epoch 260, Loss: 0.06451672315597534, Val Loss: 0.0645933523774147\n",
      "Epoch 261, Loss: 0.06425970047712326, Val Loss: 0.06436032801866531\n",
      "Epoch 262, Loss: 0.06400446593761444, Val Loss: 0.06414379924535751\n",
      "Epoch 263, Loss: 0.06375076621770859, Val Loss: 0.0638931468129158\n",
      "Epoch 264, Loss: 0.06349792331457138, Val Loss: 0.06368733942508698\n",
      "Epoch 265, Loss: 0.06324587762355804, Val Loss: 0.06343182176351547\n",
      "Epoch 266, Loss: 0.06299471110105515, Val Loss: 0.06323323398828506\n",
      "Epoch 267, Loss: 0.06274542957544327, Val Loss: 0.06297527253627777\n",
      "Epoch 268, Loss: 0.062496114522218704, Val Loss: 0.06277941912412643\n",
      "Epoch 269, Loss: 0.06224710866808891, Val Loss: 0.06252849847078323\n",
      "Epoch 270, Loss: 0.0619986318051815, Val Loss: 0.06232775002717972\n",
      "Epoch 271, Loss: 0.061749327927827835, Val Loss: 0.06208908557891846\n",
      "Epoch 272, Loss: 0.0615016333758831, Val Loss: 0.06187516823410988\n",
      "Epoch 273, Loss: 0.061253923922777176, Val Loss: 0.06164999678730965\n",
      "Epoch 274, Loss: 0.06100736930966377, Val Loss: 0.06142723187804222\n",
      "Epoch 275, Loss: 0.06076108664274216, Val Loss: 0.06121571734547615\n",
      "Epoch 276, Loss: 0.06051551178097725, Val Loss: 0.06098296493291855\n",
      "Epoch 277, Loss: 0.060270898044109344, Val Loss: 0.06078585609793663\n",
      "Epoch 278, Loss: 0.06002708524465561, Val Loss: 0.06054121255874634\n",
      "Epoch 279, Loss: 0.05978371202945709, Val Loss: 0.060357287526130676\n",
      "Epoch 280, Loss: 0.05954153463244438, Val Loss: 0.06010061502456665\n",
      "Epoch 281, Loss: 0.0593009777367115, Val Loss: 0.05993353947997093\n",
      "Epoch 282, Loss: 0.059060823172330856, Val Loss: 0.0596623420715332\n",
      "Epoch 283, Loss: 0.05882209166884422, Val Loss: 0.059503212571144104\n",
      "Epoch 284, Loss: 0.058579929172992706, Val Loss: 0.059223949909210205\n",
      "Epoch 285, Loss: 0.058336663991212845, Val Loss: 0.05904868617653847\n",
      "Epoch 286, Loss: 0.0580856055021286, Val Loss: 0.05878905579447746\n",
      "Epoch 287, Loss: 0.05783591419458389, Val Loss: 0.05858083441853523\n",
      "Epoch 288, Loss: 0.05758953094482422, Val Loss: 0.05837470665574074\n",
      "Epoch 289, Loss: 0.05734962970018387, Val Loss: 0.0581367164850235\n",
      "Epoch 290, Loss: 0.05711347237229347, Val Loss: 0.05796005204319954\n",
      "Epoch 291, Loss: 0.05687320977449417, Val Loss: 0.057704780250787735\n",
      "Epoch 292, Loss: 0.05662897229194641, Val Loss: 0.05751132220029831\n",
      "Epoch 293, Loss: 0.056380972266197205, Val Loss: 0.05727626010775566\n",
      "Epoch 294, Loss: 0.0561351552605629, Val Loss: 0.05705919489264488\n",
      "Epoch 295, Loss: 0.055892277508974075, Val Loss: 0.05685541033744812\n",
      "Epoch 296, Loss: 0.055651355534791946, Val Loss: 0.0566239133477211\n",
      "Epoch 297, Loss: 0.05541209131479263, Val Loss: 0.05643004551529884\n",
      "Epoch 298, Loss: 0.05517097935080528, Val Loss: 0.05619273707270622\n",
      "Epoch 299, Loss: 0.05492941290140152, Val Loss: 0.05598907545208931\n",
      "Epoch 300, Loss: 0.05468669533729553, Val Loss: 0.05575963482260704\n",
      "Epoch 301, Loss: 0.05444555729627609, Val Loss: 0.05554445460438728\n",
      "Epoch 302, Loss: 0.05420471727848053, Val Loss: 0.05532478168606758\n",
      "Epoch 303, Loss: 0.05396479740738869, Val Loss: 0.05510510131716728\n",
      "Epoch 304, Loss: 0.053725700825452805, Val Loss: 0.05489300936460495\n",
      "Epoch 305, Loss: 0.0534861721098423, Val Loss: 0.05466834083199501\n",
      "Epoch 306, Loss: 0.053247131407260895, Val Loss: 0.05445823445916176\n",
      "Epoch 307, Loss: 0.053008776158094406, Val Loss: 0.05422879010438919\n",
      "Epoch 308, Loss: 0.05276942253112793, Val Loss: 0.05401817336678505\n",
      "Epoch 309, Loss: 0.05253049358725548, Val Loss: 0.05378991737961769\n",
      "Epoch 310, Loss: 0.05229267477989197, Val Loss: 0.05358558148145676\n",
      "Epoch 311, Loss: 0.0520552359521389, Val Loss: 0.053356532007455826\n",
      "Epoch 312, Loss: 0.05181913822889328, Val Loss: 0.05316348001360893\n",
      "Epoch 313, Loss: 0.05158577114343643, Val Loss: 0.05292929708957672\n",
      "Epoch 314, Loss: 0.05135396122932434, Val Loss: 0.05274968594312668\n",
      "Epoch 315, Loss: 0.051121022552251816, Val Loss: 0.05250893160700798\n",
      "Epoch 316, Loss: 0.050893522799015045, Val Loss: 0.05233504995703697\n",
      "Epoch 317, Loss: 0.05065939947962761, Val Loss: 0.05208157002925873\n",
      "Epoch 318, Loss: 0.05042128264904022, Val Loss: 0.05188126116991043\n",
      "Epoch 319, Loss: 0.05017084628343582, Val Loss: 0.051635656505823135\n",
      "Epoch 320, Loss: 0.049921825528144836, Val Loss: 0.05142277106642723\n",
      "Epoch 321, Loss: 0.049685895442962646, Val Loss: 0.0512327179312706\n",
      "Epoch 322, Loss: 0.049462828785181046, Val Loss: 0.05101122707128525\n",
      "Epoch 323, Loss: 0.04924146831035614, Val Loss: 0.050820641219615936\n",
      "Epoch 324, Loss: 0.04900512844324112, Val Loss: 0.05057832598686218\n",
      "Epoch 325, Loss: 0.048760347068309784, Val Loss: 0.0503668412566185\n",
      "Epoch 326, Loss: 0.04851760342717171, Val Loss: 0.05016069486737251\n",
      "Epoch 327, Loss: 0.048287421464920044, Val Loss: 0.04994763806462288\n",
      "Epoch 328, Loss: 0.0480632558465004, Val Loss: 0.049756668508052826\n",
      "Epoch 329, Loss: 0.04783393442630768, Val Loss: 0.04952831566333771\n",
      "Epoch 330, Loss: 0.04759752377867699, Val Loss: 0.04932010546326637\n",
      "Epoch 331, Loss: 0.04735740274190903, Val Loss: 0.049104638397693634\n",
      "Epoch 332, Loss: 0.047122929245233536, Val Loss: 0.048894260078668594\n",
      "Epoch 333, Loss: 0.046894900500774384, Val Loss: 0.04869813844561577\n",
      "Epoch 334, Loss: 0.04666785150766373, Val Loss: 0.048482391983270645\n",
      "Epoch 335, Loss: 0.046438466757535934, Val Loss: 0.04827937111258507\n",
      "Epoch 336, Loss: 0.04620441421866417, Val Loss: 0.04806288704276085\n",
      "Epoch 337, Loss: 0.04597019404172897, Val Loss: 0.04785539209842682\n",
      "Epoch 338, Loss: 0.04573846235871315, Val Loss: 0.04765240475535393\n",
      "Epoch 339, Loss: 0.045510292053222656, Val Loss: 0.04744447395205498\n",
      "Epoch 340, Loss: 0.045283637940883636, Val Loss: 0.04724498838186264\n",
      "Epoch 341, Loss: 0.04505614936351776, Val Loss: 0.04703305661678314\n",
      "Epoch 342, Loss: 0.04482637345790863, Val Loss: 0.04683009162545204\n",
      "Epoch 343, Loss: 0.04459474980831146, Val Loss: 0.046619463711977005\n",
      "Epoch 344, Loss: 0.04436332732439041, Val Loss: 0.04641374945640564\n",
      "Epoch 345, Loss: 0.04413357004523277, Val Loss: 0.04620911553502083\n",
      "Epoch 346, Loss: 0.04390373080968857, Val Loss: 0.046001601964235306\n",
      "Epoch 347, Loss: 0.04367491975426674, Val Loss: 0.04579842835664749\n",
      "Epoch 348, Loss: 0.043447189033031464, Val Loss: 0.04558883234858513\n",
      "Epoch 349, Loss: 0.04321941360831261, Val Loss: 0.04538467898964882\n",
      "Epoch 350, Loss: 0.04299119487404823, Val Loss: 0.04517572745680809\n",
      "Epoch 351, Loss: 0.042763181030750275, Val Loss: 0.04497070610523224\n",
      "Epoch 352, Loss: 0.042534902691841125, Val Loss: 0.04475897550582886\n",
      "Epoch 353, Loss: 0.04230644553899765, Val Loss: 0.04455220699310303\n",
      "Epoch 354, Loss: 0.04207821190357208, Val Loss: 0.04433979094028473\n",
      "Epoch 355, Loss: 0.04184992238879204, Val Loss: 0.0441327802836895\n",
      "Epoch 356, Loss: 0.04162167012691498, Val Loss: 0.04392286017537117\n",
      "Epoch 357, Loss: 0.0413946658372879, Val Loss: 0.04371747002005577\n",
      "Epoch 358, Loss: 0.04116753488779068, Val Loss: 0.04350854828953743\n",
      "Epoch 359, Loss: 0.04094179347157478, Val Loss: 0.04330456256866455\n",
      "Epoch 360, Loss: 0.04071659967303276, Val Loss: 0.04309726506471634\n",
      "Epoch 361, Loss: 0.04049269109964371, Val Loss: 0.042894136160612106\n",
      "Epoch 362, Loss: 0.040268998593091965, Val Loss: 0.04268830642104149\n",
      "Epoch 363, Loss: 0.04004702344536781, Val Loss: 0.0424874946475029\n",
      "Epoch 364, Loss: 0.039825063198804855, Val Loss: 0.04228386655449867\n",
      "Epoch 365, Loss: 0.03960463032126427, Val Loss: 0.042082834988832474\n",
      "Epoch 366, Loss: 0.03938066214323044, Val Loss: 0.04187523573637009\n",
      "Epoch 367, Loss: 0.03915563225746155, Val Loss: 0.04167015478014946\n",
      "Epoch 368, Loss: 0.038927268236875534, Val Loss: 0.04145900905132294\n",
      "Epoch 369, Loss: 0.03869964927434921, Val Loss: 0.04125281795859337\n",
      "Epoch 370, Loss: 0.038473233580589294, Val Loss: 0.04104891046881676\n",
      "Epoch 371, Loss: 0.03825000673532486, Val Loss: 0.040849026292562485\n",
      "Epoch 372, Loss: 0.03802916035056114, Val Loss: 0.04065093770623207\n",
      "Epoch 373, Loss: 0.037809427827596664, Val Loss: 0.040452614426612854\n",
      "Epoch 374, Loss: 0.03759109228849411, Val Loss: 0.04025401547551155\n",
      "Epoch 375, Loss: 0.037372201681137085, Val Loss: 0.04005345702171326\n",
      "Epoch 376, Loss: 0.03715373948216438, Val Loss: 0.0398515909910202\n",
      "Epoch 377, Loss: 0.036932673305273056, Val Loss: 0.03964846208691597\n",
      "Epoch 378, Loss: 0.03671164810657501, Val Loss: 0.039448730647563934\n",
      "Epoch 379, Loss: 0.03649097681045532, Val Loss: 0.03924770653247833\n",
      "Epoch 380, Loss: 0.03627073019742966, Val Loss: 0.03904886916279793\n",
      "Epoch 381, Loss: 0.036051537841558456, Val Loss: 0.03885097801685333\n",
      "Epoch 382, Loss: 0.03583412617444992, Val Loss: 0.03865475952625275\n",
      "Epoch 383, Loss: 0.03561768680810928, Val Loss: 0.0384630411863327\n",
      "Epoch 384, Loss: 0.035402365028858185, Val Loss: 0.038270026445388794\n",
      "Epoch 385, Loss: 0.03518887609243393, Val Loss: 0.0380825400352478\n",
      "Epoch 386, Loss: 0.03497596085071564, Val Loss: 0.03789059817790985\n",
      "Epoch 387, Loss: 0.034764211624860764, Val Loss: 0.03770333155989647\n",
      "Epoch 388, Loss: 0.034549664705991745, Val Loss: 0.03750641271471977\n",
      "Epoch 389, Loss: 0.03433540463447571, Val Loss: 0.03731568902730942\n",
      "Epoch 390, Loss: 0.03411699831485748, Val Loss: 0.037115372717380524\n",
      "Epoch 391, Loss: 0.03389614075422287, Val Loss: 0.03692234680056572\n",
      "Epoch 392, Loss: 0.03366996347904205, Val Loss: 0.03672131523489952\n",
      "Epoch 393, Loss: 0.033437345176935196, Val Loss: 0.036515671759843826\n",
      "Epoch 394, Loss: 0.0332007110118866, Val Loss: 0.03630008175969124\n",
      "Epoch 395, Loss: 0.03297267481684685, Val Loss: 0.03609291836619377\n",
      "Epoch 396, Loss: 0.03275521472096443, Val Loss: 0.03590510040521622\n",
      "Epoch 397, Loss: 0.03254401311278343, Val Loss: 0.03571401163935661\n",
      "Epoch 398, Loss: 0.0323350764811039, Val Loss: 0.035533636808395386\n",
      "Epoch 399, Loss: 0.03212597221136093, Val Loss: 0.03534151241183281\n",
      "Epoch 400, Loss: 0.031917721033096313, Val Loss: 0.035166095942258835\n",
      "Epoch 401, Loss: 0.03170866519212723, Val Loss: 0.03497101366519928\n",
      "Epoch 402, Loss: 0.03150058910250664, Val Loss: 0.03479478135704994\n",
      "Epoch 403, Loss: 0.03129059821367264, Val Loss: 0.03459592163562775\n",
      "Epoch 404, Loss: 0.0310813058167696, Val Loss: 0.03441620245575905\n",
      "Epoch 405, Loss: 0.030870769172906876, Val Loss: 0.03421530872583389\n",
      "Epoch 406, Loss: 0.03066084533929825, Val Loss: 0.034032464027404785\n",
      "Epoch 407, Loss: 0.030451472848653793, Val Loss: 0.03383932262659073\n",
      "Epoch 408, Loss: 0.030242934823036194, Val Loss: 0.033656902611255646\n",
      "Epoch 409, Loss: 0.030035488307476044, Val Loss: 0.03347187116742134\n",
      "Epoch 410, Loss: 0.029829859733581543, Val Loss: 0.033284641802310944\n",
      "Epoch 411, Loss: 0.02962510846555233, Val Loss: 0.033105332404375076\n",
      "Epoch 412, Loss: 0.029421117156744003, Val Loss: 0.032913532108068466\n",
      "Epoch 413, Loss: 0.029216844588518143, Val Loss: 0.03273500129580498\n",
      "Epoch 414, Loss: 0.02901136316359043, Val Loss: 0.032539065927267075\n",
      "Epoch 415, Loss: 0.028805484995245934, Val Loss: 0.032357972115278244\n",
      "Epoch 416, Loss: 0.02859606221318245, Val Loss: 0.03216703608632088\n",
      "Epoch 417, Loss: 0.0283878855407238, Val Loss: 0.031999364495277405\n",
      "Epoch 418, Loss: 0.02818259410560131, Val Loss: 0.031798552721738815\n",
      "Epoch 419, Loss: 0.027977142482995987, Val Loss: 0.03164646029472351\n",
      "Epoch 420, Loss: 0.02777859754860401, Val Loss: 0.03144151344895363\n",
      "Epoch 421, Loss: 0.027586378157138824, Val Loss: 0.0313236266374588\n",
      "Epoch 422, Loss: 0.027397260069847107, Val Loss: 0.031108606606721878\n",
      "Epoch 423, Loss: 0.027222076430916786, Val Loss: 0.03102678433060646\n",
      "Epoch 424, Loss: 0.02703282982110977, Val Loss: 0.030775098130106926\n",
      "Epoch 425, Loss: 0.026850048452615738, Val Loss: 0.030636783689260483\n",
      "Epoch 426, Loss: 0.026610005646944046, Val Loss: 0.030355902388691902\n",
      "Epoch 427, Loss: 0.026375966146588326, Val Loss: 0.03018162213265896\n",
      "Epoch 428, Loss: 0.02617322839796543, Val Loss: 0.030077625066041946\n",
      "Epoch 429, Loss: 0.026002436876296997, Val Loss: 0.029859885573387146\n",
      "Epoch 430, Loss: 0.025829901918768883, Val Loss: 0.029731011018157005\n",
      "Epoch 431, Loss: 0.025617163628339767, Val Loss: 0.029495850205421448\n",
      "Epoch 432, Loss: 0.025406062602996826, Val Loss: 0.029328372329473495\n",
      "Epoch 433, Loss: 0.02521948330104351, Val Loss: 0.029226448386907578\n",
      "Epoch 434, Loss: 0.025046169757843018, Val Loss: 0.02900979295372963\n",
      "Epoch 435, Loss: 0.02486215904355049, Val Loss: 0.02887371927499771\n",
      "Epoch 436, Loss: 0.024658268317580223, Val Loss: 0.02869429998099804\n",
      "Epoch 437, Loss: 0.024466674774885178, Val Loss: 0.02852054126560688\n",
      "Epoch 438, Loss: 0.02429349534213543, Val Loss: 0.028412269428372383\n",
      "Epoch 439, Loss: 0.024112988263368607, Val Loss: 0.028196828439831734\n",
      "Epoch 440, Loss: 0.02392176352441311, Val Loss: 0.028047239407896996\n",
      "Epoch 441, Loss: 0.023732472211122513, Val Loss: 0.02790745534002781\n",
      "Epoch 442, Loss: 0.023555325344204903, Val Loss: 0.027718650177121162\n",
      "Epoch 443, Loss: 0.023379983380436897, Val Loss: 0.02759581059217453\n",
      "Epoch 444, Loss: 0.02319451980292797, Val Loss: 0.027407586574554443\n",
      "Epoch 445, Loss: 0.023009516298770905, Val Loss: 0.02724955603480339\n",
      "Epoch 446, Loss: 0.02283286117017269, Val Loss: 0.027128851041197777\n",
      "Epoch 447, Loss: 0.022660531103610992, Val Loss: 0.026941200718283653\n",
      "Epoch 448, Loss: 0.022484799847006798, Val Loss: 0.026809897273778915\n",
      "Epoch 449, Loss: 0.02230626530945301, Val Loss: 0.026657721027731895\n",
      "Epoch 450, Loss: 0.02213258296251297, Val Loss: 0.026493480429053307\n",
      "Epoch 451, Loss: 0.02196294255554676, Val Loss: 0.026376504451036453\n",
      "Epoch 452, Loss: 0.02179151587188244, Val Loss: 0.026205558329820633\n",
      "Epoch 453, Loss: 0.021618550643324852, Val Loss: 0.02606346271932125\n",
      "Epoch 454, Loss: 0.021448733285069466, Val Loss: 0.025938572362065315\n",
      "Epoch 455, Loss: 0.021282194182276726, Val Loss: 0.02576509863138199\n",
      "Epoch 456, Loss: 0.02111554518342018, Val Loss: 0.025644801557064056\n",
      "Epoch 457, Loss: 0.02094755321741104, Val Loss: 0.025489455088973045\n",
      "Epoch 458, Loss: 0.02078089490532875, Val Loss: 0.02533866837620735\n",
      "Epoch 459, Loss: 0.020617451518774033, Val Loss: 0.025221796706318855\n",
      "Epoch 460, Loss: 0.020455490797758102, Val Loss: 0.025052405893802643\n",
      "Epoch 461, Loss: 0.020293131470680237, Val Loss: 0.024930715560913086\n",
      "Epoch 462, Loss: 0.02013062871992588, Val Loss: 0.024783514440059662\n",
      "Epoch 463, Loss: 0.01996983028948307, Val Loss: 0.02463800646364689\n",
      "Epoch 464, Loss: 0.019811606034636497, Val Loss: 0.024520596489310265\n",
      "Epoch 465, Loss: 0.019653847441077232, Val Loss: 0.024364899843931198\n",
      "Epoch 466, Loss: 0.019496440887451172, Val Loss: 0.0242445170879364\n",
      "Epoch 467, Loss: 0.019340453669428825, Val Loss: 0.024104436859488487\n",
      "Epoch 468, Loss: 0.019185487180948257, Val Loss: 0.023970674723386765\n",
      "Epoch 469, Loss: 0.019031653180718422, Val Loss: 0.023848040029406548\n",
      "Epoch 470, Loss: 0.018879259005188942, Val Loss: 0.02370809018611908\n",
      "Epoch 471, Loss: 0.01872754655778408, Val Loss: 0.02358567714691162\n",
      "Epoch 472, Loss: 0.018576595932245255, Val Loss: 0.02345423959195614\n",
      "Epoch 473, Loss: 0.018427157774567604, Val Loss: 0.0233272947371006\n",
      "Epoch 474, Loss: 0.018278712406754494, Val Loss: 0.02320389822125435\n",
      "Epoch 475, Loss: 0.018130648881196976, Val Loss: 0.023073557764291763\n",
      "Epoch 476, Loss: 0.017983943223953247, Val Loss: 0.022952675819396973\n",
      "Epoch 477, Loss: 0.017838211730122566, Val Loss: 0.02282816357910633\n",
      "Epoch 478, Loss: 0.017692970111966133, Val Loss: 0.02270662784576416\n",
      "Epoch 479, Loss: 0.017548926174640656, Val Loss: 0.022585991770029068\n",
      "Epoch 480, Loss: 0.01740599423646927, Val Loss: 0.022464264184236526\n",
      "Epoch 481, Loss: 0.017264267429709435, Val Loss: 0.022353122010827065\n",
      "Epoch 482, Loss: 0.017123952507972717, Val Loss: 0.02222609519958496\n",
      "Epoch 483, Loss: 0.016984952613711357, Val Loss: 0.02212088368833065\n",
      "Epoch 484, Loss: 0.01684708148241043, Val Loss: 0.02199563942849636\n",
      "Epoch 485, Loss: 0.016710171476006508, Val Loss: 0.021886460483074188\n",
      "Epoch 486, Loss: 0.016574503853917122, Val Loss: 0.021775033324956894\n",
      "Epoch 487, Loss: 0.016439814120531082, Val Loss: 0.021654359996318817\n",
      "Epoch 488, Loss: 0.016306543722748756, Val Loss: 0.021555736660957336\n",
      "Epoch 489, Loss: 0.016174282878637314, Val Loss: 0.02143832854926586\n",
      "Epoch 490, Loss: 0.016042592003941536, Val Loss: 0.021332766860723495\n",
      "Epoch 491, Loss: 0.015911873430013657, Val Loss: 0.021231383085250854\n",
      "Epoch 492, Loss: 0.015782015398144722, Val Loss: 0.02111024409532547\n",
      "Epoch 493, Loss: 0.015653591603040695, Val Loss: 0.0210234634578228\n",
      "Epoch 494, Loss: 0.01552671380341053, Val Loss: 0.0208954568952322\n",
      "Epoch 495, Loss: 0.01540030725300312, Val Loss: 0.02080952562391758\n",
      "Epoch 496, Loss: 0.015274354256689548, Val Loss: 0.02069312520325184\n",
      "Epoch 497, Loss: 0.015148822218179703, Val Loss: 0.020587913691997528\n",
      "Epoch 498, Loss: 0.015022356063127518, Val Loss: 0.020490307360887527\n",
      "Epoch 499, Loss: 0.014892258681356907, Val Loss: 0.020360710099339485\n",
      "Epoch 500, Loss: 0.01475679874420166, Val Loss: 0.02027362398803234\n",
      "Epoch 501, Loss: 0.01462551485747099, Val Loss: 0.02016376331448555\n",
      "Epoch 502, Loss: 0.014503245241940022, Val Loss: 0.020071912556886673\n",
      "Epoch 503, Loss: 0.014384569600224495, Val Loss: 0.019973764196038246\n",
      "Epoch 504, Loss: 0.014267122372984886, Val Loss: 0.01987350545823574\n",
      "Epoch 505, Loss: 0.014150870032608509, Val Loss: 0.01979043520987034\n",
      "Epoch 506, Loss: 0.014035438187420368, Val Loss: 0.019685283303260803\n",
      "Epoch 507, Loss: 0.013920790515840054, Val Loss: 0.01961507648229599\n",
      "Epoch 508, Loss: 0.013806448318064213, Val Loss: 0.01950414478778839\n",
      "Epoch 509, Loss: 0.013692834414541721, Val Loss: 0.0194354560226202\n",
      "Epoch 510, Loss: 0.013579932041466236, Val Loss: 0.01933259330689907\n",
      "Epoch 511, Loss: 0.013467788696289062, Val Loss: 0.019246064126491547\n",
      "Epoch 512, Loss: 0.013357020914554596, Val Loss: 0.01917174831032753\n",
      "Epoch 513, Loss: 0.01324792392551899, Val Loss: 0.01906244084239006\n",
      "Epoch 514, Loss: 0.013140379451215267, Val Loss: 0.019007045775651932\n",
      "Epoch 515, Loss: 0.013033886440098286, Val Loss: 0.018902625888586044\n",
      "Epoch 516, Loss: 0.01292821578681469, Val Loss: 0.01882772333920002\n",
      "Epoch 517, Loss: 0.01282347273081541, Val Loss: 0.018752921372652054\n",
      "Epoch 518, Loss: 0.012720155529677868, Val Loss: 0.018651073798537254\n",
      "Epoch 519, Loss: 0.01261768490076065, Val Loss: 0.018602097406983376\n",
      "Epoch 520, Loss: 0.012516411952674389, Val Loss: 0.018488459289073944\n",
      "Epoch 521, Loss: 0.012415647506713867, Val Loss: 0.01843939907848835\n",
      "Epoch 522, Loss: 0.012315568514168262, Val Loss: 0.018337275832891464\n",
      "Epoch 523, Loss: 0.012216431088745594, Val Loss: 0.018271014094352722\n",
      "Epoch 524, Loss: 0.012118172831833363, Val Loss: 0.018195170909166336\n",
      "Epoch 525, Loss: 0.012021053582429886, Val Loss: 0.018109537661075592\n",
      "Epoch 526, Loss: 0.011924901977181435, Val Loss: 0.01805710792541504\n",
      "Epoch 527, Loss: 0.011829681694507599, Val Loss: 0.017957456409931183\n",
      "Epoch 528, Loss: 0.01173530425876379, Val Loss: 0.01791890524327755\n",
      "Epoch 529, Loss: 0.01164161041378975, Val Loss: 0.01781284809112549\n",
      "Epoch 530, Loss: 0.011548662558197975, Val Loss: 0.017771780490875244\n",
      "Epoch 531, Loss: 0.011456499807536602, Val Loss: 0.017678551375865936\n",
      "Epoch 532, Loss: 0.01136515662074089, Val Loss: 0.017616091296076775\n",
      "Epoch 533, Loss: 0.011274797841906548, Val Loss: 0.01755693182349205\n",
      "Epoch 534, Loss: 0.011185595765709877, Val Loss: 0.017463048920035362\n",
      "Epoch 535, Loss: 0.011097152717411518, Val Loss: 0.017432425171136856\n",
      "Epoch 536, Loss: 0.011009634472429752, Val Loss: 0.01732531562447548\n",
      "Epoch 537, Loss: 0.010922716930508614, Val Loss: 0.017297277227044106\n",
      "Epoch 538, Loss: 0.010836158879101276, Val Loss: 0.017202092334628105\n",
      "Epoch 539, Loss: 0.01075019408017397, Val Loss: 0.017149515450000763\n",
      "Epoch 540, Loss: 0.010665123350918293, Val Loss: 0.01708993874490261\n",
      "Epoch 541, Loss: 0.010581010021269321, Val Loss: 0.017004940658807755\n",
      "Epoch 542, Loss: 0.010497850365936756, Val Loss: 0.016971338540315628\n",
      "Epoch 543, Loss: 0.010414943099021912, Val Loss: 0.016877181828022003\n",
      "Epoch 544, Loss: 0.010332922451198101, Val Loss: 0.016845576465129852\n",
      "Epoch 545, Loss: 0.010251552797853947, Val Loss: 0.01676114648580551\n",
      "Epoch 546, Loss: 0.010170876048505306, Val Loss: 0.01671617664396763\n",
      "Epoch 547, Loss: 0.010091262869536877, Val Loss: 0.016646230593323708\n",
      "Epoch 548, Loss: 0.010012631304562092, Val Loss: 0.016591830179095268\n",
      "Epoch 549, Loss: 0.00993475504219532, Val Loss: 0.016528604552149773\n",
      "Epoch 550, Loss: 0.009857446886599064, Val Loss: 0.016472918912768364\n",
      "Epoch 551, Loss: 0.009780504740774632, Val Loss: 0.016408130526542664\n",
      "Epoch 552, Loss: 0.0097042890265584, Val Loss: 0.016356639564037323\n",
      "Epoch 553, Loss: 0.009629537351429462, Val Loss: 0.016293462365865707\n",
      "Epoch 554, Loss: 0.00955570675432682, Val Loss: 0.01624135859310627\n",
      "Epoch 555, Loss: 0.009482556022703648, Val Loss: 0.01618015766143799\n",
      "Epoch 556, Loss: 0.009409543126821518, Val Loss: 0.016126617789268494\n",
      "Epoch 557, Loss: 0.009337883442640305, Val Loss: 0.01607169583439827\n",
      "Epoch 558, Loss: 0.009267166256904602, Val Loss: 0.016011090949177742\n",
      "Epoch 559, Loss: 0.009197006933391094, Val Loss: 0.015965308994054794\n",
      "Epoch 560, Loss: 0.009127826429903507, Val Loss: 0.01590188778936863\n",
      "Epoch 561, Loss: 0.009059341624379158, Val Loss: 0.01585833914577961\n",
      "Epoch 562, Loss: 0.008991630747914314, Val Loss: 0.01580202206969261\n",
      "Epoch 563, Loss: 0.008924423716962337, Val Loss: 0.015750223770737648\n",
      "Epoch 564, Loss: 0.008857970125973225, Val Loss: 0.015704559162259102\n",
      "Epoch 565, Loss: 0.008792288601398468, Val Loss: 0.015645291656255722\n",
      "Epoch 566, Loss: 0.008727224543690681, Val Loss: 0.015606720931828022\n",
      "Epoch 567, Loss: 0.008662659674882889, Val Loss: 0.015545252710580826\n",
      "Epoch 568, Loss: 0.008598771877586842, Val Loss: 0.015505007468163967\n",
      "Epoch 569, Loss: 0.008535596542060375, Val Loss: 0.015452365390956402\n",
      "Epoch 570, Loss: 0.008472810499370098, Val Loss: 0.015404640696942806\n",
      "Epoch 571, Loss: 0.008410492911934853, Val Loss: 0.015369132161140442\n",
      "Epoch 572, Loss: 0.008348925039172173, Val Loss: 0.015311121940612793\n",
      "Epoch 573, Loss: 0.008287815377116203, Val Loss: 0.015290267765522003\n",
      "Epoch 574, Loss: 0.008226840756833553, Val Loss: 0.015220707282423973\n",
      "Epoch 575, Loss: 0.008166346698999405, Val Loss: 0.015201142057776451\n",
      "Epoch 576, Loss: 0.008106355555355549, Val Loss: 0.015141927637159824\n",
      "Epoch 577, Loss: 0.00804701168090105, Val Loss: 0.015102236531674862\n",
      "Epoch 578, Loss: 0.007988549768924713, Val Loss: 0.015073731541633606\n",
      "Epoch 579, Loss: 0.007930712774395943, Val Loss: 0.015016231685876846\n",
      "Epoch 580, Loss: 0.007873826660215855, Val Loss: 0.01500521320849657\n",
      "Epoch 581, Loss: 0.00781717337667942, Val Loss: 0.014943337999284267\n",
      "Epoch 582, Loss: 0.007761168293654919, Val Loss: 0.014932245947420597\n",
      "Epoch 583, Loss: 0.007705611642450094, Val Loss: 0.014878218993544579\n",
      "Epoch 584, Loss: 0.007650628220289946, Val Loss: 0.014850074425339699\n",
      "Epoch 585, Loss: 0.007596271578222513, Val Loss: 0.01481164526194334\n",
      "Epoch 586, Loss: 0.007542372215539217, Val Loss: 0.01476920023560524\n",
      "Epoch 587, Loss: 0.00748909916728735, Val Loss: 0.014738869853317738\n",
      "Epoch 588, Loss: 0.007436169311404228, Val Loss: 0.014698321931064129\n",
      "Epoch 589, Loss: 0.0073839714750647545, Val Loss: 0.014669857919216156\n",
      "Epoch 590, Loss: 0.0073321182280778885, Val Loss: 0.014632047154009342\n",
      "Epoch 591, Loss: 0.007280787453055382, Val Loss: 0.014604034833610058\n",
      "Epoch 592, Loss: 0.007229917217046022, Val Loss: 0.014567992649972439\n",
      "Epoch 593, Loss: 0.007179633714258671, Val Loss: 0.014542276971042156\n",
      "Epoch 594, Loss: 0.00712980842217803, Val Loss: 0.014501912519335747\n",
      "Epoch 595, Loss: 0.007080621086061001, Val Loss: 0.014482475817203522\n",
      "Epoch 596, Loss: 0.007031927816569805, Val Loss: 0.014437590725719929\n",
      "Epoch 597, Loss: 0.0069836825132369995, Val Loss: 0.014427553862333298\n",
      "Epoch 598, Loss: 0.006936087738722563, Val Loss: 0.014372064732015133\n",
      "Epoch 599, Loss: 0.006889043841511011, Val Loss: 0.014364836737513542\n",
      "Epoch 600, Loss: 0.0068422784097492695, Val Loss: 0.014321175403892994\n",
      "Epoch 601, Loss: 0.0067961253225803375, Val Loss: 0.01429455541074276\n",
      "Epoch 602, Loss: 0.006750321481376886, Val Loss: 0.014278058893978596\n",
      "Epoch 603, Loss: 0.00670518446713686, Val Loss: 0.01423047948628664\n",
      "Epoch 604, Loss: 0.006660480983555317, Val Loss: 0.014228754676878452\n",
      "Epoch 605, Loss: 0.006615994963794947, Val Loss: 0.014174827374517918\n",
      "Epoch 606, Loss: 0.006572123616933823, Val Loss: 0.01416861079633236\n",
      "Epoch 607, Loss: 0.006528426427394152, Val Loss: 0.01412966288626194\n",
      "Epoch 608, Loss: 0.006485377438366413, Val Loss: 0.014106771908700466\n",
      "Epoch 609, Loss: 0.0064428988844156265, Val Loss: 0.014087491668760777\n",
      "Epoch 610, Loss: 0.00640101358294487, Val Loss: 0.014050092548131943\n",
      "Epoch 611, Loss: 0.006359616760164499, Val Loss: 0.014039208181202412\n",
      "Epoch 612, Loss: 0.006318552419543266, Val Loss: 0.014004752039909363\n",
      "Epoch 613, Loss: 0.006277995649725199, Val Loss: 0.013981735333800316\n",
      "Epoch 614, Loss: 0.006237791385501623, Val Loss: 0.013963344506919384\n",
      "Epoch 615, Loss: 0.006198160815984011, Val Loss: 0.01393025740981102\n",
      "Epoch 616, Loss: 0.006158830597996712, Val Loss: 0.013912595808506012\n",
      "Epoch 617, Loss: 0.0061200461350381374, Val Loss: 0.013884921558201313\n",
      "Epoch 618, Loss: 0.006081684026867151, Val Loss: 0.013858025893568993\n",
      "Epoch 619, Loss: 0.006043542176485062, Val Loss: 0.013834219425916672\n",
      "Epoch 620, Loss: 0.006005886942148209, Val Loss: 0.013811263255774975\n",
      "Epoch 621, Loss: 0.005968539509922266, Val Loss: 0.01378237921744585\n",
      "Epoch 622, Loss: 0.00593147799372673, Val Loss: 0.013762091286480427\n",
      "Epoch 623, Loss: 0.005894968286156654, Val Loss: 0.013742277398705482\n",
      "Epoch 624, Loss: 0.005858621560037136, Val Loss: 0.01371710654348135\n",
      "Epoch 625, Loss: 0.005822543054819107, Val Loss: 0.013699766248464584\n",
      "Epoch 626, Loss: 0.005786361638456583, Val Loss: 0.013675559312105179\n",
      "Epoch 627, Loss: 0.0057505033910274506, Val Loss: 0.013656631112098694\n",
      "Epoch 628, Loss: 0.005715100094676018, Val Loss: 0.013638234697282314\n",
      "Epoch 629, Loss: 0.005680416245013475, Val Loss: 0.013611934147775173\n",
      "Epoch 630, Loss: 0.005646138451993465, Val Loss: 0.013593236915767193\n",
      "Epoch 631, Loss: 0.0056121861562132835, Val Loss: 0.01357000321149826\n",
      "Epoch 632, Loss: 0.005578570067882538, Val Loss: 0.013548466376960278\n",
      "Epoch 633, Loss: 0.005545205902308226, Val Loss: 0.013532564975321293\n",
      "Epoch 634, Loss: 0.0055121611803770065, Val Loss: 0.013507451862096786\n",
      "Epoch 635, Loss: 0.005479306913912296, Val Loss: 0.013492758385837078\n",
      "Epoch 636, Loss: 0.00544667849317193, Val Loss: 0.013477015309035778\n",
      "Epoch 637, Loss: 0.005414474289864302, Val Loss: 0.013444649986922741\n",
      "Epoch 638, Loss: 0.00538242794573307, Val Loss: 0.013437510468065739\n",
      "Epoch 639, Loss: 0.005350541323423386, Val Loss: 0.013412542641162872\n",
      "Epoch 640, Loss: 0.005318820476531982, Val Loss: 0.013391554355621338\n",
      "Epoch 641, Loss: 0.0052875252440571785, Val Loss: 0.013379445299506187\n",
      "Epoch 642, Loss: 0.005256465170532465, Val Loss: 0.013358085416257381\n",
      "Epoch 643, Loss: 0.005225574132055044, Val Loss: 0.013336150906980038\n",
      "Epoch 644, Loss: 0.005195102654397488, Val Loss: 0.013326169922947884\n",
      "Epoch 645, Loss: 0.005164795089513063, Val Loss: 0.01330721378326416\n",
      "Epoch 646, Loss: 0.00513480557128787, Val Loss: 0.013281655497848988\n",
      "Epoch 647, Loss: 0.00510529987514019, Val Loss: 0.013270726427435875\n",
      "Epoch 648, Loss: 0.005076061934232712, Val Loss: 0.013247177936136723\n",
      "Epoch 649, Loss: 0.005046942736953497, Val Loss: 0.013233410194516182\n",
      "Epoch 650, Loss: 0.005018163938075304, Val Loss: 0.013218271546065807\n"
     ]
    }
   ],
   "source": [
    "from flashrag.prompt import PromptTemplate\n",
    "import pickle\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "config_taqa = set_config(dataset_name='taqa', split = split)\n",
    "template_taqa= PromptTemplate(\n",
    "        config = config_taqa,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "config_triviaqa= set_config(dataset_name='triviaqa', split = split)\n",
    "template_triviaqa= PromptTemplate(\n",
    "        config = config_triviaqa,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "\n",
    "\n",
    "loading_hidden = True\n",
    "if not loading_hidden:\n",
    "    taqa_hidden_states, triviaqa_hidden_states = load_datasets(config_taqa, config_triviaqa, template_taqa, template_triviaqa)\n",
    "else:\n",
    "    file_path_triviaqa = '/cs/student/projects2/dsml/cdiezmar/hidden_states/triviaqa.pkl'\n",
    "    file_path_taqa = '/cs/student/projects2/dsml/cdiezmar/hidden_states/taqa.pkl'\n",
    "\n",
    "    with open(file_path_triviaqa, 'rb') as file_triviaqa:\n",
    "        triviaqa_hidden_states = pickle.load(file_triviaqa)\n",
    "        print('Length triviaqa: ', len(triviaqa_hidden_states))\n",
    "        triviaqa_hidden_states_tensor= torch.tensor(triviaqa_hidden_states, dtype=torch.float32)\n",
    "    with open(file_path_taqa, 'rb') as file_taqa:\n",
    "        taqa_hidden_states = pickle.load(file_taqa)\n",
    "        print('Length taqa: ', len(taqa_hidden_states))\n",
    "        taqa_hidden_states_tensor= torch.tensor(taqa_hidden_states, dtype=torch.float32)\n",
    "train_inputs, train_labels, val_inputs, val_labels = prepare_data(taqa_hidden_states_tensor, triviaqa_hidden_states_tensor)\n",
    "\n",
    "input_size = train_inputs.size(1)\n",
    "num_classes = 2  # Time-sensitive or not\n",
    "\n",
    "# LSTM Parameters\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "epochs = 650\n",
    "lr = 1e-4\n",
    "\n",
    "# Train LSTM Classifier\n",
    "lstm_model = train_lstm_classifier_cuda(train_inputs, train_labels, val_inputs, val_labels, input_size, hidden_size, num_layers, num_classes, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `question` in template\n",
      "Find `question` in template\n",
      "Length triviaqa:  78785\n",
      "Length taqa:  10148\n",
      "Epoch 1, Loss: 0.695809543132782, Val Loss: 0.6892292499542236\n",
      "Epoch 2, Loss: 0.6893008351325989, Val Loss: 0.6828580498695374\n",
      "Epoch 3, Loss: 0.6830213069915771, Val Loss: 0.6768442988395691\n",
      "Epoch 4, Loss: 0.6770941019058228, Val Loss: 0.671237051486969\n",
      "Epoch 5, Loss: 0.6715642809867859, Val Loss: 0.666034460067749\n",
      "Epoch 6, Loss: 0.6664344668388367, Val Loss: 0.6612151861190796\n",
      "Epoch 7, Loss: 0.6616871356964111, Val Loss: 0.656724750995636\n",
      "Epoch 8, Loss: 0.6572630405426025, Val Loss: 0.6524710655212402\n",
      "Epoch 9, Loss: 0.6530705094337463, Val Loss: 0.6483303308486938\n",
      "Epoch 10, Loss: 0.648987889289856, Val Loss: 0.6442299485206604\n",
      "Epoch 11, Loss: 0.6449424028396606, Val Loss: 0.6401236057281494\n",
      "Epoch 12, Loss: 0.6408910155296326, Val Loss: 0.6359972357749939\n",
      "Epoch 13, Loss: 0.6368218064308167, Val Loss: 0.6318440437316895\n",
      "Epoch 14, Loss: 0.6327299475669861, Val Loss: 0.6276520490646362\n",
      "Epoch 15, Loss: 0.6286022067070007, Val Loss: 0.6234153509140015\n",
      "Epoch 16, Loss: 0.6244322061538696, Val Loss: 0.6191173195838928\n",
      "Epoch 17, Loss: 0.6202028393745422, Val Loss: 0.614745557308197\n",
      "Epoch 18, Loss: 0.6159014105796814, Val Loss: 0.6102683544158936\n",
      "Epoch 19, Loss: 0.611495852470398, Val Loss: 0.6056814789772034\n",
      "Epoch 20, Loss: 0.6069816946983337, Val Loss: 0.6009531021118164\n",
      "Epoch 21, Loss: 0.6023278832435608, Val Loss: 0.5960816740989685\n",
      "Epoch 22, Loss: 0.5975343585014343, Val Loss: 0.5910690426826477\n",
      "Epoch 23, Loss: 0.5926023721694946, Val Loss: 0.585936963558197\n",
      "Epoch 24, Loss: 0.5875532627105713, Val Loss: 0.5806893110275269\n",
      "Epoch 25, Loss: 0.5823899507522583, Val Loss: 0.5753297209739685\n",
      "Epoch 26, Loss: 0.5771158933639526, Val Loss: 0.5698543190956116\n",
      "Epoch 27, Loss: 0.5717270374298096, Val Loss: 0.5642427206039429\n",
      "Epoch 28, Loss: 0.5662050843238831, Val Loss: 0.5584778189659119\n",
      "Epoch 29, Loss: 0.5605347156524658, Val Loss: 0.5525404214859009\n",
      "Epoch 30, Loss: 0.5546982884407043, Val Loss: 0.5464447140693665\n",
      "Epoch 31, Loss: 0.5487093329429626, Val Loss: 0.540188193321228\n",
      "Epoch 32, Loss: 0.5425642728805542, Val Loss: 0.533779501914978\n",
      "Epoch 33, Loss: 0.5362716317176819, Val Loss: 0.5272025465965271\n",
      "Epoch 34, Loss: 0.5298140645027161, Val Loss: 0.5204476714134216\n",
      "Epoch 35, Loss: 0.5231825113296509, Val Loss: 0.5134996771812439\n",
      "Epoch 36, Loss: 0.5163610577583313, Val Loss: 0.5063655376434326\n",
      "Epoch 37, Loss: 0.5093587040901184, Val Loss: 0.4990691840648651\n",
      "Epoch 38, Loss: 0.5022013783454895, Val Loss: 0.4916360378265381\n",
      "Epoch 39, Loss: 0.4949145019054413, Val Loss: 0.48410889506340027\n",
      "Epoch 40, Loss: 0.4875379502773285, Val Loss: 0.4765411913394928\n",
      "Epoch 41, Loss: 0.48012077808380127, Val Loss: 0.4689694941043854\n",
      "Epoch 42, Loss: 0.47270023822784424, Val Loss: 0.46145787835121155\n",
      "Epoch 43, Loss: 0.4653417766094208, Val Loss: 0.45403337478637695\n",
      "Epoch 44, Loss: 0.45807158946990967, Val Loss: 0.44663143157958984\n",
      "Epoch 45, Loss: 0.4508276879787445, Val Loss: 0.4392423927783966\n",
      "Epoch 46, Loss: 0.44360291957855225, Val Loss: 0.43191033601760864\n",
      "Epoch 47, Loss: 0.43644285202026367, Val Loss: 0.4246804416179657\n",
      "Epoch 48, Loss: 0.42938950657844543, Val Loss: 0.4175639748573303\n",
      "Epoch 49, Loss: 0.42245346307754517, Val Loss: 0.4105457365512848\n",
      "Epoch 50, Loss: 0.415617436170578, Val Loss: 0.40364205837249756\n",
      "Epoch 51, Loss: 0.4088975191116333, Val Loss: 0.39689046144485474\n",
      "Epoch 52, Loss: 0.4023248553276062, Val Loss: 0.39032483100891113\n",
      "Epoch 53, Loss: 0.3959265947341919, Val Loss: 0.3839552402496338\n",
      "Epoch 54, Loss: 0.38972240686416626, Val Loss: 0.37780097126960754\n",
      "Epoch 55, Loss: 0.383741557598114, Val Loss: 0.3718821108341217\n",
      "Epoch 56, Loss: 0.37800365686416626, Val Loss: 0.3661888837814331\n",
      "Epoch 57, Loss: 0.3725004494190216, Val Loss: 0.3607475757598877\n",
      "Epoch 58, Loss: 0.3672557771205902, Val Loss: 0.3556034564971924\n",
      "Epoch 59, Loss: 0.36231091618537903, Val Loss: 0.3507825434207916\n",
      "Epoch 60, Loss: 0.35768768191337585, Val Loss: 0.3462608754634857\n",
      "Epoch 61, Loss: 0.35335659980773926, Val Loss: 0.3420204520225525\n",
      "Epoch 62, Loss: 0.3492971956729889, Val Loss: 0.3380165696144104\n",
      "Epoch 63, Loss: 0.3454649746417999, Val Loss: 0.334237277507782\n",
      "Epoch 64, Loss: 0.3418462872505188, Val Loss: 0.33068519830703735\n",
      "Epoch 65, Loss: 0.3384403884410858, Val Loss: 0.32739588618278503\n",
      "Epoch 66, Loss: 0.33528217673301697, Val Loss: 0.3243451714515686\n",
      "Epoch 67, Loss: 0.33235231041908264, Val Loss: 0.3214503228664398\n",
      "Epoch 68, Loss: 0.32956427335739136, Val Loss: 0.3187495768070221\n",
      "Epoch 69, Loss: 0.32694941759109497, Val Loss: 0.3163643181324005\n",
      "Epoch 70, Loss: 0.3246627151966095, Val Loss: 0.3142330050468445\n",
      "Epoch 71, Loss: 0.32263848185539246, Val Loss: 0.31212401390075684\n",
      "Epoch 72, Loss: 0.32063600420951843, Val Loss: 0.3100030720233917\n",
      "Epoch 73, Loss: 0.3186156451702118, Val Loss: 0.30792534351348877\n",
      "Epoch 74, Loss: 0.3166349232196808, Val Loss: 0.3059360384941101\n",
      "Epoch 75, Loss: 0.31474485993385315, Val Loss: 0.30406081676483154\n",
      "Epoch 76, Loss: 0.3129546344280243, Val Loss: 0.3021738827228546\n",
      "Epoch 77, Loss: 0.311117023229599, Val Loss: 0.3000952899456024\n",
      "Epoch 78, Loss: 0.3090326488018036, Val Loss: 0.29773077368736267\n",
      "Epoch 79, Loss: 0.306615948677063, Val Loss: 0.29533496499061584\n",
      "Epoch 80, Loss: 0.30416786670684814, Val Loss: 0.2932807505130768\n",
      "Epoch 81, Loss: 0.3020889163017273, Val Loss: 0.2914644479751587\n",
      "Epoch 82, Loss: 0.3002667725086212, Val Loss: 0.2896307110786438\n",
      "Epoch 83, Loss: 0.29842492938041687, Val Loss: 0.2876756489276886\n",
      "Epoch 84, Loss: 0.2964573800563812, Val Loss: 0.28553828597068787\n",
      "Epoch 85, Loss: 0.2942989766597748, Val Loss: 0.28311291337013245\n",
      "Epoch 86, Loss: 0.29184165596961975, Val Loss: 0.2803395986557007\n",
      "Epoch 87, Loss: 0.2889782786369324, Val Loss: 0.27711281180381775\n",
      "Epoch 88, Loss: 0.2856043875217438, Val Loss: 0.2734043300151825\n",
      "Epoch 89, Loss: 0.2816799283027649, Val Loss: 0.26953402161598206\n",
      "Epoch 90, Loss: 0.2775910198688507, Val Loss: 0.2663048505783081\n",
      "Epoch 91, Loss: 0.27416783571243286, Val Loss: 0.26374050974845886\n",
      "Epoch 92, Loss: 0.27147430181503296, Val Loss: 0.26118630170822144\n",
      "Epoch 93, Loss: 0.2688036262989044, Val Loss: 0.25839120149612427\n",
      "Epoch 94, Loss: 0.2658752501010895, Val Loss: 0.2553698718547821\n",
      "Epoch 95, Loss: 0.2626955211162567, Val Loss: 0.252179354429245\n",
      "Epoch 96, Loss: 0.2593294680118561, Val Loss: 0.2489374577999115\n",
      "Epoch 97, Loss: 0.2559128999710083, Val Loss: 0.24572600424289703\n",
      "Epoch 98, Loss: 0.25253036618232727, Val Loss: 0.24242517352104187\n",
      "Epoch 99, Loss: 0.2490539848804474, Val Loss: 0.23903028666973114\n",
      "Epoch 100, Loss: 0.24547645449638367, Val Loss: 0.23558363318443298\n",
      "Epoch 101, Loss: 0.24185000360012054, Val Loss: 0.2321414351463318\n",
      "Epoch 102, Loss: 0.23822297155857086, Val Loss: 0.22871626913547516\n",
      "Epoch 103, Loss: 0.23461252450942993, Val Loss: 0.22522930800914764\n",
      "Epoch 104, Loss: 0.23094436526298523, Val Loss: 0.2215949445962906\n",
      "Epoch 105, Loss: 0.22712525725364685, Val Loss: 0.21771319210529327\n",
      "Epoch 106, Loss: 0.2230522334575653, Val Loss: 0.213593989610672\n",
      "Epoch 107, Loss: 0.21873170137405396, Val Loss: 0.2091902792453766\n",
      "Epoch 108, Loss: 0.21411624550819397, Val Loss: 0.20454156398773193\n",
      "Epoch 109, Loss: 0.20925500988960266, Val Loss: 0.1998058706521988\n",
      "Epoch 110, Loss: 0.204340860247612, Val Loss: 0.19530020654201508\n",
      "Epoch 111, Loss: 0.19970223307609558, Val Loss: 0.1916743814945221\n",
      "Epoch 112, Loss: 0.19597387313842773, Val Loss: 0.18795061111450195\n",
      "Epoch 113, Loss: 0.19210085272789001, Val Loss: 0.18370486795902252\n",
      "Epoch 114, Loss: 0.18761582672595978, Val Loss: 0.17921000719070435\n",
      "Epoch 115, Loss: 0.18288175761699677, Val Loss: 0.1748407632112503\n",
      "Epoch 116, Loss: 0.1782418191432953, Val Loss: 0.17051725089550018\n",
      "Epoch 117, Loss: 0.17365048825740814, Val Loss: 0.16613470017910004\n",
      "Epoch 118, Loss: 0.16902215778827667, Val Loss: 0.16181538999080658\n",
      "Epoch 119, Loss: 0.16445058584213257, Val Loss: 0.15768784284591675\n",
      "Epoch 120, Loss: 0.16013383865356445, Val Loss: 0.1538154035806656\n",
      "Epoch 121, Loss: 0.15608997642993927, Val Loss: 0.15004722774028778\n",
      "Epoch 122, Loss: 0.15218214690685272, Val Loss: 0.14626076817512512\n",
      "Epoch 123, Loss: 0.14825929701328278, Val Loss: 0.1423850953578949\n",
      "Epoch 124, Loss: 0.14425089955329895, Val Loss: 0.13848842680454254\n",
      "Epoch 125, Loss: 0.14020729064941406, Val Loss: 0.13463237881660461\n",
      "Epoch 126, Loss: 0.13621944189071655, Val Loss: 0.13094079494476318\n",
      "Epoch 127, Loss: 0.1323702037334442, Val Loss: 0.12738513946533203\n",
      "Epoch 128, Loss: 0.12868139147758484, Val Loss: 0.12395883351564407\n",
      "Epoch 129, Loss: 0.12509457767009735, Val Loss: 0.1205005869269371\n",
      "Epoch 130, Loss: 0.12150870263576508, Val Loss: 0.11715317517518997\n",
      "Epoch 131, Loss: 0.11802055686712265, Val Loss: 0.11394301801919937\n",
      "Epoch 132, Loss: 0.11470130831003189, Val Loss: 0.11084641516208649\n",
      "Epoch 133, Loss: 0.1114756241440773, Val Loss: 0.1077505350112915\n",
      "Epoch 134, Loss: 0.10831169784069061, Val Loss: 0.10475906729698181\n",
      "Epoch 135, Loss: 0.10520729422569275, Val Loss: 0.10179830342531204\n",
      "Epoch 136, Loss: 0.10217868536710739, Val Loss: 0.09901412576436996\n",
      "Epoch 137, Loss: 0.09927135705947876, Val Loss: 0.09624940901994705\n",
      "Epoch 138, Loss: 0.09649834781885147, Val Loss: 0.09372233599424362\n",
      "Epoch 139, Loss: 0.0938621535897255, Val Loss: 0.09119974076747894\n",
      "Epoch 140, Loss: 0.09133791923522949, Val Loss: 0.08888603001832962\n",
      "Epoch 141, Loss: 0.08890735357999802, Val Loss: 0.08656594902276993\n",
      "Epoch 142, Loss: 0.0865522027015686, Val Loss: 0.08437713980674744\n",
      "Epoch 143, Loss: 0.08428788930177689, Val Loss: 0.08230005204677582\n",
      "Epoch 144, Loss: 0.08210662007331848, Val Loss: 0.08025165647268295\n",
      "Epoch 145, Loss: 0.08001759648323059, Val Loss: 0.07836789637804031\n",
      "Epoch 146, Loss: 0.07801354676485062, Val Loss: 0.07645273208618164\n",
      "Epoch 147, Loss: 0.0760878324508667, Val Loss: 0.07469525933265686\n",
      "Epoch 148, Loss: 0.07423289865255356, Val Loss: 0.0729549303650856\n",
      "Epoch 149, Loss: 0.07245523482561111, Val Loss: 0.0713147222995758\n",
      "Epoch 150, Loss: 0.07075634598731995, Val Loss: 0.06974595785140991\n",
      "Epoch 151, Loss: 0.06911560148000717, Val Loss: 0.06816835701465607\n",
      "Epoch 152, Loss: 0.06752390414476395, Val Loss: 0.06671994924545288\n",
      "Epoch 153, Loss: 0.06597527861595154, Val Loss: 0.06519465893507004\n",
      "Epoch 154, Loss: 0.06446896493434906, Val Loss: 0.06383167207241058\n",
      "Epoch 155, Loss: 0.06302312761545181, Val Loss: 0.06246381253004074\n",
      "Epoch 156, Loss: 0.06166183203458786, Val Loss: 0.06116943061351776\n",
      "Epoch 157, Loss: 0.06032220646739006, Val Loss: 0.059887517243623734\n",
      "Epoch 158, Loss: 0.05900200083851814, Val Loss: 0.05862093344330788\n",
      "Epoch 159, Loss: 0.057742368429899216, Val Loss: 0.0575125627219677\n",
      "Epoch 160, Loss: 0.05654790252447128, Val Loss: 0.05636592209339142\n",
      "Epoch 161, Loss: 0.0554182268679142, Val Loss: 0.05542455241084099\n",
      "Epoch 162, Loss: 0.054340749979019165, Val Loss: 0.05430242791771889\n",
      "Epoch 163, Loss: 0.05329420417547226, Val Loss: 0.0534721277654171\n",
      "Epoch 164, Loss: 0.05227752402424812, Val Loss: 0.05243217200040817\n",
      "Epoch 165, Loss: 0.05129794403910637, Val Loss: 0.05159838870167732\n",
      "Epoch 166, Loss: 0.05035277456045151, Val Loss: 0.05074666813015938\n",
      "Epoch 167, Loss: 0.049450140446424484, Val Loss: 0.0498623251914978\n",
      "Epoch 168, Loss: 0.048593949526548386, Val Loss: 0.049175869673490524\n",
      "Epoch 169, Loss: 0.047766294330358505, Val Loss: 0.048304054886102676\n",
      "Epoch 170, Loss: 0.046949602663517, Val Loss: 0.04759705439209938\n",
      "Epoch 171, Loss: 0.04615272581577301, Val Loss: 0.04688555374741554\n",
      "Epoch 172, Loss: 0.04539426788687706, Val Loss: 0.04613108187913895\n",
      "Epoch 173, Loss: 0.04466577619314194, Val Loss: 0.04553118348121643\n",
      "Epoch 174, Loss: 0.04395008087158203, Val Loss: 0.04480172321200371\n",
      "Epoch 175, Loss: 0.04324622079730034, Val Loss: 0.04418574646115303\n",
      "Epoch 176, Loss: 0.042566049844026566, Val Loss: 0.0435827411711216\n",
      "Epoch 177, Loss: 0.0419093556702137, Val Loss: 0.04292677342891693\n",
      "Epoch 178, Loss: 0.04126949608325958, Val Loss: 0.04240729659795761\n",
      "Epoch 179, Loss: 0.04064224660396576, Val Loss: 0.04177672043442726\n",
      "Epoch 180, Loss: 0.0400288850069046, Val Loss: 0.041238781064748764\n",
      "Epoch 181, Loss: 0.039432454854249954, Val Loss: 0.0407106839120388\n",
      "Epoch 182, Loss: 0.0388534814119339, Val Loss: 0.040129248052835464\n",
      "Epoch 183, Loss: 0.038293033838272095, Val Loss: 0.0396762490272522\n",
      "Epoch 184, Loss: 0.03774179518222809, Val Loss: 0.03910991922020912\n",
      "Epoch 185, Loss: 0.037199314683675766, Val Loss: 0.038642749190330505\n",
      "Epoch 186, Loss: 0.03666996955871582, Val Loss: 0.038171518594026566\n",
      "Epoch 187, Loss: 0.03615894541144371, Val Loss: 0.03767302632331848\n",
      "Epoch 188, Loss: 0.035662028938531876, Val Loss: 0.03726370632648468\n",
      "Epoch 189, Loss: 0.0351744219660759, Val Loss: 0.03677124157547951\n",
      "Epoch 190, Loss: 0.034697018563747406, Val Loss: 0.03637290373444557\n",
      "Epoch 191, Loss: 0.0342288613319397, Val Loss: 0.03593187779188156\n",
      "Epoch 192, Loss: 0.0337708555161953, Val Loss: 0.035529401153326035\n",
      "Epoch 193, Loss: 0.03332381322979927, Val Loss: 0.03512785956263542\n",
      "Epoch 194, Loss: 0.03288772329688072, Val Loss: 0.03472643718123436\n",
      "Epoch 195, Loss: 0.03246060386300087, Val Loss: 0.03434445708990097\n",
      "Epoch 196, Loss: 0.03204263001680374, Val Loss: 0.033960942178964615\n",
      "Epoch 197, Loss: 0.031631771475076675, Val Loss: 0.03360333293676376\n",
      "Epoch 198, Loss: 0.0312302615493536, Val Loss: 0.0332355834543705\n",
      "Epoch 199, Loss: 0.03083662874996662, Val Loss: 0.032889269292354584\n",
      "Epoch 200, Loss: 0.030450161546468735, Val Loss: 0.032533302903175354\n",
      "Epoch 201, Loss: 0.030070506036281586, Val Loss: 0.03220205008983612\n",
      "Epoch 202, Loss: 0.029698612168431282, Val Loss: 0.03186775743961334\n",
      "Epoch 203, Loss: 0.029331760480999947, Val Loss: 0.03155731037259102\n",
      "Epoch 204, Loss: 0.028973091393709183, Val Loss: 0.031225688755512238\n",
      "Epoch 205, Loss: 0.028620155528187752, Val Loss: 0.03093855455517769\n",
      "Epoch 206, Loss: 0.028275221586227417, Val Loss: 0.030599143356084824\n",
      "Epoch 207, Loss: 0.02793942019343376, Val Loss: 0.03038139082491398\n",
      "Epoch 208, Loss: 0.02762044034898281, Val Loss: 0.030025165528059006\n",
      "Epoch 209, Loss: 0.027329763397574425, Val Loss: 0.029939427971839905\n",
      "Epoch 210, Loss: 0.02706538699567318, Val Loss: 0.02948915958404541\n",
      "Epoch 211, Loss: 0.026742832735180855, Val Loss: 0.029236121103167534\n",
      "Epoch 212, Loss: 0.026344895362854004, Val Loss: 0.028951074928045273\n",
      "Epoch 213, Loss: 0.026034744456410408, Val Loss: 0.028668582439422607\n",
      "Epoch 214, Loss: 0.025804201140999794, Val Loss: 0.028489185497164726\n",
      "Epoch 215, Loss: 0.025483854115009308, Val Loss: 0.028151441365480423\n",
      "Epoch 216, Loss: 0.025150639936327934, Val Loss: 0.02790692448616028\n",
      "Epoch 217, Loss: 0.02491699531674385, Val Loss: 0.02774069830775261\n",
      "Epoch 218, Loss: 0.024639014154672623, Val Loss: 0.027415387332439423\n",
      "Epoch 219, Loss: 0.024323197081685066, Val Loss: 0.027181195095181465\n",
      "Epoch 220, Loss: 0.02408679388463497, Val Loss: 0.027012797072529793\n",
      "Epoch 221, Loss: 0.02382739633321762, Val Loss: 0.026715612038969994\n",
      "Epoch 222, Loss: 0.02353300154209137, Val Loss: 0.026489952579140663\n",
      "Epoch 223, Loss: 0.02329520508646965, Val Loss: 0.026313336566090584\n",
      "Epoch 224, Loss: 0.02304559387266636, Val Loss: 0.02604886330664158\n",
      "Epoch 225, Loss: 0.022773506119847298, Val Loss: 0.025834010913968086\n",
      "Epoch 226, Loss: 0.022545674815773964, Val Loss: 0.02565411478281021\n",
      "Epoch 227, Loss: 0.02230561338365078, Val Loss: 0.02540012076497078\n",
      "Epoch 228, Loss: 0.022048979997634888, Val Loss: 0.02519373781979084\n",
      "Epoch 229, Loss: 0.021823463961482048, Val Loss: 0.025018630549311638\n",
      "Epoch 230, Loss: 0.02159767411649227, Val Loss: 0.024786680936813354\n",
      "Epoch 231, Loss: 0.021353455260396004, Val Loss: 0.024590441957116127\n",
      "Epoch 232, Loss: 0.021131182089447975, Val Loss: 0.02441701851785183\n",
      "Epoch 233, Loss: 0.020918818190693855, Val Loss: 0.02419617399573326\n",
      "Epoch 234, Loss: 0.020688233897089958, Val Loss: 0.02399972453713417\n",
      "Epoch 235, Loss: 0.02046973817050457, Val Loss: 0.023825939744710922\n",
      "Epoch 236, Loss: 0.020265383645892143, Val Loss: 0.023625239729881287\n",
      "Epoch 237, Loss: 0.020049748942255974, Val Loss: 0.023441320285201073\n",
      "Epoch 238, Loss: 0.019836878404021263, Val Loss: 0.023272020742297173\n",
      "Epoch 239, Loss: 0.019637925550341606, Val Loss: 0.023088274523615837\n",
      "Epoch 240, Loss: 0.019435448572039604, Val Loss: 0.022908244282007217\n",
      "Epoch 241, Loss: 0.019230343401432037, Val Loss: 0.022735845297574997\n",
      "Epoch 242, Loss: 0.019035527482628822, Val Loss: 0.02256595343351364\n",
      "Epoch 243, Loss: 0.01884426921606064, Val Loss: 0.02239341102540493\n",
      "Epoch 244, Loss: 0.018648002296686172, Val Loss: 0.02222680300474167\n",
      "Epoch 245, Loss: 0.01845649816095829, Val Loss: 0.02206755056977272\n",
      "Epoch 246, Loss: 0.018272340297698975, Val Loss: 0.021905330941081047\n",
      "Epoch 247, Loss: 0.018087882548570633, Val Loss: 0.02174333855509758\n",
      "Epoch 248, Loss: 0.01790100894868374, Val Loss: 0.02158745564520359\n",
      "Epoch 249, Loss: 0.01772022247314453, Val Loss: 0.021431654691696167\n",
      "Epoch 250, Loss: 0.017543572932481766, Val Loss: 0.02128196321427822\n",
      "Epoch 251, Loss: 0.01736694574356079, Val Loss: 0.021127397194504738\n",
      "Epoch 252, Loss: 0.017191285267472267, Val Loss: 0.02097824215888977\n",
      "Epoch 253, Loss: 0.017018869519233704, Val Loss: 0.0208352692425251\n",
      "Epoch 254, Loss: 0.016850614920258522, Val Loss: 0.020686578005552292\n",
      "Epoch 255, Loss: 0.01668456755578518, Val Loss: 0.02054830826818943\n",
      "Epoch 256, Loss: 0.01651841402053833, Val Loss: 0.020399708300828934\n",
      "Epoch 257, Loss: 0.01635296829044819, Val Loss: 0.020259741693735123\n",
      "Epoch 258, Loss: 0.01619129069149494, Val Loss: 0.020126700401306152\n",
      "Epoch 259, Loss: 0.016033779829740524, Val Loss: 0.019981974735856056\n",
      "Epoch 260, Loss: 0.01587587594985962, Val Loss: 0.01985170505940914\n",
      "Epoch 261, Loss: 0.015718968585133553, Val Loss: 0.019715366885066032\n",
      "Epoch 262, Loss: 0.0155641483142972, Val Loss: 0.019582193344831467\n",
      "Epoch 263, Loss: 0.015412628650665283, Val Loss: 0.019455919042229652\n",
      "Epoch 264, Loss: 0.01526351273059845, Val Loss: 0.01932014524936676\n",
      "Epoch 265, Loss: 0.015115409158170223, Val Loss: 0.019193610176444054\n",
      "Epoch 266, Loss: 0.014968741685152054, Val Loss: 0.019066279754042625\n",
      "Epoch 267, Loss: 0.014824319630861282, Val Loss: 0.018938472494482994\n",
      "Epoch 268, Loss: 0.014682382345199585, Val Loss: 0.01882225275039673\n",
      "Epoch 269, Loss: 0.014542202465236187, Val Loss: 0.018693238496780396\n",
      "Epoch 270, Loss: 0.01440404262393713, Val Loss: 0.01858007349073887\n",
      "Epoch 271, Loss: 0.01426592655479908, Val Loss: 0.01845467835664749\n",
      "Epoch 272, Loss: 0.01412972342222929, Val Loss: 0.018340233713388443\n",
      "Epoch 273, Loss: 0.013994921930134296, Val Loss: 0.01822221465408802\n",
      "Epoch 274, Loss: 0.0138615183532238, Val Loss: 0.0181049145758152\n",
      "Epoch 275, Loss: 0.013729783706367016, Val Loss: 0.01799360290169716\n",
      "Epoch 276, Loss: 0.01359948143362999, Val Loss: 0.01787419617176056\n",
      "Epoch 277, Loss: 0.013470562174916267, Val Loss: 0.01777130551636219\n",
      "Epoch 278, Loss: 0.013343032449483871, Val Loss: 0.01764672063291073\n",
      "Epoch 279, Loss: 0.01321741659194231, Val Loss: 0.01756194978952408\n",
      "Epoch 280, Loss: 0.01309372577816248, Val Loss: 0.017425408586859703\n",
      "Epoch 281, Loss: 0.012972615659236908, Val Loss: 0.01737213507294655\n",
      "Epoch 282, Loss: 0.012857696041464806, Val Loss: 0.017218831926584244\n",
      "Epoch 283, Loss: 0.01275163609534502, Val Loss: 0.0172283872961998\n",
      "Epoch 284, Loss: 0.012657947838306427, Val Loss: 0.01705360598862171\n",
      "Epoch 285, Loss: 0.012571124359965324, Val Loss: 0.017083769664168358\n",
      "Epoch 286, Loss: 0.012463690713047981, Val Loss: 0.016845762729644775\n",
      "Epoch 287, Loss: 0.012322846800088882, Val Loss: 0.01677273027598858\n",
      "Epoch 288, Loss: 0.01216525211930275, Val Loss: 0.016661154106259346\n",
      "Epoch 289, Loss: 0.012049617245793343, Val Loss: 0.016548674553632736\n",
      "Epoch 290, Loss: 0.011975237168371677, Val Loss: 0.016573578119277954\n",
      "Epoch 291, Loss: 0.011889592744410038, Val Loss: 0.016372645273804665\n",
      "Epoch 292, Loss: 0.011766303330659866, Val Loss: 0.01630314625799656\n",
      "Epoch 293, Loss: 0.011629784479737282, Val Loss: 0.016221925616264343\n",
      "Epoch 294, Loss: 0.011530840769410133, Val Loss: 0.016102828085422516\n",
      "Epoch 295, Loss: 0.011457061395049095, Val Loss: 0.016101069748401642\n",
      "Epoch 296, Loss: 0.011360954493284225, Val Loss: 0.015923362225294113\n",
      "Epoch 297, Loss: 0.011240127496421337, Val Loss: 0.015852227807044983\n",
      "Epoch 298, Loss: 0.011130694299936295, Val Loss: 0.015812205150723457\n",
      "Epoch 299, Loss: 0.011048325337469578, Val Loss: 0.015687670558691025\n",
      "Epoch 300, Loss: 0.010966910049319267, Val Loss: 0.015663955360651016\n",
      "Epoch 301, Loss: 0.010865704156458378, Val Loss: 0.015532074496150017\n",
      "Epoch 302, Loss: 0.010759839788079262, Val Loss: 0.01545597892254591\n",
      "Epoch 303, Loss: 0.010669529438018799, Val Loss: 0.015426084399223328\n",
      "Epoch 304, Loss: 0.010590391233563423, Val Loss: 0.01530621200799942\n",
      "Epoch 305, Loss: 0.010507187806069851, Val Loss: 0.015276414342224598\n",
      "Epoch 306, Loss: 0.010413640178740025, Val Loss: 0.015167001634836197\n",
      "Epoch 307, Loss: 0.010319201275706291, Val Loss: 0.015097128227353096\n",
      "Epoch 308, Loss: 0.010234509594738483, Val Loss: 0.015060650184750557\n",
      "Epoch 309, Loss: 0.010157239623367786, Val Loss: 0.014955750666558743\n",
      "Epoch 310, Loss: 0.010077457875013351, Val Loss: 0.014924401417374611\n",
      "Epoch 311, Loss: 0.00999234989285469, Val Loss: 0.014825252816081047\n",
      "Epoch 312, Loss: 0.00990671943873167, Val Loss: 0.01476519089192152\n",
      "Epoch 313, Loss: 0.009825089015066624, Val Loss: 0.014715881086885929\n",
      "Epoch 314, Loss: 0.0097489720210433, Val Loss: 0.014627382159233093\n",
      "Epoch 315, Loss: 0.009674843400716782, Val Loss: 0.01459889393299818\n",
      "Epoch 316, Loss: 0.00959786120802164, Val Loss: 0.014503865502774715\n",
      "Epoch 317, Loss: 0.00951793696731329, Val Loss: 0.014454411342740059\n",
      "Epoch 318, Loss: 0.009439575485885143, Val Loss: 0.014395330101251602\n",
      "Epoch 319, Loss: 0.009364952333271503, Val Loss: 0.014321672730147839\n",
      "Epoch 320, Loss: 0.009293750859797001, Val Loss: 0.014291620813310146\n",
      "Epoch 321, Loss: 0.009223260916769505, Val Loss: 0.014203284867107868\n",
      "Epoch 322, Loss: 0.0091514578089118, Val Loss: 0.014169749803841114\n",
      "Epoch 323, Loss: 0.009077265858650208, Val Loss: 0.014091975055634975\n",
      "Epoch 324, Loss: 0.0090040797367692, Val Loss: 0.014041803777217865\n",
      "Epoch 325, Loss: 0.008932903409004211, Val Loss: 0.013989826664328575\n",
      "Epoch 326, Loss: 0.008863822557032108, Val Loss: 0.013924884609878063\n",
      "Epoch 327, Loss: 0.008795700035989285, Val Loss: 0.013889136724174023\n",
      "Epoch 328, Loss: 0.00872845109552145, Val Loss: 0.013814072124660015\n",
      "Epoch 329, Loss: 0.008662190288305283, Val Loss: 0.013783459551632404\n",
      "Epoch 330, Loss: 0.008595365099608898, Val Loss: 0.013710219413042068\n",
      "Epoch 331, Loss: 0.008528538979589939, Val Loss: 0.013671993277966976\n",
      "Epoch 332, Loss: 0.008462253957986832, Val Loss: 0.013615410774946213\n",
      "Epoch 333, Loss: 0.008397147990763187, Val Loss: 0.013563617132604122\n",
      "Epoch 334, Loss: 0.008333805948495865, Val Loss: 0.01352474931627512\n",
      "Epoch 335, Loss: 0.008271666243672371, Val Loss: 0.013464539311826229\n",
      "Epoch 336, Loss: 0.00821007788181305, Val Loss: 0.013428603298962116\n",
      "Epoch 337, Loss: 0.008148525841534138, Val Loss: 0.013366549275815487\n",
      "Epoch 338, Loss: 0.008088084869086742, Val Loss: 0.013340539298951626\n",
      "Epoch 339, Loss: 0.008028365671634674, Val Loss: 0.013276143930852413\n",
      "Epoch 340, Loss: 0.007969101890921593, Val Loss: 0.013257160782814026\n",
      "Epoch 341, Loss: 0.007911193184554577, Val Loss: 0.013188374228775501\n",
      "Epoch 342, Loss: 0.00785408541560173, Val Loss: 0.013179216533899307\n",
      "Epoch 343, Loss: 0.007797688711434603, Val Loss: 0.01310579665005207\n",
      "Epoch 344, Loss: 0.007741639390587807, Val Loss: 0.013098659925162792\n",
      "Epoch 345, Loss: 0.007686138618737459, Val Loss: 0.013021324761211872\n",
      "Epoch 346, Loss: 0.007630395703017712, Val Loss: 0.013013117015361786\n",
      "Epoch 347, Loss: 0.007573801092803478, Val Loss: 0.012935454957187176\n",
      "Epoch 348, Loss: 0.007514966186136007, Val Loss: 0.012917800806462765\n",
      "Epoch 349, Loss: 0.007454532664269209, Val Loss: 0.012847909703850746\n",
      "Epoch 350, Loss: 0.007396596018224955, Val Loss: 0.012826256453990936\n",
      "Epoch 351, Loss: 0.0073411352932453156, Val Loss: 0.012761139310896397\n",
      "Epoch 352, Loss: 0.007286557462066412, Val Loss: 0.012738407589495182\n",
      "Epoch 353, Loss: 0.00723244296386838, Val Loss: 0.012680280022323132\n",
      "Epoch 354, Loss: 0.00717897666618228, Val Loss: 0.012660342268645763\n",
      "Epoch 355, Loss: 0.00712690269574523, Val Loss: 0.01260803546756506\n",
      "Epoch 356, Loss: 0.007076186593621969, Val Loss: 0.012591132894158363\n",
      "Epoch 357, Loss: 0.007027063053101301, Val Loss: 0.012539826333522797\n",
      "Epoch 358, Loss: 0.006979062687605619, Val Loss: 0.012529201805591583\n",
      "Epoch 359, Loss: 0.006931329146027565, Val Loss: 0.012476436793804169\n",
      "Epoch 360, Loss: 0.0068853809498250484, Val Loss: 0.012475049123167992\n",
      "Epoch 361, Loss: 0.006840995047241449, Val Loss: 0.012419179081916809\n",
      "Epoch 362, Loss: 0.006796153262257576, Val Loss: 0.012417150661349297\n",
      "Epoch 363, Loss: 0.0067497361451387405, Val Loss: 0.012354496866464615\n",
      "Epoch 364, Loss: 0.00670035183429718, Val Loss: 0.012342126108705997\n",
      "Epoch 365, Loss: 0.006647826638072729, Val Loss: 0.012281305156648159\n",
      "Epoch 366, Loss: 0.006593993864953518, Val Loss: 0.012258542701601982\n",
      "Epoch 367, Loss: 0.006541156210005283, Val Loss: 0.01221577450633049\n",
      "Epoch 368, Loss: 0.006491781212389469, Val Loss: 0.012186187319457531\n",
      "Epoch 369, Loss: 0.006446394603699446, Val Loss: 0.012166377156972885\n",
      "Epoch 370, Loss: 0.006405110936611891, Val Loss: 0.012130724266171455\n",
      "Epoch 371, Loss: 0.006365705747157335, Val Loss: 0.012120227329432964\n",
      "Epoch 372, Loss: 0.006325600668787956, Val Loss: 0.01207819301635027\n",
      "Epoch 373, Loss: 0.006283372640609741, Val Loss: 0.01206460315734148\n",
      "Epoch 374, Loss: 0.00623862212523818, Val Loss: 0.012020477093756199\n",
      "Epoch 375, Loss: 0.0061923544853925705, Val Loss: 0.012000472284853458\n",
      "Epoch 376, Loss: 0.006145999301224947, Val Loss: 0.011964124627411366\n",
      "Epoch 377, Loss: 0.006100635975599289, Val Loss: 0.011938294395804405\n",
      "Epoch 378, Loss: 0.0060569667257368565, Val Loss: 0.011914064176380634\n",
      "Epoch 379, Loss: 0.006015773396939039, Val Loss: 0.011886030435562134\n",
      "Epoch 380, Loss: 0.005975811742246151, Val Loss: 0.011868223547935486\n",
      "Epoch 381, Loss: 0.005936511792242527, Val Loss: 0.011838770471513271\n",
      "Epoch 382, Loss: 0.0058970325626432896, Val Loss: 0.01182391494512558\n",
      "Epoch 383, Loss: 0.0058569032698869705, Val Loss: 0.011792207136750221\n",
      "Epoch 384, Loss: 0.005816146265715361, Val Loss: 0.011772183701395988\n",
      "Epoch 385, Loss: 0.005774862132966518, Val Loss: 0.011740056797862053\n",
      "Epoch 386, Loss: 0.005734114442020655, Val Loss: 0.011717895045876503\n",
      "Epoch 387, Loss: 0.005693821236491203, Val Loss: 0.01168886199593544\n",
      "Epoch 388, Loss: 0.005654000677168369, Val Loss: 0.011666923761367798\n",
      "Epoch 389, Loss: 0.00561528280377388, Val Loss: 0.011644020676612854\n",
      "Epoch 390, Loss: 0.005577166564762592, Val Loss: 0.011621154844760895\n",
      "Epoch 391, Loss: 0.005539515055716038, Val Loss: 0.011599897406995296\n",
      "Epoch 392, Loss: 0.005502595566213131, Val Loss: 0.011575862765312195\n",
      "Epoch 393, Loss: 0.0054662879556417465, Val Loss: 0.011556059122085571\n",
      "Epoch 394, Loss: 0.005430095829069614, Val Loss: 0.011531202122569084\n",
      "Epoch 395, Loss: 0.005394136067479849, Val Loss: 0.011513052508234978\n",
      "Epoch 396, Loss: 0.005358457565307617, Val Loss: 0.011489467695355415\n",
      "Epoch 397, Loss: 0.005323069170117378, Val Loss: 0.011472671292722225\n",
      "Epoch 398, Loss: 0.005288411397486925, Val Loss: 0.011449404992163181\n",
      "Epoch 399, Loss: 0.0052547031082212925, Val Loss: 0.011436055414378643\n",
      "Epoch 400, Loss: 0.005222681909799576, Val Loss: 0.011415386572480202\n",
      "Epoch 401, Loss: 0.00519202183932066, Val Loss: 0.011407521553337574\n",
      "Epoch 402, Loss: 0.005163727793842554, Val Loss: 0.011388923972845078\n",
      "Epoch 403, Loss: 0.005137837026268244, Val Loss: 0.011391349136829376\n",
      "Epoch 404, Loss: 0.00511632626876235, Val Loss: 0.011377907358109951\n",
      "Epoch 405, Loss: 0.005097202956676483, Val Loss: 0.011389967054128647\n",
      "Epoch 406, Loss: 0.005080734845250845, Val Loss: 0.011368629522621632\n",
      "Epoch 407, Loss: 0.005056976806372404, Val Loss: 0.011362225748598576\n",
      "Epoch 408, Loss: 0.005020271521061659, Val Loss: 0.011304591782391071\n",
      "Epoch 409, Loss: 0.004962615668773651, Val Loss: 0.011264457367360592\n",
      "Epoch 410, Loss: 0.004901932552456856, Val Loss: 0.011226266622543335\n",
      "Epoch 411, Loss: 0.00485588051378727, Val Loss: 0.011214757338166237\n",
      "Epoch 412, Loss: 0.004832737613469362, Val Loss: 0.011225078254938126\n",
      "Epoch 413, Loss: 0.004821650683879852, Val Loss: 0.01120893657207489\n",
      "Epoch 414, Loss: 0.004802067764103413, Val Loss: 0.011193281970918179\n",
      "Epoch 415, Loss: 0.00476505421102047, Val Loss: 0.011148324236273766\n",
      "Epoch 416, Loss: 0.00471534812822938, Val Loss: 0.011124205775558949\n",
      "Epoch 417, Loss: 0.00467423303052783, Val Loss: 0.011117066256701946\n",
      "Epoch 418, Loss: 0.004650083836168051, Val Loss: 0.011111686937510967\n",
      "Epoch 419, Loss: 0.004633506294339895, Val Loss: 0.011109818704426289\n",
      "Epoch 420, Loss: 0.004610032774507999, Val Loss: 0.011077594943344593\n",
      "Epoch 421, Loss: 0.0045734127052128315, Val Loss: 0.011055201292037964\n",
      "Epoch 422, Loss: 0.004533260595053434, Val Loss: 0.011034938506782055\n",
      "Epoch 423, Loss: 0.004501256626099348, Val Loss: 0.011024830862879753\n",
      "Epoch 424, Loss: 0.004479819443076849, Val Loss: 0.011024203151464462\n",
      "Epoch 425, Loss: 0.004459130112081766, Val Loss: 0.011004488915205002\n",
      "Epoch 426, Loss: 0.004430743400007486, Val Loss: 0.010989636182785034\n",
      "Epoch 427, Loss: 0.004397221375256777, Val Loss: 0.010967310518026352\n",
      "Epoch 428, Loss: 0.004364487715065479, Val Loss: 0.010953378863632679\n",
      "Epoch 429, Loss: 0.004337749909609556, Val Loss: 0.010948569513857365\n",
      "Epoch 430, Loss: 0.004315757192671299, Val Loss: 0.010934282094240189\n",
      "Epoch 431, Loss: 0.004291889723390341, Val Loss: 0.010923082008957863\n",
      "Epoch 432, Loss: 0.004262344911694527, Val Loss: 0.010902629233896732\n",
      "Epoch 433, Loss: 0.004230498801916838, Val Loss: 0.010890006087720394\n",
      "Epoch 434, Loss: 0.004202425945550203, Val Loss: 0.01088156457990408\n",
      "Epoch 435, Loss: 0.004179156385362148, Val Loss: 0.010870375670492649\n",
      "Epoch 436, Loss: 0.00415700301527977, Val Loss: 0.010862039402127266\n",
      "Epoch 437, Loss: 0.004131975118070841, Val Loss: 0.010845269076526165\n",
      "Epoch 438, Loss: 0.004104440566152334, Val Loss: 0.01083292905241251\n",
      "Epoch 439, Loss: 0.004077593795955181, Val Loss: 0.010821503587067127\n",
      "Epoch 440, Loss: 0.004052919801324606, Val Loss: 0.010810131207108498\n",
      "Epoch 441, Loss: 0.004029946867376566, Val Loss: 0.010802825912833214\n",
      "Epoch 442, Loss: 0.004007233306765556, Val Loss: 0.010789887979626656\n",
      "Epoch 443, Loss: 0.0039840154349803925, Val Loss: 0.010779691860079765\n",
      "Epoch 444, Loss: 0.0039594462141394615, Val Loss: 0.010765815153717995\n",
      "Epoch 445, Loss: 0.003934638109058142, Val Loss: 0.010754619725048542\n",
      "Epoch 446, Loss: 0.003910737112164497, Val Loss: 0.0107483621686697\n",
      "Epoch 447, Loss: 0.0038885956164449453, Val Loss: 0.01073969341814518\n",
      "Epoch 448, Loss: 0.0038663793820887804, Val Loss: 0.010732131078839302\n",
      "Epoch 449, Loss: 0.0038438825868070126, Val Loss: 0.01071974914520979\n",
      "Epoch 450, Loss: 0.003821147372946143, Val Loss: 0.010709372349083424\n",
      "Epoch 451, Loss: 0.003798245918005705, Val Loss: 0.010697958059608936\n",
      "Epoch 452, Loss: 0.0037758417893201113, Val Loss: 0.010687744244933128\n",
      "Epoch 453, Loss: 0.00375393801368773, Val Loss: 0.010678362101316452\n",
      "Epoch 454, Loss: 0.0037324954755604267, Val Loss: 0.010668487288057804\n",
      "Epoch 455, Loss: 0.0037113777361810207, Val Loss: 0.010660381056368351\n",
      "Epoch 456, Loss: 0.0036904443986713886, Val Loss: 0.010649577714502811\n",
      "Epoch 457, Loss: 0.003669161582365632, Val Loss: 0.010640367865562439\n",
      "Epoch 458, Loss: 0.0036475048400461674, Val Loss: 0.010630006901919842\n",
      "Epoch 459, Loss: 0.0036261300556361675, Val Loss: 0.010620431043207645\n",
      "Epoch 460, Loss: 0.003605227218940854, Val Loss: 0.01061303075402975\n",
      "Epoch 461, Loss: 0.003585172351449728, Val Loss: 0.010604368522763252\n",
      "Epoch 462, Loss: 0.0035652488004416227, Val Loss: 0.010598021559417248\n",
      "Epoch 463, Loss: 0.003544681705534458, Val Loss: 0.010588491335511208\n",
      "Epoch 464, Loss: 0.0035239355638623238, Val Loss: 0.010581815615296364\n",
      "Epoch 465, Loss: 0.0035031381994485855, Val Loss: 0.010573482140898705\n",
      "Epoch 466, Loss: 0.003483495442196727, Val Loss: 0.010565532371401787\n",
      "Epoch 467, Loss: 0.0034648398868739605, Val Loss: 0.01056038960814476\n",
      "Epoch 468, Loss: 0.0034462762996554375, Val Loss: 0.01055194903165102\n",
      "Epoch 469, Loss: 0.0034272209741175175, Val Loss: 0.010546444915235043\n",
      "Epoch 470, Loss: 0.0034072946291416883, Val Loss: 0.010537275113165379\n",
      "Epoch 471, Loss: 0.003387316595762968, Val Loss: 0.01052964199334383\n",
      "Epoch 472, Loss: 0.0033675162121653557, Val Loss: 0.010521955788135529\n",
      "Epoch 473, Loss: 0.003348365891724825, Val Loss: 0.010513335466384888\n",
      "Epoch 474, Loss: 0.0033300037030130625, Val Loss: 0.010505667887628078\n",
      "Epoch 475, Loss: 0.003311901120468974, Val Loss: 0.010497172363102436\n",
      "Epoch 476, Loss: 0.003293687477707863, Val Loss: 0.010489147156476974\n",
      "Epoch 477, Loss: 0.003274989314377308, Val Loss: 0.010480819270014763\n",
      "Epoch 478, Loss: 0.0032565579749643803, Val Loss: 0.010473238304257393\n",
      "Epoch 479, Loss: 0.0032384395599365234, Val Loss: 0.010465583764016628\n",
      "Epoch 480, Loss: 0.0032206776086241007, Val Loss: 0.010458631440997124\n",
      "Epoch 481, Loss: 0.0032032248564064503, Val Loss: 0.010452622547745705\n",
      "Epoch 482, Loss: 0.0031858363654464483, Val Loss: 0.010445916093885899\n",
      "Epoch 483, Loss: 0.003168693510815501, Val Loss: 0.010439706966280937\n",
      "Epoch 484, Loss: 0.0031515294685959816, Val Loss: 0.01043226569890976\n",
      "Epoch 485, Loss: 0.003134366124868393, Val Loss: 0.010425014421343803\n",
      "Epoch 486, Loss: 0.00311711011454463, Val Loss: 0.01041807234287262\n",
      "Epoch 487, Loss: 0.0030998215079307556, Val Loss: 0.010410633869469166\n",
      "Epoch 488, Loss: 0.0030825468711555004, Val Loss: 0.010402766987681389\n",
      "Epoch 489, Loss: 0.0030653926078230143, Val Loss: 0.010395711287856102\n",
      "Epoch 490, Loss: 0.0030489240307360888, Val Loss: 0.010389906354248524\n",
      "Epoch 491, Loss: 0.003032602835446596, Val Loss: 0.010384730994701385\n",
      "Epoch 492, Loss: 0.0030162245966494083, Val Loss: 0.010378148406744003\n",
      "Epoch 493, Loss: 0.0029999897815287113, Val Loss: 0.01037205196917057\n",
      "Epoch 494, Loss: 0.002983884885907173, Val Loss: 0.010364355519413948\n",
      "Epoch 495, Loss: 0.0029679799918085337, Val Loss: 0.010357675142586231\n",
      "Epoch 496, Loss: 0.0029523076955229044, Val Loss: 0.010352547280490398\n",
      "Epoch 497, Loss: 0.002936745062470436, Val Loss: 0.010346028953790665\n",
      "Epoch 498, Loss: 0.002921072067692876, Val Loss: 0.01033957488834858\n",
      "Epoch 499, Loss: 0.0029055560007691383, Val Loss: 0.010331740602850914\n",
      "Epoch 500, Loss: 0.0028899500612169504, Val Loss: 0.010325589217245579\n",
      "Epoch 501, Loss: 0.002874752040952444, Val Loss: 0.010319688357412815\n",
      "Epoch 502, Loss: 0.0028595696203410625, Val Loss: 0.010314418002963066\n",
      "Epoch 503, Loss: 0.0028444635681807995, Val Loss: 0.010307339951395988\n",
      "Epoch 504, Loss: 0.002829429693520069, Val Loss: 0.010300339199602604\n",
      "Epoch 505, Loss: 0.002814775798469782, Val Loss: 0.01029386930167675\n",
      "Epoch 506, Loss: 0.0028000108432024717, Val Loss: 0.010286972858011723\n",
      "Epoch 507, Loss: 0.002785381628200412, Val Loss: 0.01028113067150116\n",
      "Epoch 508, Loss: 0.0027707943227142096, Val Loss: 0.010274428874254227\n",
      "Epoch 509, Loss: 0.0027562561444938183, Val Loss: 0.010266538709402084\n",
      "Epoch 510, Loss: 0.002741902368143201, Val Loss: 0.010260498151183128\n",
      "Epoch 511, Loss: 0.0027279749047011137, Val Loss: 0.010253327898681164\n",
      "Epoch 512, Loss: 0.002713955007493496, Val Loss: 0.010248025879263878\n",
      "Epoch 513, Loss: 0.002699775155633688, Val Loss: 0.010240796953439713\n",
      "Epoch 514, Loss: 0.00268563162535429, Val Loss: 0.010233565233647823\n",
      "Epoch 515, Loss: 0.0026717521250247955, Val Loss: 0.010227224789559841\n",
      "Epoch 516, Loss: 0.0026584723964333534, Val Loss: 0.01021884847432375\n",
      "Epoch 517, Loss: 0.002645175438374281, Val Loss: 0.010213850066065788\n",
      "Epoch 518, Loss: 0.0026315504219383, Val Loss: 0.010206698440015316\n",
      "Epoch 519, Loss: 0.0026175896637141705, Val Loss: 0.010200328193604946\n",
      "Epoch 520, Loss: 0.00260392134077847, Val Loss: 0.010193146765232086\n",
      "Epoch 521, Loss: 0.002590559422969818, Val Loss: 0.010184741578996181\n",
      "Epoch 522, Loss: 0.0025776501279324293, Val Loss: 0.010179441422224045\n",
      "Epoch 523, Loss: 0.0025646367575973272, Val Loss: 0.010171173140406609\n",
      "Epoch 524, Loss: 0.0025513176806271076, Val Loss: 0.010164735838770866\n",
      "Epoch 525, Loss: 0.002538134343922138, Val Loss: 0.010157301090657711\n",
      "Epoch 526, Loss: 0.0025251784827560186, Val Loss: 0.010152352973818779\n",
      "Epoch 527, Loss: 0.00251234183087945, Val Loss: 0.010147220455110073\n",
      "Epoch 528, Loss: 0.002499738708138466, Val Loss: 0.010140374302864075\n",
      "Epoch 529, Loss: 0.002487101359292865, Val Loss: 0.010134145617485046\n",
      "Epoch 530, Loss: 0.0024745988193899393, Val Loss: 0.01012871228158474\n",
      "Epoch 531, Loss: 0.002462292555719614, Val Loss: 0.01012193039059639\n",
      "Epoch 532, Loss: 0.0024500947911292315, Val Loss: 0.01011605467647314\n",
      "Epoch 533, Loss: 0.002437922405079007, Val Loss: 0.010109818540513515\n",
      "Epoch 534, Loss: 0.0024257181212306023, Val Loss: 0.010104217566549778\n",
      "Epoch 535, Loss: 0.0024138179142028093, Val Loss: 0.010098449885845184\n",
      "Epoch 536, Loss: 0.0024018557742238045, Val Loss: 0.010093609802424908\n",
      "Epoch 537, Loss: 0.0023899206425994635, Val Loss: 0.010086205787956715\n",
      "Epoch 538, Loss: 0.0023781557101756334, Val Loss: 0.010079702362418175\n",
      "Epoch 539, Loss: 0.002366461092606187, Val Loss: 0.010074013844132423\n",
      "Epoch 540, Loss: 0.0023550083860754967, Val Loss: 0.010068502277135849\n",
      "Epoch 541, Loss: 0.0023434418253600597, Val Loss: 0.01006367802619934\n",
      "Epoch 542, Loss: 0.002332048723474145, Val Loss: 0.01005583442747593\n",
      "Epoch 543, Loss: 0.0023206276819109917, Val Loss: 0.010051269084215164\n",
      "Epoch 544, Loss: 0.0023092536721378565, Val Loss: 0.010045595467090607\n",
      "Epoch 545, Loss: 0.0022979259956628084, Val Loss: 0.010039166547358036\n",
      "Epoch 546, Loss: 0.0022867810912430286, Val Loss: 0.010033922269940376\n",
      "Epoch 547, Loss: 0.002275683917105198, Val Loss: 0.010027999058365822\n",
      "Epoch 548, Loss: 0.0022646363358944654, Val Loss: 0.010022897273302078\n",
      "Epoch 549, Loss: 0.0022536159958690405, Val Loss: 0.010018173605203629\n",
      "Epoch 550, Loss: 0.002242704387754202, Val Loss: 0.010013257153332233\n",
      "Epoch 551, Loss: 0.002231874503195286, Val Loss: 0.010007399134337902\n",
      "Epoch 552, Loss: 0.002221012255176902, Val Loss: 0.01000039279460907\n",
      "Epoch 553, Loss: 0.002210259670391679, Val Loss: 0.009996197186410427\n",
      "Epoch 554, Loss: 0.0021996148861944675, Val Loss: 0.009991273283958435\n",
      "Epoch 555, Loss: 0.0021890727803111076, Val Loss: 0.00998692773282528\n",
      "Epoch 556, Loss: 0.0021786075085401535, Val Loss: 0.009980592876672745\n",
      "Epoch 557, Loss: 0.0021680868230760098, Val Loss: 0.009976651519536972\n",
      "Epoch 558, Loss: 0.0021575605496764183, Val Loss: 0.009970326907932758\n",
      "Epoch 559, Loss: 0.0021471672225743532, Val Loss: 0.009966914542019367\n",
      "Epoch 560, Loss: 0.0021368232555687428, Val Loss: 0.009963077493011951\n",
      "Epoch 561, Loss: 0.0021265181712806225, Val Loss: 0.009956919588148594\n",
      "Epoch 562, Loss: 0.002116207731887698, Val Loss: 0.00995255634188652\n",
      "Epoch 563, Loss: 0.002106029773131013, Val Loss: 0.00994756817817688\n",
      "Epoch 564, Loss: 0.0020958338864147663, Val Loss: 0.00994280818849802\n",
      "Epoch 565, Loss: 0.0020857842173427343, Val Loss: 0.009940718300640583\n",
      "Epoch 566, Loss: 0.002075746888294816, Val Loss: 0.00993530172854662\n",
      "Epoch 567, Loss: 0.0020657700952142477, Val Loss: 0.009932754561305046\n",
      "Epoch 568, Loss: 0.002055682009086013, Val Loss: 0.009927250444889069\n",
      "Epoch 569, Loss: 0.002045758068561554, Val Loss: 0.009921371005475521\n",
      "Epoch 570, Loss: 0.00203604600392282, Val Loss: 0.009920992888510227\n",
      "Epoch 571, Loss: 0.002026341389864683, Val Loss: 0.009916016831994057\n",
      "Epoch 572, Loss: 0.0020165713503956795, Val Loss: 0.00991401169449091\n",
      "Epoch 573, Loss: 0.002006831578910351, Val Loss: 0.009908482432365417\n",
      "Epoch 574, Loss: 0.0019971535075455904, Val Loss: 0.009903546422719955\n",
      "Epoch 575, Loss: 0.001987538067623973, Val Loss: 0.009901431389153004\n",
      "Epoch 576, Loss: 0.001978139393031597, Val Loss: 0.009894481860101223\n",
      "Epoch 577, Loss: 0.0019685912411659956, Val Loss: 0.00989252794533968\n",
      "Epoch 578, Loss: 0.0019590146839618683, Val Loss: 0.009888973087072372\n",
      "Epoch 579, Loss: 0.001949778408743441, Val Loss: 0.009882856160402298\n",
      "Epoch 580, Loss: 0.0019406729843467474, Val Loss: 0.009883064776659012\n",
      "Epoch 581, Loss: 0.0019313714001327753, Val Loss: 0.009877661243081093\n",
      "Epoch 582, Loss: 0.0019220017129555345, Val Loss: 0.009875882416963577\n",
      "Epoch 583, Loss: 0.001912661362439394, Val Loss: 0.009872917085886002\n",
      "Epoch 584, Loss: 0.0019037204328924417, Val Loss: 0.009866815991699696\n",
      "Epoch 585, Loss: 0.0018947600619867444, Val Loss: 0.00986628606915474\n",
      "Epoch 586, Loss: 0.0018855780363082886, Val Loss: 0.009862671606242657\n",
      "Epoch 587, Loss: 0.0018765550339594483, Val Loss: 0.009857133030891418\n",
      "Epoch 588, Loss: 0.0018677215557545424, Val Loss: 0.009857326745986938\n",
      "Epoch 589, Loss: 0.0018589715473353863, Val Loss: 0.00985017605125904\n",
      "Epoch 590, Loss: 0.0018502121092751622, Val Loss: 0.009849180467426777\n",
      "Epoch 591, Loss: 0.0018413314828649163, Val Loss: 0.009843490086495876\n",
      "Epoch 592, Loss: 0.001832563546486199, Val Loss: 0.009840828366577625\n",
      "Epoch 593, Loss: 0.0018240725621581078, Val Loss: 0.009841986000537872\n",
      "Epoch 594, Loss: 0.0018155818106606603, Val Loss: 0.009834768250584602\n",
      "Epoch 595, Loss: 0.0018069770885631442, Val Loss: 0.009834653697907925\n",
      "Epoch 596, Loss: 0.0017982542049139738, Val Loss: 0.009831581264734268\n",
      "Epoch 597, Loss: 0.0017898515798151493, Val Loss: 0.009826787747442722\n",
      "Epoch 598, Loss: 0.0017816965701058507, Val Loss: 0.009829461574554443\n",
      "Epoch 599, Loss: 0.0017733699642121792, Val Loss: 0.009823407977819443\n",
      "Epoch 600, Loss: 0.0017649539513513446, Val Loss: 0.009821227751672268\n",
      "Epoch 601, Loss: 0.001756665064021945, Val Loss: 0.009821157902479172\n",
      "Epoch 602, Loss: 0.0017486021388322115, Val Loss: 0.00981516856700182\n",
      "Epoch 603, Loss: 0.0017404567915946245, Val Loss: 0.009814930148422718\n",
      "Epoch 604, Loss: 0.0017322689527645707, Val Loss: 0.009812646545469761\n",
      "Epoch 605, Loss: 0.0017242529429495335, Val Loss: 0.009809986688196659\n",
      "Epoch 606, Loss: 0.0017162950243800879, Val Loss: 0.009811470285058022\n",
      "Epoch 607, Loss: 0.0017083790153265, Val Loss: 0.009807353839278221\n",
      "Epoch 608, Loss: 0.0017004021210595965, Val Loss: 0.009804272092878819\n",
      "Epoch 609, Loss: 0.0016925461823120713, Val Loss: 0.009803513064980507\n",
      "Epoch 610, Loss: 0.0016847532242536545, Val Loss: 0.009800764732062817\n",
      "Epoch 611, Loss: 0.0016769388457760215, Val Loss: 0.009800476022064686\n",
      "Epoch 612, Loss: 0.001669212244451046, Val Loss: 0.009799875319004059\n",
      "Epoch 613, Loss: 0.0016615656204521656, Val Loss: 0.009797188453376293\n",
      "Epoch 614, Loss: 0.0016539129428565502, Val Loss: 0.009795927442610264\n",
      "Epoch 615, Loss: 0.0016462294152006507, Val Loss: 0.009795406833291054\n",
      "Epoch 616, Loss: 0.00163868663366884, Val Loss: 0.009795030578970909\n",
      "Epoch 617, Loss: 0.0016311624785885215, Val Loss: 0.009793184697628021\n",
      "Epoch 618, Loss: 0.0016235481016337872, Val Loss: 0.009790800511837006\n",
      "Epoch 619, Loss: 0.0016160366358235478, Val Loss: 0.00979047454893589\n",
      "Epoch 620, Loss: 0.0016086020041257143, Val Loss: 0.009787541814148426\n",
      "Epoch 621, Loss: 0.0016011890256777406, Val Loss: 0.00978829525411129\n",
      "Epoch 622, Loss: 0.0015938100405037403, Val Loss: 0.009787815622985363\n",
      "Epoch 623, Loss: 0.0015864218585193157, Val Loss: 0.00978439673781395\n",
      "Epoch 624, Loss: 0.0015791876940056682, Val Loss: 0.00978588592261076\n",
      "Epoch 625, Loss: 0.0015719688963145018, Val Loss: 0.009785131551325321\n",
      "Epoch 626, Loss: 0.001564695150591433, Val Loss: 0.009783989749848843\n",
      "Epoch 627, Loss: 0.0015575322322547436, Val Loss: 0.009785646572709084\n",
      "Epoch 628, Loss: 0.0015503871254622936, Val Loss: 0.009782707318663597\n",
      "Epoch 629, Loss: 0.0015432231593877077, Val Loss: 0.009782640263438225\n",
      "Epoch 630, Loss: 0.0015361126279458404, Val Loss: 0.009781727567315102\n",
      "Epoch 631, Loss: 0.001529111061245203, Val Loss: 0.009782514534890652\n",
      "Epoch 632, Loss: 0.0015220445347949862, Val Loss: 0.009785573929548264\n",
      "Epoch 633, Loss: 0.0015151450643315911, Val Loss: 0.009779638610780239\n",
      "Epoch 634, Loss: 0.0015081815654411912, Val Loss: 0.009781760163605213\n",
      "Epoch 635, Loss: 0.0015011975774541497, Val Loss: 0.009780342690646648\n",
      "Epoch 636, Loss: 0.0014942643465474248, Val Loss: 0.009778880514204502\n",
      "Epoch 637, Loss: 0.0014874589396640658, Val Loss: 0.0097795519977808\n",
      "Epoch 638, Loss: 0.001480705919675529, Val Loss: 0.009778344072401524\n",
      "Epoch 639, Loss: 0.0014738746685907245, Val Loss: 0.009776944294571877\n",
      "Epoch 640, Loss: 0.001467159716412425, Val Loss: 0.009778088890016079\n",
      "Epoch 641, Loss: 0.0014604934258386493, Val Loss: 0.009777610190212727\n",
      "Epoch 642, Loss: 0.001453810022212565, Val Loss: 0.009778429754078388\n",
      "Epoch 643, Loss: 0.00144726422149688, Val Loss: 0.009780063293874264\n",
      "Epoch 644, Loss: 0.0014407063135877252, Val Loss: 0.009776154533028603\n",
      "Epoch 645, Loss: 0.0014342002104967833, Val Loss: 0.00977825652807951\n",
      "Epoch 646, Loss: 0.0014276218134909868, Val Loss: 0.009777740575373173\n",
      "Epoch 647, Loss: 0.001421201741322875, Val Loss: 0.009774316102266312\n",
      "Epoch 648, Loss: 0.001414854428730905, Val Loss: 0.009777815081179142\n",
      "Epoch 649, Loss: 0.0014085292350500822, Val Loss: 0.009773961268365383\n",
      "Epoch 650, Loss: 0.0014020989183336496, Val Loss: 0.009773604571819305\n"
     ]
    }
   ],
   "source": [
    "from flashrag.prompt import PromptTemplate\n",
    "import pickle\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "config_taqa = set_config(dataset_name='taqa', split = split)\n",
    "template_taqa= PromptTemplate(\n",
    "        config = config_taqa,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "config_triviaqa= set_config(dataset_name='triviaqa', split = split)\n",
    "template_triviaqa= PromptTemplate(\n",
    "        config = config_triviaqa,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "\n",
    "\n",
    "loading_hidden = True\n",
    "if not loading_hidden:\n",
    "    taqa_hidden_states, triviaqa_hidden_states = load_datasets(config_taqa, config_triviaqa, template_taqa, template_triviaqa)\n",
    "else:\n",
    "    file_path_triviaqa = '/cs/student/projects2/dsml/cdiezmar/hidden_states/triviaqa.pkl'\n",
    "    file_path_taqa = '/cs/student/projects2/dsml/cdiezmar/hidden_states/taqa.pkl'\n",
    "\n",
    "    with open(file_path_triviaqa, 'rb') as file_triviaqa:\n",
    "        triviaqa_hidden_states = pickle.load(file_triviaqa)\n",
    "        print('Length triviaqa: ', len(triviaqa_hidden_states))\n",
    "        triviaqa_hidden_states_tensor= torch.tensor(triviaqa_hidden_states, dtype=torch.float32)\n",
    "    with open(file_path_taqa, 'rb') as file_taqa:\n",
    "        taqa_hidden_states = pickle.load(file_taqa)\n",
    "        print('Length taqa: ', len(taqa_hidden_states))\n",
    "        taqa_hidden_states_tensor= torch.tensor(taqa_hidden_states, dtype=torch.float32)\n",
    "train_inputs, train_labels, val_inputs, val_labels = prepare_data(taqa_hidden_states_tensor, triviaqa_hidden_states_tensor)\n",
    "\n",
    "input_size = train_inputs.size(1)\n",
    "num_classes = 2  # Time-sensitive or not\n",
    "\n",
    "# GRU Parameters\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "epochs = 650\n",
    "lr = 1e-4\n",
    "\n",
    "# Train GRU Classifier\n",
    "gru_model = train_gru_classifier_cuda(train_inputs, train_labels, val_inputs, val_labels, input_size, hidden_size, num_layers, num_classes, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `question` in template\n",
      "Find `question` in template\n",
      "Length triviaqa:  78785\n",
      "Length taqa:  10148\n",
      "Epoch 1, Loss: 0.6834548115730286, Val Loss: 0.6367180943489075\n",
      "Epoch 2, Loss: 0.6372912526130676, Val Loss: 0.5956288576126099\n",
      "Epoch 3, Loss: 0.5968227982521057, Val Loss: 0.5598422884941101\n",
      "Epoch 4, Loss: 0.5616674423217773, Val Loss: 0.528995156288147\n",
      "Epoch 5, Loss: 0.5314736366271973, Val Loss: 0.5027759671211243\n",
      "Epoch 6, Loss: 0.5058567523956299, Val Loss: 0.4808236360549927\n",
      "Epoch 7, Loss: 0.48445889353752136, Val Loss: 0.46279096603393555\n",
      "Epoch 8, Loss: 0.466939240694046, Val Loss: 0.448172003030777\n",
      "Epoch 9, Loss: 0.45274847745895386, Val Loss: 0.4363589882850647\n",
      "Epoch 10, Loss: 0.4413072168827057, Val Loss: 0.4263599216938019\n",
      "Epoch 11, Loss: 0.4316355586051941, Val Loss: 0.4175047278404236\n",
      "Epoch 12, Loss: 0.4230915307998657, Val Loss: 0.40941962599754333\n",
      "Epoch 13, Loss: 0.4152972400188446, Val Loss: 0.40192490816116333\n",
      "Epoch 14, Loss: 0.4080960750579834, Val Loss: 0.3950011134147644\n",
      "Epoch 15, Loss: 0.4014422297477722, Val Loss: 0.3885863125324249\n",
      "Epoch 16, Loss: 0.39526742696762085, Val Loss: 0.38268911838531494\n",
      "Epoch 17, Loss: 0.3895912170410156, Val Loss: 0.37725529074668884\n",
      "Epoch 18, Loss: 0.3843715190887451, Val Loss: 0.3722841441631317\n",
      "Epoch 19, Loss: 0.3796217143535614, Val Loss: 0.3677119016647339\n",
      "Epoch 20, Loss: 0.3752667009830475, Val Loss: 0.36352643370628357\n",
      "Epoch 21, Loss: 0.3712880611419678, Val Loss: 0.359763503074646\n",
      "Epoch 22, Loss: 0.3677322566509247, Val Loss: 0.35638687014579773\n",
      "Epoch 23, Loss: 0.3645677864551544, Val Loss: 0.35330766439437866\n",
      "Epoch 24, Loss: 0.36170586943626404, Val Loss: 0.3504222631454468\n",
      "Epoch 25, Loss: 0.3590277433395386, Val Loss: 0.3477046489715576\n",
      "Epoch 26, Loss: 0.35649776458740234, Val Loss: 0.34519273042678833\n",
      "Epoch 27, Loss: 0.35415351390838623, Val Loss: 0.3429289758205414\n",
      "Epoch 28, Loss: 0.3520495593547821, Val Loss: 0.34090566635131836\n",
      "Epoch 29, Loss: 0.3501826226711273, Val Loss: 0.33906716108322144\n",
      "Epoch 30, Loss: 0.34849607944488525, Val Loss: 0.3373298943042755\n",
      "Epoch 31, Loss: 0.34690427780151367, Val Loss: 0.33566397428512573\n",
      "Epoch 32, Loss: 0.34536266326904297, Val Loss: 0.33403080701828003\n",
      "Epoch 33, Loss: 0.3438076674938202, Val Loss: 0.33236053586006165\n",
      "Epoch 34, Loss: 0.34218406677246094, Val Loss: 0.3305932283401489\n",
      "Epoch 35, Loss: 0.3404729664325714, Val Loss: 0.32883161306381226\n",
      "Epoch 36, Loss: 0.338734894990921, Val Loss: 0.32720935344696045\n",
      "Epoch 37, Loss: 0.3371235132217407, Val Loss: 0.3257516920566559\n",
      "Epoch 38, Loss: 0.3356614410877228, Val Loss: 0.3242815136909485\n",
      "Epoch 39, Loss: 0.33415794372558594, Val Loss: 0.3226819932460785\n",
      "Epoch 40, Loss: 0.33250752091407776, Val Loss: 0.3210247755050659\n",
      "Epoch 41, Loss: 0.3308069109916687, Val Loss: 0.3193782866001129\n",
      "Epoch 42, Loss: 0.32912585139274597, Val Loss: 0.3176954388618469\n",
      "Epoch 43, Loss: 0.32739683985710144, Val Loss: 0.3159725069999695\n",
      "Epoch 44, Loss: 0.3256177008152008, Val Loss: 0.3142065405845642\n",
      "Epoch 45, Loss: 0.32379230856895447, Val Loss: 0.3124035596847534\n",
      "Epoch 46, Loss: 0.32191574573516846, Val Loss: 0.3105638921260834\n",
      "Epoch 47, Loss: 0.31998008489608765, Val Loss: 0.3086938261985779\n",
      "Epoch 48, Loss: 0.31798428297042847, Val Loss: 0.30676907300949097\n",
      "Epoch 49, Loss: 0.31591901183128357, Val Loss: 0.3047315180301666\n",
      "Epoch 50, Loss: 0.31374529004096985, Val Loss: 0.3025379180908203\n",
      "Epoch 51, Loss: 0.3114185631275177, Val Loss: 0.3001728951931\n",
      "Epoch 52, Loss: 0.30890971422195435, Val Loss: 0.2976343035697937\n",
      "Epoch 53, Loss: 0.3062297999858856, Val Loss: 0.2949342131614685\n",
      "Epoch 54, Loss: 0.3033713698387146, Val Loss: 0.29216268658638\n",
      "Epoch 55, Loss: 0.3004004955291748, Val Loss: 0.28935641050338745\n",
      "Epoch 56, Loss: 0.2973751127719879, Val Loss: 0.28654205799102783\n",
      "Epoch 57, Loss: 0.2943374216556549, Val Loss: 0.2836821377277374\n",
      "Epoch 58, Loss: 0.2912544906139374, Val Loss: 0.2807532250881195\n",
      "Epoch 59, Loss: 0.288103848695755, Val Loss: 0.27768808603286743\n",
      "Epoch 60, Loss: 0.28481563925743103, Val Loss: 0.2744552493095398\n",
      "Epoch 61, Loss: 0.28135815262794495, Val Loss: 0.2710374593734741\n",
      "Epoch 62, Loss: 0.27772247791290283, Val Loss: 0.26743417978286743\n",
      "Epoch 63, Loss: 0.27391067147254944, Val Loss: 0.2637042999267578\n",
      "Epoch 64, Loss: 0.26996031403541565, Val Loss: 0.259926438331604\n",
      "Epoch 65, Loss: 0.2659285366535187, Val Loss: 0.25609657168388367\n",
      "Epoch 66, Loss: 0.26182717084884644, Val Loss: 0.2522391378879547\n",
      "Epoch 67, Loss: 0.2576994299888611, Val Loss: 0.24839778244495392\n",
      "Epoch 68, Loss: 0.2535935342311859, Val Loss: 0.24454674124717712\n",
      "Epoch 69, Loss: 0.24948732554912567, Val Loss: 0.24063505232334137\n",
      "Epoch 70, Loss: 0.24533650279045105, Val Loss: 0.23660223186016083\n",
      "Epoch 71, Loss: 0.24107809364795685, Val Loss: 0.23240476846694946\n",
      "Epoch 72, Loss: 0.23665779829025269, Val Loss: 0.22801508009433746\n",
      "Epoch 73, Loss: 0.23204822838306427, Val Loss: 0.2234383523464203\n",
      "Epoch 74, Loss: 0.22725644707679749, Val Loss: 0.21871936321258545\n",
      "Epoch 75, Loss: 0.2223299890756607, Val Loss: 0.2139250636100769\n",
      "Epoch 76, Loss: 0.21733781695365906, Val Loss: 0.2090895175933838\n",
      "Epoch 77, Loss: 0.2123103141784668, Val Loss: 0.20425459742546082\n",
      "Epoch 78, Loss: 0.20728863775730133, Val Loss: 0.1994561105966568\n",
      "Epoch 79, Loss: 0.2022971659898758, Val Loss: 0.19469621777534485\n",
      "Epoch 80, Loss: 0.19733832776546478, Val Loss: 0.18998341262340546\n",
      "Epoch 81, Loss: 0.19242249429225922, Val Loss: 0.18531110882759094\n",
      "Epoch 82, Loss: 0.1875443458557129, Val Loss: 0.18063189089298248\n",
      "Epoch 83, Loss: 0.18265695869922638, Val Loss: 0.17595189809799194\n",
      "Epoch 84, Loss: 0.17777228355407715, Val Loss: 0.17125922441482544\n",
      "Epoch 85, Loss: 0.17287735641002655, Val Loss: 0.166587695479393\n",
      "Epoch 86, Loss: 0.16800539195537567, Val Loss: 0.16193410754203796\n",
      "Epoch 87, Loss: 0.16315282881259918, Val Loss: 0.15728838741779327\n",
      "Epoch 88, Loss: 0.15830151736736298, Val Loss: 0.15271326899528503\n",
      "Epoch 89, Loss: 0.15352939069271088, Val Loss: 0.14847342669963837\n",
      "Epoch 90, Loss: 0.14913314580917358, Val Loss: 0.14427126944065094\n",
      "Epoch 91, Loss: 0.1447358876466751, Val Loss: 0.14009429514408112\n",
      "Epoch 92, Loss: 0.14032810926437378, Val Loss: 0.1360975205898285\n",
      "Epoch 93, Loss: 0.13610772788524628, Val Loss: 0.1322607845067978\n",
      "Epoch 94, Loss: 0.13208718597888947, Val Loss: 0.12849721312522888\n",
      "Epoch 95, Loss: 0.12816785275936127, Val Loss: 0.12479650229215622\n",
      "Epoch 96, Loss: 0.12432020902633667, Val Loss: 0.12116067856550217\n",
      "Epoch 97, Loss: 0.12053847312927246, Val Loss: 0.11762874573469162\n",
      "Epoch 98, Loss: 0.11687246710062027, Val Loss: 0.11421266943216324\n",
      "Epoch 99, Loss: 0.11333833634853363, Val Loss: 0.11093764007091522\n",
      "Epoch 100, Loss: 0.10994887351989746, Val Loss: 0.10776561498641968\n",
      "Epoch 101, Loss: 0.10665636509656906, Val Loss: 0.10467351973056793\n",
      "Epoch 102, Loss: 0.10344123095273972, Val Loss: 0.10166674107313156\n",
      "Epoch 103, Loss: 0.10032632201910019, Val Loss: 0.0987466350197792\n",
      "Epoch 104, Loss: 0.0973154604434967, Val Loss: 0.09591013193130493\n",
      "Epoch 105, Loss: 0.09440681338310242, Val Loss: 0.09314950555562973\n",
      "Epoch 106, Loss: 0.09159103780984879, Val Loss: 0.0904838815331459\n",
      "Epoch 107, Loss: 0.08889181166887283, Val Loss: 0.08788274973630905\n",
      "Epoch 108, Loss: 0.08624034374952316, Val Loss: 0.0853961631655693\n",
      "Epoch 109, Loss: 0.08366921544075012, Val Loss: 0.0830027312040329\n",
      "Epoch 110, Loss: 0.08120552450418472, Val Loss: 0.08065170794725418\n",
      "Epoch 111, Loss: 0.07882880419492722, Val Loss: 0.07838006317615509\n",
      "Epoch 112, Loss: 0.076535165309906, Val Loss: 0.07621907442808151\n",
      "Epoch 113, Loss: 0.0743289440870285, Val Loss: 0.07412800937891006\n",
      "Epoch 114, Loss: 0.07220303267240524, Val Loss: 0.07208140194416046\n",
      "Epoch 115, Loss: 0.07012936472892761, Val Loss: 0.07011941820383072\n",
      "Epoch 116, Loss: 0.06812851130962372, Val Loss: 0.06823217868804932\n",
      "Epoch 117, Loss: 0.06621057540178299, Val Loss: 0.06637244671583176\n",
      "Epoch 118, Loss: 0.06436450034379959, Val Loss: 0.06457672268152237\n",
      "Epoch 119, Loss: 0.06259409338235855, Val Loss: 0.06286732852458954\n",
      "Epoch 120, Loss: 0.06089431047439575, Val Loss: 0.06120685115456581\n",
      "Epoch 121, Loss: 0.05925300344824791, Val Loss: 0.05958497151732445\n",
      "Epoch 122, Loss: 0.057659730315208435, Val Loss: 0.058042947202920914\n",
      "Epoch 123, Loss: 0.05612417310476303, Val Loss: 0.05657187104225159\n",
      "Epoch 124, Loss: 0.05464857071638107, Val Loss: 0.055169813334941864\n",
      "Epoch 125, Loss: 0.053252674639225006, Val Loss: 0.053873609751462936\n",
      "Epoch 126, Loss: 0.05195716395974159, Val Loss: 0.052638571709394455\n",
      "Epoch 127, Loss: 0.050716910511255264, Val Loss: 0.051419466733932495\n",
      "Epoch 128, Loss: 0.04950134456157684, Val Loss: 0.05023699626326561\n",
      "Epoch 129, Loss: 0.04830491915345192, Val Loss: 0.049110352993011475\n",
      "Epoch 130, Loss: 0.04715081304311752, Val Loss: 0.04801390320062637\n",
      "Epoch 131, Loss: 0.04604712128639221, Val Loss: 0.04696991667151451\n",
      "Epoch 132, Loss: 0.045003343373537064, Val Loss: 0.04598623141646385\n",
      "Epoch 133, Loss: 0.044011857360601425, Val Loss: 0.04502912983298302\n",
      "Epoch 134, Loss: 0.04305446147918701, Val Loss: 0.04411108046770096\n",
      "Epoch 135, Loss: 0.04212387278676033, Val Loss: 0.04322638735175133\n",
      "Epoch 136, Loss: 0.0412142388522625, Val Loss: 0.042367592453956604\n",
      "Epoch 137, Loss: 0.04033776745200157, Val Loss: 0.04154482111334801\n",
      "Epoch 138, Loss: 0.03949486091732979, Val Loss: 0.04077538102865219\n",
      "Epoch 139, Loss: 0.03869093209505081, Val Loss: 0.04003090783953667\n",
      "Epoch 140, Loss: 0.03791842237114906, Val Loss: 0.03932010754942894\n",
      "Epoch 141, Loss: 0.03716744855046272, Val Loss: 0.03862599655985832\n",
      "Epoch 142, Loss: 0.03643547371029854, Val Loss: 0.03794168308377266\n",
      "Epoch 143, Loss: 0.03572465106844902, Val Loss: 0.037289634346961975\n",
      "Epoch 144, Loss: 0.03503882884979248, Val Loss: 0.03666260465979576\n",
      "Epoch 145, Loss: 0.034377966076135635, Val Loss: 0.03605859354138374\n",
      "Epoch 146, Loss: 0.033739056438207626, Val Loss: 0.03548631817102432\n",
      "Epoch 147, Loss: 0.03311963379383087, Val Loss: 0.0349300354719162\n",
      "Epoch 148, Loss: 0.03252124413847923, Val Loss: 0.034388426691293716\n",
      "Epoch 149, Loss: 0.03194277733564377, Val Loss: 0.033870477229356766\n",
      "Epoch 150, Loss: 0.031383320689201355, Val Loss: 0.03336383402347565\n",
      "Epoch 151, Loss: 0.030833980068564415, Val Loss: 0.03287728875875473\n",
      "Epoch 152, Loss: 0.030303480103611946, Val Loss: 0.032411299645900726\n",
      "Epoch 153, Loss: 0.029792530462145805, Val Loss: 0.031962838023900986\n",
      "Epoch 154, Loss: 0.02929759956896305, Val Loss: 0.031530704349279404\n",
      "Epoch 155, Loss: 0.02881525084376335, Val Loss: 0.031108345836400986\n",
      "Epoch 156, Loss: 0.02834649197757244, Val Loss: 0.030689703300595284\n",
      "Epoch 157, Loss: 0.02788735367357731, Val Loss: 0.03028765320777893\n",
      "Epoch 158, Loss: 0.02744423970580101, Val Loss: 0.02989274635910988\n",
      "Epoch 159, Loss: 0.027017978951334953, Val Loss: 0.02951277792453766\n",
      "Epoch 160, Loss: 0.026603708043694496, Val Loss: 0.02914639562368393\n",
      "Epoch 161, Loss: 0.026197344064712524, Val Loss: 0.028794415295124054\n",
      "Epoch 162, Loss: 0.025805506855249405, Val Loss: 0.028453070670366287\n",
      "Epoch 163, Loss: 0.025423280894756317, Val Loss: 0.02811955101788044\n",
      "Epoch 164, Loss: 0.025051087141036987, Val Loss: 0.027793223038315773\n",
      "Epoch 165, Loss: 0.024686597287654877, Val Loss: 0.027473248541355133\n",
      "Epoch 166, Loss: 0.024334192276000977, Val Loss: 0.02716839499771595\n",
      "Epoch 167, Loss: 0.023991785943508148, Val Loss: 0.026872606948018074\n",
      "Epoch 168, Loss: 0.023655548691749573, Val Loss: 0.026588739827275276\n",
      "Epoch 169, Loss: 0.023326503112912178, Val Loss: 0.026313576847314835\n",
      "Epoch 170, Loss: 0.023006372153759003, Val Loss: 0.026040785014629364\n",
      "Epoch 171, Loss: 0.022694580256938934, Val Loss: 0.025770599022507668\n",
      "Epoch 172, Loss: 0.022390037775039673, Val Loss: 0.02550315670669079\n",
      "Epoch 173, Loss: 0.02209446020424366, Val Loss: 0.025245361030101776\n",
      "Epoch 174, Loss: 0.021805480122566223, Val Loss: 0.024997249245643616\n",
      "Epoch 175, Loss: 0.021520236507058144, Val Loss: 0.024759624153375626\n",
      "Epoch 176, Loss: 0.02124285325407982, Val Loss: 0.024528533220291138\n",
      "Epoch 177, Loss: 0.02097122184932232, Val Loss: 0.024301882833242416\n",
      "Epoch 178, Loss: 0.02070673182606697, Val Loss: 0.024073349311947823\n",
      "Epoch 179, Loss: 0.02044719271361828, Val Loss: 0.023850571364164352\n",
      "Epoch 180, Loss: 0.0201935525983572, Val Loss: 0.023627400398254395\n",
      "Epoch 181, Loss: 0.01994457095861435, Val Loss: 0.023410508409142494\n",
      "Epoch 182, Loss: 0.019700461998581886, Val Loss: 0.0232022013515234\n",
      "Epoch 183, Loss: 0.01946226879954338, Val Loss: 0.02300146594643593\n",
      "Epoch 184, Loss: 0.019229311496019363, Val Loss: 0.02280355803668499\n",
      "Epoch 185, Loss: 0.0190002229064703, Val Loss: 0.02260601706802845\n",
      "Epoch 186, Loss: 0.018775656819343567, Val Loss: 0.02241428941488266\n",
      "Epoch 187, Loss: 0.018555322661995888, Val Loss: 0.022228600457310677\n",
      "Epoch 188, Loss: 0.018340233713388443, Val Loss: 0.022046716883778572\n",
      "Epoch 189, Loss: 0.01812881976366043, Val Loss: 0.021870695054531097\n",
      "Epoch 190, Loss: 0.017921360209584236, Val Loss: 0.021697981283068657\n",
      "Epoch 191, Loss: 0.017718499526381493, Val Loss: 0.021523939445614815\n",
      "Epoch 192, Loss: 0.017519881948828697, Val Loss: 0.021349960938096046\n",
      "Epoch 193, Loss: 0.017323166131973267, Val Loss: 0.02118573524057865\n",
      "Epoch 194, Loss: 0.017131764441728592, Val Loss: 0.021021487191319466\n",
      "Epoch 195, Loss: 0.016943473368883133, Val Loss: 0.020862549543380737\n",
      "Epoch 196, Loss: 0.016758903861045837, Val Loss: 0.020704615861177444\n",
      "Epoch 197, Loss: 0.016576817259192467, Val Loss: 0.020550483837723732\n",
      "Epoch 198, Loss: 0.016398802399635315, Val Loss: 0.02039485052227974\n",
      "Epoch 199, Loss: 0.01622348092496395, Val Loss: 0.020247992128133774\n",
      "Epoch 200, Loss: 0.016050737351179123, Val Loss: 0.02010260336101055\n",
      "Epoch 201, Loss: 0.015881288796663284, Val Loss: 0.019964367151260376\n",
      "Epoch 202, Loss: 0.015715179964900017, Val Loss: 0.01982646808028221\n",
      "Epoch 203, Loss: 0.015552189201116562, Val Loss: 0.019687965512275696\n",
      "Epoch 204, Loss: 0.015391845256090164, Val Loss: 0.019548989832401276\n",
      "Epoch 205, Loss: 0.015234574675559998, Val Loss: 0.01941484399139881\n",
      "Epoch 206, Loss: 0.015081081539392471, Val Loss: 0.0192874725908041\n",
      "Epoch 207, Loss: 0.014930503442883492, Val Loss: 0.01916070468723774\n",
      "Epoch 208, Loss: 0.014785027131438255, Val Loss: 0.019051378592848778\n",
      "Epoch 209, Loss: 0.014647044241428375, Val Loss: 0.01892995461821556\n",
      "Epoch 210, Loss: 0.014518273994326591, Val Loss: 0.018847553059458733\n",
      "Epoch 211, Loss: 0.014395822770893574, Val Loss: 0.018716443330049515\n",
      "Epoch 212, Loss: 0.014260997995734215, Val Loss: 0.01859760843217373\n",
      "Epoch 213, Loss: 0.014103722758591175, Val Loss: 0.018447019159793854\n",
      "Epoch 214, Loss: 0.013945951126515865, Val Loss: 0.018333638086915016\n",
      "Epoch 215, Loss: 0.013816364109516144, Val Loss: 0.018258197233080864\n",
      "Epoch 216, Loss: 0.013707309029996395, Val Loss: 0.018132634460926056\n",
      "Epoch 217, Loss: 0.013589379377663136, Val Loss: 0.018036996945738792\n",
      "Epoch 218, Loss: 0.013452024199068546, Val Loss: 0.01790796034038067\n",
      "Epoch 219, Loss: 0.013314975425601006, Val Loss: 0.01781059429049492\n",
      "Epoch 220, Loss: 0.013199555687606335, Val Loss: 0.01774696819484234\n",
      "Epoch 221, Loss: 0.013093815185129642, Val Loss: 0.017623377963900566\n",
      "Epoch 222, Loss: 0.012978708371520042, Val Loss: 0.017536619678139687\n",
      "Epoch 223, Loss: 0.012852850370109081, Val Loss: 0.017427649348974228\n",
      "Epoch 224, Loss: 0.012734165415167809, Val Loss: 0.017333952710032463\n",
      "Epoch 225, Loss: 0.012628573924303055, Val Loss: 0.017273973673582077\n",
      "Epoch 226, Loss: 0.01252621691673994, Val Loss: 0.017157457768917084\n",
      "Epoch 227, Loss: 0.01241518184542656, Val Loss: 0.01707986369729042\n",
      "Epoch 228, Loss: 0.012301520444452763, Val Loss: 0.01698734052479267\n",
      "Epoch 229, Loss: 0.01219554990530014, Val Loss: 0.01689492166042328\n",
      "Epoch 230, Loss: 0.012096518650650978, Val Loss: 0.01683831214904785\n",
      "Epoch 231, Loss: 0.011998494155704975, Val Loss: 0.01673041470348835\n",
      "Epoch 232, Loss: 0.011895590461790562, Val Loss: 0.01666168123483658\n",
      "Epoch 233, Loss: 0.011791756376624107, Val Loss: 0.0165775865316391\n",
      "Epoch 234, Loss: 0.011693193577229977, Val Loss: 0.016491534188389778\n",
      "Epoch 235, Loss: 0.011599309742450714, Val Loss: 0.01643802411854267\n",
      "Epoch 236, Loss: 0.011506988666951656, Val Loss: 0.016338132321834564\n",
      "Epoch 237, Loss: 0.011412936262786388, Val Loss: 0.016279427334666252\n",
      "Epoch 238, Loss: 0.011317098513245583, Val Loss: 0.01619233936071396\n",
      "Epoch 239, Loss: 0.0112227201461792, Val Loss: 0.016119031235575676\n",
      "Epoch 240, Loss: 0.011131450533866882, Val Loss: 0.016059333458542824\n",
      "Epoch 241, Loss: 0.011043651029467583, Val Loss: 0.01597204990684986\n",
      "Epoch 242, Loss: 0.010956376791000366, Val Loss: 0.015921534970402718\n",
      "Epoch 243, Loss: 0.010868990793824196, Val Loss: 0.015832412987947464\n",
      "Epoch 244, Loss: 0.010780788958072662, Val Loss: 0.015776043757796288\n",
      "Epoch 245, Loss: 0.010693722404539585, Val Loss: 0.01570223644375801\n",
      "Epoch 246, Loss: 0.010608510114252567, Val Loss: 0.015632089227437973\n",
      "Epoch 247, Loss: 0.010524990037083626, Val Loss: 0.015577845275402069\n",
      "Epoch 248, Loss: 0.010443408042192459, Val Loss: 0.015499171800911427\n",
      "Epoch 249, Loss: 0.010362581349909306, Val Loss: 0.015452387742698193\n",
      "Epoch 250, Loss: 0.010282094590365887, Val Loss: 0.015372672118246555\n",
      "Epoch 251, Loss: 0.010201847180724144, Val Loss: 0.015327096916735172\n",
      "Epoch 252, Loss: 0.010122490115463734, Val Loss: 0.015248646028339863\n",
      "Epoch 253, Loss: 0.010042985901236534, Val Loss: 0.015199129469692707\n",
      "Epoch 254, Loss: 0.009965097531676292, Val Loss: 0.015129621140658855\n",
      "Epoch 255, Loss: 0.009888382628560066, Val Loss: 0.015072495676577091\n",
      "Epoch 256, Loss: 0.009813014417886734, Val Loss: 0.015014687553048134\n",
      "Epoch 257, Loss: 0.009738773107528687, Val Loss: 0.01495292503386736\n",
      "Epoch 258, Loss: 0.009665417484939098, Val Loss: 0.01490179542452097\n",
      "Epoch 259, Loss: 0.009592958725988865, Val Loss: 0.014835584908723831\n",
      "Epoch 260, Loss: 0.009521652944386005, Val Loss: 0.014791062101721764\n",
      "Epoch 261, Loss: 0.009451597929000854, Val Loss: 0.014720821753144264\n",
      "Epoch 262, Loss: 0.009382675401866436, Val Loss: 0.01468964759260416\n",
      "Epoch 263, Loss: 0.009315300732851028, Val Loss: 0.014611813239753246\n",
      "Epoch 264, Loss: 0.009250925853848457, Val Loss: 0.014601476490497589\n",
      "Epoch 265, Loss: 0.009189416654407978, Val Loss: 0.014512680470943451\n",
      "Epoch 266, Loss: 0.00913333985954523, Val Loss: 0.01453965064138174\n",
      "Epoch 267, Loss: 0.009083263576030731, Val Loss: 0.014438350684940815\n",
      "Epoch 268, Loss: 0.00903693214058876, Val Loss: 0.014482531696557999\n",
      "Epoch 269, Loss: 0.008983408100903034, Val Loss: 0.014343383722007275\n",
      "Epoch 270, Loss: 0.008908824063837528, Val Loss: 0.014325764030218124\n",
      "Epoch 271, Loss: 0.00881491880863905, Val Loss: 0.014221821911633015\n",
      "Epoch 272, Loss: 0.008730309084057808, Val Loss: 0.014176087453961372\n",
      "Epoch 273, Loss: 0.008674124255776405, Val Loss: 0.01419191062450409\n",
      "Epoch 274, Loss: 0.008635343983769417, Val Loss: 0.01410657912492752\n",
      "Epoch 275, Loss: 0.008587999269366264, Val Loss: 0.014104950241744518\n",
      "Epoch 276, Loss: 0.008518344722688198, Val Loss: 0.01400588359683752\n",
      "Epoch 277, Loss: 0.008440772071480751, Val Loss: 0.013965515419840813\n",
      "Epoch 278, Loss: 0.008378230966627598, Val Loss: 0.013953609392046928\n",
      "Epoch 279, Loss: 0.008333297446370125, Val Loss: 0.013885920867323875\n",
      "Epoch 280, Loss: 0.008287807926535606, Val Loss: 0.013882488943636417\n",
      "Epoch 281, Loss: 0.008228234946727753, Val Loss: 0.013799392618238926\n",
      "Epoch 282, Loss: 0.008161067962646484, Val Loss: 0.013765262439846992\n",
      "Epoch 283, Loss: 0.008101188577711582, Val Loss: 0.013744700700044632\n",
      "Epoch 284, Loss: 0.00805298425257206, Val Loss: 0.013687539845705032\n",
      "Epoch 285, Loss: 0.008006666786968708, Val Loss: 0.013677540235221386\n",
      "Epoch 286, Loss: 0.00795238558202982, Val Loss: 0.01360698789358139\n",
      "Epoch 287, Loss: 0.007892386987805367, Val Loss: 0.013574366457760334\n",
      "Epoch 288, Loss: 0.007835744880139828, Val Loss: 0.013543596491217613\n",
      "Epoch 289, Loss: 0.007785682566463947, Val Loss: 0.013493590988218784\n",
      "Epoch 290, Loss: 0.0077386521734297276, Val Loss: 0.013480954803526402\n",
      "Epoch 291, Loss: 0.007688623853027821, Val Loss: 0.013419129885733128\n",
      "Epoch 292, Loss: 0.00763428770005703, Val Loss: 0.01338963396847248\n",
      "Epoch 293, Loss: 0.007580993231385946, Val Loss: 0.01335143018513918\n",
      "Epoch 294, Loss: 0.007531126495450735, Val Loss: 0.01331005897372961\n",
      "Epoch 295, Loss: 0.007484664209187031, Val Loss: 0.013292789459228516\n",
      "Epoch 296, Loss: 0.007437827531248331, Val Loss: 0.013238847255706787\n",
      "Epoch 297, Loss: 0.007389233447611332, Val Loss: 0.013215783052146435\n",
      "Epoch 298, Loss: 0.00733930291607976, Val Loss: 0.01317128911614418\n",
      "Epoch 299, Loss: 0.007289701607078314, Val Loss: 0.013136648572981358\n",
      "Epoch 300, Loss: 0.0072432043962180614, Val Loss: 0.013110286556184292\n",
      "Epoch 301, Loss: 0.007197980768978596, Val Loss: 0.013066322542726994\n",
      "Epoch 302, Loss: 0.007153291720896959, Val Loss: 0.013046244159340858\n",
      "Epoch 303, Loss: 0.007107758428901434, Val Loss: 0.013000399805605412\n",
      "Epoch 304, Loss: 0.007061501033604145, Val Loss: 0.012974465265870094\n",
      "Epoch 305, Loss: 0.007015664130449295, Val Loss: 0.01293918676674366\n",
      "Epoch 306, Loss: 0.00697077251970768, Val Loss: 0.012904868461191654\n",
      "Epoch 307, Loss: 0.00692737614735961, Val Loss: 0.01288002822548151\n",
      "Epoch 308, Loss: 0.006884594913572073, Val Loss: 0.012840356677770615\n",
      "Epoch 309, Loss: 0.006842099595814943, Val Loss: 0.012820345349609852\n",
      "Epoch 310, Loss: 0.006799399387091398, Val Loss: 0.012780359014868736\n",
      "Epoch 311, Loss: 0.006756267976015806, Val Loss: 0.01275629922747612\n",
      "Epoch 312, Loss: 0.006713527254760265, Val Loss: 0.012720790691673756\n",
      "Epoch 313, Loss: 0.0066709150560200214, Val Loss: 0.0126922857016325\n",
      "Epoch 314, Loss: 0.006629333831369877, Val Loss: 0.012663878500461578\n",
      "Epoch 315, Loss: 0.006588383577764034, Val Loss: 0.012631903402507305\n",
      "Epoch 316, Loss: 0.0065474919974803925, Val Loss: 0.012607590295374393\n",
      "Epoch 317, Loss: 0.006507324054837227, Val Loss: 0.0125730624422431\n",
      "Epoch 318, Loss: 0.006467243190854788, Val Loss: 0.012551064603030682\n",
      "Epoch 319, Loss: 0.00642738863825798, Val Loss: 0.012516221031546593\n",
      "Epoch 320, Loss: 0.006387761794030666, Val Loss: 0.012494103983044624\n",
      "Epoch 321, Loss: 0.006348402239382267, Val Loss: 0.012461945414543152\n",
      "Epoch 322, Loss: 0.006309289950877428, Val Loss: 0.012440073303878307\n",
      "Epoch 323, Loss: 0.006270678713917732, Val Loss: 0.012408637441694736\n",
      "Epoch 324, Loss: 0.006232353858649731, Val Loss: 0.012384824454784393\n",
      "Epoch 325, Loss: 0.006194365210831165, Val Loss: 0.012355772778391838\n",
      "Epoch 326, Loss: 0.006156649440526962, Val Loss: 0.01233269926160574\n",
      "Epoch 327, Loss: 0.0061192587018013, Val Loss: 0.012303887866437435\n",
      "Epoch 328, Loss: 0.0060821473598480225, Val Loss: 0.012280252762138844\n",
      "Epoch 329, Loss: 0.006045464891940355, Val Loss: 0.012253093533217907\n",
      "Epoch 330, Loss: 0.0060094245709478855, Val Loss: 0.012231588363647461\n",
      "Epoch 331, Loss: 0.005973423831164837, Val Loss: 0.012203546240925789\n",
      "Epoch 332, Loss: 0.0059376670978963375, Val Loss: 0.012183180078864098\n",
      "Epoch 333, Loss: 0.005902453791350126, Val Loss: 0.012155324220657349\n",
      "Epoch 334, Loss: 0.005867646541446447, Val Loss: 0.012136423029005527\n",
      "Epoch 335, Loss: 0.005833299830555916, Val Loss: 0.012107057496905327\n",
      "Epoch 336, Loss: 0.00579919433221221, Val Loss: 0.012092412449419498\n",
      "Epoch 337, Loss: 0.005765446461737156, Val Loss: 0.012059727683663368\n",
      "Epoch 338, Loss: 0.005732925608754158, Val Loss: 0.012050390243530273\n",
      "Epoch 339, Loss: 0.005701479502022266, Val Loss: 0.01201649196445942\n",
      "Epoch 340, Loss: 0.005671888589859009, Val Loss: 0.012022661045193672\n",
      "Epoch 341, Loss: 0.005645488388836384, Val Loss: 0.011984696611762047\n",
      "Epoch 342, Loss: 0.005623944569379091, Val Loss: 0.012020919471979141\n",
      "Epoch 343, Loss: 0.005609187763184309, Val Loss: 0.011980598792433739\n",
      "Epoch 344, Loss: 0.005599191877990961, Val Loss: 0.012034947983920574\n",
      "Epoch 345, Loss: 0.005586595740169287, Val Loss: 0.011954600922763348\n",
      "Epoch 346, Loss: 0.005552938207983971, Val Loss: 0.011944926343858242\n",
      "Epoch 347, Loss: 0.005490521900355816, Val Loss: 0.011847855523228645\n",
      "Epoch 348, Loss: 0.005421302281320095, Val Loss: 0.011820352636277676\n",
      "Epoch 349, Loss: 0.005379822105169296, Val Loss: 0.011836511082947254\n",
      "Epoch 350, Loss: 0.005369048565626144, Val Loss: 0.011807962320744991\n",
      "Epoch 351, Loss: 0.005361628253012896, Val Loss: 0.011825270019471645\n",
      "Epoch 352, Loss: 0.005330371204763651, Val Loss: 0.011746362783014774\n",
      "Epoch 353, Loss: 0.005277510732412338, Val Loss: 0.011721759103238583\n",
      "Epoch 354, Loss: 0.005231792572885752, Val Loss: 0.011716206558048725\n",
      "Epoch 355, Loss: 0.005209940951317549, Val Loss: 0.011691177263855934\n",
      "Epoch 356, Loss: 0.005197206977754831, Val Loss: 0.011700861155986786\n",
      "Epoch 357, Loss: 0.005170455202460289, Val Loss: 0.011639931239187717\n",
      "Epoch 358, Loss: 0.005128370597958565, Val Loss: 0.011618796736001968\n",
      "Epoch 359, Loss: 0.005089821759611368, Val Loss: 0.011610694229602814\n",
      "Epoch 360, Loss: 0.005066749174147844, Val Loss: 0.011586389504373074\n",
      "Epoch 361, Loss: 0.005048997234553099, Val Loss: 0.011592109687626362\n",
      "Epoch 362, Loss: 0.0050222305580973625, Val Loss: 0.011542493477463722\n",
      "Epoch 363, Loss: 0.004986270796507597, Val Loss: 0.011524803936481476\n",
      "Epoch 364, Loss: 0.004954165313392878, Val Loss: 0.011514520272612572\n",
      "Epoch 365, Loss: 0.004931347444653511, Val Loss: 0.011487677693367004\n",
      "Epoch 366, Loss: 0.004910895600914955, Val Loss: 0.011488509364426136\n",
      "Epoch 367, Loss: 0.004884449765086174, Val Loss: 0.011450808495283127\n",
      "Epoch 368, Loss: 0.004852672107517719, Val Loss: 0.011434759944677353\n",
      "Epoch 369, Loss: 0.0048233140259981155, Val Loss: 0.011424577794969082\n",
      "Epoch 370, Loss: 0.004800045397132635, Val Loss: 0.011402206495404243\n",
      "Epoch 371, Loss: 0.004777983296662569, Val Loss: 0.011399419978260994\n",
      "Epoch 372, Loss: 0.004752663895487785, Val Loss: 0.011367227882146835\n",
      "Epoch 373, Loss: 0.004723981022834778, Val Loss: 0.011354130692780018\n",
      "Epoch 374, Loss: 0.004696228541433811, Val Loss: 0.011340495198965073\n",
      "Epoch 375, Loss: 0.004671948961913586, Val Loss: 0.01131895836442709\n",
      "Epoch 376, Loss: 0.004649297799915075, Val Loss: 0.011314653791487217\n",
      "Epoch 377, Loss: 0.004625482484698296, Val Loss: 0.011284993030130863\n",
      "Epoch 378, Loss: 0.0045994860120117664, Val Loss: 0.011274226009845734\n",
      "Epoch 379, Loss: 0.004572825040668249, Val Loss: 0.011253593489527702\n",
      "Epoch 380, Loss: 0.004547544755041599, Val Loss: 0.011235596612095833\n",
      "Epoch 381, Loss: 0.004523849580436945, Val Loss: 0.011228576302528381\n",
      "Epoch 382, Loss: 0.0045011104084551334, Val Loss: 0.011204379610717297\n",
      "Epoch 383, Loss: 0.00447775749489665, Val Loss: 0.011198165826499462\n",
      "Epoch 384, Loss: 0.0044531445018947124, Val Loss: 0.011172205209732056\n",
      "Epoch 385, Loss: 0.004428296349942684, Val Loss: 0.011158589273691177\n",
      "Epoch 386, Loss: 0.004403762053698301, Val Loss: 0.011142515577375889\n",
      "Epoch 387, Loss: 0.004380231723189354, Val Loss: 0.011123600415885448\n",
      "Epoch 388, Loss: 0.004357441328465939, Val Loss: 0.011114941909909248\n",
      "Epoch 389, Loss: 0.004334847442805767, Val Loss: 0.011092844419181347\n",
      "Epoch 390, Loss: 0.0043120007030665874, Val Loss: 0.011084447614848614\n",
      "Epoch 391, Loss: 0.004288800992071629, Val Loss: 0.01106190588325262\n",
      "Epoch 392, Loss: 0.004265204071998596, Val Loss: 0.011048157699406147\n",
      "Epoch 393, Loss: 0.0042418562807142735, Val Loss: 0.01103180181235075\n",
      "Epoch 394, Loss: 0.0042185974307358265, Val Loss: 0.011014147661626339\n",
      "Epoch 395, Loss: 0.004195581655949354, Val Loss: 0.01100284606218338\n",
      "Epoch 396, Loss: 0.004173276945948601, Val Loss: 0.010982758365571499\n",
      "Epoch 397, Loss: 0.004151104483753443, Val Loss: 0.010972562246024609\n",
      "Epoch 398, Loss: 0.00412923377007246, Val Loss: 0.010949821211397648\n",
      "Epoch 399, Loss: 0.004107131622731686, Val Loss: 0.01093931682407856\n",
      "Epoch 400, Loss: 0.004085224587470293, Val Loss: 0.01091996394097805\n",
      "Epoch 401, Loss: 0.004063462372869253, Val Loss: 0.01090945303440094\n",
      "Epoch 402, Loss: 0.00404169037938118, Val Loss: 0.010893432423472404\n",
      "Epoch 403, Loss: 0.004020392429083586, Val Loss: 0.010879576206207275\n",
      "Epoch 404, Loss: 0.003999193664640188, Val Loss: 0.010868328623473644\n",
      "Epoch 405, Loss: 0.003978170920163393, Val Loss: 0.0108533576130867\n",
      "Epoch 406, Loss: 0.003957446664571762, Val Loss: 0.010843552649021149\n",
      "Epoch 407, Loss: 0.003936725202947855, Val Loss: 0.010827958583831787\n",
      "Epoch 408, Loss: 0.0039159259758889675, Val Loss: 0.010820298455655575\n",
      "Epoch 409, Loss: 0.0038951721508055925, Val Loss: 0.010802894830703735\n",
      "Epoch 410, Loss: 0.0038741594180464745, Val Loss: 0.010797034949064255\n",
      "Epoch 411, Loss: 0.003853393020108342, Val Loss: 0.010779635980725288\n",
      "Epoch 412, Loss: 0.0038330573588609695, Val Loss: 0.01077351439744234\n",
      "Epoch 413, Loss: 0.003813178278505802, Val Loss: 0.010756048373878002\n",
      "Epoch 414, Loss: 0.0037931278347969055, Val Loss: 0.010749751701951027\n",
      "Epoch 415, Loss: 0.003773435950279236, Val Loss: 0.010733245871961117\n",
      "Epoch 416, Loss: 0.0037536686286330223, Val Loss: 0.010728253982961178\n",
      "Epoch 417, Loss: 0.0037340126000344753, Val Loss: 0.010711975395679474\n",
      "Epoch 418, Loss: 0.003714545164257288, Val Loss: 0.010706841014325619\n",
      "Epoch 419, Loss: 0.0036953194066882133, Val Loss: 0.010689876042306423\n",
      "Epoch 420, Loss: 0.0036760359071195126, Val Loss: 0.010686998255550861\n",
      "Epoch 421, Loss: 0.003656926564872265, Val Loss: 0.01066851057112217\n",
      "Epoch 422, Loss: 0.003638220252469182, Val Loss: 0.010666907764971256\n",
      "Epoch 423, Loss: 0.0036194673739373684, Val Loss: 0.010646108537912369\n",
      "Epoch 424, Loss: 0.003600927535444498, Val Loss: 0.01064915768802166\n",
      "Epoch 425, Loss: 0.0035828249529004097, Val Loss: 0.010625326074659824\n",
      "Epoch 426, Loss: 0.0035652900114655495, Val Loss: 0.010634738951921463\n",
      "Epoch 427, Loss: 0.003548248438164592, Val Loss: 0.010604764334857464\n",
      "Epoch 428, Loss: 0.0035322150215506554, Val Loss: 0.01062426995486021\n",
      "Epoch 429, Loss: 0.003516973229125142, Val Loss: 0.010588756762444973\n",
      "Epoch 430, Loss: 0.003502538660541177, Val Loss: 0.010620594024658203\n",
      "Epoch 431, Loss: 0.0034890680108219385, Val Loss: 0.010574701242148876\n",
      "Epoch 432, Loss: 0.003475175704807043, Val Loss: 0.010617323219776154\n",
      "Epoch 433, Loss: 0.003459870582446456, Val Loss: 0.010556336492300034\n",
      "Epoch 434, Loss: 0.003440421773120761, Val Loss: 0.01058660726994276\n",
      "Epoch 435, Loss: 0.0034181757364422083, Val Loss: 0.010527489706873894\n",
      "Epoch 436, Loss: 0.0033928074408322573, Val Loss: 0.010536681860685349\n",
      "Epoch 437, Loss: 0.0033683134242892265, Val Loss: 0.010507564060389996\n",
      "Epoch 438, Loss: 0.00334720010869205, Val Loss: 0.010498110204935074\n",
      "Epoch 439, Loss: 0.0033303298987448215, Val Loss: 0.010505923070013523\n",
      "Epoch 440, Loss: 0.003316581016406417, Val Loss: 0.010477892123162746\n",
      "Epoch 441, Loss: 0.003303526435047388, Val Loss: 0.010503372177481651\n",
      "Epoch 442, Loss: 0.003289231099188328, Val Loss: 0.010460150428116322\n",
      "Epoch 443, Loss: 0.0032721911557018757, Val Loss: 0.010478831827640533\n",
      "Epoch 444, Loss: 0.003253128845244646, Val Loss: 0.010441254824399948\n",
      "Epoch 445, Loss: 0.0032331461552530527, Val Loss: 0.010443811304867268\n",
      "Epoch 446, Loss: 0.0032142314594238997, Val Loss: 0.010430687107145786\n",
      "Epoch 447, Loss: 0.003197407117113471, Val Loss: 0.010417620651423931\n",
      "Epoch 448, Loss: 0.0031824703328311443, Val Loss: 0.010426047258079052\n",
      "Epoch 449, Loss: 0.0031680853571742773, Val Loss: 0.010399364866316319\n",
      "Epoch 450, Loss: 0.0031534505542367697, Val Loss: 0.010412742383778095\n",
      "Epoch 451, Loss: 0.0031378704588860273, Val Loss: 0.010381522588431835\n",
      "Epoch 452, Loss: 0.0031211148016154766, Val Loss: 0.010389230214059353\n",
      "Epoch 453, Loss: 0.0031038664747029543, Val Loss: 0.010369070805609226\n",
      "Epoch 454, Loss: 0.0030868975445628166, Val Loss: 0.010364864021539688\n",
      "Epoch 455, Loss: 0.003070785664021969, Val Loss: 0.01035987213253975\n",
      "Epoch 456, Loss: 0.003055406967177987, Val Loss: 0.010343028232455254\n",
      "Epoch 457, Loss: 0.003040730021893978, Val Loss: 0.010350920259952545\n",
      "Epoch 458, Loss: 0.003026076592504978, Val Loss: 0.010326542891561985\n",
      "Epoch 459, Loss: 0.0030110725201666355, Val Loss: 0.010338197462260723\n",
      "Epoch 460, Loss: 0.0029956959187984467, Val Loss: 0.010312885046005249\n",
      "Epoch 461, Loss: 0.002979961223900318, Val Loss: 0.010316585190594196\n",
      "Epoch 462, Loss: 0.0029641296714544296, Val Loss: 0.010300347581505775\n",
      "Epoch 463, Loss: 0.002948709297925234, Val Loss: 0.010296372696757317\n",
      "Epoch 464, Loss: 0.0029336425941437483, Val Loss: 0.01029360480606556\n",
      "Epoch 465, Loss: 0.0029190347995609045, Val Loss: 0.010278240777552128\n",
      "Epoch 466, Loss: 0.0029045799747109413, Val Loss: 0.010282037779688835\n",
      "Epoch 467, Loss: 0.002890191040933132, Val Loss: 0.010263563133776188\n",
      "Epoch 468, Loss: 0.002875435398891568, Val Loss: 0.010270930826663971\n",
      "Epoch 469, Loss: 0.002860515844076872, Val Loss: 0.010252166539430618\n",
      "Epoch 470, Loss: 0.002845731796696782, Val Loss: 0.010255970060825348\n",
      "Epoch 471, Loss: 0.0028310506604611874, Val Loss: 0.010242154821753502\n",
      "Epoch 472, Loss: 0.002816487569361925, Val Loss: 0.01024066936224699\n",
      "Epoch 473, Loss: 0.0028022504411637783, Val Loss: 0.01023152470588684\n",
      "Epoch 474, Loss: 0.002788316458463669, Val Loss: 0.010223103687167168\n",
      "Epoch 475, Loss: 0.002774404129013419, Val Loss: 0.010220978409051895\n",
      "Epoch 476, Loss: 0.0027604964561760426, Val Loss: 0.010208871215581894\n",
      "Epoch 477, Loss: 0.0027467021718621254, Val Loss: 0.010211263783276081\n",
      "Epoch 478, Loss: 0.0027328680735081434, Val Loss: 0.010195422917604446\n",
      "Epoch 479, Loss: 0.002718839794397354, Val Loss: 0.010197117924690247\n",
      "Epoch 480, Loss: 0.0027047726325690746, Val Loss: 0.01018183771520853\n",
      "Epoch 481, Loss: 0.0026905967388302088, Val Loss: 0.010181245394051075\n",
      "Epoch 482, Loss: 0.002676298376172781, Val Loss: 0.01016923040151596\n",
      "Epoch 483, Loss: 0.0026619392447173595, Val Loss: 0.010164374485611916\n",
      "Epoch 484, Loss: 0.0026479146908968687, Val Loss: 0.010156666859984398\n",
      "Epoch 485, Loss: 0.0026339227333664894, Val Loss: 0.010150199756026268\n",
      "Epoch 486, Loss: 0.0026200104039162397, Val Loss: 0.010146505199372768\n",
      "Epoch 487, Loss: 0.0026064773555845022, Val Loss: 0.010136407800018787\n",
      "Epoch 488, Loss: 0.0025929545518010855, Val Loss: 0.01013470534235239\n",
      "Epoch 489, Loss: 0.0025794999673962593, Val Loss: 0.010123785585165024\n",
      "Epoch 490, Loss: 0.0025660230312496424, Val Loss: 0.010122984647750854\n",
      "Epoch 491, Loss: 0.0025525137316435575, Val Loss: 0.010112977586686611\n",
      "Epoch 492, Loss: 0.0025389434304088354, Val Loss: 0.010111743584275246\n",
      "Epoch 493, Loss: 0.002525682095438242, Val Loss: 0.010102479718625546\n",
      "Epoch 494, Loss: 0.002512766048312187, Val Loss: 0.010100394487380981\n",
      "Epoch 495, Loss: 0.002499987604096532, Val Loss: 0.010090932250022888\n",
      "Epoch 496, Loss: 0.002487197518348694, Val Loss: 0.010087874718010426\n",
      "Epoch 497, Loss: 0.002474487293511629, Val Loss: 0.010081981308758259\n",
      "Epoch 498, Loss: 0.002461901633068919, Val Loss: 0.010076489299535751\n",
      "Epoch 499, Loss: 0.002449201652780175, Val Loss: 0.010071913711726665\n",
      "Epoch 500, Loss: 0.0024364872369915247, Val Loss: 0.010064364410936832\n",
      "Epoch 501, Loss: 0.0024238487239927053, Val Loss: 0.010060160420835018\n",
      "Epoch 502, Loss: 0.002411260036751628, Val Loss: 0.010051514022052288\n",
      "Epoch 503, Loss: 0.002398728160187602, Val Loss: 0.010048787109553814\n",
      "Epoch 504, Loss: 0.0023863129317760468, Val Loss: 0.010039976797997952\n",
      "Epoch 505, Loss: 0.0023737687151879072, Val Loss: 0.010038369335234165\n",
      "Epoch 506, Loss: 0.002361464314162731, Val Loss: 0.010028313845396042\n",
      "Epoch 507, Loss: 0.0023491187021136284, Val Loss: 0.010026133619248867\n",
      "Epoch 508, Loss: 0.002337043173611164, Val Loss: 0.01001697313040495\n",
      "Epoch 509, Loss: 0.002324888715520501, Val Loss: 0.01001447718590498\n",
      "Epoch 510, Loss: 0.0023128469474613667, Val Loss: 0.010006772354245186\n",
      "Epoch 511, Loss: 0.002300866646692157, Val Loss: 0.010001802816987038\n",
      "Epoch 512, Loss: 0.0022889073006808758, Val Loss: 0.009995692409574986\n",
      "Epoch 513, Loss: 0.00227698078379035, Val Loss: 0.009990043938159943\n",
      "Epoch 514, Loss: 0.002265156712383032, Val Loss: 0.009985207580029964\n",
      "Epoch 515, Loss: 0.0022534101735800505, Val Loss: 0.009978609159588814\n",
      "Epoch 516, Loss: 0.0022415914572775364, Val Loss: 0.009975829161703587\n",
      "Epoch 517, Loss: 0.0022297955583781004, Val Loss: 0.009968799538910389\n",
      "Epoch 518, Loss: 0.002218100242316723, Val Loss: 0.009968343190848827\n",
      "Epoch 519, Loss: 0.002206620294600725, Val Loss: 0.009958510287106037\n",
      "Epoch 520, Loss: 0.00219540111720562, Val Loss: 0.009958701208233833\n",
      "Epoch 521, Loss: 0.0021843065042048693, Val Loss: 0.009949696250259876\n",
      "Epoch 522, Loss: 0.0021734717302024364, Val Loss: 0.00994973722845316\n",
      "Epoch 523, Loss: 0.0021628043614327908, Val Loss: 0.009939520619809628\n",
      "Epoch 524, Loss: 0.002152184024453163, Val Loss: 0.009938628412783146\n",
      "Epoch 525, Loss: 0.0021416188683360815, Val Loss: 0.009928420186042786\n",
      "Epoch 526, Loss: 0.0021311917807906866, Val Loss: 0.009929059073328972\n",
      "Epoch 527, Loss: 0.002120784018188715, Val Loss: 0.00991878379136324\n",
      "Epoch 528, Loss: 0.0021103844046592712, Val Loss: 0.009920540265738964\n",
      "Epoch 529, Loss: 0.002100166864693165, Val Loss: 0.009909186512231827\n",
      "Epoch 530, Loss: 0.0020899877417832613, Val Loss: 0.009911510162055492\n",
      "Epoch 531, Loss: 0.002079950412735343, Val Loss: 0.009900631383061409\n",
      "Epoch 532, Loss: 0.0020698343869298697, Val Loss: 0.009902169927954674\n",
      "Epoch 533, Loss: 0.002059798687696457, Val Loss: 0.009889454580843449\n",
      "Epoch 534, Loss: 0.002049771137535572, Val Loss: 0.009892852045595646\n",
      "Epoch 535, Loss: 0.0020399042405188084, Val Loss: 0.00988061260432005\n",
      "Epoch 536, Loss: 0.002030004281550646, Val Loss: 0.009884025901556015\n",
      "Epoch 537, Loss: 0.002020191168412566, Val Loss: 0.009871662594377995\n",
      "Epoch 538, Loss: 0.002010381082072854, Val Loss: 0.009875303134322166\n",
      "Epoch 539, Loss: 0.002000642940402031, Val Loss: 0.009862861596047878\n",
      "Epoch 540, Loss: 0.0019910295959562063, Val Loss: 0.009867534972727299\n",
      "Epoch 541, Loss: 0.001981403911486268, Val Loss: 0.009855407290160656\n",
      "Epoch 542, Loss: 0.001971928868442774, Val Loss: 0.009861317463219166\n",
      "Epoch 543, Loss: 0.0019624584820121527, Val Loss: 0.009847457520663738\n",
      "Epoch 544, Loss: 0.0019529175478965044, Val Loss: 0.009852319955825806\n",
      "Epoch 545, Loss: 0.0019434827845543623, Val Loss: 0.009837206453084946\n",
      "Epoch 546, Loss: 0.001934064901433885, Val Loss: 0.0098448246717453\n",
      "Epoch 547, Loss: 0.0019247038289904594, Val Loss: 0.00983032863587141\n",
      "Epoch 548, Loss: 0.0019153793109580874, Val Loss: 0.009838945232331753\n",
      "Epoch 549, Loss: 0.0019061919301748276, Val Loss: 0.009822332300245762\n",
      "Epoch 550, Loss: 0.0018970087403431535, Val Loss: 0.009830774739384651\n",
      "Epoch 551, Loss: 0.0018879080889746547, Val Loss: 0.009814715012907982\n",
      "Epoch 552, Loss: 0.0018788145389407873, Val Loss: 0.009822281077504158\n",
      "Epoch 553, Loss: 0.0018697651103138924, Val Loss: 0.009807449765503407\n",
      "Epoch 554, Loss: 0.0018607648089528084, Val Loss: 0.0098155802115798\n",
      "Epoch 555, Loss: 0.001851815264672041, Val Loss: 0.00980118103325367\n",
      "Epoch 556, Loss: 0.001842809608206153, Val Loss: 0.009808380156755447\n",
      "Epoch 557, Loss: 0.0018339003436267376, Val Loss: 0.009793790057301521\n",
      "Epoch 558, Loss: 0.0018250826979056, Val Loss: 0.009800391271710396\n",
      "Epoch 559, Loss: 0.0018162730848416686, Val Loss: 0.00978686474263668\n",
      "Epoch 560, Loss: 0.0018074066611006856, Val Loss: 0.009792905300855637\n",
      "Epoch 561, Loss: 0.0017986749298870564, Val Loss: 0.00977999996393919\n",
      "Epoch 562, Loss: 0.001790145761333406, Val Loss: 0.009785245172679424\n",
      "Epoch 563, Loss: 0.0017816706094890833, Val Loss: 0.009774411097168922\n",
      "Epoch 564, Loss: 0.0017730392282828689, Val Loss: 0.009778390638530254\n",
      "Epoch 565, Loss: 0.001764595741406083, Val Loss: 0.009768241085112095\n",
      "Epoch 566, Loss: 0.0017563309520483017, Val Loss: 0.009770198725163937\n",
      "Epoch 567, Loss: 0.0017479313537478447, Val Loss: 0.009762817062437534\n",
      "Epoch 568, Loss: 0.00173970649484545, Val Loss: 0.009762708097696304\n",
      "Epoch 569, Loss: 0.0017315043369308114, Val Loss: 0.009756837971508503\n",
      "Epoch 570, Loss: 0.0017232417594641447, Val Loss: 0.009754742495715618\n",
      "Epoch 571, Loss: 0.0017151767387986183, Val Loss: 0.00975046120584011\n",
      "Epoch 572, Loss: 0.0017070512985810637, Val Loss: 0.009747869335114956\n",
      "Epoch 573, Loss: 0.001699049142189324, Val Loss: 0.009744872339069843\n",
      "Epoch 574, Loss: 0.0016911730635911226, Val Loss: 0.009741303510963917\n",
      "Epoch 575, Loss: 0.0016832581022754312, Val Loss: 0.009738697670400143\n",
      "Epoch 576, Loss: 0.001675390056334436, Val Loss: 0.009733720682561398\n",
      "Epoch 577, Loss: 0.0016675523947924376, Val Loss: 0.009731579571962357\n",
      "Epoch 578, Loss: 0.0016597699141129851, Val Loss: 0.009726148098707199\n",
      "Epoch 579, Loss: 0.0016520463395863771, Val Loss: 0.009727198630571365\n",
      "Epoch 580, Loss: 0.0016441872576251626, Val Loss: 0.009721282869577408\n",
      "Epoch 581, Loss: 0.0016365130431950092, Val Loss: 0.009722648188471794\n",
      "Epoch 582, Loss: 0.0016288978513330221, Val Loss: 0.009715238586068153\n",
      "Epoch 583, Loss: 0.0016212647315114737, Val Loss: 0.009715928696095943\n",
      "Epoch 584, Loss: 0.0016136933118104935, Val Loss: 0.009710581041872501\n",
      "Epoch 585, Loss: 0.0016059835907071829, Val Loss: 0.009707571938633919\n",
      "Epoch 586, Loss: 0.0015981431351974607, Val Loss: 0.0097043476998806\n",
      "Epoch 587, Loss: 0.001590165076777339, Val Loss: 0.009701184928417206\n",
      "Epoch 588, Loss: 0.0015820814296603203, Val Loss: 0.00969729945063591\n",
      "Epoch 589, Loss: 0.0015736735658720136, Val Loss: 0.009693835861980915\n",
      "Epoch 590, Loss: 0.0015658076154068112, Val Loss: 0.009691509418189526\n",
      "Epoch 591, Loss: 0.0015582696069031954, Val Loss: 0.009688175283372402\n",
      "Epoch 592, Loss: 0.0015508841024711728, Val Loss: 0.009686717763543129\n",
      "Epoch 593, Loss: 0.0015436522662639618, Val Loss: 0.00968296267092228\n",
      "Epoch 594, Loss: 0.0015365115832537413, Val Loss: 0.009681181982159615\n",
      "Epoch 595, Loss: 0.001529422588646412, Val Loss: 0.009676165878772736\n",
      "Epoch 596, Loss: 0.001522252568975091, Val Loss: 0.009675072506070137\n",
      "Epoch 597, Loss: 0.001515079289674759, Val Loss: 0.009672141633927822\n",
      "Epoch 598, Loss: 0.0015080699231475592, Val Loss: 0.009666944853961468\n",
      "Epoch 599, Loss: 0.0015011340146884322, Val Loss: 0.009667440317571163\n",
      "Epoch 600, Loss: 0.0014940999681130052, Val Loss: 0.009661253541707993\n",
      "Epoch 601, Loss: 0.0014871022431179881, Val Loss: 0.009663119912147522\n",
      "Epoch 602, Loss: 0.0014801950892433524, Val Loss: 0.009657771326601505\n",
      "Epoch 603, Loss: 0.0014731795527040958, Val Loss: 0.00965963676571846\n",
      "Epoch 604, Loss: 0.0014663273468613625, Val Loss: 0.009652388282120228\n",
      "Epoch 605, Loss: 0.0014594030799344182, Val Loss: 0.009655723348259926\n",
      "Epoch 606, Loss: 0.0014526183949783444, Val Loss: 0.009648614563047886\n",
      "Epoch 607, Loss: 0.0014458223013207316, Val Loss: 0.009651150554418564\n",
      "Epoch 608, Loss: 0.001439046929590404, Val Loss: 0.009645740501582623\n",
      "Epoch 609, Loss: 0.0014322949573397636, Val Loss: 0.009645131416618824\n",
      "Epoch 610, Loss: 0.0014256361173465848, Val Loss: 0.009643258526921272\n",
      "Epoch 611, Loss: 0.0014190830988809466, Val Loss: 0.009640033356845379\n",
      "Epoch 612, Loss: 0.001412539859302342, Val Loss: 0.009641441516578197\n",
      "Epoch 613, Loss: 0.0014060164103284478, Val Loss: 0.009635848924517632\n",
      "Epoch 614, Loss: 0.0013995629269629717, Val Loss: 0.009636051952838898\n",
      "Epoch 615, Loss: 0.0013930720742791891, Val Loss: 0.009630314074456692\n",
      "Epoch 616, Loss: 0.0013866377994418144, Val Loss: 0.009633592329919338\n",
      "Epoch 617, Loss: 0.0013802474131807685, Val Loss: 0.009628222323954105\n",
      "Epoch 618, Loss: 0.0013739803107455373, Val Loss: 0.00962987169623375\n",
      "Epoch 619, Loss: 0.0013676396338269114, Val Loss: 0.009626134298741817\n",
      "Epoch 620, Loss: 0.001361320959404111, Val Loss: 0.009625773876905441\n",
      "Epoch 621, Loss: 0.0013550176518037915, Val Loss: 0.009623024612665176\n",
      "Epoch 622, Loss: 0.001348765566945076, Val Loss: 0.00962146557867527\n",
      "Epoch 623, Loss: 0.0013425147626549006, Val Loss: 0.009619643911719322\n",
      "Epoch 624, Loss: 0.0013363283360376954, Val Loss: 0.009616907685995102\n",
      "Epoch 625, Loss: 0.001330027007497847, Val Loss: 0.009616276249289513\n",
      "Epoch 626, Loss: 0.0013239331310614944, Val Loss: 0.009613984264433384\n",
      "Epoch 627, Loss: 0.0013177868677303195, Val Loss: 0.009613334201276302\n",
      "Epoch 628, Loss: 0.0013115749461576343, Val Loss: 0.009610244072973728\n",
      "Epoch 629, Loss: 0.0013053547590970993, Val Loss: 0.009608753025531769\n",
      "Epoch 630, Loss: 0.0012992533156648278, Val Loss: 0.009607107378542423\n",
      "Epoch 631, Loss: 0.0012931199744343758, Val Loss: 0.009605232626199722\n",
      "Epoch 632, Loss: 0.001286902348510921, Val Loss: 0.009604496881365776\n",
      "Epoch 633, Loss: 0.0012808454921469092, Val Loss: 0.009603619575500488\n",
      "Epoch 634, Loss: 0.0012746814172714949, Val Loss: 0.00960304494947195\n",
      "Epoch 635, Loss: 0.0012685757828876376, Val Loss: 0.009599432349205017\n",
      "Epoch 636, Loss: 0.001262311707250774, Val Loss: 0.009598773904144764\n",
      "Epoch 637, Loss: 0.0012561827898025513, Val Loss: 0.009595689363777637\n",
      "Epoch 638, Loss: 0.0012498735450208187, Val Loss: 0.009596220217645168\n",
      "Epoch 639, Loss: 0.0012436803663149476, Val Loss: 0.009592557325959206\n",
      "Epoch 640, Loss: 0.0012374857906252146, Val Loss: 0.009591750800609589\n",
      "Epoch 641, Loss: 0.0012312865583226085, Val Loss: 0.00959005206823349\n",
      "Epoch 642, Loss: 0.0012251404114067554, Val Loss: 0.00958945695310831\n",
      "Epoch 643, Loss: 0.0012191312853246927, Val Loss: 0.009586812928318977\n",
      "Epoch 644, Loss: 0.0012131247203797102, Val Loss: 0.009584559127688408\n",
      "Epoch 645, Loss: 0.0012072545941919088, Val Loss: 0.009584282524883747\n",
      "Epoch 646, Loss: 0.001201456761918962, Val Loss: 0.009581172838807106\n",
      "Epoch 647, Loss: 0.0011957824463024735, Val Loss: 0.009580975398421288\n",
      "Epoch 648, Loss: 0.001190154580399394, Val Loss: 0.009580392390489578\n",
      "Epoch 649, Loss: 0.0011846749112010002, Val Loss: 0.009579041972756386\n",
      "Epoch 650, Loss: 0.001179252052679658, Val Loss: 0.009577560238540173\n"
     ]
    }
   ],
   "source": [
    "from flashrag.prompt import PromptTemplate\n",
    "import pickle\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "config_taqa = set_config(dataset_name='taqa', split = split)\n",
    "template_taqa= PromptTemplate(\n",
    "        config = config_taqa,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "config_triviaqa= set_config(dataset_name='triviaqa', split = split)\n",
    "template_triviaqa= PromptTemplate(\n",
    "        config = config_triviaqa,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "\n",
    "\n",
    "loading_hidden = True\n",
    "if not loading_hidden:\n",
    "    taqa_hidden_states, triviaqa_hidden_states = load_datasets(config_taqa, config_triviaqa, template_taqa, template_triviaqa)\n",
    "else:\n",
    "    file_path_triviaqa = '/cs/student/projects2/dsml/cdiezmar/hidden_states/triviaqa.pkl'\n",
    "    file_path_taqa = '/cs/student/projects2/dsml/cdiezmar/hidden_states/taqa.pkl'\n",
    "\n",
    "    with open(file_path_triviaqa, 'rb') as file_triviaqa:\n",
    "        triviaqa_hidden_states = pickle.load(file_triviaqa)\n",
    "        print('Length triviaqa: ', len(triviaqa_hidden_states))\n",
    "        triviaqa_hidden_states_tensor= torch.tensor(triviaqa_hidden_states, dtype=torch.float32)\n",
    "    with open(file_path_taqa, 'rb') as file_taqa:\n",
    "        taqa_hidden_states = pickle.load(file_taqa)\n",
    "        print('Length taqa: ', len(taqa_hidden_states))\n",
    "        taqa_hidden_states_tensor= torch.tensor(taqa_hidden_states, dtype=torch.float32)\n",
    "train_inputs, train_labels, val_inputs, val_labels = prepare_data(taqa_hidden_states_tensor, triviaqa_hidden_states_tensor)\n",
    "\n",
    "input_size = train_inputs.size(1)\n",
    "num_classes = 2  # Time-sensitive or not\n",
    "\n",
    "# RNN Parameters\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "epochs = 650\n",
    "lr = 1e-4\n",
    "\n",
    "# Train GRU Classifier\n",
    "rnn_model = train_rnn_classifier_cuda(train_inputs, train_labels, val_inputs, val_labels, input_size, hidden_size, num_layers, num_classes, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `question` in template\n",
      "Find `question` in template\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length triviaqa:  78785\n",
      "Length taqa:  10148\n",
      "Epoch 1, Loss: 0.717808187007904, Val Loss: 0.5694388747215271\n",
      "Epoch 2, Loss: 0.5713568925857544, Val Loss: 0.4750271439552307\n",
      "Epoch 3, Loss: 0.4786933958530426, Val Loss: 0.4181862473487854\n",
      "Epoch 4, Loss: 0.42348888516426086, Val Loss: 0.38501614332199097\n",
      "Epoch 5, Loss: 0.3917642831802368, Val Loss: 0.36577853560447693\n",
      "Epoch 6, Loss: 0.3737366199493408, Val Loss: 0.3546333611011505\n",
      "Epoch 7, Loss: 0.36359703540802, Val Loss: 0.3483542501926422\n",
      "Epoch 8, Loss: 0.3581359386444092, Val Loss: 0.3449462652206421\n",
      "Epoch 9, Loss: 0.3553542494773865, Val Loss: 0.3428073525428772\n",
      "Epoch 10, Loss: 0.353658527135849, Val Loss: 0.3406047224998474\n",
      "Epoch 11, Loss: 0.3517110049724579, Val Loss: 0.3376754820346832\n",
      "Epoch 12, Loss: 0.34886470437049866, Val Loss: 0.33398741483688354\n",
      "Epoch 13, Loss: 0.34509485960006714, Val Loss: 0.32977601885795593\n",
      "Epoch 14, Loss: 0.34066203236579895, Val Loss: 0.3252825140953064\n",
      "Epoch 15, Loss: 0.33586785197257996, Val Loss: 0.32075467705726624\n",
      "Epoch 16, Loss: 0.3310061991214752, Val Loss: 0.3163810670375824\n",
      "Epoch 17, Loss: 0.32625967264175415, Val Loss: 0.3121638894081116\n",
      "Epoch 18, Loss: 0.3216114342212677, Val Loss: 0.30783432722091675\n",
      "Epoch 19, Loss: 0.31680330634117126, Val Loss: 0.3030797839164734\n",
      "Epoch 20, Loss: 0.31156125664711, Val Loss: 0.2978270947933197\n",
      "Epoch 21, Loss: 0.3058214485645294, Val Loss: 0.2923586368560791\n",
      "Epoch 22, Loss: 0.29990145564079285, Val Loss: 0.2866687476634979\n",
      "Epoch 23, Loss: 0.29379066824913025, Val Loss: 0.2806439995765686\n",
      "Epoch 24, Loss: 0.2873484194278717, Val Loss: 0.2746725082397461\n",
      "Epoch 25, Loss: 0.2810245752334595, Val Loss: 0.2692345380783081\n",
      "Epoch 26, Loss: 0.2753371000289917, Val Loss: 0.26406821608543396\n",
      "Epoch 27, Loss: 0.2699449062347412, Val Loss: 0.258583664894104\n",
      "Epoch 28, Loss: 0.2641405463218689, Val Loss: 0.25248682498931885\n",
      "Epoch 29, Loss: 0.25761085748672485, Val Loss: 0.24596966803073883\n",
      "Epoch 30, Loss: 0.25059178471565247, Val Loss: 0.2394842654466629\n",
      "Epoch 31, Loss: 0.243580624461174, Val Loss: 0.23321843147277832\n",
      "Epoch 32, Loss: 0.23679643869400024, Val Loss: 0.22699597477912903\n",
      "Epoch 33, Loss: 0.23009100556373596, Val Loss: 0.220502570271492\n",
      "Epoch 34, Loss: 0.22317524254322052, Val Loss: 0.21358837187290192\n",
      "Epoch 35, Loss: 0.2159285694360733, Val Loss: 0.2063695639371872\n",
      "Epoch 36, Loss: 0.20845206081867218, Val Loss: 0.19909484684467316\n",
      "Epoch 37, Loss: 0.20091821253299713, Val Loss: 0.19195130467414856\n",
      "Epoch 38, Loss: 0.19345584511756897, Val Loss: 0.1849478930234909\n",
      "Epoch 39, Loss: 0.18607228994369507, Val Loss: 0.17807340621948242\n",
      "Epoch 40, Loss: 0.17878153920173645, Val Loss: 0.17129135131835938\n",
      "Epoch 41, Loss: 0.17159754037857056, Val Loss: 0.16467830538749695\n",
      "Epoch 42, Loss: 0.164632648229599, Val Loss: 0.15833337604999542\n",
      "Epoch 43, Loss: 0.15799309313297272, Val Loss: 0.152137890458107\n",
      "Epoch 44, Loss: 0.15152348577976227, Val Loss: 0.14601658284664154\n",
      "Epoch 45, Loss: 0.14515282213687897, Val Loss: 0.14005960524082184\n",
      "Epoch 46, Loss: 0.1389504373073578, Val Loss: 0.134436696767807\n",
      "Epoch 47, Loss: 0.13308674097061157, Val Loss: 0.1292097419500351\n",
      "Epoch 48, Loss: 0.12766000628471375, Val Loss: 0.12431015819311142\n",
      "Epoch 49, Loss: 0.1225619688630104, Val Loss: 0.11978147178888321\n",
      "Epoch 50, Loss: 0.11783677339553833, Val Loss: 0.11561089009046555\n",
      "Epoch 51, Loss: 0.11347982287406921, Val Loss: 0.11171410977840424\n",
      "Epoch 52, Loss: 0.10941857844591141, Val Loss: 0.1080237478017807\n",
      "Epoch 53, Loss: 0.10558880120515823, Val Loss: 0.10450371354818344\n",
      "Epoch 54, Loss: 0.1019572839140892, Val Loss: 0.10116715729236603\n",
      "Epoch 55, Loss: 0.09854359924793243, Val Loss: 0.09805643558502197\n",
      "Epoch 56, Loss: 0.09537433087825775, Val Loss: 0.09518752247095108\n",
      "Epoch 57, Loss: 0.09244228899478912, Val Loss: 0.09251828491687775\n",
      "Epoch 58, Loss: 0.08969923853874207, Val Loss: 0.0900028795003891\n",
      "Epoch 59, Loss: 0.08710167557001114, Val Loss: 0.08759437501430511\n",
      "Epoch 60, Loss: 0.08460427820682526, Val Loss: 0.08527161926031113\n",
      "Epoch 61, Loss: 0.08219032734632492, Val Loss: 0.08303473889827728\n",
      "Epoch 62, Loss: 0.07986002415418625, Val Loss: 0.08087828755378723\n",
      "Epoch 63, Loss: 0.0776076391339302, Val Loss: 0.07880181074142456\n",
      "Epoch 64, Loss: 0.07543009519577026, Val Loss: 0.07678154855966568\n",
      "Epoch 65, Loss: 0.07331791520118713, Val Loss: 0.07479418814182281\n",
      "Epoch 66, Loss: 0.07125750929117203, Val Loss: 0.07282322645187378\n",
      "Epoch 67, Loss: 0.069239042699337, Val Loss: 0.07089147716760635\n",
      "Epoch 68, Loss: 0.06727449595928192, Val Loss: 0.06903955340385437\n",
      "Epoch 69, Loss: 0.06539276242256165, Val Loss: 0.06723728030920029\n",
      "Epoch 70, Loss: 0.06356247514486313, Val Loss: 0.06547559797763824\n",
      "Epoch 71, Loss: 0.06176847591996193, Val Loss: 0.06375624239444733\n",
      "Epoch 72, Loss: 0.06001274287700653, Val Loss: 0.062077321112155914\n",
      "Epoch 73, Loss: 0.05829896777868271, Val Loss: 0.06044630706310272\n",
      "Epoch 74, Loss: 0.05663212016224861, Val Loss: 0.058865711092948914\n",
      "Epoch 75, Loss: 0.055024176836013794, Val Loss: 0.057329870760440826\n",
      "Epoch 76, Loss: 0.053463686257600784, Val Loss: 0.05583704635500908\n",
      "Epoch 77, Loss: 0.051951926201581955, Val Loss: 0.05439874529838562\n",
      "Epoch 78, Loss: 0.05049585923552513, Val Loss: 0.053013499826192856\n",
      "Epoch 79, Loss: 0.04909259453415871, Val Loss: 0.05168300122022629\n",
      "Epoch 80, Loss: 0.0477425754070282, Val Loss: 0.05039246752858162\n",
      "Epoch 81, Loss: 0.04643883928656578, Val Loss: 0.04915149509906769\n",
      "Epoch 82, Loss: 0.04518083482980728, Val Loss: 0.04795075207948685\n",
      "Epoch 83, Loss: 0.04396045207977295, Val Loss: 0.04678816348314285\n",
      "Epoch 84, Loss: 0.04277908429503441, Val Loss: 0.04566667601466179\n",
      "Epoch 85, Loss: 0.04164482653141022, Val Loss: 0.044585514813661575\n",
      "Epoch 86, Loss: 0.04055703803896904, Val Loss: 0.0435556136071682\n",
      "Epoch 87, Loss: 0.03951264172792435, Val Loss: 0.04256594553589821\n",
      "Epoch 88, Loss: 0.03850674629211426, Val Loss: 0.04161832481622696\n",
      "Epoch 89, Loss: 0.03753506392240524, Val Loss: 0.04072115942835808\n",
      "Epoch 90, Loss: 0.0366034097969532, Val Loss: 0.039860665798187256\n",
      "Epoch 91, Loss: 0.035706888884305954, Val Loss: 0.039034757763147354\n",
      "Epoch 92, Loss: 0.0348440445959568, Val Loss: 0.03824763745069504\n",
      "Epoch 93, Loss: 0.034015145152807236, Val Loss: 0.037486717104911804\n",
      "Epoch 94, Loss: 0.03321944922208786, Val Loss: 0.03675660863518715\n",
      "Epoch 95, Loss: 0.03245564550161362, Val Loss: 0.03605739399790764\n",
      "Epoch 96, Loss: 0.031718380749225616, Val Loss: 0.03538072854280472\n",
      "Epoch 97, Loss: 0.031007125973701477, Val Loss: 0.034728940576314926\n",
      "Epoch 98, Loss: 0.030323144048452377, Val Loss: 0.03409571200609207\n",
      "Epoch 99, Loss: 0.029663847759366035, Val Loss: 0.03350168094038963\n",
      "Epoch 100, Loss: 0.029026787728071213, Val Loss: 0.03291911259293556\n",
      "Epoch 101, Loss: 0.028411343693733215, Val Loss: 0.03236160799860954\n",
      "Epoch 102, Loss: 0.02781580574810505, Val Loss: 0.03183110058307648\n",
      "Epoch 103, Loss: 0.027241213247179985, Val Loss: 0.03132210299372673\n",
      "Epoch 104, Loss: 0.02668602764606476, Val Loss: 0.030818210914731026\n",
      "Epoch 105, Loss: 0.02614891156554222, Val Loss: 0.030348271131515503\n",
      "Epoch 106, Loss: 0.025625215843319893, Val Loss: 0.029897574335336685\n",
      "Epoch 107, Loss: 0.02511952444911003, Val Loss: 0.029440078884363174\n",
      "Epoch 108, Loss: 0.024631928652524948, Val Loss: 0.02902296744287014\n",
      "Epoch 109, Loss: 0.024167219176888466, Val Loss: 0.02860519103705883\n",
      "Epoch 110, Loss: 0.023716716095805168, Val Loss: 0.02819289267063141\n",
      "Epoch 111, Loss: 0.023275673389434814, Val Loss: 0.027810469269752502\n",
      "Epoch 112, Loss: 0.022845124825835228, Val Loss: 0.027431312948465347\n",
      "Epoch 113, Loss: 0.022428084164857864, Val Loss: 0.02707045152783394\n",
      "Epoch 114, Loss: 0.02202545665204525, Val Loss: 0.026730241253972054\n",
      "Epoch 115, Loss: 0.021635791286826134, Val Loss: 0.026390885934233665\n",
      "Epoch 116, Loss: 0.021258220076560974, Val Loss: 0.026067141443490982\n",
      "Epoch 117, Loss: 0.020889440551400185, Val Loss: 0.02575482428073883\n",
      "Epoch 118, Loss: 0.020529627799987793, Val Loss: 0.02543346770107746\n",
      "Epoch 119, Loss: 0.020178521052002907, Val Loss: 0.025128545239567757\n",
      "Epoch 120, Loss: 0.01983884535729885, Val Loss: 0.024831825867295265\n",
      "Epoch 121, Loss: 0.01950886845588684, Val Loss: 0.024521512910723686\n",
      "Epoch 122, Loss: 0.01918899640440941, Val Loss: 0.024247489869594574\n",
      "Epoch 123, Loss: 0.018875127658247948, Val Loss: 0.023965613916516304\n",
      "Epoch 124, Loss: 0.018569761887192726, Val Loss: 0.023692045360803604\n",
      "Epoch 125, Loss: 0.0182720385491848, Val Loss: 0.023457013070583344\n",
      "Epoch 126, Loss: 0.017983319237828255, Val Loss: 0.02318517677485943\n",
      "Epoch 127, Loss: 0.017703454941511154, Val Loss: 0.02296862006187439\n",
      "Epoch 128, Loss: 0.017427824437618256, Val Loss: 0.022715259343385696\n",
      "Epoch 129, Loss: 0.0171583890914917, Val Loss: 0.02248430997133255\n",
      "Epoch 130, Loss: 0.01689676195383072, Val Loss: 0.02226453460752964\n",
      "Epoch 131, Loss: 0.016642054542899132, Val Loss: 0.022022617980837822\n",
      "Epoch 132, Loss: 0.016394617035984993, Val Loss: 0.021820513531565666\n",
      "Epoch 133, Loss: 0.016152676194906235, Val Loss: 0.021592076867818832\n",
      "Epoch 134, Loss: 0.015916336327791214, Val Loss: 0.0213934276252985\n",
      "Epoch 135, Loss: 0.015685733407735825, Val Loss: 0.021187549456954002\n",
      "Epoch 136, Loss: 0.015460164286196232, Val Loss: 0.02099040523171425\n",
      "Epoch 137, Loss: 0.015240850858390331, Val Loss: 0.020805716514587402\n",
      "Epoch 138, Loss: 0.015026615932583809, Val Loss: 0.020605789497494698\n",
      "Epoch 139, Loss: 0.014816741459071636, Val Loss: 0.020433466881513596\n",
      "Epoch 140, Loss: 0.014611121267080307, Val Loss: 0.020241964608430862\n",
      "Epoch 141, Loss: 0.014410310424864292, Val Loss: 0.02007247507572174\n",
      "Epoch 142, Loss: 0.014213706366717815, Val Loss: 0.019898710772395134\n",
      "Epoch 143, Loss: 0.014020668342709541, Val Loss: 0.019730344414711\n",
      "Epoch 144, Loss: 0.013833021745085716, Val Loss: 0.019573500379920006\n",
      "Epoch 145, Loss: 0.013649323023855686, Val Loss: 0.019405122846364975\n",
      "Epoch 146, Loss: 0.013469264842569828, Val Loss: 0.019263533875346184\n",
      "Epoch 147, Loss: 0.013293222524225712, Val Loss: 0.01909429207444191\n",
      "Epoch 148, Loss: 0.013120717369019985, Val Loss: 0.01896749436855316\n",
      "Epoch 149, Loss: 0.012952573597431183, Val Loss: 0.018797392025589943\n",
      "Epoch 150, Loss: 0.012788957916200161, Val Loss: 0.018698174506425858\n",
      "Epoch 151, Loss: 0.012631596066057682, Val Loss: 0.018511775881052017\n",
      "Epoch 152, Loss: 0.012480882927775383, Val Loss: 0.018458029255270958\n",
      "Epoch 153, Loss: 0.012335333973169327, Val Loss: 0.018249858170747757\n",
      "Epoch 154, Loss: 0.01218870747834444, Val Loss: 0.018189242109656334\n",
      "Epoch 155, Loss: 0.012032733298838139, Val Loss: 0.01799720898270607\n",
      "Epoch 156, Loss: 0.011873344890773296, Val Loss: 0.017880287021398544\n",
      "Epoch 157, Loss: 0.011728965677320957, Val Loss: 0.01779910922050476\n",
      "Epoch 158, Loss: 0.011600184254348278, Val Loss: 0.0176361296325922\n",
      "Epoch 159, Loss: 0.011471882462501526, Val Loss: 0.01757211424410343\n",
      "Epoch 160, Loss: 0.011334624141454697, Val Loss: 0.017415978014469147\n",
      "Epoch 161, Loss: 0.01119526568800211, Val Loss: 0.017309071496129036\n",
      "Epoch 162, Loss: 0.01106703095138073, Val Loss: 0.017239224165678024\n",
      "Epoch 163, Loss: 0.010949584655463696, Val Loss: 0.0170944444835186\n",
      "Epoch 164, Loss: 0.010830777697265148, Val Loss: 0.017027750611305237\n",
      "Epoch 165, Loss: 0.010705896653234959, Val Loss: 0.016893917694687843\n",
      "Epoch 166, Loss: 0.01058129407465458, Val Loss: 0.0167925376445055\n",
      "Epoch 167, Loss: 0.010465848259627819, Val Loss: 0.016729341819882393\n",
      "Epoch 168, Loss: 0.010357714258134365, Val Loss: 0.0165999922901392\n",
      "Epoch 169, Loss: 0.010247931815683842, Val Loss: 0.016540013253688812\n",
      "Epoch 170, Loss: 0.010133357718586922, Val Loss: 0.016421005129814148\n",
      "Epoch 171, Loss: 0.01001882553100586, Val Loss: 0.016336118802428246\n",
      "Epoch 172, Loss: 0.009909742511808872, Val Loss: 0.01626531407237053\n",
      "Epoch 173, Loss: 0.009807120077311993, Val Loss: 0.01615087501704693\n",
      "Epoch 174, Loss: 0.009706206619739532, Val Loss: 0.016084447503089905\n",
      "Epoch 175, Loss: 0.009604302234947681, Val Loss: 0.015974583104252815\n",
      "Epoch 176, Loss: 0.00950104184448719, Val Loss: 0.01590769551694393\n",
      "Epoch 177, Loss: 0.00939891766756773, Val Loss: 0.015827953815460205\n",
      "Epoch 178, Loss: 0.009300455451011658, Val Loss: 0.015748919919133186\n",
      "Epoch 179, Loss: 0.00920554157346487, Val Loss: 0.015684593468904495\n",
      "Epoch 180, Loss: 0.009113370440900326, Val Loss: 0.01559061836451292\n",
      "Epoch 181, Loss: 0.009020435623824596, Val Loss: 0.015524345450103283\n",
      "Epoch 182, Loss: 0.00892879068851471, Val Loss: 0.015424061566591263\n",
      "Epoch 183, Loss: 0.008838511072099209, Val Loss: 0.015355581417679787\n",
      "Epoch 184, Loss: 0.008749669417738914, Val Loss: 0.015273223631083965\n",
      "Epoch 185, Loss: 0.008660905994474888, Val Loss: 0.015212616883218288\n",
      "Epoch 186, Loss: 0.008573440834879875, Val Loss: 0.015148092061281204\n",
      "Epoch 187, Loss: 0.008488570339977741, Val Loss: 0.015081822872161865\n",
      "Epoch 188, Loss: 0.008405664004385471, Val Loss: 0.015022022649645805\n",
      "Epoch 189, Loss: 0.00832421611994505, Val Loss: 0.014944839291274548\n",
      "Epoch 190, Loss: 0.008244149386882782, Val Loss: 0.014889858663082123\n",
      "Epoch 191, Loss: 0.008166286163032055, Val Loss: 0.014812363311648369\n",
      "Epoch 192, Loss: 0.00809080433100462, Val Loss: 0.014773416332900524\n",
      "Epoch 193, Loss: 0.008016775362193584, Val Loss: 0.01470049750059843\n",
      "Epoch 194, Loss: 0.007943440228700638, Val Loss: 0.01467170100659132\n",
      "Epoch 195, Loss: 0.007870340719819069, Val Loss: 0.014596100896596909\n",
      "Epoch 196, Loss: 0.007797484286129475, Val Loss: 0.014560841023921967\n",
      "Epoch 197, Loss: 0.007723165675997734, Val Loss: 0.01447900477796793\n",
      "Epoch 198, Loss: 0.007647071499377489, Val Loss: 0.014424588531255722\n",
      "Epoch 199, Loss: 0.007569822948426008, Val Loss: 0.014349019154906273\n",
      "Epoch 200, Loss: 0.007494629360735416, Val Loss: 0.01428903453052044\n",
      "Epoch 201, Loss: 0.0074247620068490505, Val Loss: 0.014237586408853531\n",
      "Epoch 202, Loss: 0.007358989678323269, Val Loss: 0.014184028841555119\n",
      "Epoch 203, Loss: 0.007296362426131964, Val Loss: 0.01415085606276989\n",
      "Epoch 204, Loss: 0.007235510740429163, Val Loss: 0.014099887572228909\n",
      "Epoch 205, Loss: 0.007175112143158913, Val Loss: 0.014066828414797783\n",
      "Epoch 206, Loss: 0.007113794796168804, Val Loss: 0.014005433768033981\n",
      "Epoch 207, Loss: 0.007049731910228729, Val Loss: 0.013959073461592197\n",
      "Epoch 208, Loss: 0.006983187515288591, Val Loss: 0.013893316499888897\n",
      "Epoch 209, Loss: 0.006915867328643799, Val Loss: 0.013840774074196815\n",
      "Epoch 210, Loss: 0.006850014440715313, Val Loss: 0.013785813003778458\n",
      "Epoch 211, Loss: 0.006786606274545193, Val Loss: 0.013741361908614635\n",
      "Epoch 212, Loss: 0.006726763676851988, Val Loss: 0.01370153483003378\n",
      "Epoch 213, Loss: 0.006669629365205765, Val Loss: 0.013661305420100689\n",
      "Epoch 214, Loss: 0.006613769102841616, Val Loss: 0.0136237982660532\n",
      "Epoch 215, Loss: 0.0065591721795499325, Val Loss: 0.013584040105342865\n",
      "Epoch 216, Loss: 0.006504508201032877, Val Loss: 0.013542324304580688\n",
      "Epoch 217, Loss: 0.0064497520215809345, Val Loss: 0.01349948812276125\n",
      "Epoch 218, Loss: 0.0063942912966012955, Val Loss: 0.013457761146128178\n",
      "Epoch 219, Loss: 0.006338970270007849, Val Loss: 0.013416673056781292\n",
      "Epoch 220, Loss: 0.006283569149672985, Val Loss: 0.013376355171203613\n",
      "Epoch 221, Loss: 0.006228524725884199, Val Loss: 0.013336495496332645\n",
      "Epoch 222, Loss: 0.006173573900014162, Val Loss: 0.013296175748109818\n",
      "Epoch 223, Loss: 0.006119180470705032, Val Loss: 0.013257632963359356\n",
      "Epoch 224, Loss: 0.0060652014799416065, Val Loss: 0.013219154439866543\n",
      "Epoch 225, Loss: 0.00601296266540885, Val Loss: 0.013182340189814568\n",
      "Epoch 226, Loss: 0.00596190057694912, Val Loss: 0.013148950412869453\n",
      "Epoch 227, Loss: 0.0059129768051207066, Val Loss: 0.01311431173235178\n",
      "Epoch 228, Loss: 0.005865968763828278, Val Loss: 0.013086779043078423\n",
      "Epoch 229, Loss: 0.005819983314722776, Val Loss: 0.013054819777607918\n",
      "Epoch 230, Loss: 0.005775445606559515, Val Loss: 0.013032644987106323\n",
      "Epoch 231, Loss: 0.00573159521445632, Val Loss: 0.01300172507762909\n",
      "Epoch 232, Loss: 0.005688605830073357, Val Loss: 0.012980892322957516\n",
      "Epoch 233, Loss: 0.00564650958403945, Val Loss: 0.012949048541486263\n",
      "Epoch 234, Loss: 0.005604997277259827, Val Loss: 0.012923420406877995\n",
      "Epoch 235, Loss: 0.005559233482927084, Val Loss: 0.012878844514489174\n",
      "Epoch 236, Loss: 0.005507816094905138, Val Loss: 0.012838988564908504\n",
      "Epoch 237, Loss: 0.005452222656458616, Val Loss: 0.01279415562748909\n",
      "Epoch 238, Loss: 0.005398379638791084, Val Loss: 0.012761120684444904\n",
      "Epoch 239, Loss: 0.005350078456103802, Val Loss: 0.0127340042963624\n",
      "Epoch 240, Loss: 0.005308251362293959, Val Loss: 0.012707019224762917\n",
      "Epoch 241, Loss: 0.005269024521112442, Val Loss: 0.012687090784311295\n",
      "Epoch 242, Loss: 0.00522951502352953, Val Loss: 0.012653843499720097\n",
      "Epoch 243, Loss: 0.005187790375202894, Val Loss: 0.012628457508981228\n",
      "Epoch 244, Loss: 0.005144353024661541, Val Loss: 0.012592321261763573\n",
      "Epoch 245, Loss: 0.0050996290519833565, Val Loss: 0.01256435364484787\n",
      "Epoch 246, Loss: 0.0050554433837533, Val Loss: 0.01253282930701971\n",
      "Epoch 247, Loss: 0.005013562738895416, Val Loss: 0.012506570667028427\n",
      "Epoch 248, Loss: 0.004973347298800945, Val Loss: 0.012485307641327381\n",
      "Epoch 249, Loss: 0.004935124889016151, Val Loss: 0.012460128404200077\n",
      "Epoch 250, Loss: 0.0048981923609972, Val Loss: 0.012444746680557728\n",
      "Epoch 251, Loss: 0.0048616621643304825, Val Loss: 0.012417582795023918\n",
      "Epoch 252, Loss: 0.0048256972804665565, Val Loss: 0.012404966168105602\n",
      "Epoch 253, Loss: 0.004789643920958042, Val Loss: 0.01237812452018261\n",
      "Epoch 254, Loss: 0.004752737004309893, Val Loss: 0.012367895804345608\n",
      "Epoch 255, Loss: 0.004714424256235361, Val Loss: 0.012339131906628609\n",
      "Epoch 256, Loss: 0.004676202777773142, Val Loss: 0.012325542978942394\n",
      "Epoch 257, Loss: 0.004637340549379587, Val Loss: 0.012288885191082954\n",
      "Epoch 258, Loss: 0.00459688063710928, Val Loss: 0.012265840545296669\n",
      "Epoch 259, Loss: 0.004556336440145969, Val Loss: 0.012234918773174286\n",
      "Epoch 260, Loss: 0.004517773631960154, Val Loss: 0.012210330925881863\n",
      "Epoch 261, Loss: 0.004481465555727482, Val Loss: 0.01218851562589407\n",
      "Epoch 262, Loss: 0.00444602919742465, Val Loss: 0.012168028391897678\n",
      "Epoch 263, Loss: 0.0044113993644714355, Val Loss: 0.01215278822928667\n",
      "Epoch 264, Loss: 0.004377499222755432, Val Loss: 0.012129825539886951\n",
      "Epoch 265, Loss: 0.004343229811638594, Val Loss: 0.012114748358726501\n",
      "Epoch 266, Loss: 0.004309314768761396, Val Loss: 0.012090767733752728\n",
      "Epoch 267, Loss: 0.004275571554899216, Val Loss: 0.012073848396539688\n",
      "Epoch 268, Loss: 0.004242022056132555, Val Loss: 0.012048562057316303\n",
      "Epoch 269, Loss: 0.004208358470350504, Val Loss: 0.012032360769808292\n",
      "Epoch 270, Loss: 0.004175455775111914, Val Loss: 0.012008600868284702\n",
      "Epoch 271, Loss: 0.00414362782612443, Val Loss: 0.011997466906905174\n",
      "Epoch 272, Loss: 0.004112984985113144, Val Loss: 0.011976275593042374\n",
      "Epoch 273, Loss: 0.004083962645381689, Val Loss: 0.011975113302469254\n",
      "Epoch 274, Loss: 0.004056992009282112, Val Loss: 0.011957879178225994\n",
      "Epoch 275, Loss: 0.004034621641039848, Val Loss: 0.011974073015153408\n",
      "Epoch 276, Loss: 0.004017357714474201, Val Loss: 0.011960430070757866\n",
      "Epoch 277, Loss: 0.004004685208201408, Val Loss: 0.011985300108790398\n",
      "Epoch 278, Loss: 0.003990259952843189, Val Loss: 0.011956805363297462\n",
      "Epoch 279, Loss: 0.0039671193808317184, Val Loss: 0.011949080973863602\n",
      "Epoch 280, Loss: 0.0039239670149981976, Val Loss: 0.011879859492182732\n",
      "Epoch 281, Loss: 0.0038647549226880074, Val Loss: 0.011843238957226276\n",
      "Epoch 282, Loss: 0.00380865391343832, Val Loss: 0.011819944716989994\n",
      "Epoch 283, Loss: 0.0037763051223009825, Val Loss: 0.011814397759735584\n",
      "Epoch 284, Loss: 0.003764066146686673, Val Loss: 0.011830710805952549\n",
      "Epoch 285, Loss: 0.0037514744326472282, Val Loss: 0.011799232102930546\n",
      "Epoch 286, Loss: 0.0037221009843051434, Val Loss: 0.011775098741054535\n",
      "Epoch 287, Loss: 0.0036773798055946827, Val Loss: 0.011737227439880371\n",
      "Epoch 288, Loss: 0.003635996952652931, Val Loss: 0.011723959818482399\n",
      "Epoch 289, Loss: 0.0036114936228841543, Val Loss: 0.011729790829122066\n",
      "Epoch 290, Loss: 0.0035955682396888733, Val Loss: 0.011710749939084053\n",
      "Epoch 291, Loss: 0.003574331058189273, Val Loss: 0.011701520532369614\n",
      "Epoch 292, Loss: 0.003542458638548851, Val Loss: 0.011668473482131958\n",
      "Epoch 293, Loss: 0.0035060695372521877, Val Loss: 0.011650904081761837\n",
      "Epoch 294, Loss: 0.0034756448585540056, Val Loss: 0.011644653975963593\n",
      "Epoch 295, Loss: 0.0034538237378001213, Val Loss: 0.011630676686763763\n",
      "Epoch 296, Loss: 0.0034339523408561945, Val Loss: 0.011627750471234322\n",
      "Epoch 297, Loss: 0.00341026671230793, Val Loss: 0.011603008955717087\n",
      "Epoch 298, Loss: 0.0033810348249971867, Val Loss: 0.011589925736188889\n",
      "Epoch 299, Loss: 0.003350234590470791, Val Loss: 0.011575049720704556\n",
      "Epoch 300, Loss: 0.003323860466480255, Val Loss: 0.011564542539417744\n",
      "Epoch 301, Loss: 0.003301512449979782, Val Loss: 0.011560849845409393\n",
      "Epoch 302, Loss: 0.0032798307947814465, Val Loss: 0.011541723273694515\n",
      "Epoch 303, Loss: 0.0032564911525696516, Val Loss: 0.011533730663359165\n",
      "Epoch 304, Loss: 0.003230463247746229, Val Loss: 0.011513355188071728\n",
      "Epoch 305, Loss: 0.0032040688674896955, Val Loss: 0.011502699926495552\n",
      "Epoch 306, Loss: 0.0031784491147845984, Val Loss: 0.011491312645375729\n",
      "Epoch 307, Loss: 0.0031543602235615253, Val Loss: 0.011479253880679607\n",
      "Epoch 308, Loss: 0.0031316999811679125, Val Loss: 0.011472804471850395\n",
      "Epoch 309, Loss: 0.0031093040015548468, Val Loss: 0.011456489562988281\n",
      "Epoch 310, Loss: 0.0030864898581057787, Val Loss: 0.011448093689978123\n",
      "Epoch 311, Loss: 0.003062981180846691, Val Loss: 0.011431551538407803\n",
      "Epoch 312, Loss: 0.0030396927613765, Val Loss: 0.011420738883316517\n",
      "Epoch 313, Loss: 0.003016522154211998, Val Loss: 0.011409874074161053\n",
      "Epoch 314, Loss: 0.0029941783286631107, Val Loss: 0.011396778747439384\n",
      "Epoch 315, Loss: 0.0029724908526986837, Val Loss: 0.011389041319489479\n",
      "Epoch 316, Loss: 0.0029515649657696486, Val Loss: 0.01137363538146019\n",
      "Epoch 317, Loss: 0.0029308171942830086, Val Loss: 0.011370951309800148\n",
      "Epoch 318, Loss: 0.002909997943788767, Val Loss: 0.011352556757628918\n",
      "Epoch 319, Loss: 0.0028885924257338047, Val Loss: 0.011348110623657703\n",
      "Epoch 320, Loss: 0.0028666583821177483, Val Loss: 0.011329572647809982\n",
      "Epoch 321, Loss: 0.0028443317860364914, Val Loss: 0.011322586797177792\n",
      "Epoch 322, Loss: 0.0028226268477737904, Val Loss: 0.011308646760880947\n",
      "Epoch 323, Loss: 0.0028013004921376705, Val Loss: 0.011298558674752712\n",
      "Epoch 324, Loss: 0.0027806200087070465, Val Loss: 0.011288126930594444\n",
      "Epoch 325, Loss: 0.002760329283773899, Val Loss: 0.01127657387405634\n",
      "Epoch 326, Loss: 0.0027403966523706913, Val Loss: 0.011269845068454742\n",
      "Epoch 327, Loss: 0.0027206866070628166, Val Loss: 0.011255394667387009\n",
      "Epoch 328, Loss: 0.002700954908505082, Val Loss: 0.011249332688748837\n",
      "Epoch 329, Loss: 0.0026810462586581707, Val Loss: 0.011236621998250484\n",
      "Epoch 330, Loss: 0.002661324804648757, Val Loss: 0.011226673610508442\n",
      "Epoch 331, Loss: 0.002641804050654173, Val Loss: 0.011213620193302631\n",
      "Epoch 332, Loss: 0.002622620202600956, Val Loss: 0.011204198002815247\n",
      "Epoch 333, Loss: 0.0026034547481685877, Val Loss: 0.01119410153478384\n",
      "Epoch 334, Loss: 0.002584518399089575, Val Loss: 0.011183987371623516\n",
      "Epoch 335, Loss: 0.0025656181387603283, Val Loss: 0.011175383813679218\n",
      "Epoch 336, Loss: 0.002546922070905566, Val Loss: 0.01116422563791275\n",
      "Epoch 337, Loss: 0.0025283214636147022, Val Loss: 0.011154083535075188\n",
      "Epoch 338, Loss: 0.0025099399499595165, Val Loss: 0.011145921424031258\n",
      "Epoch 339, Loss: 0.0024917302653193474, Val Loss: 0.01113501749932766\n",
      "Epoch 340, Loss: 0.0024735783226788044, Val Loss: 0.011126963421702385\n",
      "Epoch 341, Loss: 0.0024555714335292578, Val Loss: 0.011114190332591534\n",
      "Epoch 342, Loss: 0.0024377957452088594, Val Loss: 0.011108801700174809\n",
      "Epoch 343, Loss: 0.0024200687184929848, Val Loss: 0.011094922199845314\n",
      "Epoch 344, Loss: 0.0024026765022426844, Val Loss: 0.011089636012911797\n",
      "Epoch 345, Loss: 0.0023854016326367855, Val Loss: 0.011074998416006565\n",
      "Epoch 346, Loss: 0.002368120476603508, Val Loss: 0.011071890592575073\n",
      "Epoch 347, Loss: 0.0023511135950684547, Val Loss: 0.011056204326450825\n",
      "Epoch 348, Loss: 0.0023339849431067705, Val Loss: 0.011052238754928112\n",
      "Epoch 349, Loss: 0.0023169463966041803, Val Loss: 0.011035405099391937\n",
      "Epoch 350, Loss: 0.002300100866705179, Val Loss: 0.01103198155760765\n",
      "Epoch 351, Loss: 0.0022831757087260485, Val Loss: 0.011017978191375732\n",
      "Epoch 352, Loss: 0.002266169060021639, Val Loss: 0.01101164985448122\n",
      "Epoch 353, Loss: 0.0022493579890578985, Val Loss: 0.011004810221493244\n",
      "Epoch 354, Loss: 0.002232976257801056, Val Loss: 0.010995921678841114\n",
      "Epoch 355, Loss: 0.0022170154843479395, Val Loss: 0.01099152211099863\n",
      "Epoch 356, Loss: 0.002201349940150976, Val Loss: 0.010976280085742474\n",
      "Epoch 357, Loss: 0.002186224330216646, Val Loss: 0.010976829566061497\n",
      "Epoch 358, Loss: 0.0021711501758545637, Val Loss: 0.010959000326693058\n",
      "Epoch 359, Loss: 0.0021561942994594574, Val Loss: 0.010964438319206238\n",
      "Epoch 360, Loss: 0.00214122561737895, Val Loss: 0.010945457965135574\n",
      "Epoch 361, Loss: 0.0021262052468955517, Val Loss: 0.01095186173915863\n",
      "Epoch 362, Loss: 0.0021111860405653715, Val Loss: 0.0109321940690279\n",
      "Epoch 363, Loss: 0.002096008975058794, Val Loss: 0.010939056985080242\n",
      "Epoch 364, Loss: 0.0020808528643101454, Val Loss: 0.010917481034994125\n",
      "Epoch 365, Loss: 0.00206564087420702, Val Loss: 0.010924523696303368\n",
      "Epoch 366, Loss: 0.0020504482090473175, Val Loss: 0.010904568247497082\n",
      "Epoch 367, Loss: 0.0020351633429527283, Val Loss: 0.010908430442214012\n",
      "Epoch 368, Loss: 0.0020198735874146223, Val Loss: 0.010889447294175625\n",
      "Epoch 369, Loss: 0.0020042292308062315, Val Loss: 0.010887696407735348\n",
      "Epoch 370, Loss: 0.0019890395924448967, Val Loss: 0.010876533575356007\n",
      "Epoch 371, Loss: 0.0019744017627090216, Val Loss: 0.010873246937990189\n",
      "Epoch 372, Loss: 0.0019600519444793463, Val Loss: 0.010865433141589165\n",
      "Epoch 373, Loss: 0.001945812371559441, Val Loss: 0.01086200401186943\n",
      "Epoch 374, Loss: 0.0019318803679198027, Val Loss: 0.010854275897145271\n",
      "Epoch 375, Loss: 0.001918007037602365, Val Loss: 0.010847616009414196\n",
      "Epoch 376, Loss: 0.0019043511711061, Val Loss: 0.010843513533473015\n",
      "Epoch 377, Loss: 0.0018909984501078725, Val Loss: 0.010833573527634144\n",
      "Epoch 378, Loss: 0.001877463306300342, Val Loss: 0.010831614956259727\n",
      "Epoch 379, Loss: 0.0018641432980075479, Val Loss: 0.010823121294379234\n",
      "Epoch 380, Loss: 0.0018508285284042358, Val Loss: 0.010821850039064884\n",
      "Epoch 381, Loss: 0.0018373819766566157, Val Loss: 0.010815620422363281\n",
      "Epoch 382, Loss: 0.0018242400838062167, Val Loss: 0.010808216407895088\n",
      "Epoch 383, Loss: 0.0018114035483449697, Val Loss: 0.010803916491568089\n",
      "Epoch 384, Loss: 0.0017986485036090016, Val Loss: 0.010797280818223953\n",
      "Epoch 385, Loss: 0.001785893808118999, Val Loss: 0.010795393027365208\n",
      "Epoch 386, Loss: 0.0017732612323015928, Val Loss: 0.010786568745970726\n",
      "Epoch 387, Loss: 0.0017607402987778187, Val Loss: 0.010785520076751709\n",
      "Epoch 388, Loss: 0.001748216338455677, Val Loss: 0.010776106268167496\n",
      "Epoch 389, Loss: 0.0017357510514557362, Val Loss: 0.010770980268716812\n",
      "Epoch 390, Loss: 0.0017234513070434332, Val Loss: 0.010768087580800056\n",
      "Epoch 391, Loss: 0.001711519667878747, Val Loss: 0.010756853967905045\n",
      "Epoch 392, Loss: 0.0016996897757053375, Val Loss: 0.010759891010820866\n",
      "Epoch 393, Loss: 0.0016878880560398102, Val Loss: 0.01074828952550888\n",
      "Epoch 394, Loss: 0.0016759554855525494, Val Loss: 0.01075319666415453\n",
      "Epoch 395, Loss: 0.0016639615641906857, Val Loss: 0.010743418708443642\n",
      "Epoch 396, Loss: 0.001651874277740717, Val Loss: 0.010741627775132656\n",
      "Epoch 397, Loss: 0.0016400800086557865, Val Loss: 0.010734783485531807\n",
      "Epoch 398, Loss: 0.0016284933080896735, Val Loss: 0.01072818972170353\n",
      "Epoch 399, Loss: 0.001617163885384798, Val Loss: 0.01072569191455841\n",
      "Epoch 400, Loss: 0.0016058736946433783, Val Loss: 0.01071612536907196\n",
      "Epoch 401, Loss: 0.00159449502825737, Val Loss: 0.010716970078647137\n",
      "Epoch 402, Loss: 0.001583486096933484, Val Loss: 0.010709048248827457\n",
      "Epoch 403, Loss: 0.0015724040567874908, Val Loss: 0.010711797513067722\n",
      "Epoch 404, Loss: 0.0015614815056324005, Val Loss: 0.010701353661715984\n",
      "Epoch 405, Loss: 0.00155063986312598, Val Loss: 0.010701878927648067\n",
      "Epoch 406, Loss: 0.0015396045055240393, Val Loss: 0.010693514719605446\n",
      "Epoch 407, Loss: 0.0015285807894542813, Val Loss: 0.010691762901842594\n",
      "Epoch 408, Loss: 0.0015178086468949914, Val Loss: 0.010686226189136505\n",
      "Epoch 409, Loss: 0.0015070742228999734, Val Loss: 0.010680737905204296\n",
      "Epoch 410, Loss: 0.0014967024326324463, Val Loss: 0.01068065781146288\n",
      "Epoch 411, Loss: 0.0014863410033285618, Val Loss: 0.010672511532902718\n",
      "Epoch 412, Loss: 0.001476135803386569, Val Loss: 0.010674539022147655\n",
      "Epoch 413, Loss: 0.001466104993596673, Val Loss: 0.01066101435571909\n",
      "Epoch 414, Loss: 0.0014559574192389846, Val Loss: 0.010664683766663074\n",
      "Epoch 415, Loss: 0.0014457209035754204, Val Loss: 0.010652793571352959\n",
      "Epoch 416, Loss: 0.0014355236198753119, Val Loss: 0.010656182654201984\n",
      "Epoch 417, Loss: 0.001425236463546753, Val Loss: 0.010648750700056553\n",
      "Epoch 418, Loss: 0.001415093895047903, Val Loss: 0.010646701790392399\n",
      "Epoch 419, Loss: 0.001405202317982912, Val Loss: 0.010644134134054184\n",
      "Epoch 420, Loss: 0.0013954187743365765, Val Loss: 0.010638720355927944\n",
      "Epoch 421, Loss: 0.0013857678277418017, Val Loss: 0.010637990199029446\n",
      "Epoch 422, Loss: 0.001376038882881403, Val Loss: 0.010632404126226902\n",
      "Epoch 423, Loss: 0.0013665106380358338, Val Loss: 0.010630563832819462\n",
      "Epoch 424, Loss: 0.001356958644464612, Val Loss: 0.010626064613461494\n",
      "Epoch 425, Loss: 0.0013474691659212112, Val Loss: 0.010623790323734283\n",
      "Epoch 426, Loss: 0.0013380403397604823, Val Loss: 0.010621673427522182\n",
      "Epoch 427, Loss: 0.0013288301415741444, Val Loss: 0.01061614602804184\n",
      "Epoch 428, Loss: 0.0013196381041780114, Val Loss: 0.01061504241079092\n",
      "Epoch 429, Loss: 0.001310462481342256, Val Loss: 0.010610113851726055\n",
      "Epoch 430, Loss: 0.0013013094430789351, Val Loss: 0.010606244206428528\n",
      "Epoch 431, Loss: 0.001292325323447585, Val Loss: 0.010603790171444416\n",
      "Epoch 432, Loss: 0.001283280085772276, Val Loss: 0.010600093752145767\n",
      "Epoch 433, Loss: 0.0012743204133585095, Val Loss: 0.010596653446555138\n",
      "Epoch 434, Loss: 0.0012654707534238696, Val Loss: 0.010595576837658882\n",
      "Epoch 435, Loss: 0.001256665913388133, Val Loss: 0.010588843375444412\n",
      "Epoch 436, Loss: 0.0012479566503316164, Val Loss: 0.010588554665446281\n",
      "Epoch 437, Loss: 0.001239385805092752, Val Loss: 0.010582043789327145\n",
      "Epoch 438, Loss: 0.001230716472491622, Val Loss: 0.010579255409538746\n",
      "Epoch 439, Loss: 0.0012220937060192227, Val Loss: 0.010579043999314308\n",
      "Epoch 440, Loss: 0.0012134823482483625, Val Loss: 0.01057342253625393\n",
      "Epoch 441, Loss: 0.0012052940437570214, Val Loss: 0.01057918556034565\n",
      "Epoch 442, Loss: 0.0011972476495429873, Val Loss: 0.010568519122898579\n",
      "Epoch 443, Loss: 0.001189177157357335, Val Loss: 0.010577020235359669\n",
      "Epoch 444, Loss: 0.001180990133434534, Val Loss: 0.010563026182353497\n",
      "Epoch 445, Loss: 0.0011723454808816314, Val Loss: 0.010566471144557\n",
      "Epoch 446, Loss: 0.001163667649962008, Val Loss: 0.010561642237007618\n",
      "Epoch 447, Loss: 0.0011553616495802999, Val Loss: 0.010557029396295547\n",
      "Epoch 448, Loss: 0.0011473852209746838, Val Loss: 0.010563069954514503\n",
      "Epoch 449, Loss: 0.0011397211346775293, Val Loss: 0.010549887083470821\n",
      "Epoch 450, Loss: 0.0011321617057546973, Val Loss: 0.010559022426605225\n",
      "Epoch 451, Loss: 0.0011243242770433426, Val Loss: 0.010544195771217346\n",
      "Epoch 452, Loss: 0.0011163733433932066, Val Loss: 0.01054977998137474\n",
      "Epoch 453, Loss: 0.001108282944187522, Val Loss: 0.01054109912365675\n",
      "Epoch 454, Loss: 0.001100343419238925, Val Loss: 0.01053854264318943\n",
      "Epoch 455, Loss: 0.0010927993571385741, Val Loss: 0.010542483069002628\n",
      "Epoch 456, Loss: 0.0010855166474357247, Val Loss: 0.010530844330787659\n",
      "Epoch 457, Loss: 0.001078398316167295, Val Loss: 0.01054326817393303\n",
      "Epoch 458, Loss: 0.0010712348157539964, Val Loss: 0.010526204481720924\n",
      "Epoch 459, Loss: 0.0010637351078912616, Val Loss: 0.010535123758018017\n",
      "Epoch 460, Loss: 0.001055740169249475, Val Loss: 0.010526573285460472\n",
      "Epoch 461, Loss: 0.0010481044882908463, Val Loss: 0.010522364638745785\n",
      "Epoch 462, Loss: 0.001041000708937645, Val Loss: 0.01052863709628582\n",
      "Epoch 463, Loss: 0.0010341060115024447, Val Loss: 0.010516641661524773\n",
      "Epoch 464, Loss: 0.001027079182676971, Val Loss: 0.010524878278374672\n",
      "Epoch 465, Loss: 0.001019909861497581, Val Loss: 0.010516801849007607\n",
      "Epoch 466, Loss: 0.0010126567212864757, Val Loss: 0.010516766458749771\n",
      "Epoch 467, Loss: 0.0010056083556264639, Val Loss: 0.010518287308514118\n",
      "Epoch 468, Loss: 0.00099884660448879, Val Loss: 0.010508747771382332\n",
      "Epoch 469, Loss: 0.0009922966128215194, Val Loss: 0.010515835136175156\n",
      "Epoch 470, Loss: 0.0009856679243966937, Val Loss: 0.010505217127501965\n",
      "Epoch 471, Loss: 0.0009789241012185812, Val Loss: 0.010511756874620914\n",
      "Epoch 472, Loss: 0.0009721373207867146, Val Loss: 0.010506435297429562\n",
      "Epoch 473, Loss: 0.0009654281311668456, Val Loss: 0.01050663460046053\n",
      "Epoch 474, Loss: 0.0009588741813786328, Val Loss: 0.010508526116609573\n",
      "Epoch 475, Loss: 0.0009525377536192536, Val Loss: 0.010499918833374977\n",
      "Epoch 476, Loss: 0.0009462995803914964, Val Loss: 0.010507697239518166\n",
      "Epoch 477, Loss: 0.0009398554102517664, Val Loss: 0.010498760268092155\n",
      "Epoch 478, Loss: 0.0009333313792012632, Val Loss: 0.010499161668121815\n",
      "Epoch 479, Loss: 0.0009270691080018878, Val Loss: 0.010500510223209858\n",
      "Epoch 480, Loss: 0.0009208801202476025, Val Loss: 0.010495040565729141\n",
      "Epoch 481, Loss: 0.000914740317966789, Val Loss: 0.010498827323317528\n",
      "Epoch 482, Loss: 0.0009086020872928202, Val Loss: 0.010495013557374477\n",
      "Epoch 483, Loss: 0.0009025649051181972, Val Loss: 0.010494505055248737\n",
      "Epoch 484, Loss: 0.0008964966982603073, Val Loss: 0.010493419133126736\n",
      "Epoch 485, Loss: 0.0008905756985768676, Val Loss: 0.01049033086746931\n",
      "Epoch 486, Loss: 0.0008846313576214015, Val Loss: 0.010493062436580658\n",
      "Epoch 487, Loss: 0.0008787439437583089, Val Loss: 0.010489791631698608\n",
      "Epoch 488, Loss: 0.0008729196269996464, Val Loss: 0.010494031012058258\n",
      "Epoch 489, Loss: 0.0008671534596942365, Val Loss: 0.010492593050003052\n",
      "Epoch 490, Loss: 0.0008614370017312467, Val Loss: 0.010492133907973766\n",
      "Epoch 491, Loss: 0.0008556695538572967, Val Loss: 0.010492149740457535\n",
      "Epoch 492, Loss: 0.0008500237017869949, Val Loss: 0.01049004215747118\n",
      "Epoch 493, Loss: 0.0008444416453130543, Val Loss: 0.010488823987543583\n",
      "Epoch 494, Loss: 0.000838883628603071, Val Loss: 0.010492014698684216\n",
      "Epoch 495, Loss: 0.0008334826561622322, Val Loss: 0.010490037500858307\n",
      "Epoch 496, Loss: 0.0008279747562482953, Val Loss: 0.010490712709724903\n",
      "Epoch 497, Loss: 0.0008226458448916674, Val Loss: 0.010488846339285374\n",
      "Epoch 498, Loss: 0.0008172980742529035, Val Loss: 0.010489053092896938\n",
      "Epoch 499, Loss: 0.0008119387202896178, Val Loss: 0.010488090105354786\n",
      "Epoch 500, Loss: 0.0008066828595474362, Val Loss: 0.010490043088793755\n"
     ]
    }
   ],
   "source": [
    "from flashrag.prompt import PromptTemplate\n",
    "import pickle\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "config_taqa = set_config(dataset_name='taqa', split = split)\n",
    "template_taqa= PromptTemplate(\n",
    "        config = config_taqa,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "config_triviaqa= set_config(dataset_name='triviaqa', split = split)\n",
    "template_triviaqa= PromptTemplate(\n",
    "        config = config_triviaqa,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "\n",
    "\n",
    "loading_hidden = True\n",
    "if not loading_hidden:\n",
    "    taqa_hidden_states, triviaqa_hidden_states = load_datasets(config_taqa, config_triviaqa, template_taqa, template_triviaqa)\n",
    "else:\n",
    "    file_path_triviaqa = '/cs/student/projects2/dsml/cdiezmar/hidden_states/triviaqa.pkl'\n",
    "    file_path_taqa = '/cs/student/projects2/dsml/cdiezmar/hidden_states/taqa.pkl'\n",
    "\n",
    "    with open(file_path_triviaqa, 'rb') as file_triviaqa:\n",
    "        triviaqa_hidden_states = pickle.load(file_triviaqa)\n",
    "        print('Length triviaqa: ', len(triviaqa_hidden_states))\n",
    "        triviaqa_hidden_states_tensor= torch.tensor(triviaqa_hidden_states, dtype=torch.float32)\n",
    "    with open(file_path_taqa, 'rb') as file_taqa:\n",
    "        taqa_hidden_states = pickle.load(file_taqa)\n",
    "        print('Length taqa: ', len(taqa_hidden_states))\n",
    "        taqa_hidden_states_tensor= torch.tensor(taqa_hidden_states, dtype=torch.float32)\n",
    "train_inputs, train_labels, val_inputs, val_labels = prepare_data(taqa_hidden_states_tensor, triviaqa_hidden_states_tensor)\n",
    "\n",
    "input_size = train_inputs.size(1)\n",
    "num_classes = 2  # Time-sensitive or not\n",
    "\n",
    "# RNN Parameters\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "epochs = 500\n",
    "lr = 1e-4\n",
    "\n",
    "# Train GRU Classifier\n",
    "bidir_rnn_model = train_bidir_rnn_classifier_cuda(train_inputs, train_labels, val_inputs, val_labels, input_size, hidden_size, num_layers, num_classes, epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge-awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `question` in template\n",
      "Find `question` in template\n",
      "Length triviaqa:  20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_171680/1763743531.py:30: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  know_hidden_states_tensor= torch.tensor(know_hidden_states, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length taqa:  10148\n",
      "Epoch 1, Loss: 0.7675870060920715, Val Loss: 0.6338747143745422\n",
      "Epoch 2, Loss: 0.6418005228042603, Val Loss: 0.5522106885910034\n",
      "Epoch 3, Loss: 0.5594211220741272, Val Loss: 0.4726037383079529\n",
      "Epoch 4, Loss: 0.47878068685531616, Val Loss: 0.4142310619354248\n",
      "Epoch 5, Loss: 0.41946959495544434, Val Loss: 0.3795351982116699\n",
      "Epoch 6, Loss: 0.38418158888816833, Val Loss: 0.35371097922325134\n",
      "Epoch 7, Loss: 0.3582218289375305, Val Loss: 0.32840079069137573\n",
      "Epoch 8, Loss: 0.33310896158218384, Val Loss: 0.30695822834968567\n",
      "Epoch 9, Loss: 0.3119732439517975, Val Loss: 0.29241374135017395\n",
      "Epoch 10, Loss: 0.297682523727417, Val Loss: 0.2820468544960022\n",
      "Epoch 11, Loss: 0.2874409258365631, Val Loss: 0.27171826362609863\n",
      "Epoch 12, Loss: 0.2770942151546478, Val Loss: 0.26038864254951477\n",
      "Epoch 13, Loss: 0.26563021540641785, Val Loss: 0.24963806569576263\n",
      "Epoch 14, Loss: 0.25467848777770996, Val Loss: 0.24091404676437378\n",
      "Epoch 15, Loss: 0.2457338273525238, Val Loss: 0.23386038839817047\n",
      "Epoch 16, Loss: 0.23846890032291412, Val Loss: 0.2269524782896042\n",
      "Epoch 17, Loss: 0.2313660979270935, Val Loss: 0.2192917913198471\n",
      "Epoch 18, Loss: 0.2235174924135208, Val Loss: 0.21131527423858643\n",
      "Epoch 19, Loss: 0.21534349024295807, Val Loss: 0.20393244922161102\n",
      "Epoch 20, Loss: 0.20774075388908386, Val Loss: 0.1974751353263855\n",
      "Epoch 21, Loss: 0.20103736221790314, Val Loss: 0.19154275953769684\n",
      "Epoch 22, Loss: 0.19483646750450134, Val Loss: 0.18556754291057587\n",
      "Epoch 23, Loss: 0.1885785162448883, Val Loss: 0.17934805154800415\n",
      "Epoch 24, Loss: 0.18207260966300964, Val Loss: 0.1731148064136505\n",
      "Epoch 25, Loss: 0.1755593866109848, Val Loss: 0.16722501814365387\n",
      "Epoch 26, Loss: 0.1694014072418213, Val Loss: 0.16183213889598846\n",
      "Epoch 27, Loss: 0.1637512594461441, Val Loss: 0.15680204331874847\n",
      "Epoch 28, Loss: 0.15846912562847137, Val Loss: 0.15189476311206818\n",
      "Epoch 29, Loss: 0.15330860018730164, Val Loss: 0.1470005363225937\n",
      "Epoch 30, Loss: 0.1481558382511139, Val Loss: 0.14220918715000153\n",
      "Epoch 31, Loss: 0.14310063421726227, Val Loss: 0.1376897543668747\n",
      "Epoch 32, Loss: 0.13831643760204315, Val Loss: 0.1335320919752121\n",
      "Epoch 33, Loss: 0.13390061259269714, Val Loss: 0.12969185411930084\n",
      "Epoch 34, Loss: 0.12981724739074707, Val Loss: 0.12605415284633636\n",
      "Epoch 35, Loss: 0.12595905363559723, Val Loss: 0.12253403663635254\n",
      "Epoch 36, Loss: 0.12224587053060532, Val Loss: 0.11912891268730164\n",
      "Epoch 37, Loss: 0.11867665499448776, Val Loss: 0.11589717119932175\n",
      "Epoch 38, Loss: 0.11530784517526627, Val Loss: 0.1128973662853241\n",
      "Epoch 39, Loss: 0.11219307035207748, Val Loss: 0.11014062911272049\n",
      "Epoch 40, Loss: 0.10933683067560196, Val Loss: 0.10758795589208603\n",
      "Epoch 41, Loss: 0.10669352114200592, Val Loss: 0.10518451780080795\n",
      "Epoch 42, Loss: 0.10420330613851547, Val Loss: 0.10289700329303741\n",
      "Epoch 43, Loss: 0.10183031857013702, Val Loss: 0.10072486847639084\n",
      "Epoch 44, Loss: 0.0995742604136467, Val Loss: 0.09868337213993073\n",
      "Epoch 45, Loss: 0.0974527895450592, Val Loss: 0.09677762538194656\n",
      "Epoch 46, Loss: 0.0954747423529625, Val Loss: 0.09499004483222961\n",
      "Epoch 47, Loss: 0.09362632781267166, Val Loss: 0.09328723698854446\n",
      "Epoch 48, Loss: 0.09187709540128708, Val Loss: 0.09163784980773926\n",
      "Epoch 49, Loss: 0.09019702672958374, Val Loss: 0.09002634882926941\n",
      "Epoch 50, Loss: 0.0885699912905693, Val Loss: 0.0884547010064125\n",
      "Epoch 51, Loss: 0.08699551969766617, Val Loss: 0.08693327754735947\n",
      "Epoch 52, Loss: 0.08548026531934738, Val Loss: 0.08546949177980423\n",
      "Epoch 53, Loss: 0.08402732014656067, Val Loss: 0.08406182378530502\n",
      "Epoch 54, Loss: 0.08263079822063446, Val Loss: 0.08270159363746643\n",
      "Epoch 55, Loss: 0.08127875626087189, Val Loss: 0.0813797116279602\n",
      "Epoch 56, Loss: 0.07996006309986115, Val Loss: 0.0800919383764267\n",
      "Epoch 57, Loss: 0.07866989076137543, Val Loss: 0.0788395032286644\n",
      "Epoch 58, Loss: 0.07741031050682068, Val Loss: 0.07762571424245834\n",
      "Epoch 59, Loss: 0.076186403632164, Val Loss: 0.07645154744386673\n",
      "Epoch 60, Loss: 0.07500148564577103, Val Loss: 0.07531388849020004\n",
      "Epoch 61, Loss: 0.07385469228029251, Val Loss: 0.07420703768730164\n",
      "Epoch 62, Loss: 0.07274206727743149, Val Loss: 0.07312577217817307\n",
      "Epoch 63, Loss: 0.07165930420160294, Val Loss: 0.07206805795431137\n",
      "Epoch 64, Loss: 0.07060442864894867, Val Loss: 0.07103534787893295\n",
      "Epoch 65, Loss: 0.06957811117172241, Val Loss: 0.07003101706504822\n",
      "Epoch 66, Loss: 0.06858226656913757, Val Loss: 0.06905806809663773\n",
      "Epoch 67, Loss: 0.0676182433962822, Val Loss: 0.0681176483631134\n",
      "Epoch 68, Loss: 0.06668546795845032, Val Loss: 0.06720878928899765\n",
      "Epoch 69, Loss: 0.06578169763088226, Val Loss: 0.06632959842681885\n",
      "Epoch 70, Loss: 0.06490418314933777, Val Loss: 0.06547825038433075\n",
      "Epoch 71, Loss: 0.06405096501111984, Val Loss: 0.06465351581573486\n",
      "Epoch 72, Loss: 0.06322114914655685, Val Loss: 0.06385449320077896\n",
      "Epoch 73, Loss: 0.0624147467315197, Val Loss: 0.06308005005121231\n",
      "Epoch 74, Loss: 0.061631716787815094, Val Loss: 0.06232834607362747\n",
      "Epoch 75, Loss: 0.060871340334415436, Val Loss: 0.06159699708223343\n",
      "Epoch 76, Loss: 0.060132212936878204, Val Loss: 0.060883570462465286\n",
      "Epoch 77, Loss: 0.059412527829408646, Val Loss: 0.060186222195625305\n",
      "Epoch 78, Loss: 0.05871080979704857, Val Loss: 0.05950408801436424\n",
      "Epoch 79, Loss: 0.05802604556083679, Val Loss: 0.05883707106113434\n",
      "Epoch 80, Loss: 0.057357750833034515, Val Loss: 0.05818549171090126\n",
      "Epoch 81, Loss: 0.056705642491579056, Val Loss: 0.057549603283405304\n",
      "Epoch 82, Loss: 0.05606931820511818, Val Loss: 0.05692930892109871\n",
      "Epoch 83, Loss: 0.05544806644320488, Val Loss: 0.05632418394088745\n",
      "Epoch 84, Loss: 0.054840993136167526, Val Loss: 0.05573359876871109\n",
      "Epoch 85, Loss: 0.054247237741947174, Val Loss: 0.0551568828523159\n",
      "Epoch 86, Loss: 0.0536661334335804, Val Loss: 0.05459343269467354\n",
      "Epoch 87, Loss: 0.05309725180268288, Val Loss: 0.05404260754585266\n",
      "Epoch 88, Loss: 0.052540358155965805, Val Loss: 0.05350380390882492\n",
      "Epoch 89, Loss: 0.05199526622891426, Val Loss: 0.052976228296756744\n",
      "Epoch 90, Loss: 0.05146162584424019, Val Loss: 0.052459124475717545\n",
      "Epoch 91, Loss: 0.05093902722001076, Val Loss: 0.05195173993706703\n",
      "Epoch 92, Loss: 0.05042700842022896, Val Loss: 0.05145355686545372\n",
      "Epoch 93, Loss: 0.04992515221238136, Val Loss: 0.05096431449055672\n",
      "Epoch 94, Loss: 0.04943312704563141, Val Loss: 0.05048392713069916\n",
      "Epoch 95, Loss: 0.0489506796002388, Val Loss: 0.05001244321465492\n",
      "Epoch 96, Loss: 0.048477653414011, Val Loss: 0.04954995959997177\n",
      "Epoch 97, Loss: 0.04801386222243309, Val Loss: 0.0490964911878109\n",
      "Epoch 98, Loss: 0.047559019178152084, Val Loss: 0.04865194112062454\n",
      "Epoch 99, Loss: 0.04711286351084709, Val Loss: 0.04821615293622017\n",
      "Epoch 100, Loss: 0.04667508974671364, Val Loss: 0.047788891941308975\n",
      "Epoch 101, Loss: 0.04624543711543083, Val Loss: 0.04736991226673126\n",
      "Epoch 102, Loss: 0.04582365229725838, Val Loss: 0.046958889812231064\n",
      "Epoch 103, Loss: 0.04540955275297165, Val Loss: 0.0465555377304554\n",
      "Epoch 104, Loss: 0.045002974569797516, Val Loss: 0.04615948721766472\n",
      "Epoch 105, Loss: 0.04460372030735016, Val Loss: 0.045770399272441864\n",
      "Epoch 106, Loss: 0.044211581349372864, Val Loss: 0.04538795351982117\n",
      "Epoch 107, Loss: 0.04382634535431862, Val Loss: 0.04501187428832054\n",
      "Epoch 108, Loss: 0.043447792530059814, Val Loss: 0.04464195296168327\n",
      "Epoch 109, Loss: 0.04307572543621063, Val Loss: 0.044278088957071304\n",
      "Epoch 110, Loss: 0.04270997270941734, Val Loss: 0.04392017796635628\n",
      "Epoch 111, Loss: 0.04235038161277771, Val Loss: 0.04356817901134491\n",
      "Epoch 112, Loss: 0.041996799409389496, Val Loss: 0.04322202503681183\n",
      "Epoch 113, Loss: 0.04164906591176987, Val Loss: 0.042881689965724945\n",
      "Epoch 114, Loss: 0.04130703583359718, Val Loss: 0.042547062039375305\n",
      "Epoch 115, Loss: 0.04097055271267891, Val Loss: 0.042218051850795746\n",
      "Epoch 116, Loss: 0.04063946008682251, Val Loss: 0.04189449921250343\n",
      "Epoch 117, Loss: 0.04031362384557724, Val Loss: 0.04157627373933792\n",
      "Epoch 118, Loss: 0.039992913603782654, Val Loss: 0.041263218969106674\n",
      "Epoch 119, Loss: 0.039677198976278305, Val Loss: 0.04095513001084328\n",
      "Epoch 120, Loss: 0.03936637192964554, Val Loss: 0.04065186157822609\n",
      "Epoch 121, Loss: 0.0390603244304657, Val Loss: 0.04035322740674019\n",
      "Epoch 122, Loss: 0.038758937269449234, Val Loss: 0.040059078484773636\n",
      "Epoch 123, Loss: 0.03846210241317749, Val Loss: 0.03976928070187569\n",
      "Epoch 124, Loss: 0.038169704377651215, Val Loss: 0.039483729749917984\n",
      "Epoch 125, Loss: 0.03788164630532265, Val Loss: 0.03920233994722366\n",
      "Epoch 126, Loss: 0.03759783133864403, Val Loss: 0.03892501816153526\n",
      "Epoch 127, Loss: 0.0373181514441967, Val Loss: 0.03865174949169159\n",
      "Epoch 128, Loss: 0.03704254329204559, Val Loss: 0.03838244825601578\n",
      "Epoch 129, Loss: 0.036770910024642944, Val Loss: 0.03811708465218544\n",
      "Epoch 130, Loss: 0.036503154784440994, Val Loss: 0.03785558417439461\n",
      "Epoch 131, Loss: 0.036239199340343475, Val Loss: 0.03759787976741791\n",
      "Epoch 132, Loss: 0.03597896546125412, Val Loss: 0.03734390065073967\n",
      "Epoch 133, Loss: 0.03572235628962517, Val Loss: 0.03709355369210243\n",
      "Epoch 134, Loss: 0.03546931594610214, Val Loss: 0.036846745759248734\n",
      "Epoch 135, Loss: 0.03521976247429848, Val Loss: 0.03660338744521141\n",
      "Epoch 136, Loss: 0.03497361019253731, Val Loss: 0.0363633893430233\n",
      "Epoch 137, Loss: 0.034730829298496246, Val Loss: 0.03612665459513664\n",
      "Epoch 138, Loss: 0.03449130058288574, Val Loss: 0.035893090069293976\n",
      "Epoch 139, Loss: 0.03425499051809311, Val Loss: 0.035662636160850525\n",
      "Epoch 140, Loss: 0.03402182459831238, Val Loss: 0.035435210913419724\n",
      "Epoch 141, Loss: 0.033791735768318176, Val Loss: 0.035210754722356796\n",
      "Epoch 142, Loss: 0.03356466069817543, Val Loss: 0.03498921915888786\n",
      "Epoch 143, Loss: 0.03334054723381996, Val Loss: 0.03477054834365845\n",
      "Epoch 144, Loss: 0.0331193283200264, Val Loss: 0.03455470874905586\n",
      "Epoch 145, Loss: 0.03290095180273056, Val Loss: 0.034341663122177124\n",
      "Epoch 146, Loss: 0.03268537297844887, Val Loss: 0.034131359308958054\n",
      "Epoch 147, Loss: 0.03247251734137535, Val Loss: 0.03392374888062477\n",
      "Epoch 148, Loss: 0.03226234018802643, Val Loss: 0.03371880203485489\n",
      "Epoch 149, Loss: 0.03205479308962822, Val Loss: 0.033516447991132736\n",
      "Epoch 150, Loss: 0.03184983506798744, Val Loss: 0.03331663832068443\n",
      "Epoch 151, Loss: 0.03164740279316902, Val Loss: 0.033119332045316696\n",
      "Epoch 152, Loss: 0.03144744038581848, Val Loss: 0.03292446956038475\n",
      "Epoch 153, Loss: 0.03124992735683918, Val Loss: 0.03273199126124382\n",
      "Epoch 154, Loss: 0.031054794788360596, Val Loss: 0.03254186362028122\n",
      "Epoch 155, Loss: 0.03086201287806034, Val Loss: 0.03235401585698128\n",
      "Epoch 156, Loss: 0.03067154437303543, Val Loss: 0.0321684293448925\n",
      "Epoch 157, Loss: 0.03048332966864109, Val Loss: 0.03198503702878952\n",
      "Epoch 158, Loss: 0.030297331511974335, Val Loss: 0.03180381655693054\n",
      "Epoch 159, Loss: 0.030113520100712776, Val Loss: 0.03162473067641258\n",
      "Epoch 160, Loss: 0.029931847006082535, Val Loss: 0.03144773468375206\n",
      "Epoch 161, Loss: 0.02975228987634182, Val Loss: 0.03127282112836838\n",
      "Epoch 162, Loss: 0.029574794694781303, Val Loss: 0.031099939718842506\n",
      "Epoch 163, Loss: 0.029399324208498, Val Loss: 0.030929069966077805\n",
      "Epoch 164, Loss: 0.02922585792839527, Val Loss: 0.030760178342461586\n",
      "Epoch 165, Loss: 0.029054349288344383, Val Loss: 0.030593229457736015\n",
      "Epoch 166, Loss: 0.028884777799248695, Val Loss: 0.03042818419635296\n",
      "Epoch 167, Loss: 0.02871708944439888, Val Loss: 0.030265023931860924\n",
      "Epoch 168, Loss: 0.028551263734698296, Val Loss: 0.030103713274002075\n",
      "Epoch 169, Loss: 0.028387272730469704, Val Loss: 0.02994421496987343\n",
      "Epoch 170, Loss: 0.02822508104145527, Val Loss: 0.029786497354507446\n",
      "Epoch 171, Loss: 0.028064657002687454, Val Loss: 0.02963053062558174\n",
      "Epoch 172, Loss: 0.027905983850359917, Val Loss: 0.029476281255483627\n",
      "Epoch 173, Loss: 0.027749011293053627, Val Loss: 0.029323717579245567\n",
      "Epoch 174, Loss: 0.027593733742833138, Val Loss: 0.02917282097041607\n",
      "Epoch 175, Loss: 0.027440102770924568, Val Loss: 0.029023563489317894\n",
      "Epoch 176, Loss: 0.027288110926747322, Val Loss: 0.028875907883048058\n",
      "Epoch 177, Loss: 0.027137719094753265, Val Loss: 0.028729839250445366\n",
      "Epoch 178, Loss: 0.026988904923200607, Val Loss: 0.028585337102413177\n",
      "Epoch 179, Loss: 0.02684163860976696, Val Loss: 0.028442375361919403\n",
      "Epoch 180, Loss: 0.02669590711593628, Val Loss: 0.028300931677222252\n",
      "Epoch 181, Loss: 0.02655167132616043, Val Loss: 0.028160978108644485\n",
      "Epoch 182, Loss: 0.026408923789858818, Val Loss: 0.028022512793540955\n",
      "Epoch 183, Loss: 0.026267632842063904, Val Loss: 0.027885474264621735\n",
      "Epoch 184, Loss: 0.0261277724057436, Val Loss: 0.02774987928569317\n",
      "Epoch 185, Loss: 0.02598932757973671, Val Loss: 0.027615688741207123\n",
      "Epoch 186, Loss: 0.025852268561720848, Val Loss: 0.027482878416776657\n",
      "Epoch 187, Loss: 0.02571658231317997, Val Loss: 0.02735142968595028\n",
      "Epoch 188, Loss: 0.025582246482372284, Val Loss: 0.02722131833434105\n",
      "Epoch 189, Loss: 0.025449227541685104, Val Loss: 0.027092523872852325\n",
      "Epoch 190, Loss: 0.02531753107905388, Val Loss: 0.026965035125613213\n",
      "Epoch 191, Loss: 0.025187114253640175, Val Loss: 0.02683882601559162\n",
      "Epoch 192, Loss: 0.025057973340153694, Val Loss: 0.026713870465755463\n",
      "Epoch 193, Loss: 0.0249300766736269, Val Loss: 0.02659016102552414\n",
      "Epoch 194, Loss: 0.024803414940834045, Val Loss: 0.026467669755220413\n",
      "Epoch 195, Loss: 0.02467796951532364, Val Loss: 0.026346389204263687\n",
      "Epoch 196, Loss: 0.024553723633289337, Val Loss: 0.026226293295621872\n",
      "Epoch 197, Loss: 0.024430645629763603, Val Loss: 0.026107370853424072\n",
      "Epoch 198, Loss: 0.024308741092681885, Val Loss: 0.025989599525928497\n",
      "Epoch 199, Loss: 0.0241879690438509, Val Loss: 0.025872979313135147\n",
      "Epoch 200, Loss: 0.024068336933851242, Val Loss: 0.025757476687431335\n",
      "Epoch 201, Loss: 0.023949814960360527, Val Loss: 0.025643080472946167\n",
      "Epoch 202, Loss: 0.023832380771636963, Val Loss: 0.025529775768518448\n",
      "Epoch 203, Loss: 0.023716045543551445, Val Loss: 0.02541755512356758\n",
      "Epoch 204, Loss: 0.023600762709975243, Val Loss: 0.025306392461061478\n",
      "Epoch 205, Loss: 0.023486537858843803, Val Loss: 0.025196269154548645\n",
      "Epoch 206, Loss: 0.023373348638415337, Val Loss: 0.025087185204029083\n",
      "Epoch 207, Loss: 0.023261182010173798, Val Loss: 0.024979110807180405\n",
      "Epoch 208, Loss: 0.023150023072957993, Val Loss: 0.02487204596400261\n",
      "Epoch 209, Loss: 0.023039860650897026, Val Loss: 0.02476595900952816\n",
      "Epoch 210, Loss: 0.022930681705474854, Val Loss: 0.024660855531692505\n",
      "Epoch 211, Loss: 0.022822465747594833, Val Loss: 0.024556707590818405\n",
      "Epoch 212, Loss: 0.022715209051966667, Val Loss: 0.02445351704955101\n",
      "Epoch 213, Loss: 0.022608887404203415, Val Loss: 0.02435125596821308\n",
      "Epoch 214, Loss: 0.022503506392240524, Val Loss: 0.024249926209449768\n",
      "Epoch 215, Loss: 0.02239902876317501, Val Loss: 0.024149496108293533\n",
      "Epoch 216, Loss: 0.02229546755552292, Val Loss: 0.024049976840615273\n",
      "Epoch 217, Loss: 0.02219279669225216, Val Loss: 0.023951338604092598\n",
      "Epoch 218, Loss: 0.022091006860136986, Val Loss: 0.02385358139872551\n",
      "Epoch 219, Loss: 0.021990086883306503, Val Loss: 0.023756692185997963\n",
      "Epoch 220, Loss: 0.021890023723244667, Val Loss: 0.023660652339458466\n",
      "Epoch 221, Loss: 0.021790804341435432, Val Loss: 0.02356545813381672\n",
      "Epoch 222, Loss: 0.02169243060052395, Val Loss: 0.023471100255846977\n",
      "Epoch 223, Loss: 0.021594880148768425, Val Loss: 0.02337755262851715\n",
      "Epoch 224, Loss: 0.021498141810297966, Val Loss: 0.023284822702407837\n",
      "Epoch 225, Loss: 0.021402211859822273, Val Loss: 0.023192884400486946\n",
      "Epoch 226, Loss: 0.021307075396180153, Val Loss: 0.023101745173335075\n",
      "Epoch 227, Loss: 0.02121272310614586, Val Loss: 0.023011384531855583\n",
      "Epoch 228, Loss: 0.021119149401783943, Val Loss: 0.022921795025467873\n",
      "Epoch 229, Loss: 0.021026330068707466, Val Loss: 0.02283295802772045\n",
      "Epoch 230, Loss: 0.02093428000807762, Val Loss: 0.02274487167596817\n",
      "Epoch 231, Loss: 0.02084297128021717, Val Loss: 0.02265753597021103\n",
      "Epoch 232, Loss: 0.02075239270925522, Val Loss: 0.022570930421352386\n",
      "Epoch 233, Loss: 0.020662549883127213, Val Loss: 0.022485051304101944\n",
      "Epoch 234, Loss: 0.02057342603802681, Val Loss: 0.02239987999200821\n",
      "Epoch 235, Loss: 0.020485015586018562, Val Loss: 0.02231542021036148\n",
      "Epoch 236, Loss: 0.020397303625941277, Val Loss: 0.022231651470065117\n",
      "Epoch 237, Loss: 0.020310280844569206, Val Loss: 0.022148575633764267\n",
      "Epoch 238, Loss: 0.0202239491045475, Val Loss: 0.022066185250878334\n",
      "Epoch 239, Loss: 0.020138293504714966, Val Loss: 0.021984461694955826\n",
      "Epoch 240, Loss: 0.020053306594491005, Val Loss: 0.021903401240706444\n",
      "Epoch 241, Loss: 0.019968979060649872, Val Loss: 0.021823005750775337\n",
      "Epoch 242, Loss: 0.019885307177901268, Val Loss: 0.021743251010775566\n",
      "Epoch 243, Loss: 0.01980227418243885, Val Loss: 0.02166413702070713\n",
      "Epoch 244, Loss: 0.019719887524843216, Val Loss: 0.02158566564321518\n",
      "Epoch 245, Loss: 0.01963813416659832, Val Loss: 0.02150781825184822\n",
      "Epoch 246, Loss: 0.01955699548125267, Val Loss: 0.02143058180809021\n",
      "Epoch 247, Loss: 0.019476475194096565, Val Loss: 0.021353961899876595\n",
      "Epoch 248, Loss: 0.019396565854549408, Val Loss: 0.02127794548869133\n",
      "Epoch 249, Loss: 0.019317256286740303, Val Loss: 0.02120252326130867\n",
      "Epoch 250, Loss: 0.019238542765378952, Val Loss: 0.021127697080373764\n",
      "Epoch 251, Loss: 0.01916041597723961, Val Loss: 0.02105345018208027\n",
      "Epoch 252, Loss: 0.019082875922322273, Val Loss: 0.020979788154363632\n",
      "Epoch 253, Loss: 0.019005903974175453, Val Loss: 0.020906692370772362\n",
      "Epoch 254, Loss: 0.018929507583379745, Val Loss: 0.02083415910601616\n",
      "Epoch 255, Loss: 0.018853669986128807, Val Loss: 0.020762184634804726\n",
      "Epoch 256, Loss: 0.01877838559448719, Val Loss: 0.020690759643912315\n",
      "Epoch 257, Loss: 0.018703656271100044, Val Loss: 0.020619884133338928\n",
      "Epoch 258, Loss: 0.018629468977451324, Val Loss: 0.02054954692721367\n",
      "Epoch 259, Loss: 0.018555818125605583, Val Loss: 0.020479746162891388\n",
      "Epoch 260, Loss: 0.01848270557820797, Val Loss: 0.02041046693921089\n",
      "Epoch 261, Loss: 0.018410108983516693, Val Loss: 0.02034170925617218\n",
      "Epoch 262, Loss: 0.01833803579211235, Val Loss: 0.020273469388484955\n",
      "Epoch 263, Loss: 0.018266484141349792, Val Loss: 0.02020573988556862\n",
      "Epoch 264, Loss: 0.018195439130067825, Val Loss: 0.020138511434197426\n",
      "Epoch 265, Loss: 0.018124893307685852, Val Loss: 0.020071785897016525\n",
      "Epoch 266, Loss: 0.018054848536849022, Val Loss: 0.02000555396080017\n",
      "Epoch 267, Loss: 0.017985302954912186, Val Loss: 0.019939804449677467\n",
      "Epoch 268, Loss: 0.01791623793542385, Val Loss: 0.01987454481422901\n",
      "Epoch 269, Loss: 0.01784765161573887, Val Loss: 0.01980975829064846\n",
      "Epoch 270, Loss: 0.017779551446437836, Val Loss: 0.019745446741580963\n",
      "Epoch 271, Loss: 0.017711922526359558, Val Loss: 0.019681600853800774\n",
      "Epoch 272, Loss: 0.01764475740492344, Val Loss: 0.019618220627307892\n",
      "Epoch 273, Loss: 0.01757805421948433, Val Loss: 0.019555293023586273\n",
      "Epoch 274, Loss: 0.017511816695332527, Val Loss: 0.019492825493216515\n",
      "Epoch 275, Loss: 0.01744602620601654, Val Loss: 0.019430799409747124\n",
      "Epoch 276, Loss: 0.01738068461418152, Val Loss: 0.01936922036111355\n",
      "Epoch 277, Loss: 0.017315790057182312, Val Loss: 0.019308075308799744\n",
      "Epoch 278, Loss: 0.017251329496502876, Val Loss: 0.019247371703386307\n",
      "Epoch 279, Loss: 0.01718731038272381, Val Loss: 0.019187090918421745\n",
      "Epoch 280, Loss: 0.017123719677329063, Val Loss: 0.019127240404486656\n",
      "Epoch 281, Loss: 0.017060555517673492, Val Loss: 0.019067808985710144\n",
      "Epoch 282, Loss: 0.0169978104531765, Val Loss: 0.01900879666209221\n",
      "Epoch 283, Loss: 0.016935482621192932, Val Loss: 0.018950190395116806\n",
      "Epoch 284, Loss: 0.016873573884367943, Val Loss: 0.018891992047429085\n",
      "Epoch 285, Loss: 0.016812069341540337, Val Loss: 0.018834203481674194\n",
      "Epoch 286, Loss: 0.016750968992710114, Val Loss: 0.01877681165933609\n",
      "Epoch 287, Loss: 0.016690274700522423, Val Loss: 0.018719816580414772\n",
      "Epoch 288, Loss: 0.01662997528910637, Val Loss: 0.018663212656974792\n",
      "Epoch 289, Loss: 0.0165700726211071, Val Loss: 0.018606990575790405\n",
      "Epoch 290, Loss: 0.016510559245944023, Val Loss: 0.018551163375377655\n",
      "Epoch 291, Loss: 0.016451431438326836, Val Loss: 0.01849570870399475\n",
      "Epoch 292, Loss: 0.016392678022384644, Val Loss: 0.01844063028693199\n",
      "Epoch 293, Loss: 0.01633431389927864, Val Loss: 0.01838592439889908\n",
      "Epoch 294, Loss: 0.016276318579912186, Val Loss: 0.01833159103989601\n",
      "Epoch 295, Loss: 0.01621869206428528, Val Loss: 0.018277620896697044\n",
      "Epoch 296, Loss: 0.016161443665623665, Val Loss: 0.01822401024401188\n",
      "Epoch 297, Loss: 0.016104549169540405, Val Loss: 0.018170762807130814\n",
      "Epoch 298, Loss: 0.016048017889261246, Val Loss: 0.018117867410182953\n",
      "Epoch 299, Loss: 0.01599184423685074, Val Loss: 0.018065322190523148\n",
      "Epoch 300, Loss: 0.015936026349663734, Val Loss: 0.01801312156021595\n",
      "Epoch 301, Loss: 0.015880554914474487, Val Loss: 0.01796126924455166\n",
      "Epoch 302, Loss: 0.015825431793928146, Val Loss: 0.017909761518239975\n",
      "Epoch 303, Loss: 0.015770653262734413, Val Loss: 0.017858587205410004\n",
      "Epoch 304, Loss: 0.015716219320893288, Val Loss: 0.017807744443416595\n",
      "Epoch 305, Loss: 0.015662115067243576, Val Loss: 0.017757238820195198\n",
      "Epoch 306, Loss: 0.015608350746333599, Val Loss: 0.017707061022520065\n",
      "Epoch 307, Loss: 0.015554914250969887, Val Loss: 0.01765720546245575\n",
      "Epoch 308, Loss: 0.01550181396305561, Val Loss: 0.0176076702773571\n",
      "Epoch 309, Loss: 0.015449034981429577, Val Loss: 0.017558453604578972\n",
      "Epoch 310, Loss: 0.015396574512124062, Val Loss: 0.01750956103205681\n",
      "Epoch 311, Loss: 0.015344439074397087, Val Loss: 0.01746097207069397\n",
      "Epoch 312, Loss: 0.015292614698410034, Val Loss: 0.0174126997590065\n",
      "Epoch 313, Loss: 0.0152411088347435, Val Loss: 0.017364732921123505\n",
      "Epoch 314, Loss: 0.015189911238849163, Val Loss: 0.017317069694399834\n",
      "Epoch 315, Loss: 0.015139020048081875, Val Loss: 0.01726970449090004\n",
      "Epoch 316, Loss: 0.015088440850377083, Val Loss: 0.01722264662384987\n",
      "Epoch 317, Loss: 0.015038159675896168, Val Loss: 0.01717587560415268\n",
      "Epoch 318, Loss: 0.01498817466199398, Val Loss: 0.017129404470324516\n",
      "Epoch 319, Loss: 0.014938493259251118, Val Loss: 0.017083218321204185\n",
      "Epoch 320, Loss: 0.014889104291796684, Val Loss: 0.017037319019436836\n",
      "Epoch 321, Loss: 0.01484000962227583, Val Loss: 0.016991712152957916\n",
      "Epoch 322, Loss: 0.01479120459407568, Val Loss: 0.016946380957961082\n",
      "Epoch 323, Loss: 0.014742681756615639, Val Loss: 0.01690133661031723\n",
      "Epoch 324, Loss: 0.014694442972540855, Val Loss: 0.016856564208865166\n",
      "Epoch 325, Loss: 0.014646492898464203, Val Loss: 0.016812073066830635\n",
      "Epoch 326, Loss: 0.014598819427192211, Val Loss: 0.016767846420407295\n",
      "Epoch 327, Loss: 0.01455142255872488, Val Loss: 0.01672389917075634\n",
      "Epoch 328, Loss: 0.014504299499094486, Val Loss: 0.016680216416716576\n",
      "Epoch 329, Loss: 0.014457449316978455, Val Loss: 0.016636796295642853\n",
      "Epoch 330, Loss: 0.014410872012376785, Val Loss: 0.01659364067018032\n",
      "Epoch 331, Loss: 0.014364558272063732, Val Loss: 0.01655074581503868\n",
      "Epoch 332, Loss: 0.014318513683974743, Val Loss: 0.016508109867572784\n",
      "Epoch 333, Loss: 0.014272731728851795, Val Loss: 0.01646573469042778\n",
      "Epoch 334, Loss: 0.014227207750082016, Val Loss: 0.016423603519797325\n",
      "Epoch 335, Loss: 0.014181944541633129, Val Loss: 0.016381729394197464\n",
      "Epoch 336, Loss: 0.014136938378214836, Val Loss: 0.01634010672569275\n",
      "Epoch 337, Loss: 0.014092187397181988, Val Loss: 0.016298728063702583\n",
      "Epoch 338, Loss: 0.014047682285308838, Val Loss: 0.016257591545581818\n",
      "Epoch 339, Loss: 0.01400343794375658, Val Loss: 0.01621670462191105\n",
      "Epoch 340, Loss: 0.013959434814751148, Val Loss: 0.01617605797946453\n",
      "Epoch 341, Loss: 0.013915681280195713, Val Loss: 0.016135651618242264\n",
      "Epoch 342, Loss: 0.013872169889509678, Val Loss: 0.0160954762250185\n",
      "Epoch 343, Loss: 0.013828898780047894, Val Loss: 0.01605553738772869\n",
      "Epoch 344, Loss: 0.013785872608423233, Val Loss: 0.016015836969017982\n",
      "Epoch 345, Loss: 0.013743084855377674, Val Loss: 0.01597636193037033\n",
      "Epoch 346, Loss: 0.013700529932975769, Val Loss: 0.015937117859721184\n",
      "Epoch 347, Loss: 0.013658209703862667, Val Loss: 0.015898097306489944\n",
      "Epoch 348, Loss: 0.013616122305393219, Val Loss: 0.01585930585861206\n",
      "Epoch 349, Loss: 0.013574264943599701, Val Loss: 0.015820730477571487\n",
      "Epoch 350, Loss: 0.013532637618482113, Val Loss: 0.015782391652464867\n",
      "Epoch 351, Loss: 0.013491236604750156, Val Loss: 0.01574426330626011\n",
      "Epoch 352, Loss: 0.013450060039758682, Val Loss: 0.015706347301602364\n",
      "Epoch 353, Loss: 0.01340910978615284, Val Loss: 0.015668656677007675\n",
      "Epoch 354, Loss: 0.013368378393352032, Val Loss: 0.01563117280602455\n",
      "Epoch 355, Loss: 0.013327865861356258, Val Loss: 0.015593907795846462\n",
      "Epoch 356, Loss: 0.013287571258842945, Val Loss: 0.015556844882667065\n",
      "Epoch 357, Loss: 0.013247493654489517, Val Loss: 0.015519996173679829\n",
      "Epoch 358, Loss: 0.013207630254328251, Val Loss: 0.015483352355659008\n",
      "Epoch 359, Loss: 0.013167982921004295, Val Loss: 0.015446917153894901\n",
      "Epoch 360, Loss: 0.013128543272614479, Val Loss: 0.01541068498045206\n",
      "Epoch 361, Loss: 0.0130893150344491, Val Loss: 0.015374653041362762\n",
      "Epoch 362, Loss: 0.013050290755927563, Val Loss: 0.015338823199272156\n",
      "Epoch 363, Loss: 0.013011478818953037, Val Loss: 0.015303188934922218\n",
      "Epoch 364, Loss: 0.012972868047654629, Val Loss: 0.015267754904925823\n",
      "Epoch 365, Loss: 0.012934461236000061, Val Loss: 0.015232516452670097\n",
      "Epoch 366, Loss: 0.01289625559002161, Val Loss: 0.015197468921542168\n",
      "Epoch 367, Loss: 0.012858248315751553, Val Loss: 0.015162616968154907\n",
      "Epoch 368, Loss: 0.012820442207157612, Val Loss: 0.015127956867218018\n",
      "Epoch 369, Loss: 0.012782835401594639, Val Loss: 0.015093485824763775\n",
      "Epoch 370, Loss: 0.012745416723191738, Val Loss: 0.01505920011550188\n",
      "Epoch 371, Loss: 0.012708197347819805, Val Loss: 0.01502510067075491\n",
      "Epoch 372, Loss: 0.012671169824898243, Val Loss: 0.014991184696555138\n",
      "Epoch 373, Loss: 0.012634333223104477, Val Loss: 0.014957454986870289\n",
      "Epoch 374, Loss: 0.01259768195450306, Val Loss: 0.01492390688508749\n",
      "Epoch 375, Loss: 0.012561225332319736, Val Loss: 0.014890539459884167\n",
      "Epoch 376, Loss: 0.012524950318038464, Val Loss: 0.014857356436550617\n",
      "Epoch 377, Loss: 0.012488864362239838, Val Loss: 0.014824341051280499\n",
      "Epoch 378, Loss: 0.012452960945665836, Val Loss: 0.01479150727391243\n",
      "Epoch 379, Loss: 0.012417236343026161, Val Loss: 0.014758847653865814\n",
      "Epoch 380, Loss: 0.012381698936223984, Val Loss: 0.014726361259818077\n",
      "Epoch 381, Loss: 0.012346340343356133, Val Loss: 0.014694048091769218\n",
      "Epoch 382, Loss: 0.012311161495745182, Val Loss: 0.014661907218396664\n",
      "Epoch 383, Loss: 0.012276156805455685, Val Loss: 0.014629931189119816\n",
      "Epoch 384, Loss: 0.01224132813513279, Val Loss: 0.014598130248486996\n",
      "Epoch 385, Loss: 0.012206674553453922, Val Loss: 0.014566490426659584\n",
      "Epoch 386, Loss: 0.012172192335128784, Val Loss: 0.01453501544892788\n",
      "Epoch 387, Loss: 0.012137887999415398, Val Loss: 0.01450370904058218\n",
      "Epoch 388, Loss: 0.012103748507797718, Val Loss: 0.014472565613687038\n",
      "Epoch 389, Loss: 0.012069780379533768, Val Loss: 0.014441589824855328\n",
      "Epoch 390, Loss: 0.012035980820655823, Val Loss: 0.014410764910280704\n",
      "Epoch 391, Loss: 0.012002354487776756, Val Loss: 0.014380103908479214\n",
      "Epoch 392, Loss: 0.011968884617090225, Val Loss: 0.014349599368870258\n",
      "Epoch 393, Loss: 0.011935584247112274, Val Loss: 0.014319255016744137\n",
      "Epoch 394, Loss: 0.01190244872123003, Val Loss: 0.014289066195487976\n",
      "Epoch 395, Loss: 0.011869472451508045, Val Loss: 0.014259028248488903\n",
      "Epoch 396, Loss: 0.011836660094559193, Val Loss: 0.014229148626327515\n",
      "Epoch 397, Loss: 0.01180400513112545, Val Loss: 0.014199421741068363\n",
      "Epoch 398, Loss: 0.011771509423851967, Val Loss: 0.01416984386742115\n",
      "Epoch 399, Loss: 0.011739173904061317, Val Loss: 0.014140415005385876\n",
      "Epoch 400, Loss: 0.01170699205249548, Val Loss: 0.01411113329231739\n",
      "Epoch 401, Loss: 0.011674966663122177, Val Loss: 0.014082002453505993\n",
      "Epoch 402, Loss: 0.011643098667263985, Val Loss: 0.014053022488951683\n",
      "Epoch 403, Loss: 0.01161138154566288, Val Loss: 0.014024185948073864\n",
      "Epoch 404, Loss: 0.011579819954931736, Val Loss: 0.013995492830872536\n",
      "Epoch 405, Loss: 0.011548405513167381, Val Loss: 0.013966943137347698\n",
      "Epoch 406, Loss: 0.011517142876982689, Val Loss: 0.013938537798821926\n",
      "Epoch 407, Loss: 0.011486027389764786, Val Loss: 0.01391027681529522\n",
      "Epoch 408, Loss: 0.01145506277680397, Val Loss: 0.013882152736186981\n",
      "Epoch 409, Loss: 0.011424243450164795, Val Loss: 0.013854165561497211\n",
      "Epoch 410, Loss: 0.011393573135137558, Val Loss: 0.013826321810483932\n",
      "Epoch 411, Loss: 0.011363046243786812, Val Loss: 0.013798614032566547\n",
      "Epoch 412, Loss: 0.011332660913467407, Val Loss: 0.01377104315906763\n",
      "Epoch 413, Loss: 0.011302425526082516, Val Loss: 0.013743609189987183\n",
      "Epoch 414, Loss: 0.011272325180470943, Val Loss: 0.013716302812099457\n",
      "Epoch 415, Loss: 0.011242368258535862, Val Loss: 0.013689138926565647\n",
      "Epoch 416, Loss: 0.011212552897632122, Val Loss: 0.013662103563547134\n",
      "Epoch 417, Loss: 0.011182878166437149, Val Loss: 0.013635204173624516\n",
      "Epoch 418, Loss: 0.01115333754569292, Val Loss: 0.01360842864960432\n",
      "Epoch 419, Loss: 0.01112393569201231, Val Loss: 0.013581786304712296\n",
      "Epoch 420, Loss: 0.011094671674072742, Val Loss: 0.013555274344980717\n",
      "Epoch 421, Loss: 0.011065543629229069, Val Loss: 0.01352888997644186\n",
      "Epoch 422, Loss: 0.01103654969483614, Val Loss: 0.013502631336450577\n",
      "Epoch 423, Loss: 0.011007686145603657, Val Loss: 0.013476500287652016\n",
      "Epoch 424, Loss: 0.010978958569467068, Val Loss: 0.013450492173433304\n",
      "Epoch 425, Loss: 0.0109503623098135, Val Loss: 0.01342461071908474\n",
      "Epoch 426, Loss: 0.010921898297965527, Val Loss: 0.0133988531306386\n",
      "Epoch 427, Loss: 0.010893560945987701, Val Loss: 0.013373221270740032\n",
      "Epoch 428, Loss: 0.010865353979170322, Val Loss: 0.01334770955145359\n",
      "Epoch 429, Loss: 0.010837276466190815, Val Loss: 0.013322316110134125\n",
      "Epoch 430, Loss: 0.010809328407049179, Val Loss: 0.013297043740749359\n",
      "Epoch 431, Loss: 0.010781504213809967, Val Loss: 0.013271892443299294\n",
      "Epoch 432, Loss: 0.010753805749118328, Val Loss: 0.01324685849249363\n",
      "Epoch 433, Loss: 0.010726233012974262, Val Loss: 0.013221941888332367\n",
      "Epoch 434, Loss: 0.010698783211410046, Val Loss: 0.01319714542478323\n",
      "Epoch 435, Loss: 0.010671463795006275, Val Loss: 0.013172462582588196\n",
      "Epoch 436, Loss: 0.010644258931279182, Val Loss: 0.013147896155714989\n",
      "Epoch 437, Loss: 0.010617181658744812, Val Loss: 0.01312344055622816\n",
      "Epoch 438, Loss: 0.010590220801532269, Val Loss: 0.013099102303385735\n",
      "Epoch 439, Loss: 0.010563382878899574, Val Loss: 0.01307487953454256\n",
      "Epoch 440, Loss: 0.010536662302911282, Val Loss: 0.013050764799118042\n",
      "Epoch 441, Loss: 0.010510062798857689, Val Loss: 0.013026763685047626\n",
      "Epoch 442, Loss: 0.010483580641448498, Val Loss: 0.01300287339836359\n",
      "Epoch 443, Loss: 0.010457213968038559, Val Loss: 0.012979092076420784\n",
      "Epoch 444, Loss: 0.010430967435240746, Val Loss: 0.01295542437583208\n",
      "Epoch 445, Loss: 0.01040483359247446, Val Loss: 0.01293186005204916\n",
      "Epoch 446, Loss: 0.010378815233707428, Val Loss: 0.012908403761684895\n",
      "Epoch 447, Loss: 0.01035290863364935, Val Loss: 0.012885058298707008\n",
      "Epoch 448, Loss: 0.010327117517590523, Val Loss: 0.012861814349889755\n",
      "Epoch 449, Loss: 0.010301442816853523, Val Loss: 0.012838679365813732\n",
      "Epoch 450, Loss: 0.010275877080857754, Val Loss: 0.01281564962118864\n",
      "Epoch 451, Loss: 0.010250420309603214, Val Loss: 0.012792722322046757\n",
      "Epoch 452, Loss: 0.010225078091025352, Val Loss: 0.01276989933103323\n",
      "Epoch 453, Loss: 0.010199841111898422, Val Loss: 0.012747183442115784\n",
      "Epoch 454, Loss: 0.01017471868544817, Val Loss: 0.012724563479423523\n",
      "Epoch 455, Loss: 0.010149702429771423, Val Loss: 0.01270204782485962\n",
      "Epoch 456, Loss: 0.010124793276190758, Val Loss: 0.012679633684456348\n",
      "Epoch 457, Loss: 0.010099992156028748, Val Loss: 0.012657320126891136\n",
      "Epoch 458, Loss: 0.010075298137962818, Val Loss: 0.012635106220841408\n",
      "Epoch 459, Loss: 0.01005071122199297, Val Loss: 0.012612988241016865\n",
      "Epoch 460, Loss: 0.010026227682828903, Val Loss: 0.012590971775352955\n",
      "Epoch 461, Loss: 0.01000184752047062, Val Loss: 0.01256905309855938\n",
      "Epoch 462, Loss: 0.009977572597563267, Val Loss: 0.01254723034799099\n",
      "Epoch 463, Loss: 0.009953400120139122, Val Loss: 0.01252550259232521\n",
      "Epoch 464, Loss: 0.009929332882165909, Val Loss: 0.012503874488174915\n",
      "Epoch 465, Loss: 0.009905364364385605, Val Loss: 0.012482338584959507\n",
      "Epoch 466, Loss: 0.009881499223411083, Val Loss: 0.012460894882678986\n",
      "Epoch 467, Loss: 0.009857733733952045, Val Loss: 0.012439551763236523\n",
      "Epoch 468, Loss: 0.009834068827331066, Val Loss: 0.012418300844728947\n",
      "Epoch 469, Loss: 0.00981050357222557, Val Loss: 0.012397137470543385\n",
      "Epoch 470, Loss: 0.009787037968635559, Val Loss: 0.012376070953905582\n",
      "Epoch 471, Loss: 0.009763669222593307, Val Loss: 0.012355093844234943\n",
      "Epoch 472, Loss: 0.00974040012806654, Val Loss: 0.012334207072854042\n",
      "Epoch 473, Loss: 0.009717226959764957, Val Loss: 0.012313412502408028\n",
      "Epoch 474, Loss: 0.009694148786365986, Val Loss: 0.012292709201574326\n",
      "Epoch 475, Loss: 0.009671167470514774, Val Loss: 0.012272091582417488\n",
      "Epoch 476, Loss: 0.009648284874856472, Val Loss: 0.012251568026840687\n",
      "Epoch 477, Loss: 0.009625492617487907, Val Loss: 0.012231123633682728\n",
      "Epoch 478, Loss: 0.009602794423699379, Val Loss: 0.012210777960717678\n",
      "Epoch 479, Loss: 0.009580194018781185, Val Loss: 0.012190515175461769\n",
      "Epoch 480, Loss: 0.009557683952152729, Val Loss: 0.01217033714056015\n",
      "Epoch 481, Loss: 0.009535267949104309, Val Loss: 0.012150244787335396\n",
      "Epoch 482, Loss: 0.009512944146990776, Val Loss: 0.012130239978432655\n",
      "Epoch 483, Loss: 0.009490709751844406, Val Loss: 0.012110318057239056\n",
      "Epoch 484, Loss: 0.009468566626310349, Val Loss: 0.01209048368036747\n",
      "Epoch 485, Loss: 0.009446512907743454, Val Loss: 0.012070730328559875\n",
      "Epoch 486, Loss: 0.009424551390111446, Val Loss: 0.01205106358975172\n",
      "Epoch 487, Loss: 0.009402677416801453, Val Loss: 0.012031476944684982\n",
      "Epoch 488, Loss: 0.009380894713103771, Val Loss: 0.01201197225600481\n",
      "Epoch 489, Loss: 0.009359197691082954, Val Loss: 0.011992551386356354\n",
      "Epoch 490, Loss: 0.009337591007351875, Val Loss: 0.011973212473094463\n",
      "Epoch 491, Loss: 0.00931607000529766, Val Loss: 0.011953956447541714\n",
      "Epoch 492, Loss: 0.009294633753597736, Val Loss: 0.011934774927794933\n",
      "Epoch 493, Loss: 0.0092732859775424, Val Loss: 0.01191567350178957\n",
      "Epoch 494, Loss: 0.009252025745809078, Val Loss: 0.011896657757461071\n",
      "Epoch 495, Loss: 0.009230844676494598, Val Loss: 0.011877715587615967\n",
      "Epoch 496, Loss: 0.009209753014147282, Val Loss: 0.011858855374157429\n",
      "Epoch 497, Loss: 0.00918874517083168, Val Loss: 0.01184007152915001\n",
      "Epoch 498, Loss: 0.00916782021522522, Val Loss: 0.011821365915238857\n",
      "Epoch 499, Loss: 0.00914698000997305, Val Loss: 0.011802737601101398\n",
      "Epoch 500, Loss: 0.00912622082978487, Val Loss: 0.011784184724092484\n"
     ]
    }
   ],
   "source": [
    "from flashrag.prompt import PromptTemplate\n",
    "import pickle\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "config_taqa = set_config(dataset_name='taqa', split = split)\n",
    "template_taqa= PromptTemplate(\n",
    "        config = config_taqa,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "config_know= set_config(dataset_name='triviaqa', split = split)\n",
    "template_know= PromptTemplate(\n",
    "        config = config_know,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "\n",
    "\n",
    "loading_hidden = True\n",
    "if not loading_hidden:\n",
    "    taqa_hidden_states, triviaqa_hidden_states = load_datasets(config_taqa, config_triviaqa, template_taqa, template_triviaqa)\n",
    "else:\n",
    "    file_path_know = '/cs/student/projects2/dsml/cdiezmar/hidden_states/knowledge-aware.pkl'\n",
    "    file_path_taqa = '/cs/student/projects2/dsml/cdiezmar/hidden_states/taqa.pkl'\n",
    "\n",
    "    with open(file_path_know, 'rb') as file_know:\n",
    "        know_hidden_states = pickle.load(file_know)\n",
    "        print('Length triviaqa: ', len(know_hidden_states))\n",
    "        know_hidden_states_tensor= torch.tensor(know_hidden_states, dtype=torch.float32)\n",
    "    with open(file_path_taqa, 'rb') as file_taqa:\n",
    "        taqa_hidden_states = pickle.load(file_taqa)\n",
    "        print('Length taqa: ', len(taqa_hidden_states))\n",
    "        taqa_hidden_states_tensor= torch.tensor(taqa_hidden_states, dtype=torch.float32)\n",
    "train_inputs, train_labels, val_inputs, val_labels = prepare_data(taqa_hidden_states_tensor, know_hidden_states_tensor)\n",
    "\n",
    "input_size = train_inputs.size(1)\n",
    "num_classes = 2  # Time-sensitive or not\n",
    "epochs = 500\n",
    "lr = 1e-4\n",
    "\n",
    "model = train_mlp_classifier_cuda(train_inputs, train_labels, val_inputs, val_labels, input_size, num_classes, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `question` in template\n",
      "Find `question` in template\n",
      "Length triviaqa:  20000\n",
      "Length taqa:  10148\n",
      "Epoch 1, Loss: 0.6843438148498535, Val Loss: 0.6842096447944641\n",
      "Epoch 2, Loss: 0.683990478515625, Val Loss: 0.683865487575531\n",
      "Epoch 3, Loss: 0.683641791343689, Val Loss: 0.6835243701934814\n",
      "Epoch 4, Loss: 0.6832959055900574, Val Loss: 0.6831863522529602\n",
      "Epoch 5, Loss: 0.6829527616500854, Val Loss: 0.6828508973121643\n",
      "Epoch 6, Loss: 0.6826122999191284, Val Loss: 0.6825178861618042\n",
      "Epoch 7, Loss: 0.6822740435600281, Val Loss: 0.6821860671043396\n",
      "Epoch 8, Loss: 0.6819368004798889, Val Loss: 0.681851863861084\n",
      "Epoch 9, Loss: 0.6815974712371826, Val Loss: 0.6815138459205627\n",
      "Epoch 10, Loss: 0.6812540292739868, Val Loss: 0.6811704039573669\n",
      "Epoch 11, Loss: 0.6809052228927612, Val Loss: 0.6808199286460876\n",
      "Epoch 12, Loss: 0.6805482506752014, Val Loss: 0.6804585456848145\n",
      "Epoch 13, Loss: 0.6801799535751343, Val Loss: 0.6800833940505981\n",
      "Epoch 14, Loss: 0.679798424243927, Val Loss: 0.6796931624412537\n",
      "Epoch 15, Loss: 0.6794025301933289, Val Loss: 0.6792880296707153\n",
      "Epoch 16, Loss: 0.6789929866790771, Val Loss: 0.6788648962974548\n",
      "Epoch 17, Loss: 0.678565502166748, Val Loss: 0.6784238815307617\n",
      "Epoch 18, Loss: 0.6781201362609863, Val Loss: 0.6779651045799255\n",
      "Epoch 19, Loss: 0.6776565313339233, Val Loss: 0.6774904131889343\n",
      "Epoch 20, Loss: 0.6771764159202576, Val Loss: 0.6769983768463135\n",
      "Epoch 21, Loss: 0.6766785979270935, Val Loss: 0.6764867901802063\n",
      "Epoch 22, Loss: 0.6761608719825745, Val Loss: 0.6759516000747681\n",
      "Epoch 23, Loss: 0.6756194829940796, Val Loss: 0.6753906011581421\n",
      "Epoch 24, Loss: 0.6750523447990417, Val Loss: 0.6748018264770508\n",
      "Epoch 25, Loss: 0.6744582056999207, Val Loss: 0.6741817593574524\n",
      "Epoch 26, Loss: 0.6738326549530029, Val Loss: 0.6735283136367798\n",
      "Epoch 27, Loss: 0.6731743812561035, Val Loss: 0.6728390455245972\n",
      "Epoch 28, Loss: 0.6724804043769836, Val Loss: 0.6721117496490479\n",
      "Epoch 29, Loss: 0.6717484593391418, Val Loss: 0.6713447570800781\n",
      "Epoch 30, Loss: 0.6709764003753662, Val Loss: 0.670538067817688\n",
      "Epoch 31, Loss: 0.6701642274856567, Val Loss: 0.6696935892105103\n",
      "Epoch 32, Loss: 0.6693137288093567, Val Loss: 0.6688110828399658\n",
      "Epoch 33, Loss: 0.6684255003929138, Val Loss: 0.6678878664970398\n",
      "Epoch 34, Loss: 0.667496383190155, Val Loss: 0.6669194102287292\n",
      "Epoch 35, Loss: 0.6665216684341431, Val Loss: 0.6658987998962402\n",
      "Epoch 36, Loss: 0.6654939651489258, Val Loss: 0.6648175120353699\n",
      "Epoch 37, Loss: 0.6644043326377869, Val Loss: 0.6636705994606018\n",
      "Epoch 38, Loss: 0.6632483601570129, Val Loss: 0.6624541878700256\n",
      "Epoch 39, Loss: 0.6620211601257324, Val Loss: 0.6611703038215637\n",
      "Epoch 40, Loss: 0.660724401473999, Val Loss: 0.6598122119903564\n",
      "Epoch 41, Loss: 0.6593533158302307, Val Loss: 0.6583752632141113\n",
      "Epoch 42, Loss: 0.6579040288925171, Val Loss: 0.6568576097488403\n",
      "Epoch 43, Loss: 0.6563736200332642, Val Loss: 0.6552485823631287\n",
      "Epoch 44, Loss: 0.6547514200210571, Val Loss: 0.6535426378250122\n",
      "Epoch 45, Loss: 0.6530319452285767, Val Loss: 0.6517356038093567\n",
      "Epoch 46, Loss: 0.6512117981910706, Val Loss: 0.6498245000839233\n",
      "Epoch 47, Loss: 0.6492873430252075, Val Loss: 0.6478021144866943\n",
      "Epoch 48, Loss: 0.6472508311271667, Val Loss: 0.6456710696220398\n",
      "Epoch 49, Loss: 0.6451053023338318, Val Loss: 0.6434169411659241\n",
      "Epoch 50, Loss: 0.6428359746932983, Val Loss: 0.6410306692123413\n",
      "Epoch 51, Loss: 0.6404340267181396, Val Loss: 0.6384987831115723\n",
      "Epoch 52, Loss: 0.6378849744796753, Val Loss: 0.635810136795044\n",
      "Epoch 53, Loss: 0.6351784467697144, Val Loss: 0.6329633593559265\n",
      "Epoch 54, Loss: 0.6323146224021912, Val Loss: 0.6299633383750916\n",
      "Epoch 55, Loss: 0.6292933821678162, Val Loss: 0.6268086433410645\n",
      "Epoch 56, Loss: 0.6261097192764282, Val Loss: 0.6234673261642456\n",
      "Epoch 57, Loss: 0.6227396726608276, Val Loss: 0.6199135780334473\n",
      "Epoch 58, Loss: 0.6191586256027222, Val Loss: 0.6161580085754395\n",
      "Epoch 59, Loss: 0.6153780221939087, Val Loss: 0.6122039556503296\n",
      "Epoch 60, Loss: 0.6113983988761902, Val Loss: 0.6080649495124817\n",
      "Epoch 61, Loss: 0.607232928276062, Val Loss: 0.6037460565567017\n",
      "Epoch 62, Loss: 0.6028870940208435, Val Loss: 0.599236249923706\n",
      "Epoch 63, Loss: 0.5983498692512512, Val Loss: 0.5945314764976501\n",
      "Epoch 64, Loss: 0.5936118364334106, Val Loss: 0.5896283388137817\n",
      "Epoch 65, Loss: 0.5886704325675964, Val Loss: 0.584510862827301\n",
      "Epoch 66, Loss: 0.5835155844688416, Val Loss: 0.5791516900062561\n",
      "Epoch 67, Loss: 0.5781204104423523, Val Loss: 0.5735456943511963\n",
      "Epoch 68, Loss: 0.5724798440933228, Val Loss: 0.5677123069763184\n",
      "Epoch 69, Loss: 0.5666092038154602, Val Loss: 0.5616660118103027\n",
      "Epoch 70, Loss: 0.5605216026306152, Val Loss: 0.5554057359695435\n",
      "Epoch 71, Loss: 0.5542154908180237, Val Loss: 0.5489396452903748\n",
      "Epoch 72, Loss: 0.5476992130279541, Val Loss: 0.5422667264938354\n",
      "Epoch 73, Loss: 0.5409700870513916, Val Loss: 0.5353977680206299\n",
      "Epoch 74, Loss: 0.5340372920036316, Val Loss: 0.5283422470092773\n",
      "Epoch 75, Loss: 0.5269122123718262, Val Loss: 0.521111249923706\n",
      "Epoch 76, Loss: 0.5196123719215393, Val Loss: 0.5136995315551758\n",
      "Epoch 77, Loss: 0.5121369957923889, Val Loss: 0.5061001777648926\n",
      "Epoch 78, Loss: 0.504476010799408, Val Loss: 0.49831607937812805\n",
      "Epoch 79, Loss: 0.4966249167919159, Val Loss: 0.49037811160087585\n",
      "Epoch 80, Loss: 0.4886193573474884, Val Loss: 0.48231241106987\n",
      "Epoch 81, Loss: 0.4804885983467102, Val Loss: 0.4741508662700653\n",
      "Epoch 82, Loss: 0.47225794196128845, Val Loss: 0.46590331196784973\n",
      "Epoch 83, Loss: 0.46393686532974243, Val Loss: 0.4575796127319336\n",
      "Epoch 84, Loss: 0.4555303156375885, Val Loss: 0.4491966962814331\n",
      "Epoch 85, Loss: 0.44705742597579956, Val Loss: 0.44077056646347046\n",
      "Epoch 86, Loss: 0.43854421377182007, Val Loss: 0.4323267638683319\n",
      "Epoch 87, Loss: 0.4300183951854706, Val Loss: 0.42388880252838135\n",
      "Epoch 88, Loss: 0.421498566865921, Val Loss: 0.41548311710357666\n",
      "Epoch 89, Loss: 0.4130023717880249, Val Loss: 0.40715181827545166\n",
      "Epoch 90, Loss: 0.40457332134246826, Val Loss: 0.3989078104496002\n",
      "Epoch 91, Loss: 0.3962371349334717, Val Loss: 0.39079055190086365\n",
      "Epoch 92, Loss: 0.38802993297576904, Val Loss: 0.38281282782554626\n",
      "Epoch 93, Loss: 0.3799641728401184, Val Loss: 0.37496837973594666\n",
      "Epoch 94, Loss: 0.372048944234848, Val Loss: 0.3672638535499573\n",
      "Epoch 95, Loss: 0.3642847537994385, Val Loss: 0.35972708463668823\n",
      "Epoch 96, Loss: 0.3566780686378479, Val Loss: 0.3523560166358948\n",
      "Epoch 97, Loss: 0.3492434024810791, Val Loss: 0.34516507387161255\n",
      "Epoch 98, Loss: 0.34200114011764526, Val Loss: 0.33816757798194885\n",
      "Epoch 99, Loss: 0.3349597156047821, Val Loss: 0.33136242628097534\n",
      "Epoch 100, Loss: 0.32813191413879395, Val Loss: 0.3247925937175751\n",
      "Epoch 101, Loss: 0.321536123752594, Val Loss: 0.3184569478034973\n",
      "Epoch 102, Loss: 0.3151629865169525, Val Loss: 0.31236377358436584\n",
      "Epoch 103, Loss: 0.3090318441390991, Val Loss: 0.3065115213394165\n",
      "Epoch 104, Loss: 0.3031339645385742, Val Loss: 0.3008849322795868\n",
      "Epoch 105, Loss: 0.29747840762138367, Val Loss: 0.2954714894294739\n",
      "Epoch 106, Loss: 0.29206445813179016, Val Loss: 0.29028719663619995\n",
      "Epoch 107, Loss: 0.28686702251434326, Val Loss: 0.28533124923706055\n",
      "Epoch 108, Loss: 0.28187915682792664, Val Loss: 0.28056788444519043\n",
      "Epoch 109, Loss: 0.27709653973579407, Val Loss: 0.2759943902492523\n",
      "Epoch 110, Loss: 0.27251869440078735, Val Loss: 0.2716476321220398\n",
      "Epoch 111, Loss: 0.26814794540405273, Val Loss: 0.2674979269504547\n",
      "Epoch 112, Loss: 0.2639670670032501, Val Loss: 0.2635214328765869\n",
      "Epoch 113, Loss: 0.2599710524082184, Val Loss: 0.2597239315509796\n",
      "Epoch 114, Loss: 0.2561500072479248, Val Loss: 0.25610247254371643\n",
      "Epoch 115, Loss: 0.2525027394294739, Val Loss: 0.25264817476272583\n",
      "Epoch 116, Loss: 0.24902412295341492, Val Loss: 0.2493516057729721\n",
      "Epoch 117, Loss: 0.24569346010684967, Val Loss: 0.24619875848293304\n",
      "Epoch 118, Loss: 0.24250656366348267, Val Loss: 0.24317964911460876\n",
      "Epoch 119, Loss: 0.23945610225200653, Val Loss: 0.240277960896492\n",
      "Epoch 120, Loss: 0.236533984541893, Val Loss: 0.23749981820583344\n",
      "Epoch 121, Loss: 0.23373441398143768, Val Loss: 0.23483391106128693\n",
      "Epoch 122, Loss: 0.23104587197303772, Val Loss: 0.23227478563785553\n",
      "Epoch 123, Loss: 0.2284650057554245, Val Loss: 0.22981594502925873\n",
      "Epoch 124, Loss: 0.22598473727703094, Val Loss: 0.22744353115558624\n",
      "Epoch 125, Loss: 0.22359023988246918, Val Loss: 0.22515396773815155\n",
      "Epoch 126, Loss: 0.22127674520015717, Val Loss: 0.22294387221336365\n",
      "Epoch 127, Loss: 0.21904081106185913, Val Loss: 0.2208121418952942\n",
      "Epoch 128, Loss: 0.21688184142112732, Val Loss: 0.21875447034835815\n",
      "Epoch 129, Loss: 0.21479906141757965, Val Loss: 0.21676255762577057\n",
      "Epoch 130, Loss: 0.21278293430805206, Val Loss: 0.21483269333839417\n",
      "Epoch 131, Loss: 0.21082986891269684, Val Loss: 0.21296142041683197\n",
      "Epoch 132, Loss: 0.20893476903438568, Val Loss: 0.21114125847816467\n",
      "Epoch 133, Loss: 0.20709228515625, Val Loss: 0.20937199890613556\n",
      "Epoch 134, Loss: 0.20529969036579132, Val Loss: 0.207646444439888\n",
      "Epoch 135, Loss: 0.20355044305324554, Val Loss: 0.20596176385879517\n",
      "Epoch 136, Loss: 0.20184224843978882, Val Loss: 0.20431312918663025\n",
      "Epoch 137, Loss: 0.20017404854297638, Val Loss: 0.20269572734832764\n",
      "Epoch 138, Loss: 0.19854050874710083, Val Loss: 0.20111113786697388\n",
      "Epoch 139, Loss: 0.19694256782531738, Val Loss: 0.199563130736351\n",
      "Epoch 140, Loss: 0.19538281857967377, Val Loss: 0.19804486632347107\n",
      "Epoch 141, Loss: 0.19385461509227753, Val Loss: 0.19655761122703552\n",
      "Epoch 142, Loss: 0.19235718250274658, Val Loss: 0.1950935572385788\n",
      "Epoch 143, Loss: 0.1908842772245407, Val Loss: 0.19365206360816956\n",
      "Epoch 144, Loss: 0.18943506479263306, Val Loss: 0.19222912192344666\n",
      "Epoch 145, Loss: 0.1880057156085968, Val Loss: 0.19082757830619812\n",
      "Epoch 146, Loss: 0.18659794330596924, Val Loss: 0.18944546580314636\n",
      "Epoch 147, Loss: 0.18520992994308472, Val Loss: 0.18808290362358093\n",
      "Epoch 148, Loss: 0.18384264409542084, Val Loss: 0.1867338865995407\n",
      "Epoch 149, Loss: 0.18249021470546722, Val Loss: 0.18539881706237793\n",
      "Epoch 150, Loss: 0.18115301430225372, Val Loss: 0.18407565355300903\n",
      "Epoch 151, Loss: 0.17982779443264008, Val Loss: 0.1827593594789505\n",
      "Epoch 152, Loss: 0.17850959300994873, Val Loss: 0.1814551055431366\n",
      "Epoch 153, Loss: 0.17720472812652588, Val Loss: 0.1801603138446808\n",
      "Epoch 154, Loss: 0.17591023445129395, Val Loss: 0.17887693643569946\n",
      "Epoch 155, Loss: 0.1746276617050171, Val Loss: 0.17760063707828522\n",
      "Epoch 156, Loss: 0.17335322499275208, Val Loss: 0.1763346791267395\n",
      "Epoch 157, Loss: 0.17208844423294067, Val Loss: 0.1750839650630951\n",
      "Epoch 158, Loss: 0.17083805799484253, Val Loss: 0.17384454607963562\n",
      "Epoch 159, Loss: 0.16959767043590546, Val Loss: 0.17261114716529846\n",
      "Epoch 160, Loss: 0.1683633029460907, Val Loss: 0.171380415558815\n",
      "Epoch 161, Loss: 0.16713450849056244, Val Loss: 0.17015685141086578\n",
      "Epoch 162, Loss: 0.16591398417949677, Val Loss: 0.16893930733203888\n",
      "Epoch 163, Loss: 0.16470041871070862, Val Loss: 0.1677282601594925\n",
      "Epoch 164, Loss: 0.16349218785762787, Val Loss: 0.16652369499206543\n",
      "Epoch 165, Loss: 0.16228841245174408, Val Loss: 0.1653238981962204\n",
      "Epoch 166, Loss: 0.16108787059783936, Val Loss: 0.16412505507469177\n",
      "Epoch 167, Loss: 0.15988649427890778, Val Loss: 0.16292470693588257\n",
      "Epoch 168, Loss: 0.15868249535560608, Val Loss: 0.1617247760295868\n",
      "Epoch 169, Loss: 0.15747970342636108, Val Loss: 0.160526305437088\n",
      "Epoch 170, Loss: 0.1562805026769638, Val Loss: 0.15933570265769958\n",
      "Epoch 171, Loss: 0.1550886332988739, Val Loss: 0.15815469622612\n",
      "Epoch 172, Loss: 0.15390489995479584, Val Loss: 0.15697632730007172\n",
      "Epoch 173, Loss: 0.1527232974767685, Val Loss: 0.1558002382516861\n",
      "Epoch 174, Loss: 0.15154322981834412, Val Loss: 0.15462280809879303\n",
      "Epoch 175, Loss: 0.15036340057849884, Val Loss: 0.1534450501203537\n",
      "Epoch 176, Loss: 0.14918455481529236, Val Loss: 0.15226677060127258\n",
      "Epoch 177, Loss: 0.1480051726102829, Val Loss: 0.15109021961688995\n",
      "Epoch 178, Loss: 0.14682796597480774, Val Loss: 0.14991481602191925\n",
      "Epoch 179, Loss: 0.145650252699852, Val Loss: 0.14874187111854553\n",
      "Epoch 180, Loss: 0.144476518034935, Val Loss: 0.1475677490234375\n",
      "Epoch 181, Loss: 0.14330065250396729, Val Loss: 0.1463957130908966\n",
      "Epoch 182, Loss: 0.14212600886821747, Val Loss: 0.14522264897823334\n",
      "Epoch 183, Loss: 0.14095087349414825, Val Loss: 0.14405368268489838\n",
      "Epoch 184, Loss: 0.13977999985218048, Val Loss: 0.14288483560085297\n",
      "Epoch 185, Loss: 0.13860909640789032, Val Loss: 0.14171643555164337\n",
      "Epoch 186, Loss: 0.13743865489959717, Val Loss: 0.140546515583992\n",
      "Epoch 187, Loss: 0.13626673817634583, Val Loss: 0.13937796652317047\n",
      "Epoch 188, Loss: 0.1350957304239273, Val Loss: 0.13820672035217285\n",
      "Epoch 189, Loss: 0.13392122089862823, Val Loss: 0.13703513145446777\n",
      "Epoch 190, Loss: 0.1327459067106247, Val Loss: 0.13585978746414185\n",
      "Epoch 191, Loss: 0.13156597316265106, Val Loss: 0.13468439877033234\n",
      "Epoch 192, Loss: 0.130384624004364, Val Loss: 0.13350962102413177\n",
      "Epoch 193, Loss: 0.12920334935188293, Val Loss: 0.13233661651611328\n",
      "Epoch 194, Loss: 0.1280246078968048, Val Loss: 0.1311628371477127\n",
      "Epoch 195, Loss: 0.12684760987758636, Val Loss: 0.12998880445957184\n",
      "Epoch 196, Loss: 0.1256718784570694, Val Loss: 0.12881281971931458\n",
      "Epoch 197, Loss: 0.12449420988559723, Val Loss: 0.12763629853725433\n",
      "Epoch 198, Loss: 0.12331586331129074, Val Loss: 0.12645873427391052\n",
      "Epoch 199, Loss: 0.12213648855686188, Val Loss: 0.12527962028980255\n",
      "Epoch 200, Loss: 0.12095677852630615, Val Loss: 0.12410194426774979\n",
      "Epoch 201, Loss: 0.11977948248386383, Val Loss: 0.12292496860027313\n",
      "Epoch 202, Loss: 0.1186024621129036, Val Loss: 0.1217498853802681\n",
      "Epoch 203, Loss: 0.11742762476205826, Val Loss: 0.12057570368051529\n",
      "Epoch 204, Loss: 0.11625494062900543, Val Loss: 0.11940094828605652\n",
      "Epoch 205, Loss: 0.11508176475763321, Val Loss: 0.11822738498449326\n",
      "Epoch 206, Loss: 0.11391030997037888, Val Loss: 0.11705383658409119\n",
      "Epoch 207, Loss: 0.11274024099111557, Val Loss: 0.11588099598884583\n",
      "Epoch 208, Loss: 0.11157150566577911, Val Loss: 0.1147082969546318\n",
      "Epoch 209, Loss: 0.11040493100881577, Val Loss: 0.11353608965873718\n",
      "Epoch 210, Loss: 0.1092398539185524, Val Loss: 0.11236691474914551\n",
      "Epoch 211, Loss: 0.10807856917381287, Val Loss: 0.11119795590639114\n",
      "Epoch 212, Loss: 0.10691745579242706, Val Loss: 0.11003097891807556\n",
      "Epoch 213, Loss: 0.1057581678032875, Val Loss: 0.1088666096329689\n",
      "Epoch 214, Loss: 0.10460076481103897, Val Loss: 0.10770457237958908\n",
      "Epoch 215, Loss: 0.10344424098730087, Val Loss: 0.10654280334711075\n",
      "Epoch 216, Loss: 0.10228754580020905, Val Loss: 0.10538049042224884\n",
      "Epoch 217, Loss: 0.1011294350028038, Val Loss: 0.10421999543905258\n",
      "Epoch 218, Loss: 0.09997306764125824, Val Loss: 0.10306719690561295\n",
      "Epoch 219, Loss: 0.09882377833127975, Val Loss: 0.10192129015922546\n",
      "Epoch 220, Loss: 0.09768137335777283, Val Loss: 0.10077855736017227\n",
      "Epoch 221, Loss: 0.09654231369495392, Val Loss: 0.09963946044445038\n",
      "Epoch 222, Loss: 0.09540697187185287, Val Loss: 0.09850224107503891\n",
      "Epoch 223, Loss: 0.0942726656794548, Val Loss: 0.0973670482635498\n",
      "Epoch 224, Loss: 0.09314024448394775, Val Loss: 0.0962354838848114\n",
      "Epoch 225, Loss: 0.0920109897851944, Val Loss: 0.09510879963636398\n",
      "Epoch 226, Loss: 0.09088598191738129, Val Loss: 0.0939873605966568\n",
      "Epoch 227, Loss: 0.0897652804851532, Val Loss: 0.0928705558180809\n",
      "Epoch 228, Loss: 0.08864917606115341, Val Loss: 0.09176012128591537\n",
      "Epoch 229, Loss: 0.08753812313079834, Val Loss: 0.09065429121255875\n",
      "Epoch 230, Loss: 0.08643212914466858, Val Loss: 0.08955463021993637\n",
      "Epoch 231, Loss: 0.085331492125988, Val Loss: 0.08845876157283783\n",
      "Epoch 232, Loss: 0.0842357873916626, Val Loss: 0.08736994117498398\n",
      "Epoch 233, Loss: 0.08314750343561172, Val Loss: 0.08628694713115692\n",
      "Epoch 234, Loss: 0.08206547796726227, Val Loss: 0.08521166443824768\n",
      "Epoch 235, Loss: 0.08099105209112167, Val Loss: 0.08414193987846375\n",
      "Epoch 236, Loss: 0.07992195338010788, Val Loss: 0.08308285474777222\n",
      "Epoch 237, Loss: 0.07886252552270889, Val Loss: 0.082029789686203\n",
      "Epoch 238, Loss: 0.07780861109495163, Val Loss: 0.08098549395799637\n",
      "Epoch 239, Loss: 0.07676292955875397, Val Loss: 0.0799475759267807\n",
      "Epoch 240, Loss: 0.07572413980960846, Val Loss: 0.07891698181629181\n",
      "Epoch 241, Loss: 0.07469334453344345, Val Loss: 0.07789518684148788\n",
      "Epoch 242, Loss: 0.07367081195116043, Val Loss: 0.0768800675868988\n",
      "Epoch 243, Loss: 0.07265395671129227, Val Loss: 0.07587376236915588\n",
      "Epoch 244, Loss: 0.07164496183395386, Val Loss: 0.07488097995519638\n",
      "Epoch 245, Loss: 0.07064826041460037, Val Loss: 0.0738988071680069\n",
      "Epoch 246, Loss: 0.06965984404087067, Val Loss: 0.07292915880680084\n",
      "Epoch 247, Loss: 0.06868193298578262, Val Loss: 0.07197068631649017\n",
      "Epoch 248, Loss: 0.06771326065063477, Val Loss: 0.07102183252573013\n",
      "Epoch 249, Loss: 0.06675408035516739, Val Loss: 0.0700833797454834\n",
      "Epoch 250, Loss: 0.06580477952957153, Val Loss: 0.06915368884801865\n",
      "Epoch 251, Loss: 0.06486407667398453, Val Loss: 0.06823208183050156\n",
      "Epoch 252, Loss: 0.06393107771873474, Val Loss: 0.0673205628991127\n",
      "Epoch 253, Loss: 0.0630079060792923, Val Loss: 0.06642003357410431\n",
      "Epoch 254, Loss: 0.06209590286016464, Val Loss: 0.06552950292825699\n",
      "Epoch 255, Loss: 0.061193887144327164, Val Loss: 0.06465157121419907\n",
      "Epoch 256, Loss: 0.060304127633571625, Val Loss: 0.06378456950187683\n",
      "Epoch 257, Loss: 0.05942501872777939, Val Loss: 0.06292681396007538\n",
      "Epoch 258, Loss: 0.05855646729469299, Val Loss: 0.06207625940442085\n",
      "Epoch 259, Loss: 0.057697173207998276, Val Loss: 0.061235882341861725\n",
      "Epoch 260, Loss: 0.0568496510386467, Val Loss: 0.06040416285395622\n",
      "Epoch 261, Loss: 0.05601135268807411, Val Loss: 0.059584684669971466\n",
      "Epoch 262, Loss: 0.05518556013703346, Val Loss: 0.058777373284101486\n",
      "Epoch 263, Loss: 0.0543706938624382, Val Loss: 0.05798234045505524\n",
      "Epoch 264, Loss: 0.05356715992093086, Val Loss: 0.05719960853457451\n",
      "Epoch 265, Loss: 0.05277569591999054, Val Loss: 0.05642574653029442\n",
      "Epoch 266, Loss: 0.05199463292956352, Val Loss: 0.05566192790865898\n",
      "Epoch 267, Loss: 0.05122452601790428, Val Loss: 0.05491030961275101\n",
      "Epoch 268, Loss: 0.050466448068618774, Val Loss: 0.054168909788131714\n",
      "Epoch 269, Loss: 0.04971902817487717, Val Loss: 0.053439732640981674\n",
      "Epoch 270, Loss: 0.04898255690932274, Val Loss: 0.05272211134433746\n",
      "Epoch 271, Loss: 0.04825747013092041, Val Loss: 0.052015576511621475\n",
      "Epoch 272, Loss: 0.047542642802000046, Val Loss: 0.05131999030709267\n",
      "Epoch 273, Loss: 0.0468384250998497, Val Loss: 0.050634901970624924\n",
      "Epoch 274, Loss: 0.046145036816596985, Val Loss: 0.049959465861320496\n",
      "Epoch 275, Loss: 0.04546135291457176, Val Loss: 0.04929400607943535\n",
      "Epoch 276, Loss: 0.044788334518671036, Val Loss: 0.04863760992884636\n",
      "Epoch 277, Loss: 0.04412563890218735, Val Loss: 0.047991715371608734\n",
      "Epoch 278, Loss: 0.0434737429022789, Val Loss: 0.04735393449664116\n",
      "Epoch 279, Loss: 0.0428311824798584, Val Loss: 0.046728454530239105\n",
      "Epoch 280, Loss: 0.04219982028007507, Val Loss: 0.04611433669924736\n",
      "Epoch 281, Loss: 0.04157956317067146, Val Loss: 0.04551000893115997\n",
      "Epoch 282, Loss: 0.04096895456314087, Val Loss: 0.04491586238145828\n",
      "Epoch 283, Loss: 0.04036845266819, Val Loss: 0.04433157667517662\n",
      "Epoch 284, Loss: 0.039777226746082306, Val Loss: 0.04375822842121124\n",
      "Epoch 285, Loss: 0.03919639810919762, Val Loss: 0.0431944839656353\n",
      "Epoch 286, Loss: 0.03862454742193222, Val Loss: 0.04263971000909805\n",
      "Epoch 287, Loss: 0.03806249424815178, Val Loss: 0.04209485277533531\n",
      "Epoch 288, Loss: 0.03751140087842941, Val Loss: 0.0415545217692852\n",
      "Epoch 289, Loss: 0.0369684062898159, Val Loss: 0.04102235287427902\n",
      "Epoch 290, Loss: 0.03643425181508064, Val Loss: 0.04050230607390404\n",
      "Epoch 291, Loss: 0.03591112047433853, Val Loss: 0.03999311849474907\n",
      "Epoch 292, Loss: 0.03539690002799034, Val Loss: 0.039493877440690994\n",
      "Epoch 293, Loss: 0.0348910391330719, Val Loss: 0.03900470957159996\n",
      "Epoch 294, Loss: 0.03439413756132126, Val Loss: 0.03852562978863716\n",
      "Epoch 295, Loss: 0.033906109631061554, Val Loss: 0.03805342689156532\n",
      "Epoch 296, Loss: 0.03342581167817116, Val Loss: 0.03758976235985756\n",
      "Epoch 297, Loss: 0.032955270260572433, Val Loss: 0.03713230416178703\n",
      "Epoch 298, Loss: 0.032491881400346756, Val Loss: 0.03668338060379028\n",
      "Epoch 299, Loss: 0.0320374071598053, Val Loss: 0.03624391183257103\n",
      "Epoch 300, Loss: 0.03159181401133537, Val Loss: 0.03581211715936661\n",
      "Epoch 301, Loss: 0.03115377388894558, Val Loss: 0.03538941964507103\n",
      "Epoch 302, Loss: 0.03072441928088665, Val Loss: 0.03497404605150223\n",
      "Epoch 303, Loss: 0.03030216507613659, Val Loss: 0.03456699103116989\n",
      "Epoch 304, Loss: 0.029887814074754715, Val Loss: 0.03416691720485687\n",
      "Epoch 305, Loss: 0.029481008648872375, Val Loss: 0.033773768693208694\n",
      "Epoch 306, Loss: 0.029081353917717934, Val Loss: 0.033388201147317886\n",
      "Epoch 307, Loss: 0.028689509257674217, Val Loss: 0.03300897032022476\n",
      "Epoch 308, Loss: 0.028304578736424446, Val Loss: 0.03263729810714722\n",
      "Epoch 309, Loss: 0.02792651392519474, Val Loss: 0.03227309137582779\n",
      "Epoch 310, Loss: 0.027555568143725395, Val Loss: 0.0319146066904068\n",
      "Epoch 311, Loss: 0.027191046625375748, Val Loss: 0.031564101576805115\n",
      "Epoch 312, Loss: 0.026833510026335716, Val Loss: 0.03121950291097164\n",
      "Epoch 313, Loss: 0.026482054963707924, Val Loss: 0.030881468206644058\n",
      "Epoch 314, Loss: 0.026137368753552437, Val Loss: 0.030550185590982437\n",
      "Epoch 315, Loss: 0.025798624381422997, Val Loss: 0.030224379152059555\n",
      "Epoch 316, Loss: 0.025465715676546097, Val Loss: 0.02990458719432354\n",
      "Epoch 317, Loss: 0.02513885498046875, Val Loss: 0.029590517282485962\n",
      "Epoch 318, Loss: 0.024818306788802147, Val Loss: 0.029282089322805405\n",
      "Epoch 319, Loss: 0.024503663182258606, Val Loss: 0.028978122398257256\n",
      "Epoch 320, Loss: 0.024194451048970222, Val Loss: 0.028679270297288895\n",
      "Epoch 321, Loss: 0.02389051392674446, Val Loss: 0.028386123478412628\n",
      "Epoch 322, Loss: 0.023592522367835045, Val Loss: 0.02809888869524002\n",
      "Epoch 323, Loss: 0.02329947054386139, Val Loss: 0.02781759575009346\n",
      "Epoch 324, Loss: 0.023011641576886177, Val Loss: 0.027542680501937866\n",
      "Epoch 325, Loss: 0.022729316726326942, Val Loss: 0.02727278508245945\n",
      "Epoch 326, Loss: 0.02245170809328556, Val Loss: 0.027008172124624252\n",
      "Epoch 327, Loss: 0.02217927947640419, Val Loss: 0.026749569922685623\n",
      "Epoch 328, Loss: 0.02191176824271679, Val Loss: 0.026494409888982773\n",
      "Epoch 329, Loss: 0.021649185568094254, Val Loss: 0.026244278997182846\n",
      "Epoch 330, Loss: 0.02139078825712204, Val Loss: 0.025999506935477257\n",
      "Epoch 331, Loss: 0.02113739773631096, Val Loss: 0.02575833909213543\n",
      "Epoch 332, Loss: 0.020887533202767372, Val Loss: 0.02552209421992302\n",
      "Epoch 333, Loss: 0.020642511546611786, Val Loss: 0.025290919467806816\n",
      "Epoch 334, Loss: 0.02040199190378189, Val Loss: 0.02506309002637863\n",
      "Epoch 335, Loss: 0.020164990797638893, Val Loss: 0.024840377271175385\n",
      "Epoch 336, Loss: 0.019932687282562256, Val Loss: 0.024621926248073578\n",
      "Epoch 337, Loss: 0.019704336300492287, Val Loss: 0.02440730854868889\n",
      "Epoch 338, Loss: 0.01947944611310959, Val Loss: 0.02419695258140564\n",
      "Epoch 339, Loss: 0.019258789718151093, Val Loss: 0.02398979477584362\n",
      "Epoch 340, Loss: 0.019041310995817184, Val Loss: 0.023785898461937904\n",
      "Epoch 341, Loss: 0.018828071653842926, Val Loss: 0.023585937917232513\n",
      "Epoch 342, Loss: 0.018618207424879074, Val Loss: 0.023387547582387924\n",
      "Epoch 343, Loss: 0.018411880359053612, Val Loss: 0.02319308929145336\n",
      "Epoch 344, Loss: 0.018209248781204224, Val Loss: 0.023001614958047867\n",
      "Epoch 345, Loss: 0.01800990290939808, Val Loss: 0.022814234718680382\n",
      "Epoch 346, Loss: 0.017813924700021744, Val Loss: 0.02263065241277218\n",
      "Epoch 347, Loss: 0.017620928585529327, Val Loss: 0.022451069205999374\n",
      "Epoch 348, Loss: 0.01743098720908165, Val Loss: 0.02227504923939705\n",
      "Epoch 349, Loss: 0.017244381830096245, Val Loss: 0.02210269309580326\n",
      "Epoch 350, Loss: 0.01706019416451454, Val Loss: 0.021934306249022484\n",
      "Epoch 351, Loss: 0.01687924936413765, Val Loss: 0.021770397201180458\n",
      "Epoch 352, Loss: 0.016701366752386093, Val Loss: 0.02160792052745819\n",
      "Epoch 353, Loss: 0.01652592606842518, Val Loss: 0.021450072526931763\n",
      "Epoch 354, Loss: 0.016353363171219826, Val Loss: 0.021293744444847107\n",
      "Epoch 355, Loss: 0.016183942556381226, Val Loss: 0.02113809622824192\n",
      "Epoch 356, Loss: 0.016017666086554527, Val Loss: 0.020982038229703903\n",
      "Epoch 357, Loss: 0.015853578224778175, Val Loss: 0.02082652598619461\n",
      "Epoch 358, Loss: 0.015691988170146942, Val Loss: 0.020673401653766632\n",
      "Epoch 359, Loss: 0.01553298532962799, Val Loss: 0.020524146035313606\n",
      "Epoch 360, Loss: 0.015376472845673561, Val Loss: 0.020379969850182533\n",
      "Epoch 361, Loss: 0.015222475863993168, Val Loss: 0.020239371806383133\n",
      "Epoch 362, Loss: 0.015070723369717598, Val Loss: 0.02010364644229412\n",
      "Epoch 363, Loss: 0.014921451918780804, Val Loss: 0.019971352070569992\n",
      "Epoch 364, Loss: 0.014773954637348652, Val Loss: 0.019843412563204765\n",
      "Epoch 365, Loss: 0.014629332348704338, Val Loss: 0.019717413932085037\n",
      "Epoch 366, Loss: 0.014486543834209442, Val Loss: 0.019592516124248505\n",
      "Epoch 367, Loss: 0.014346093870699406, Val Loss: 0.01946648582816124\n",
      "Epoch 368, Loss: 0.014207632280886173, Val Loss: 0.019340064376592636\n",
      "Epoch 369, Loss: 0.014071181416511536, Val Loss: 0.019213277846574783\n",
      "Epoch 370, Loss: 0.013936758041381836, Val Loss: 0.019086187705397606\n",
      "Epoch 371, Loss: 0.013804212212562561, Val Loss: 0.018962278962135315\n",
      "Epoch 372, Loss: 0.013673685491085052, Val Loss: 0.0188402459025383\n",
      "Epoch 373, Loss: 0.013544950634241104, Val Loss: 0.01872183382511139\n",
      "Epoch 374, Loss: 0.013418290764093399, Val Loss: 0.018605856224894524\n",
      "Epoch 375, Loss: 0.013293043710291386, Val Loss: 0.018493346869945526\n",
      "Epoch 376, Loss: 0.013169681653380394, Val Loss: 0.018383856862783432\n",
      "Epoch 377, Loss: 0.013048260472714901, Val Loss: 0.018276922404766083\n",
      "Epoch 378, Loss: 0.012928261421620846, Val Loss: 0.018171779811382294\n",
      "Epoch 379, Loss: 0.012810243293642998, Val Loss: 0.018069198355078697\n",
      "Epoch 380, Loss: 0.012693815864622593, Val Loss: 0.017967194318771362\n",
      "Epoch 381, Loss: 0.012578743509948254, Val Loss: 0.017866402864456177\n",
      "Epoch 382, Loss: 0.012465499341487885, Val Loss: 0.017765769734978676\n",
      "Epoch 383, Loss: 0.012353654019534588, Val Loss: 0.017666270956397057\n",
      "Epoch 384, Loss: 0.012243380770087242, Val Loss: 0.01756700687110424\n",
      "Epoch 385, Loss: 0.012134583666920662, Val Loss: 0.017468620091676712\n",
      "Epoch 386, Loss: 0.012027108110487461, Val Loss: 0.017371490597724915\n",
      "Epoch 387, Loss: 0.011921503581106663, Val Loss: 0.017276063561439514\n",
      "Epoch 388, Loss: 0.011817044578492641, Val Loss: 0.017182590439915657\n",
      "Epoch 389, Loss: 0.011713855899870396, Val Loss: 0.017090875655412674\n",
      "Epoch 390, Loss: 0.011612399481236935, Val Loss: 0.017000025138258934\n",
      "Epoch 391, Loss: 0.011511772871017456, Val Loss: 0.016912108287215233\n",
      "Epoch 392, Loss: 0.011412665247917175, Val Loss: 0.016825320199131966\n",
      "Epoch 393, Loss: 0.011315052397549152, Val Loss: 0.016741186380386353\n",
      "Epoch 394, Loss: 0.011218572966754436, Val Loss: 0.016658781096339226\n",
      "Epoch 395, Loss: 0.011123590171337128, Val Loss: 0.016576234251260757\n",
      "Epoch 396, Loss: 0.011029423214495182, Val Loss: 0.016495579853653908\n",
      "Epoch 397, Loss: 0.010936818085610867, Val Loss: 0.01641559600830078\n",
      "Epoch 398, Loss: 0.010845131240785122, Val Loss: 0.016337061300873756\n",
      "Epoch 399, Loss: 0.010754890739917755, Val Loss: 0.016259169206023216\n",
      "Epoch 400, Loss: 0.010665536858141422, Val Loss: 0.016182074323296547\n",
      "Epoch 401, Loss: 0.010577365756034851, Val Loss: 0.016105907037854195\n",
      "Epoch 402, Loss: 0.010490511544048786, Val Loss: 0.016029832884669304\n",
      "Epoch 403, Loss: 0.010404475033283234, Val Loss: 0.015955179929733276\n",
      "Epoch 404, Loss: 0.010319553315639496, Val Loss: 0.015881136059761047\n",
      "Epoch 405, Loss: 0.010235628113150597, Val Loss: 0.015808487311005592\n",
      "Epoch 406, Loss: 0.010152951814234257, Val Loss: 0.015737220644950867\n",
      "Epoch 407, Loss: 0.010070906020700932, Val Loss: 0.015666943043470383\n",
      "Epoch 408, Loss: 0.009990253485739231, Val Loss: 0.0155988410115242\n",
      "Epoch 409, Loss: 0.009910475462675095, Val Loss: 0.015531647019088268\n",
      "Epoch 410, Loss: 0.00983148068189621, Val Loss: 0.01546635664999485\n",
      "Epoch 411, Loss: 0.009753748774528503, Val Loss: 0.015402151271700859\n",
      "Epoch 412, Loss: 0.009676510468125343, Val Loss: 0.015338624827563763\n",
      "Epoch 413, Loss: 0.009600527584552765, Val Loss: 0.015275593847036362\n",
      "Epoch 414, Loss: 0.009525175206363201, Val Loss: 0.015215136110782623\n",
      "Epoch 415, Loss: 0.00945108849555254, Val Loss: 0.015153301879763603\n",
      "Epoch 416, Loss: 0.009377408772706985, Val Loss: 0.015092025510966778\n",
      "Epoch 417, Loss: 0.00930479820817709, Val Loss: 0.015029621310532093\n",
      "Epoch 418, Loss: 0.009233164601027966, Val Loss: 0.014966776594519615\n",
      "Epoch 419, Loss: 0.009162165224552155, Val Loss: 0.014903354458510876\n",
      "Epoch 420, Loss: 0.009092157706618309, Val Loss: 0.014839697629213333\n",
      "Epoch 421, Loss: 0.009022632613778114, Val Loss: 0.01477782242000103\n",
      "Epoch 422, Loss: 0.008954204618930817, Val Loss: 0.014716233126819134\n",
      "Epoch 423, Loss: 0.008886436000466347, Val Loss: 0.014658141881227493\n",
      "Epoch 424, Loss: 0.008819344453513622, Val Loss: 0.014603020623326302\n",
      "Epoch 425, Loss: 0.008753209374845028, Val Loss: 0.014549972489476204\n",
      "Epoch 426, Loss: 0.00868777371942997, Val Loss: 0.014498457312583923\n",
      "Epoch 427, Loss: 0.008622972294688225, Val Loss: 0.014448284171521664\n",
      "Epoch 428, Loss: 0.008558948524296284, Val Loss: 0.014398153871297836\n",
      "Epoch 429, Loss: 0.008495648391544819, Val Loss: 0.014348075725138187\n",
      "Epoch 430, Loss: 0.008432896807789803, Val Loss: 0.014297320507466793\n",
      "Epoch 431, Loss: 0.008371115662157536, Val Loss: 0.014246220700442791\n",
      "Epoch 432, Loss: 0.0083098029717803, Val Loss: 0.014195523224771023\n",
      "Epoch 433, Loss: 0.008249292150139809, Val Loss: 0.014144377782940865\n",
      "Epoch 434, Loss: 0.008189292624592781, Val Loss: 0.014094130136072636\n",
      "Epoch 435, Loss: 0.008130356669425964, Val Loss: 0.014044621959328651\n",
      "Epoch 436, Loss: 0.008071532472968102, Val Loss: 0.01399588119238615\n",
      "Epoch 437, Loss: 0.008013763464987278, Val Loss: 0.013948033563792706\n",
      "Epoch 438, Loss: 0.007956475019454956, Val Loss: 0.0139009365811944\n",
      "Epoch 439, Loss: 0.007899618707597256, Val Loss: 0.013855023309588432\n",
      "Epoch 440, Loss: 0.007843668572604656, Val Loss: 0.013808843679726124\n",
      "Epoch 441, Loss: 0.007788027636706829, Val Loss: 0.01376250572502613\n",
      "Epoch 442, Loss: 0.007732983212918043, Val Loss: 0.013716851361095905\n",
      "Epoch 443, Loss: 0.007678847759962082, Val Loss: 0.013671976514160633\n",
      "Epoch 444, Loss: 0.007624947465956211, Val Loss: 0.013627228327095509\n",
      "Epoch 445, Loss: 0.0075718858279287815, Val Loss: 0.013583003543317318\n",
      "Epoch 446, Loss: 0.007519140839576721, Val Loss: 0.013538515195250511\n",
      "Epoch 447, Loss: 0.0074670989997684956, Val Loss: 0.01349653024226427\n",
      "Epoch 448, Loss: 0.007415665313601494, Val Loss: 0.013454562053084373\n",
      "Epoch 449, Loss: 0.007364511955529451, Val Loss: 0.013413277454674244\n",
      "Epoch 450, Loss: 0.00731404684484005, Val Loss: 0.013372384011745453\n",
      "Epoch 451, Loss: 0.007264138199388981, Val Loss: 0.013333077542483807\n",
      "Epoch 452, Loss: 0.0072147296741604805, Val Loss: 0.013294230215251446\n",
      "Epoch 453, Loss: 0.0071656121872365475, Val Loss: 0.01325573306530714\n",
      "Epoch 454, Loss: 0.007117018103599548, Val Loss: 0.013218077830970287\n",
      "Epoch 455, Loss: 0.007069162093102932, Val Loss: 0.013180458918213844\n",
      "Epoch 456, Loss: 0.007021661382168531, Val Loss: 0.013142624869942665\n",
      "Epoch 457, Loss: 0.006974674295634031, Val Loss: 0.013104580342769623\n",
      "Epoch 458, Loss: 0.006928149610757828, Val Loss: 0.013067484833300114\n",
      "Epoch 459, Loss: 0.0068821231834590435, Val Loss: 0.013030258007347584\n",
      "Epoch 460, Loss: 0.00683626951649785, Val Loss: 0.012993987649679184\n",
      "Epoch 461, Loss: 0.00679133040830493, Val Loss: 0.012957848608493805\n",
      "Epoch 462, Loss: 0.0067465053871273994, Val Loss: 0.01292130071669817\n",
      "Epoch 463, Loss: 0.006702045910060406, Val Loss: 0.0128856236115098\n",
      "Epoch 464, Loss: 0.0066582742147147655, Val Loss: 0.01285097748041153\n",
      "Epoch 465, Loss: 0.006614829879254103, Val Loss: 0.012816591188311577\n",
      "Epoch 466, Loss: 0.006571854930371046, Val Loss: 0.01278320699930191\n",
      "Epoch 467, Loss: 0.006529242731630802, Val Loss: 0.012750381603837013\n",
      "Epoch 468, Loss: 0.006487071048468351, Val Loss: 0.012717627920210361\n",
      "Epoch 469, Loss: 0.0064452337101101875, Val Loss: 0.012685621157288551\n",
      "Epoch 470, Loss: 0.0064037879928946495, Val Loss: 0.012654155492782593\n",
      "Epoch 471, Loss: 0.006362760439515114, Val Loss: 0.012623238377273083\n",
      "Epoch 472, Loss: 0.0063221994787454605, Val Loss: 0.012591295875608921\n",
      "Epoch 473, Loss: 0.006281992886215448, Val Loss: 0.012561078183352947\n",
      "Epoch 474, Loss: 0.006242109462618828, Val Loss: 0.012530621141195297\n",
      "Epoch 475, Loss: 0.006202675402164459, Val Loss: 0.01250174455344677\n",
      "Epoch 476, Loss: 0.006163508631289005, Val Loss: 0.01247198972851038\n",
      "Epoch 477, Loss: 0.006124747451394796, Val Loss: 0.012442773208022118\n",
      "Epoch 478, Loss: 0.006086303852498531, Val Loss: 0.01241358183324337\n",
      "Epoch 479, Loss: 0.006048400886356831, Val Loss: 0.012384729459881783\n",
      "Epoch 480, Loss: 0.0060106790624558926, Val Loss: 0.012356387451291084\n",
      "Epoch 481, Loss: 0.005973324179649353, Val Loss: 0.012328604236245155\n",
      "Epoch 482, Loss: 0.005936369299888611, Val Loss: 0.012300567701458931\n",
      "Epoch 483, Loss: 0.005899706389755011, Val Loss: 0.012272879481315613\n",
      "Epoch 484, Loss: 0.005863326136022806, Val Loss: 0.012246496975421906\n",
      "Epoch 485, Loss: 0.005827359389513731, Val Loss: 0.012220215052366257\n",
      "Epoch 486, Loss: 0.005791821517050266, Val Loss: 0.01219331193715334\n",
      "Epoch 487, Loss: 0.0057563697919249535, Val Loss: 0.012167212553322315\n",
      "Epoch 488, Loss: 0.005721230525523424, Val Loss: 0.012141627259552479\n",
      "Epoch 489, Loss: 0.005686508491635323, Val Loss: 0.012116546742618084\n",
      "Epoch 490, Loss: 0.005652225576341152, Val Loss: 0.012091982178390026\n",
      "Epoch 491, Loss: 0.005618223920464516, Val Loss: 0.012067153118550777\n",
      "Epoch 492, Loss: 0.005584355443716049, Val Loss: 0.012042437680065632\n",
      "Epoch 493, Loss: 0.005550640635192394, Val Loss: 0.012018361128866673\n",
      "Epoch 494, Loss: 0.005517576355487108, Val Loss: 0.011994825676083565\n",
      "Epoch 495, Loss: 0.005484417546540499, Val Loss: 0.011970307677984238\n",
      "Epoch 496, Loss: 0.0054518599063158035, Val Loss: 0.011945147998631\n",
      "Epoch 497, Loss: 0.005419466644525528, Val Loss: 0.011920709162950516\n",
      "Epoch 498, Loss: 0.005387192126363516, Val Loss: 0.011896355077624321\n",
      "Epoch 499, Loss: 0.005355542525649071, Val Loss: 0.011872307397425175\n",
      "Epoch 500, Loss: 0.005323878023773432, Val Loss: 0.011847158893942833\n",
      "Epoch 501, Loss: 0.005292609333992004, Val Loss: 0.011822712607681751\n",
      "Epoch 502, Loss: 0.005261619109660387, Val Loss: 0.011796918697655201\n",
      "Epoch 503, Loss: 0.005230790004134178, Val Loss: 0.011773063801229\n",
      "Epoch 504, Loss: 0.005200129467993975, Val Loss: 0.011750087141990662\n",
      "Epoch 505, Loss: 0.005170028191059828, Val Loss: 0.011726506985723972\n",
      "Epoch 506, Loss: 0.005139877554029226, Val Loss: 0.011703401803970337\n",
      "Epoch 507, Loss: 0.005110128317028284, Val Loss: 0.011680804193019867\n",
      "Epoch 508, Loss: 0.0050806328654289246, Val Loss: 0.011659123934805393\n",
      "Epoch 509, Loss: 0.00505138048902154, Val Loss: 0.011637240648269653\n",
      "Epoch 510, Loss: 0.005022271070629358, Val Loss: 0.011616776697337627\n",
      "Epoch 511, Loss: 0.004993551876395941, Val Loss: 0.011595395393669605\n",
      "Epoch 512, Loss: 0.004964972380548716, Val Loss: 0.011575192213058472\n",
      "Epoch 513, Loss: 0.004936616402119398, Val Loss: 0.01155485212802887\n",
      "Epoch 514, Loss: 0.004908611066639423, Val Loss: 0.01153477281332016\n",
      "Epoch 515, Loss: 0.004880666267126799, Val Loss: 0.011514917016029358\n",
      "Epoch 516, Loss: 0.004853199701756239, Val Loss: 0.011495411396026611\n",
      "Epoch 517, Loss: 0.004825786221772432, Val Loss: 0.0114766675978899\n",
      "Epoch 518, Loss: 0.004798674024641514, Val Loss: 0.011456461623311043\n",
      "Epoch 519, Loss: 0.004771815612912178, Val Loss: 0.011435828171670437\n",
      "Epoch 520, Loss: 0.004745126701891422, Val Loss: 0.011414519511163235\n",
      "Epoch 521, Loss: 0.004718728829175234, Val Loss: 0.011393103748559952\n",
      "Epoch 522, Loss: 0.004692407790571451, Val Loss: 0.011372657492756844\n",
      "Epoch 523, Loss: 0.004666531458497047, Val Loss: 0.011351678520441055\n",
      "Epoch 524, Loss: 0.004640703555196524, Val Loss: 0.011331929825246334\n",
      "Epoch 525, Loss: 0.004615351092070341, Val Loss: 0.011311586014926434\n",
      "Epoch 526, Loss: 0.004589868709445, Val Loss: 0.011291157454252243\n",
      "Epoch 527, Loss: 0.004564815200865269, Val Loss: 0.01127214077860117\n",
      "Epoch 528, Loss: 0.004539797082543373, Val Loss: 0.011253779754042625\n",
      "Epoch 529, Loss: 0.0045151179656386375, Val Loss: 0.011235256679356098\n",
      "Epoch 530, Loss: 0.004490535240620375, Val Loss: 0.011217893101274967\n",
      "Epoch 531, Loss: 0.0044663273729383945, Val Loss: 0.011200366541743279\n",
      "Epoch 532, Loss: 0.004442163277417421, Val Loss: 0.011183365248143673\n",
      "Epoch 533, Loss: 0.004418317694216967, Val Loss: 0.011167478747665882\n",
      "Epoch 534, Loss: 0.0043946136720478535, Val Loss: 0.011151334270834923\n",
      "Epoch 535, Loss: 0.004370995797216892, Val Loss: 0.011134490370750427\n",
      "Epoch 536, Loss: 0.004347658716142178, Val Loss: 0.011118329130113125\n",
      "Epoch 537, Loss: 0.004324565175920725, Val Loss: 0.011101799085736275\n",
      "Epoch 538, Loss: 0.004301520995795727, Val Loss: 0.011085745878517628\n",
      "Epoch 539, Loss: 0.0042787473648786545, Val Loss: 0.01106993481516838\n",
      "Epoch 540, Loss: 0.004256224725395441, Val Loss: 0.0110536590218544\n",
      "Epoch 541, Loss: 0.004233707208186388, Val Loss: 0.011036783456802368\n",
      "Epoch 542, Loss: 0.004211511462926865, Val Loss: 0.011020953767001629\n",
      "Epoch 543, Loss: 0.0041893478482961655, Val Loss: 0.011004638858139515\n",
      "Epoch 544, Loss: 0.0041673979721963406, Val Loss: 0.010989783331751823\n",
      "Epoch 545, Loss: 0.004145794548094273, Val Loss: 0.01097394060343504\n",
      "Epoch 546, Loss: 0.0041240728460252285, Val Loss: 0.010958386585116386\n",
      "Epoch 547, Loss: 0.004102925304323435, Val Loss: 0.010942764580249786\n",
      "Epoch 548, Loss: 0.004081439692527056, Val Loss: 0.010927173309028149\n",
      "Epoch 549, Loss: 0.004060446284711361, Val Loss: 0.010912152007222176\n",
      "Epoch 550, Loss: 0.00403950922191143, Val Loss: 0.010896976105868816\n",
      "Epoch 551, Loss: 0.004018757492303848, Val Loss: 0.01088254339993\n",
      "Epoch 552, Loss: 0.003998172003775835, Val Loss: 0.010867570526897907\n",
      "Epoch 553, Loss: 0.003977723885327578, Val Loss: 0.010854164138436317\n",
      "Epoch 554, Loss: 0.003957475535571575, Val Loss: 0.010840142145752907\n",
      "Epoch 555, Loss: 0.003937253728508949, Val Loss: 0.010826567187905312\n",
      "Epoch 556, Loss: 0.00391730759292841, Val Loss: 0.010811994783580303\n",
      "Epoch 557, Loss: 0.0038974378257989883, Val Loss: 0.010797404684126377\n",
      "Epoch 558, Loss: 0.003877913812175393, Val Loss: 0.010783327743411064\n",
      "Epoch 559, Loss: 0.003858148818835616, Val Loss: 0.010769950225949287\n",
      "Epoch 560, Loss: 0.0038389412220567465, Val Loss: 0.010755558498203754\n",
      "Epoch 561, Loss: 0.003819589037448168, Val Loss: 0.010741869919002056\n",
      "Epoch 562, Loss: 0.0038005367387086153, Val Loss: 0.010728527791798115\n",
      "Epoch 563, Loss: 0.00378157920204103, Val Loss: 0.010715339332818985\n",
      "Epoch 564, Loss: 0.0037627166602760553, Val Loss: 0.01070205308496952\n",
      "Epoch 565, Loss: 0.003744022687897086, Val Loss: 0.010688633657991886\n",
      "Epoch 566, Loss: 0.0037253969348967075, Val Loss: 0.010675991885364056\n",
      "Epoch 567, Loss: 0.0037068987730890512, Val Loss: 0.010662979446351528\n",
      "Epoch 568, Loss: 0.0036885663866996765, Val Loss: 0.010650225915014744\n",
      "Epoch 569, Loss: 0.003670481499284506, Val Loss: 0.010637806728482246\n",
      "Epoch 570, Loss: 0.003652410814538598, Val Loss: 0.010624379850924015\n",
      "Epoch 571, Loss: 0.003634452121332288, Val Loss: 0.010611752979457378\n",
      "Epoch 572, Loss: 0.003616774920374155, Val Loss: 0.010599018074572086\n",
      "Epoch 573, Loss: 0.0035990767646580935, Val Loss: 0.010586665943264961\n",
      "Epoch 574, Loss: 0.003581633558496833, Val Loss: 0.010574913583695889\n",
      "Epoch 575, Loss: 0.003564179874956608, Val Loss: 0.01056446973234415\n",
      "Epoch 576, Loss: 0.0035469522699713707, Val Loss: 0.010552843101322651\n",
      "Epoch 577, Loss: 0.0035298217553645372, Val Loss: 0.010542194359004498\n",
      "Epoch 578, Loss: 0.0035128556191921234, Val Loss: 0.010531285777688026\n",
      "Epoch 579, Loss: 0.0034959595650434494, Val Loss: 0.01052137091755867\n",
      "Epoch 580, Loss: 0.003479174803942442, Val Loss: 0.010511023923754692\n",
      "Epoch 581, Loss: 0.0034626894630491734, Val Loss: 0.010500055737793446\n",
      "Epoch 582, Loss: 0.0034460839815437794, Val Loss: 0.010489179752767086\n",
      "Epoch 583, Loss: 0.0034296526573598385, Val Loss: 0.010479187592864037\n",
      "Epoch 584, Loss: 0.0034133177250623703, Val Loss: 0.010469428263604641\n",
      "Epoch 585, Loss: 0.0033971841912716627, Val Loss: 0.010459419339895248\n",
      "Epoch 586, Loss: 0.0033811870962381363, Val Loss: 0.010450302623212337\n",
      "Epoch 587, Loss: 0.0033651827834546566, Val Loss: 0.010441227816045284\n",
      "Epoch 588, Loss: 0.00334936217404902, Val Loss: 0.01043178141117096\n",
      "Epoch 589, Loss: 0.003333648666739464, Val Loss: 0.010422879830002785\n",
      "Epoch 590, Loss: 0.003318051341921091, Val Loss: 0.010412925854325294\n",
      "Epoch 591, Loss: 0.0033024894073605537, Val Loss: 0.010404268279671669\n",
      "Epoch 592, Loss: 0.003287140280008316, Val Loss: 0.010395456105470657\n",
      "Epoch 593, Loss: 0.003271787893027067, Val Loss: 0.010385682806372643\n",
      "Epoch 594, Loss: 0.0032565968576818705, Val Loss: 0.010376839898526669\n",
      "Epoch 595, Loss: 0.003241468919441104, Val Loss: 0.010368687100708485\n",
      "Epoch 596, Loss: 0.003226556582376361, Val Loss: 0.010360205546021461\n",
      "Epoch 597, Loss: 0.0032116989605128765, Val Loss: 0.010351437143981457\n",
      "Epoch 598, Loss: 0.0031968315597623587, Val Loss: 0.010342917405068874\n",
      "Epoch 599, Loss: 0.0031821690499782562, Val Loss: 0.01033428031951189\n",
      "Epoch 600, Loss: 0.0031675598584115505, Val Loss: 0.010326356627047062\n",
      "Epoch 601, Loss: 0.0031531061977148056, Val Loss: 0.010317612439393997\n",
      "Epoch 602, Loss: 0.0031386183109134436, Val Loss: 0.010308518074452877\n",
      "Epoch 603, Loss: 0.003124409122392535, Val Loss: 0.010299425572156906\n",
      "Epoch 604, Loss: 0.0031102467328310013, Val Loss: 0.010289010591804981\n",
      "Epoch 605, Loss: 0.003096097381785512, Val Loss: 0.010278433561325073\n",
      "Epoch 606, Loss: 0.0030820872634649277, Val Loss: 0.01026865653693676\n",
      "Epoch 607, Loss: 0.0030681679490953684, Val Loss: 0.010257910937070847\n",
      "Epoch 608, Loss: 0.003054314060136676, Val Loss: 0.010248417966067791\n",
      "Epoch 609, Loss: 0.0030405924189835787, Val Loss: 0.010239452123641968\n",
      "Epoch 610, Loss: 0.0030269892886281013, Val Loss: 0.010231485590338707\n",
      "Epoch 611, Loss: 0.0030134227126836777, Val Loss: 0.010223988443613052\n",
      "Epoch 612, Loss: 0.0029999297112226486, Val Loss: 0.010217207483947277\n",
      "Epoch 613, Loss: 0.0029865887481719255, Val Loss: 0.010210300795733929\n",
      "Epoch 614, Loss: 0.0029731926042586565, Val Loss: 0.010203436948359013\n",
      "Epoch 615, Loss: 0.002960126381367445, Val Loss: 0.010196416638791561\n",
      "Epoch 616, Loss: 0.002946957014501095, Val Loss: 0.010189559310674667\n",
      "Epoch 617, Loss: 0.0029339611064642668, Val Loss: 0.010183104313910007\n",
      "Epoch 618, Loss: 0.0029209856875240803, Val Loss: 0.010176194831728935\n",
      "Epoch 619, Loss: 0.002908191876485944, Val Loss: 0.010169968008995056\n",
      "Epoch 620, Loss: 0.002895363373681903, Val Loss: 0.010162684135138988\n",
      "Epoch 621, Loss: 0.0028827169444411993, Val Loss: 0.010155227035284042\n",
      "Epoch 622, Loss: 0.002869976218789816, Val Loss: 0.010148152709007263\n",
      "Epoch 623, Loss: 0.0028575442265719175, Val Loss: 0.010141624137759209\n",
      "Epoch 624, Loss: 0.002845194423571229, Val Loss: 0.010134298354387283\n",
      "Epoch 625, Loss: 0.0028327913023531437, Val Loss: 0.010128023102879524\n",
      "Epoch 626, Loss: 0.0028204480186104774, Val Loss: 0.010121376253664494\n",
      "Epoch 627, Loss: 0.0028081852942705154, Val Loss: 0.010114972479641438\n",
      "Epoch 628, Loss: 0.002796105109155178, Val Loss: 0.010108844377100468\n",
      "Epoch 629, Loss: 0.0027841096743941307, Val Loss: 0.010102094151079655\n",
      "Epoch 630, Loss: 0.0027720800135284662, Val Loss: 0.010094980709254742\n",
      "Epoch 631, Loss: 0.0027602470945566893, Val Loss: 0.010088387876749039\n",
      "Epoch 632, Loss: 0.0027484314050525427, Val Loss: 0.010079831816256046\n",
      "Epoch 633, Loss: 0.002736601047217846, Val Loss: 0.01007287111133337\n",
      "Epoch 634, Loss: 0.002725042402744293, Val Loss: 0.01006635744124651\n",
      "Epoch 635, Loss: 0.0027133869007229805, Val Loss: 0.01006036065518856\n",
      "Epoch 636, Loss: 0.002701840829104185, Val Loss: 0.010051990859210491\n",
      "Epoch 637, Loss: 0.002690503839403391, Val Loss: 0.010045605711638927\n",
      "Epoch 638, Loss: 0.0026789770927280188, Val Loss: 0.010038445703685284\n",
      "Epoch 639, Loss: 0.002667735330760479, Val Loss: 0.010032215155661106\n",
      "Epoch 640, Loss: 0.0026565210428088903, Val Loss: 0.010026616975665092\n",
      "Epoch 641, Loss: 0.0026453323662281036, Val Loss: 0.01002027839422226\n",
      "Epoch 642, Loss: 0.002634285716339946, Val Loss: 0.01001355703920126\n",
      "Epoch 643, Loss: 0.002623174572363496, Val Loss: 0.010006999596953392\n",
      "Epoch 644, Loss: 0.0026121654082089663, Val Loss: 0.010001018643379211\n",
      "Epoch 645, Loss: 0.0026013574097305536, Val Loss: 0.009995149448513985\n",
      "Epoch 646, Loss: 0.0025905787479132414, Val Loss: 0.009988936595618725\n",
      "Epoch 647, Loss: 0.0025797442067414522, Val Loss: 0.009982832707464695\n",
      "Epoch 648, Loss: 0.0025690835900604725, Val Loss: 0.00997690949589014\n",
      "Epoch 649, Loss: 0.002558416686952114, Val Loss: 0.009971130639314651\n",
      "Epoch 650, Loss: 0.0025477996096014977, Val Loss: 0.009965122677385807\n"
     ]
    }
   ],
   "source": [
    "from flashrag.prompt import PromptTemplate\n",
    "import pickle\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "config_taqa = set_config(dataset_name='taqa', split = split)\n",
    "template_taqa= PromptTemplate(\n",
    "        config = config_taqa,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "config_know= set_config(dataset_name='triviaqa', split = split)\n",
    "template_know= PromptTemplate(\n",
    "        config = config_know,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "\n",
    "\n",
    "loading_hidden = True\n",
    "if not loading_hidden:\n",
    "    taqa_hidden_states, triviaqa_hidden_states = load_datasets(config_taqa, config_triviaqa, template_taqa, template_triviaqa)\n",
    "else:\n",
    "    file_path_know = '/cs/student/projects2/dsml/cdiezmar/hidden_states/knowledge-aware.pkl'\n",
    "    file_path_taqa = '/cs/student/projects2/dsml/cdiezmar/hidden_states/taqa.pkl'\n",
    "\n",
    "    with open(file_path_know, 'rb') as file_know:\n",
    "        know_hidden_states = pickle.load(file_know)\n",
    "        print('Length triviaqa: ', len(know_hidden_states))\n",
    "        know_hidden_states_tensor= torch.tensor(know_hidden_states, dtype=torch.float32)\n",
    "    with open(file_path_taqa, 'rb') as file_taqa:\n",
    "        taqa_hidden_states = pickle.load(file_taqa)\n",
    "        print('Length taqa: ', len(taqa_hidden_states))\n",
    "        taqa_hidden_states_tensor= torch.tensor(taqa_hidden_states, dtype=torch.float32)\n",
    "train_inputs, train_labels, val_inputs, val_labels = prepare_data(taqa_hidden_states_tensor, know_hidden_states_tensor)\n",
    "\n",
    "input_size = train_inputs.size(1)\n",
    "num_classes = 2  # Time-sensitive or not\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "epochs = 650\n",
    "lr = 1e-4\n",
    "\n",
    "# Train LSTM Classifier\n",
    "lstm_model = train_lstm_classifier_cuda(train_inputs, train_labels, val_inputs, val_labels, input_size, hidden_size, num_layers, num_classes, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `question` in template\n",
      "Find `question` in template\n",
      "Length triviaqa:  20000\n",
      "Length taqa:  10148\n",
      "Epoch 1, Loss: 0.6926335096359253, Val Loss: 0.6899766325950623\n",
      "Epoch 2, Loss: 0.6900738477706909, Val Loss: 0.687498152256012\n",
      "Epoch 3, Loss: 0.6875885725021362, Val Loss: 0.6851382851600647\n",
      "Epoch 4, Loss: 0.685215413570404, Val Loss: 0.6828591227531433\n",
      "Epoch 5, Loss: 0.6829231381416321, Val Loss: 0.6806137561798096\n",
      "Epoch 6, Loss: 0.6806706786155701, Val Loss: 0.6783788204193115\n",
      "Epoch 7, Loss: 0.6784321665763855, Val Loss: 0.6761313080787659\n",
      "Epoch 8, Loss: 0.6761780381202698, Val Loss: 0.6738216280937195\n",
      "Epoch 9, Loss: 0.6738597750663757, Val Loss: 0.671405017375946\n",
      "Epoch 10, Loss: 0.6714377999305725, Val Loss: 0.6688687205314636\n",
      "Epoch 11, Loss: 0.6688997745513916, Val Loss: 0.6662190556526184\n",
      "Epoch 12, Loss: 0.666247546672821, Val Loss: 0.6634514927864075\n",
      "Epoch 13, Loss: 0.6634727120399475, Val Loss: 0.6605425477027893\n",
      "Epoch 14, Loss: 0.6605536937713623, Val Loss: 0.657477855682373\n",
      "Epoch 15, Loss: 0.6574780941009521, Val Loss: 0.6542522311210632\n",
      "Epoch 16, Loss: 0.654242992401123, Val Loss: 0.6508744359016418\n",
      "Epoch 17, Loss: 0.6508602499961853, Val Loss: 0.6473087668418884\n",
      "Epoch 18, Loss: 0.6472928524017334, Val Loss: 0.6435363292694092\n",
      "Epoch 19, Loss: 0.6435153484344482, Val Loss: 0.6395664811134338\n",
      "Epoch 20, Loss: 0.6395343542098999, Val Loss: 0.6354085803031921\n",
      "Epoch 21, Loss: 0.6353620886802673, Val Loss: 0.6310740113258362\n",
      "Epoch 22, Loss: 0.6310133934020996, Val Loss: 0.6265438795089722\n",
      "Epoch 23, Loss: 0.626469075679779, Val Loss: 0.6218017339706421\n",
      "Epoch 24, Loss: 0.6217122673988342, Val Loss: 0.6168531179428101\n",
      "Epoch 25, Loss: 0.6167488694190979, Val Loss: 0.6116889715194702\n",
      "Epoch 26, Loss: 0.6115689873695374, Val Loss: 0.606275200843811\n",
      "Epoch 27, Loss: 0.6061391830444336, Val Loss: 0.600577175617218\n",
      "Epoch 28, Loss: 0.6004253625869751, Val Loss: 0.5945990681648254\n",
      "Epoch 29, Loss: 0.5944309234619141, Val Loss: 0.588340699672699\n",
      "Epoch 30, Loss: 0.5881556868553162, Val Loss: 0.5818121433258057\n",
      "Epoch 31, Loss: 0.5816116333007812, Val Loss: 0.5749974250793457\n",
      "Epoch 32, Loss: 0.574786901473999, Val Loss: 0.5678850412368774\n",
      "Epoch 33, Loss: 0.5676699280738831, Val Loss: 0.5604599118232727\n",
      "Epoch 34, Loss: 0.5602427124977112, Val Loss: 0.5527384877204895\n",
      "Epoch 35, Loss: 0.5525205135345459, Val Loss: 0.5446914434432983\n",
      "Epoch 36, Loss: 0.5444721579551697, Val Loss: 0.5363094806671143\n",
      "Epoch 37, Loss: 0.5360884666442871, Val Loss: 0.5275823473930359\n",
      "Epoch 38, Loss: 0.5273587107658386, Val Loss: 0.5185248851776123\n",
      "Epoch 39, Loss: 0.5182973146438599, Val Loss: 0.5091583728790283\n",
      "Epoch 40, Loss: 0.5089254379272461, Val Loss: 0.49952587485313416\n",
      "Epoch 41, Loss: 0.4992865025997162, Val Loss: 0.48964253067970276\n",
      "Epoch 42, Loss: 0.4893970489501953, Val Loss: 0.47949638962745667\n",
      "Epoch 43, Loss: 0.47924360632896423, Val Loss: 0.46912822127342224\n",
      "Epoch 44, Loss: 0.4688652455806732, Val Loss: 0.4585859775543213\n",
      "Epoch 45, Loss: 0.4583069086074829, Val Loss: 0.44786691665649414\n",
      "Epoch 46, Loss: 0.44756725430488586, Val Loss: 0.4369615912437439\n",
      "Epoch 47, Loss: 0.4366440176963806, Val Loss: 0.4258856177330017\n",
      "Epoch 48, Loss: 0.4255545735359192, Val Loss: 0.41469472646713257\n",
      "Epoch 49, Loss: 0.41435426473617554, Val Loss: 0.4034551680088043\n",
      "Epoch 50, Loss: 0.40310052037239075, Val Loss: 0.3921544551849365\n",
      "Epoch 51, Loss: 0.3917800486087799, Val Loss: 0.3807470500469208\n",
      "Epoch 52, Loss: 0.38035473227500916, Val Loss: 0.3692629337310791\n",
      "Epoch 53, Loss: 0.36885344982147217, Val Loss: 0.357771635055542\n",
      "Epoch 54, Loss: 0.3573299050331116, Val Loss: 0.34637024998664856\n",
      "Epoch 55, Loss: 0.3458733856678009, Val Loss: 0.3350929915904999\n",
      "Epoch 56, Loss: 0.3345315754413605, Val Loss: 0.3239530324935913\n",
      "Epoch 57, Loss: 0.32333090901374817, Val Loss: 0.3129536509513855\n",
      "Epoch 58, Loss: 0.3122779428958893, Val Loss: 0.3021169900894165\n",
      "Epoch 59, Loss: 0.3013864755630493, Val Loss: 0.29151439666748047\n",
      "Epoch 60, Loss: 0.2907238304615021, Val Loss: 0.28115347027778625\n",
      "Epoch 61, Loss: 0.2803076207637787, Val Loss: 0.271026074886322\n",
      "Epoch 62, Loss: 0.2701333165168762, Val Loss: 0.261135071516037\n",
      "Epoch 63, Loss: 0.26020288467407227, Val Loss: 0.2514966130256653\n",
      "Epoch 64, Loss: 0.2505328953266144, Val Loss: 0.24210603535175323\n",
      "Epoch 65, Loss: 0.24111182987689972, Val Loss: 0.23298655450344086\n",
      "Epoch 66, Loss: 0.23195181787014008, Val Loss: 0.22413672506809235\n",
      "Epoch 67, Loss: 0.2230488508939743, Val Loss: 0.21558621525764465\n",
      "Epoch 68, Loss: 0.21443524956703186, Val Loss: 0.20734505355358124\n",
      "Epoch 69, Loss: 0.2061271071434021, Val Loss: 0.1994314044713974\n",
      "Epoch 70, Loss: 0.19815021753311157, Val Loss: 0.19183765351772308\n",
      "Epoch 71, Loss: 0.19049771130084991, Val Loss: 0.1845356822013855\n",
      "Epoch 72, Loss: 0.18314002454280853, Val Loss: 0.17751987278461456\n",
      "Epoch 73, Loss: 0.17606805264949799, Val Loss: 0.17078308761119843\n",
      "Epoch 74, Loss: 0.16926714777946472, Val Loss: 0.1643255352973938\n",
      "Epoch 75, Loss: 0.16273608803749084, Val Loss: 0.1581348031759262\n",
      "Epoch 76, Loss: 0.15647117793560028, Val Loss: 0.15219976007938385\n",
      "Epoch 77, Loss: 0.15046359598636627, Val Loss: 0.14650897681713104\n",
      "Epoch 78, Loss: 0.14471426606178284, Val Loss: 0.14105011522769928\n",
      "Epoch 79, Loss: 0.1392177939414978, Val Loss: 0.13580322265625\n",
      "Epoch 80, Loss: 0.13397248089313507, Val Loss: 0.13077224791049957\n",
      "Epoch 81, Loss: 0.1289837658405304, Val Loss: 0.12597273290157318\n",
      "Epoch 82, Loss: 0.12423119693994522, Val Loss: 0.12139889597892761\n",
      "Epoch 83, Loss: 0.11969228833913803, Val Loss: 0.11704869568347931\n",
      "Epoch 84, Loss: 0.11535567790269852, Val Loss: 0.11292023956775665\n",
      "Epoch 85, Loss: 0.11121122539043427, Val Loss: 0.10901395976543427\n",
      "Epoch 86, Loss: 0.107266366481781, Val Loss: 0.10531311482191086\n",
      "Epoch 87, Loss: 0.10352961719036102, Val Loss: 0.10178375244140625\n",
      "Epoch 88, Loss: 0.09997837245464325, Val Loss: 0.09839887171983719\n",
      "Epoch 89, Loss: 0.09660224616527557, Val Loss: 0.09516224265098572\n",
      "Epoch 90, Loss: 0.09338768571615219, Val Loss: 0.0920964851975441\n",
      "Epoch 91, Loss: 0.09033004939556122, Val Loss: 0.08918778598308563\n",
      "Epoch 92, Loss: 0.08741969615221024, Val Loss: 0.08641324192285538\n",
      "Epoch 93, Loss: 0.0846368744969368, Val Loss: 0.08375507593154907\n",
      "Epoch 94, Loss: 0.08197105675935745, Val Loss: 0.08120755851268768\n",
      "Epoch 95, Loss: 0.07941674441099167, Val Loss: 0.07877051830291748\n",
      "Epoch 96, Loss: 0.07696559280157089, Val Loss: 0.07644309103488922\n",
      "Epoch 97, Loss: 0.07461192458868027, Val Loss: 0.0742182806134224\n",
      "Epoch 98, Loss: 0.07235078513622284, Val Loss: 0.07209247350692749\n",
      "Epoch 99, Loss: 0.07017967104911804, Val Loss: 0.07006154954433441\n",
      "Epoch 100, Loss: 0.06809669733047485, Val Loss: 0.06811989098787308\n",
      "Epoch 101, Loss: 0.0660969614982605, Val Loss: 0.06626790016889572\n",
      "Epoch 102, Loss: 0.06417842954397202, Val Loss: 0.06449731439352036\n",
      "Epoch 103, Loss: 0.062332428991794586, Val Loss: 0.06280261278152466\n",
      "Epoch 104, Loss: 0.06055929884314537, Val Loss: 0.06117533519864082\n",
      "Epoch 105, Loss: 0.058856040239334106, Val Loss: 0.05961842089891434\n",
      "Epoch 106, Loss: 0.05722122639417648, Val Loss: 0.05812279134988785\n",
      "Epoch 107, Loss: 0.05564749240875244, Val Loss: 0.056685201823711395\n",
      "Epoch 108, Loss: 0.054134778678417206, Val Loss: 0.05530065670609474\n",
      "Epoch 109, Loss: 0.05267871916294098, Val Loss: 0.053965918719768524\n",
      "Epoch 110, Loss: 0.05127532035112381, Val Loss: 0.05267901346087456\n",
      "Epoch 111, Loss: 0.049922578036785126, Val Loss: 0.0514400415122509\n",
      "Epoch 112, Loss: 0.04862125590443611, Val Loss: 0.050246767699718475\n",
      "Epoch 113, Loss: 0.04736799746751785, Val Loss: 0.04909362643957138\n",
      "Epoch 114, Loss: 0.04615719988942146, Val Loss: 0.04797982797026634\n",
      "Epoch 115, Loss: 0.04498619958758354, Val Loss: 0.04690692946314812\n",
      "Epoch 116, Loss: 0.04385659098625183, Val Loss: 0.04586951807141304\n",
      "Epoch 117, Loss: 0.04276443272829056, Val Loss: 0.04486891254782677\n",
      "Epoch 118, Loss: 0.041710346937179565, Val Loss: 0.04390382394194603\n",
      "Epoch 119, Loss: 0.04069335013628006, Val Loss: 0.04297193884849548\n",
      "Epoch 120, Loss: 0.03971051052212715, Val Loss: 0.04206910729408264\n",
      "Epoch 121, Loss: 0.03875960037112236, Val Loss: 0.041197270154953\n",
      "Epoch 122, Loss: 0.03784136846661568, Val Loss: 0.04035233333706856\n",
      "Epoch 123, Loss: 0.036952752619981766, Val Loss: 0.03953653201460838\n",
      "Epoch 124, Loss: 0.036095526069402695, Val Loss: 0.03874530270695686\n",
      "Epoch 125, Loss: 0.035264752805233, Val Loss: 0.03798190876841545\n",
      "Epoch 126, Loss: 0.03446418419480324, Val Loss: 0.037241388112306595\n",
      "Epoch 127, Loss: 0.03368702530860901, Val Loss: 0.03652385249733925\n",
      "Epoch 128, Loss: 0.03293386101722717, Val Loss: 0.03583100810647011\n",
      "Epoch 129, Loss: 0.03220545873045921, Val Loss: 0.03516019135713577\n",
      "Epoch 130, Loss: 0.031499870121479034, Val Loss: 0.03450886160135269\n",
      "Epoch 131, Loss: 0.030814068391919136, Val Loss: 0.03388049826025963\n",
      "Epoch 132, Loss: 0.030150102451443672, Val Loss: 0.03327234461903572\n",
      "Epoch 133, Loss: 0.029507094994187355, Val Loss: 0.03268378600478172\n",
      "Epoch 134, Loss: 0.028882550075650215, Val Loss: 0.03211463987827301\n",
      "Epoch 135, Loss: 0.02827715128660202, Val Loss: 0.031562596559524536\n",
      "Epoch 136, Loss: 0.027689067646861076, Val Loss: 0.03102867491543293\n",
      "Epoch 137, Loss: 0.027119625359773636, Val Loss: 0.030511735007166862\n",
      "Epoch 138, Loss: 0.0265665203332901, Val Loss: 0.030008835718035698\n",
      "Epoch 139, Loss: 0.0260282251983881, Val Loss: 0.02952333725988865\n",
      "Epoch 140, Loss: 0.02550729736685753, Val Loss: 0.02905041165649891\n",
      "Epoch 141, Loss: 0.025000523775815964, Val Loss: 0.028593070805072784\n",
      "Epoch 142, Loss: 0.024511344730854034, Val Loss: 0.028148530051112175\n",
      "Epoch 143, Loss: 0.024035828188061714, Val Loss: 0.02771608531475067\n",
      "Epoch 144, Loss: 0.023572569712996483, Val Loss: 0.027298826724290848\n",
      "Epoch 145, Loss: 0.023123743012547493, Val Loss: 0.026895247399806976\n",
      "Epoch 146, Loss: 0.02268800139427185, Val Loss: 0.0265028178691864\n",
      "Epoch 147, Loss: 0.022262398153543472, Val Loss: 0.026122750714421272\n",
      "Epoch 148, Loss: 0.02184957079589367, Val Loss: 0.025752592831850052\n",
      "Epoch 149, Loss: 0.02144777588546276, Val Loss: 0.025394363328814507\n",
      "Epoch 150, Loss: 0.021058587357401848, Val Loss: 0.025044359266757965\n",
      "Epoch 151, Loss: 0.02067929320037365, Val Loss: 0.02470247447490692\n",
      "Epoch 152, Loss: 0.020308751612901688, Val Loss: 0.024371756240725517\n",
      "Epoch 153, Loss: 0.01994902826845646, Val Loss: 0.02404910884797573\n",
      "Epoch 154, Loss: 0.01959841139614582, Val Loss: 0.023734472692012787\n",
      "Epoch 155, Loss: 0.019257908686995506, Val Loss: 0.02342892251908779\n",
      "Epoch 156, Loss: 0.01892727054655552, Val Loss: 0.023128602653741837\n",
      "Epoch 157, Loss: 0.018604058772325516, Val Loss: 0.022837059572339058\n",
      "Epoch 158, Loss: 0.018290692940354347, Val Loss: 0.022553611546754837\n",
      "Epoch 159, Loss: 0.017984634265303612, Val Loss: 0.022278383374214172\n",
      "Epoch 160, Loss: 0.017687872052192688, Val Loss: 0.022009244188666344\n",
      "Epoch 161, Loss: 0.017397617921233177, Val Loss: 0.021747378632426262\n",
      "Epoch 162, Loss: 0.017115021124482155, Val Loss: 0.02149256132543087\n",
      "Epoch 163, Loss: 0.016840653494000435, Val Loss: 0.0212449561804533\n",
      "Epoch 164, Loss: 0.016572292894124985, Val Loss: 0.02100292034447193\n",
      "Epoch 165, Loss: 0.016310781240463257, Val Loss: 0.020767850801348686\n",
      "Epoch 166, Loss: 0.016055796295404434, Val Loss: 0.020538927987217903\n",
      "Epoch 167, Loss: 0.015807203948497772, Val Loss: 0.020314758643507957\n",
      "Epoch 168, Loss: 0.015564734116196632, Val Loss: 0.02009645476937294\n",
      "Epoch 169, Loss: 0.01532821822911501, Val Loss: 0.019884375855326653\n",
      "Epoch 170, Loss: 0.01509798038750887, Val Loss: 0.01967628113925457\n",
      "Epoch 171, Loss: 0.014872550033032894, Val Loss: 0.019473794847726822\n",
      "Epoch 172, Loss: 0.014653263613581657, Val Loss: 0.01927538402378559\n",
      "Epoch 173, Loss: 0.014438731595873833, Val Loss: 0.019081396982073784\n",
      "Epoch 174, Loss: 0.014229502528905869, Val Loss: 0.01889239251613617\n",
      "Epoch 175, Loss: 0.014025433920323849, Val Loss: 0.018706604838371277\n",
      "Epoch 176, Loss: 0.013825044967234135, Val Loss: 0.018525579944252968\n",
      "Epoch 177, Loss: 0.013629795983433723, Val Loss: 0.018348094075918198\n",
      "Epoch 178, Loss: 0.013438261114060879, Val Loss: 0.018174627795815468\n",
      "Epoch 179, Loss: 0.013250556774437428, Val Loss: 0.018003785982728004\n",
      "Epoch 180, Loss: 0.013066918589174747, Val Loss: 0.01783684268593788\n",
      "Epoch 181, Loss: 0.01288810558617115, Val Loss: 0.017673498019576073\n",
      "Epoch 182, Loss: 0.012713346630334854, Val Loss: 0.01751258596777916\n",
      "Epoch 183, Loss: 0.012542263604700565, Val Loss: 0.017354944720864296\n",
      "Epoch 184, Loss: 0.012374581769108772, Val Loss: 0.017201194539666176\n",
      "Epoch 185, Loss: 0.012211167253553867, Val Loss: 0.017050210386514664\n",
      "Epoch 186, Loss: 0.012050995603203773, Val Loss: 0.016902809962630272\n",
      "Epoch 187, Loss: 0.011894174851477146, Val Loss: 0.0167581494897604\n",
      "Epoch 188, Loss: 0.011740525253117085, Val Loss: 0.0166166964918375\n",
      "Epoch 189, Loss: 0.011590378358960152, Val Loss: 0.016478223726153374\n",
      "Epoch 190, Loss: 0.011443011462688446, Val Loss: 0.016342338174581528\n",
      "Epoch 191, Loss: 0.011298777535557747, Val Loss: 0.016209956258535385\n",
      "Epoch 192, Loss: 0.011158212088048458, Val Loss: 0.016080358996987343\n",
      "Epoch 193, Loss: 0.011019719764590263, Val Loss: 0.01595270447432995\n",
      "Epoch 194, Loss: 0.010883909650146961, Val Loss: 0.015827758237719536\n",
      "Epoch 195, Loss: 0.010750965215265751, Val Loss: 0.01570555567741394\n",
      "Epoch 196, Loss: 0.010620694607496262, Val Loss: 0.015586783178150654\n",
      "Epoch 197, Loss: 0.0104932626709342, Val Loss: 0.015470054931938648\n",
      "Epoch 198, Loss: 0.010368281044065952, Val Loss: 0.01535552553832531\n",
      "Epoch 199, Loss: 0.010245328769087791, Val Loss: 0.015243484638631344\n",
      "Epoch 200, Loss: 0.010124823078513145, Val Loss: 0.015133274719119072\n",
      "Epoch 201, Loss: 0.010006053373217583, Val Loss: 0.01502534095197916\n",
      "Epoch 202, Loss: 0.009889909997582436, Val Loss: 0.014918966218829155\n",
      "Epoch 203, Loss: 0.00977552030235529, Val Loss: 0.01481441780924797\n",
      "Epoch 204, Loss: 0.009663119912147522, Val Loss: 0.014712574891746044\n",
      "Epoch 205, Loss: 0.0095527907833457, Val Loss: 0.0146121084690094\n",
      "Epoch 206, Loss: 0.009444566443562508, Val Loss: 0.014513907954096794\n",
      "Epoch 207, Loss: 0.009338496252894402, Val Loss: 0.01441818568855524\n",
      "Epoch 208, Loss: 0.00923445075750351, Val Loss: 0.014322716742753983\n",
      "Epoch 209, Loss: 0.009131449274718761, Val Loss: 0.01422872208058834\n",
      "Epoch 210, Loss: 0.009030754677951336, Val Loss: 0.014135781675577164\n",
      "Epoch 211, Loss: 0.008931814692914486, Val Loss: 0.014044935815036297\n",
      "Epoch 212, Loss: 0.008834468200802803, Val Loss: 0.013955717906355858\n",
      "Epoch 213, Loss: 0.008738731034100056, Val Loss: 0.013867589645087719\n",
      "Epoch 214, Loss: 0.008644690737128258, Val Loss: 0.013781602494418621\n",
      "Epoch 215, Loss: 0.008551920764148235, Val Loss: 0.013696597889065742\n",
      "Epoch 216, Loss: 0.008461066521704197, Val Loss: 0.01361400168389082\n",
      "Epoch 217, Loss: 0.008371567353606224, Val Loss: 0.01353227999061346\n",
      "Epoch 218, Loss: 0.008283895440399647, Val Loss: 0.013451230712234974\n",
      "Epoch 219, Loss: 0.00819708313792944, Val Loss: 0.013371494598686695\n",
      "Epoch 220, Loss: 0.008111744187772274, Val Loss: 0.013293063268065453\n",
      "Epoch 221, Loss: 0.008027839474380016, Val Loss: 0.013216455467045307\n",
      "Epoch 222, Loss: 0.007945197634398937, Val Loss: 0.013141315430402756\n",
      "Epoch 223, Loss: 0.007863916456699371, Val Loss: 0.013067482970654964\n",
      "Epoch 224, Loss: 0.007783751469105482, Val Loss: 0.012994577176868916\n",
      "Epoch 225, Loss: 0.0077048721723258495, Val Loss: 0.012922250665724277\n",
      "Epoch 226, Loss: 0.007627233397215605, Val Loss: 0.012852045707404613\n",
      "Epoch 227, Loss: 0.007550936657935381, Val Loss: 0.012782970443367958\n",
      "Epoch 228, Loss: 0.007475589867681265, Val Loss: 0.012714503332972527\n",
      "Epoch 229, Loss: 0.007401913870126009, Val Loss: 0.012648007832467556\n",
      "Epoch 230, Loss: 0.007329012732952833, Val Loss: 0.012581842951476574\n",
      "Epoch 231, Loss: 0.007256862707436085, Val Loss: 0.012517561204731464\n",
      "Epoch 232, Loss: 0.007186233066022396, Val Loss: 0.012453826144337654\n",
      "Epoch 233, Loss: 0.00711654732003808, Val Loss: 0.012391062453389168\n",
      "Epoch 234, Loss: 0.00704785343259573, Val Loss: 0.012329084798693657\n",
      "Epoch 235, Loss: 0.006980488542467356, Val Loss: 0.012268482707440853\n",
      "Epoch 236, Loss: 0.00691397488117218, Val Loss: 0.012208721600472927\n",
      "Epoch 237, Loss: 0.006848768796771765, Val Loss: 0.012149736285209656\n",
      "Epoch 238, Loss: 0.0067843059077858925, Val Loss: 0.01209261454641819\n",
      "Epoch 239, Loss: 0.00672134617343545, Val Loss: 0.012035333551466465\n",
      "Epoch 240, Loss: 0.006659034639596939, Val Loss: 0.011978869326412678\n",
      "Epoch 241, Loss: 0.006597399711608887, Val Loss: 0.01192410197108984\n",
      "Epoch 242, Loss: 0.0065368651412427425, Val Loss: 0.011869986541569233\n",
      "Epoch 243, Loss: 0.00647697364911437, Val Loss: 0.011816329322755337\n",
      "Epoch 244, Loss: 0.006417867261916399, Val Loss: 0.011763136833906174\n",
      "Epoch 245, Loss: 0.006359632592648268, Val Loss: 0.011711043305695057\n",
      "Epoch 246, Loss: 0.00630198186263442, Val Loss: 0.011658885516226292\n",
      "Epoch 247, Loss: 0.006245024036616087, Val Loss: 0.011608279310166836\n",
      "Epoch 248, Loss: 0.006189186591655016, Val Loss: 0.011558306403458118\n",
      "Epoch 249, Loss: 0.006133761256933212, Val Loss: 0.011509297415614128\n",
      "Epoch 250, Loss: 0.006079202517867088, Val Loss: 0.011460598558187485\n",
      "Epoch 251, Loss: 0.006025570444762707, Val Loss: 0.011412462219595909\n",
      "Epoch 252, Loss: 0.005972288548946381, Val Loss: 0.011366396211087704\n",
      "Epoch 253, Loss: 0.0059203291311860085, Val Loss: 0.011318318545818329\n",
      "Epoch 254, Loss: 0.0058688013814389706, Val Loss: 0.011272085830569267\n",
      "Epoch 255, Loss: 0.00581812858581543, Val Loss: 0.011226016096770763\n",
      "Epoch 256, Loss: 0.005767999216914177, Val Loss: 0.011180775240063667\n",
      "Epoch 257, Loss: 0.005718318745493889, Val Loss: 0.011136393994092941\n",
      "Epoch 258, Loss: 0.005669194273650646, Val Loss: 0.011091820895671844\n",
      "Epoch 259, Loss: 0.005620928015559912, Val Loss: 0.011048421263694763\n",
      "Epoch 260, Loss: 0.00557310227304697, Val Loss: 0.01100549940019846\n",
      "Epoch 261, Loss: 0.005525913089513779, Val Loss: 0.01096329279243946\n",
      "Epoch 262, Loss: 0.005479522980749607, Val Loss: 0.010921532288193703\n",
      "Epoch 263, Loss: 0.005433260928839445, Val Loss: 0.010880140587687492\n",
      "Epoch 264, Loss: 0.005387853365391493, Val Loss: 0.010839415714144707\n",
      "Epoch 265, Loss: 0.005343024153262377, Val Loss: 0.01079829316586256\n",
      "Epoch 266, Loss: 0.005298445466905832, Val Loss: 0.010758085176348686\n",
      "Epoch 267, Loss: 0.00525450287386775, Val Loss: 0.010717974975705147\n",
      "Epoch 268, Loss: 0.005210828501731157, Val Loss: 0.010678804479539394\n",
      "Epoch 269, Loss: 0.005167872179299593, Val Loss: 0.01064055971801281\n",
      "Epoch 270, Loss: 0.005125547759234905, Val Loss: 0.01060311496257782\n",
      "Epoch 271, Loss: 0.005083595402538776, Val Loss: 0.010565919801592827\n",
      "Epoch 272, Loss: 0.005042209289968014, Val Loss: 0.010529567487537861\n",
      "Epoch 273, Loss: 0.005001100245863199, Val Loss: 0.010494055226445198\n",
      "Epoch 274, Loss: 0.004960464313626289, Val Loss: 0.01045866310596466\n",
      "Epoch 275, Loss: 0.004920341540127993, Val Loss: 0.010424669831991196\n",
      "Epoch 276, Loss: 0.004880603402853012, Val Loss: 0.010391035117208958\n",
      "Epoch 277, Loss: 0.004841575864702463, Val Loss: 0.010357603430747986\n",
      "Epoch 278, Loss: 0.004802570212632418, Val Loss: 0.010324216447770596\n",
      "Epoch 279, Loss: 0.004763753153383732, Val Loss: 0.010292182676494122\n",
      "Epoch 280, Loss: 0.0047256783582270145, Val Loss: 0.010259998962283134\n",
      "Epoch 281, Loss: 0.004687788896262646, Val Loss: 0.010228477418422699\n",
      "Epoch 282, Loss: 0.00465031573548913, Val Loss: 0.010197790339589119\n",
      "Epoch 283, Loss: 0.004613342694938183, Val Loss: 0.010167502798140049\n",
      "Epoch 284, Loss: 0.004576667677611113, Val Loss: 0.010136924684047699\n",
      "Epoch 285, Loss: 0.0045402697287499905, Val Loss: 0.010106448084115982\n",
      "Epoch 286, Loss: 0.004504323471337557, Val Loss: 0.010076437145471573\n",
      "Epoch 287, Loss: 0.004468702711164951, Val Loss: 0.010045569390058517\n",
      "Epoch 288, Loss: 0.004433527588844299, Val Loss: 0.010015202686190605\n",
      "Epoch 289, Loss: 0.00439854059368372, Val Loss: 0.009984229691326618\n",
      "Epoch 290, Loss: 0.00436389772221446, Val Loss: 0.009953826665878296\n",
      "Epoch 291, Loss: 0.004329415503889322, Val Loss: 0.009923379868268967\n",
      "Epoch 292, Loss: 0.0042955223470926285, Val Loss: 0.009893098846077919\n",
      "Epoch 293, Loss: 0.004261883441358805, Val Loss: 0.00986429676413536\n",
      "Epoch 294, Loss: 0.004228423349559307, Val Loss: 0.009836345911026001\n",
      "Epoch 295, Loss: 0.004195270594209433, Val Loss: 0.009809092618525028\n",
      "Epoch 296, Loss: 0.004162428434938192, Val Loss: 0.009782395325601101\n",
      "Epoch 297, Loss: 0.0041298046708106995, Val Loss: 0.009757619351148605\n",
      "Epoch 298, Loss: 0.004097623750567436, Val Loss: 0.00973240565508604\n",
      "Epoch 299, Loss: 0.004065317567437887, Val Loss: 0.009708348661661148\n",
      "Epoch 300, Loss: 0.004033472388982773, Val Loss: 0.009683826938271523\n",
      "Epoch 301, Loss: 0.004002234898507595, Val Loss: 0.009660676121711731\n",
      "Epoch 302, Loss: 0.003971446305513382, Val Loss: 0.009636808186769485\n",
      "Epoch 303, Loss: 0.003941079135984182, Val Loss: 0.009612498804926872\n",
      "Epoch 304, Loss: 0.003911269828677177, Val Loss: 0.00958792120218277\n",
      "Epoch 305, Loss: 0.0038816514424979687, Val Loss: 0.009562831372022629\n",
      "Epoch 306, Loss: 0.003852345049381256, Val Loss: 0.00953766331076622\n",
      "Epoch 307, Loss: 0.0038233648519963026, Val Loss: 0.009511900134384632\n",
      "Epoch 308, Loss: 0.003794520627707243, Val Loss: 0.00948659423738718\n",
      "Epoch 309, Loss: 0.0037661048118025064, Val Loss: 0.009461375884711742\n",
      "Epoch 310, Loss: 0.0037381225265562534, Val Loss: 0.009437081404030323\n",
      "Epoch 311, Loss: 0.0037104259245097637, Val Loss: 0.009413513354957104\n",
      "Epoch 312, Loss: 0.0036831421311944723, Val Loss: 0.00939068105071783\n",
      "Epoch 313, Loss: 0.0036561391316354275, Val Loss: 0.009367013350129128\n",
      "Epoch 314, Loss: 0.0036294616293162107, Val Loss: 0.009344954043626785\n",
      "Epoch 315, Loss: 0.0036029077600687742, Val Loss: 0.00932289008051157\n",
      "Epoch 316, Loss: 0.003576785558834672, Val Loss: 0.009302026592195034\n",
      "Epoch 317, Loss: 0.0035507818683981895, Val Loss: 0.009281064383685589\n",
      "Epoch 318, Loss: 0.003524788888171315, Val Loss: 0.009260665625333786\n",
      "Epoch 319, Loss: 0.003499067621305585, Val Loss: 0.009239306673407555\n",
      "Epoch 320, Loss: 0.003473173128440976, Val Loss: 0.00921741034835577\n",
      "Epoch 321, Loss: 0.003446969436481595, Val Loss: 0.009195303544402122\n",
      "Epoch 322, Loss: 0.003419991349801421, Val Loss: 0.009173042140901089\n",
      "Epoch 323, Loss: 0.0033923748414963484, Val Loss: 0.009152333252131939\n",
      "Epoch 324, Loss: 0.0033655392471700907, Val Loss: 0.009131330996751785\n",
      "Epoch 325, Loss: 0.0033402573317289352, Val Loss: 0.0091114966198802\n",
      "Epoch 326, Loss: 0.003316242014989257, Val Loss: 0.009091362357139587\n",
      "Epoch 327, Loss: 0.003292839741334319, Val Loss: 0.009071974083781242\n",
      "Epoch 328, Loss: 0.0032699890434741974, Val Loss: 0.009052636101841927\n",
      "Epoch 329, Loss: 0.0032475776970386505, Val Loss: 0.009033303707838058\n",
      "Epoch 330, Loss: 0.0032254511024802923, Val Loss: 0.009014509618282318\n",
      "Epoch 331, Loss: 0.0032034660689532757, Val Loss: 0.00899592973291874\n",
      "Epoch 332, Loss: 0.003181830048561096, Val Loss: 0.00897737592458725\n",
      "Epoch 333, Loss: 0.00316044082865119, Val Loss: 0.008958660066127777\n",
      "Epoch 334, Loss: 0.0031392150558531284, Val Loss: 0.008940833620727062\n",
      "Epoch 335, Loss: 0.0031182034872472286, Val Loss: 0.008923494257032871\n",
      "Epoch 336, Loss: 0.0030974396504461765, Val Loss: 0.008906551636755466\n",
      "Epoch 337, Loss: 0.003076944500207901, Val Loss: 0.00888898503035307\n",
      "Epoch 338, Loss: 0.003056704066693783, Val Loss: 0.008872129954397678\n",
      "Epoch 339, Loss: 0.0030366198625415564, Val Loss: 0.008854654617607594\n",
      "Epoch 340, Loss: 0.003016745438799262, Val Loss: 0.008838648907840252\n",
      "Epoch 341, Loss: 0.0029971078038215637, Val Loss: 0.008822387084364891\n",
      "Epoch 342, Loss: 0.0029777465388178825, Val Loss: 0.008807044476270676\n",
      "Epoch 343, Loss: 0.002958473516628146, Val Loss: 0.00879181083291769\n",
      "Epoch 344, Loss: 0.002939477562904358, Val Loss: 0.008776462636888027\n",
      "Epoch 345, Loss: 0.002920642727985978, Val Loss: 0.008760903030633926\n",
      "Epoch 346, Loss: 0.0029018442146480083, Val Loss: 0.00874631293118\n",
      "Epoch 347, Loss: 0.002883490640670061, Val Loss: 0.008731714449822903\n",
      "Epoch 348, Loss: 0.0028651307802647352, Val Loss: 0.008716910146176815\n",
      "Epoch 349, Loss: 0.0028470270335674286, Val Loss: 0.008702567778527737\n",
      "Epoch 350, Loss: 0.002829092089086771, Val Loss: 0.008688589558005333\n",
      "Epoch 351, Loss: 0.002811404410749674, Val Loss: 0.008674869313836098\n",
      "Epoch 352, Loss: 0.002793786581605673, Val Loss: 0.008660800755023956\n",
      "Epoch 353, Loss: 0.002776333363726735, Val Loss: 0.008647242560982704\n",
      "Epoch 354, Loss: 0.0027591518592089415, Val Loss: 0.008633697405457497\n",
      "Epoch 355, Loss: 0.002742015989497304, Val Loss: 0.008619855158030987\n",
      "Epoch 356, Loss: 0.0027250766288489103, Val Loss: 0.008606547489762306\n",
      "Epoch 357, Loss: 0.0027083836030215025, Val Loss: 0.008593497797846794\n",
      "Epoch 358, Loss: 0.002691635163500905, Val Loss: 0.008580336347222328\n",
      "Epoch 359, Loss: 0.002675167052075267, Val Loss: 0.008567546494305134\n",
      "Epoch 360, Loss: 0.0026588942855596542, Val Loss: 0.008554212749004364\n",
      "Epoch 361, Loss: 0.0026427083648741245, Val Loss: 0.008542344905436039\n",
      "Epoch 362, Loss: 0.0026266800705343485, Val Loss: 0.008529605343937874\n",
      "Epoch 363, Loss: 0.002610880648717284, Val Loss: 0.00851724948734045\n",
      "Epoch 364, Loss: 0.002595233963802457, Val Loss: 0.008504406549036503\n",
      "Epoch 365, Loss: 0.002579578896984458, Val Loss: 0.008492741733789444\n",
      "Epoch 366, Loss: 0.0025641252286732197, Val Loss: 0.00848042219877243\n",
      "Epoch 367, Loss: 0.0025488093961030245, Val Loss: 0.008468549698591232\n",
      "Epoch 368, Loss: 0.0025336723774671555, Val Loss: 0.008456270210444927\n",
      "Epoch 369, Loss: 0.002518611028790474, Val Loss: 0.008444405160844326\n",
      "Epoch 370, Loss: 0.0025037568993866444, Val Loss: 0.00843318272382021\n",
      "Epoch 371, Loss: 0.0024890124332159758, Val Loss: 0.008421818725764751\n",
      "Epoch 372, Loss: 0.00247437902726233, Val Loss: 0.008410520851612091\n",
      "Epoch 373, Loss: 0.002459925599396229, Val Loss: 0.00840006023645401\n",
      "Epoch 374, Loss: 0.0024455594830214977, Val Loss: 0.008388804271817207\n",
      "Epoch 375, Loss: 0.002431288128718734, Val Loss: 0.008378459140658379\n",
      "Epoch 376, Loss: 0.0024171594996005297, Val Loss: 0.008367430418729782\n",
      "Epoch 377, Loss: 0.002403066959232092, Val Loss: 0.008356355130672455\n",
      "Epoch 378, Loss: 0.0023891564924269915, Val Loss: 0.00834633968770504\n",
      "Epoch 379, Loss: 0.0023754637222737074, Val Loss: 0.008335964754223824\n",
      "Epoch 380, Loss: 0.0023617586120963097, Val Loss: 0.008325623348355293\n",
      "Epoch 381, Loss: 0.002348219742998481, Val Loss: 0.008314967155456543\n",
      "Epoch 382, Loss: 0.002334859222173691, Val Loss: 0.008305328898131847\n",
      "Epoch 383, Loss: 0.0023215399123728275, Val Loss: 0.008295672945678234\n",
      "Epoch 384, Loss: 0.0023083193227648735, Val Loss: 0.008285602554678917\n",
      "Epoch 385, Loss: 0.00229520071297884, Val Loss: 0.008275963366031647\n",
      "Epoch 386, Loss: 0.002282279310747981, Val Loss: 0.008266986347734928\n",
      "Epoch 387, Loss: 0.0022693732753396034, Val Loss: 0.008257437497377396\n",
      "Epoch 388, Loss: 0.0022566106636077166, Val Loss: 0.008248240686953068\n",
      "Epoch 389, Loss: 0.002243880880996585, Val Loss: 0.008238735608756542\n",
      "Epoch 390, Loss: 0.0022313366644084454, Val Loss: 0.008229529485106468\n",
      "Epoch 391, Loss: 0.002218963112682104, Val Loss: 0.008220513351261616\n",
      "Epoch 392, Loss: 0.002206480363383889, Val Loss: 0.008211717940866947\n",
      "Epoch 393, Loss: 0.002194225089624524, Val Loss: 0.00820265244692564\n",
      "Epoch 394, Loss: 0.0021821155678480864, Val Loss: 0.008193215355277061\n",
      "Epoch 395, Loss: 0.0021699846256524324, Val Loss: 0.008184733800590038\n",
      "Epoch 396, Loss: 0.0021579591557383537, Val Loss: 0.008175867609679699\n",
      "Epoch 397, Loss: 0.002146159065887332, Val Loss: 0.00816726591438055\n",
      "Epoch 398, Loss: 0.002134362468495965, Val Loss: 0.008158872835338116\n",
      "Epoch 399, Loss: 0.0021226131357252598, Val Loss: 0.008150164037942886\n",
      "Epoch 400, Loss: 0.002111081499606371, Val Loss: 0.008142179809510708\n",
      "Epoch 401, Loss: 0.0020994541700929403, Val Loss: 0.008133702911436558\n",
      "Epoch 402, Loss: 0.0020880992524325848, Val Loss: 0.008126018568873405\n",
      "Epoch 403, Loss: 0.0020767473615705967, Val Loss: 0.008117577992379665\n",
      "Epoch 404, Loss: 0.002065452514216304, Val Loss: 0.008109426125884056\n",
      "Epoch 405, Loss: 0.002054371638223529, Val Loss: 0.008101359009742737\n",
      "Epoch 406, Loss: 0.002043265150859952, Val Loss: 0.00809411983937025\n",
      "Epoch 407, Loss: 0.002032287884503603, Val Loss: 0.00808680895715952\n",
      "Epoch 408, Loss: 0.0020213425159454346, Val Loss: 0.008079313673079014\n",
      "Epoch 409, Loss: 0.002010600408539176, Val Loss: 0.008071663789451122\n",
      "Epoch 410, Loss: 0.001999811502173543, Val Loss: 0.008064361289143562\n",
      "Epoch 411, Loss: 0.0019892307464033365, Val Loss: 0.008057397790253162\n",
      "Epoch 412, Loss: 0.0019786530174314976, Val Loss: 0.008049541153013706\n",
      "Epoch 413, Loss: 0.0019682354759424925, Val Loss: 0.008042479865252972\n",
      "Epoch 414, Loss: 0.0019578475039452314, Val Loss: 0.008035351522266865\n",
      "Epoch 415, Loss: 0.0019475283334031701, Val Loss: 0.0080284858122468\n",
      "Epoch 416, Loss: 0.0019373252289369702, Val Loss: 0.008021879009902477\n",
      "Epoch 417, Loss: 0.0019272143254056573, Val Loss: 0.00801429059356451\n",
      "Epoch 418, Loss: 0.0019171888707205653, Val Loss: 0.008008074015378952\n",
      "Epoch 419, Loss: 0.0019072294235229492, Val Loss: 0.008001265116035938\n",
      "Epoch 420, Loss: 0.0018973519327118993, Val Loss: 0.00799479614943266\n",
      "Epoch 421, Loss: 0.0018875427776947618, Val Loss: 0.007988190278410912\n",
      "Epoch 422, Loss: 0.001877916045486927, Val Loss: 0.00798177532851696\n",
      "Epoch 423, Loss: 0.001868230989202857, Val Loss: 0.007975165732204914\n",
      "Epoch 424, Loss: 0.0018586450023576617, Val Loss: 0.007967899553477764\n",
      "Epoch 425, Loss: 0.0018491486553102732, Val Loss: 0.007961695082485676\n",
      "Epoch 426, Loss: 0.0018397086532786489, Val Loss: 0.007954956963658333\n",
      "Epoch 427, Loss: 0.0018303659744560719, Val Loss: 0.007948622107505798\n",
      "Epoch 428, Loss: 0.0018210832495242357, Val Loss: 0.007942226715385914\n",
      "Epoch 429, Loss: 0.001811807043850422, Val Loss: 0.007935963571071625\n",
      "Epoch 430, Loss: 0.0018026951001957059, Val Loss: 0.00792966689914465\n",
      "Epoch 431, Loss: 0.0017935856012627482, Val Loss: 0.007923802360892296\n",
      "Epoch 432, Loss: 0.0017845978727564216, Val Loss: 0.007917609997093678\n",
      "Epoch 433, Loss: 0.0017756677698343992, Val Loss: 0.007911878637969494\n",
      "Epoch 434, Loss: 0.0017667773645371199, Val Loss: 0.007905666716396809\n",
      "Epoch 435, Loss: 0.001758006401360035, Val Loss: 0.007899551652371883\n",
      "Epoch 436, Loss: 0.0017491840990260243, Val Loss: 0.007893850095570087\n",
      "Epoch 437, Loss: 0.0017405175603926182, Val Loss: 0.007887691259384155\n",
      "Epoch 438, Loss: 0.001731877913698554, Val Loss: 0.007881429977715015\n",
      "Epoch 439, Loss: 0.001723276567645371, Val Loss: 0.007875259965658188\n",
      "Epoch 440, Loss: 0.001714706071652472, Val Loss: 0.007869239896535873\n",
      "Epoch 441, Loss: 0.001706301816739142, Val Loss: 0.007863537408411503\n",
      "Epoch 442, Loss: 0.0016979167703539133, Val Loss: 0.007857647724449635\n",
      "Epoch 443, Loss: 0.0016896181041374803, Val Loss: 0.007851648144423962\n",
      "Epoch 444, Loss: 0.0016813576221466064, Val Loss: 0.007845649495720863\n",
      "Epoch 445, Loss: 0.0016731020295992494, Val Loss: 0.007840130478143692\n",
      "Epoch 446, Loss: 0.0016648578457534313, Val Loss: 0.007834008894860744\n",
      "Epoch 447, Loss: 0.0016568058636039495, Val Loss: 0.007827406749129295\n",
      "Epoch 448, Loss: 0.001648797420784831, Val Loss: 0.00782081950455904\n",
      "Epoch 449, Loss: 0.0016408551018685102, Val Loss: 0.007814722135663033\n",
      "Epoch 450, Loss: 0.0016329501522704959, Val Loss: 0.00780875189229846\n",
      "Epoch 451, Loss: 0.001625129021704197, Val Loss: 0.007802889682352543\n",
      "Epoch 452, Loss: 0.0016174113843590021, Val Loss: 0.007797651924192905\n",
      "Epoch 453, Loss: 0.0016096909530460835, Val Loss: 0.007791783194988966\n",
      "Epoch 454, Loss: 0.0016020609764382243, Val Loss: 0.007786722853779793\n",
      "Epoch 455, Loss: 0.0015944570768624544, Val Loss: 0.007781386375427246\n",
      "Epoch 456, Loss: 0.0015869146445766091, Val Loss: 0.007776559796184301\n",
      "Epoch 457, Loss: 0.001579432049766183, Val Loss: 0.0077723609283566475\n",
      "Epoch 458, Loss: 0.0015720451483502984, Val Loss: 0.007767871953547001\n",
      "Epoch 459, Loss: 0.001564697246067226, Val Loss: 0.007762731984257698\n",
      "Epoch 460, Loss: 0.0015573549317196012, Val Loss: 0.007757825776934624\n",
      "Epoch 461, Loss: 0.001550001441501081, Val Loss: 0.007753389421850443\n",
      "Epoch 462, Loss: 0.0015427778707817197, Val Loss: 0.00774924224242568\n",
      "Epoch 463, Loss: 0.0015355725772678852, Val Loss: 0.007745400071144104\n",
      "Epoch 464, Loss: 0.0015284710098057985, Val Loss: 0.007740887347608805\n",
      "Epoch 465, Loss: 0.0015213668812066317, Val Loss: 0.007736629340797663\n",
      "Epoch 466, Loss: 0.001514329924248159, Val Loss: 0.007732965052127838\n",
      "Epoch 467, Loss: 0.0015073310350999236, Val Loss: 0.007729494012892246\n",
      "Epoch 468, Loss: 0.0015004200395196676, Val Loss: 0.0077252695336937904\n",
      "Epoch 469, Loss: 0.0014935520011931658, Val Loss: 0.007722130045294762\n",
      "Epoch 470, Loss: 0.0014867122517898679, Val Loss: 0.007718556560575962\n",
      "Epoch 471, Loss: 0.0014799066120758653, Val Loss: 0.007714431267231703\n",
      "Epoch 472, Loss: 0.0014731261180713773, Val Loss: 0.0077115814201533794\n",
      "Epoch 473, Loss: 0.0014664062764495611, Val Loss: 0.007707927376031876\n",
      "Epoch 474, Loss: 0.0014597282279282808, Val Loss: 0.007703946903347969\n",
      "Epoch 475, Loss: 0.0014530839398503304, Val Loss: 0.007700871676206589\n",
      "Epoch 476, Loss: 0.0014465437270700932, Val Loss: 0.007697571534663439\n",
      "Epoch 477, Loss: 0.0014400084037333727, Val Loss: 0.007693995255976915\n",
      "Epoch 478, Loss: 0.0014335146406665444, Val Loss: 0.007690316531807184\n",
      "Epoch 479, Loss: 0.0014270828105509281, Val Loss: 0.0076864114962518215\n",
      "Epoch 480, Loss: 0.0014206661144271493, Val Loss: 0.0076833157800138\n",
      "Epoch 481, Loss: 0.0014143610605970025, Val Loss: 0.007679833099246025\n",
      "Epoch 482, Loss: 0.0014080330729484558, Val Loss: 0.00767588010057807\n",
      "Epoch 483, Loss: 0.0014017829671502113, Val Loss: 0.007672297768294811\n",
      "Epoch 484, Loss: 0.0013955308822914958, Val Loss: 0.007669062353670597\n",
      "Epoch 485, Loss: 0.0013893366558477283, Val Loss: 0.007665327284485102\n",
      "Epoch 486, Loss: 0.0013831964461132884, Val Loss: 0.007662075571715832\n",
      "Epoch 487, Loss: 0.001377066015265882, Val Loss: 0.007658451329916716\n",
      "Epoch 488, Loss: 0.001371020800434053, Val Loss: 0.007655316032469273\n",
      "Epoch 489, Loss: 0.001364975469186902, Val Loss: 0.007652604021131992\n",
      "Epoch 490, Loss: 0.0013589943991973996, Val Loss: 0.007649489212781191\n",
      "Epoch 491, Loss: 0.001353041036054492, Val Loss: 0.007646511308848858\n",
      "Epoch 492, Loss: 0.0013470883714035153, Val Loss: 0.0076432120986282825\n",
      "Epoch 493, Loss: 0.001341204042546451, Val Loss: 0.007639915682375431\n",
      "Epoch 494, Loss: 0.0013353804824873805, Val Loss: 0.007636812049895525\n",
      "Epoch 495, Loss: 0.0013295720564201474, Val Loss: 0.007633791770786047\n",
      "Epoch 496, Loss: 0.0013237650273367763, Val Loss: 0.007630986627191305\n",
      "Epoch 497, Loss: 0.0013180364621803164, Val Loss: 0.007627747021615505\n",
      "Epoch 498, Loss: 0.0013123396784067154, Val Loss: 0.007625065743923187\n",
      "Epoch 499, Loss: 0.0013066757237538695, Val Loss: 0.007622076198458672\n",
      "Epoch 500, Loss: 0.0013010991970077157, Val Loss: 0.007619451731443405\n",
      "Epoch 501, Loss: 0.001295499037951231, Val Loss: 0.007616478484123945\n",
      "Epoch 502, Loss: 0.0012899579014629126, Val Loss: 0.007613871246576309\n",
      "Epoch 503, Loss: 0.0012844474986195564, Val Loss: 0.00761124212294817\n",
      "Epoch 504, Loss: 0.0012789644533768296, Val Loss: 0.007608858402818441\n",
      "Epoch 505, Loss: 0.0012734815245494246, Val Loss: 0.007606091443449259\n",
      "Epoch 506, Loss: 0.0012681226944550872, Val Loss: 0.007603492587804794\n",
      "Epoch 507, Loss: 0.0012627383694052696, Val Loss: 0.00760116521269083\n",
      "Epoch 508, Loss: 0.0012573649873957038, Val Loss: 0.0075984918512403965\n",
      "Epoch 509, Loss: 0.0012520980089902878, Val Loss: 0.007596373558044434\n",
      "Epoch 510, Loss: 0.0012467694468796253, Val Loss: 0.007593576330691576\n",
      "Epoch 511, Loss: 0.0012415108503773808, Val Loss: 0.00759161589667201\n",
      "Epoch 512, Loss: 0.0012362855486571789, Val Loss: 0.007588926702737808\n",
      "Epoch 513, Loss: 0.0012310676975175738, Val Loss: 0.007586491294205189\n",
      "Epoch 514, Loss: 0.0012259123614057899, Val Loss: 0.007584101986140013\n",
      "Epoch 515, Loss: 0.001220786478370428, Val Loss: 0.007581531070172787\n",
      "Epoch 516, Loss: 0.0012157037854194641, Val Loss: 0.007579287979751825\n",
      "Epoch 517, Loss: 0.0012106264475733042, Val Loss: 0.007577113341540098\n",
      "Epoch 518, Loss: 0.001205612439662218, Val Loss: 0.007574424147605896\n",
      "Epoch 519, Loss: 0.0012006033211946487, Val Loss: 0.00757239805534482\n",
      "Epoch 520, Loss: 0.00119566370267421, Val Loss: 0.007570136804133654\n",
      "Epoch 521, Loss: 0.0011907400330528617, Val Loss: 0.007568497210741043\n",
      "Epoch 522, Loss: 0.0011858148500323296, Val Loss: 0.007566686253994703\n",
      "Epoch 523, Loss: 0.0011809248244389892, Val Loss: 0.0075647565536201\n",
      "Epoch 524, Loss: 0.0011760961497202516, Val Loss: 0.007562503218650818\n",
      "Epoch 525, Loss: 0.0011712497798725963, Val Loss: 0.0075602540746331215\n",
      "Epoch 526, Loss: 0.0011664886260405183, Val Loss: 0.00755816837772727\n",
      "Epoch 527, Loss: 0.0011617110576480627, Val Loss: 0.00755513459444046\n",
      "Epoch 528, Loss: 0.0011569891357794404, Val Loss: 0.0075533101335167885\n",
      "Epoch 529, Loss: 0.0011522688437253237, Val Loss: 0.007550907786935568\n",
      "Epoch 530, Loss: 0.0011475696228444576, Val Loss: 0.00754843233153224\n",
      "Epoch 531, Loss: 0.0011429459555074573, Val Loss: 0.0075468081049621105\n",
      "Epoch 532, Loss: 0.0011383091332390904, Val Loss: 0.007544361986219883\n",
      "Epoch 533, Loss: 0.001133705023676157, Val Loss: 0.007542348466813564\n",
      "Epoch 534, Loss: 0.0011291267583146691, Val Loss: 0.007540808990597725\n",
      "Epoch 535, Loss: 0.0011245864443480968, Val Loss: 0.00753881735727191\n",
      "Epoch 536, Loss: 0.0011200762819498777, Val Loss: 0.007537157274782658\n",
      "Epoch 537, Loss: 0.0011155690299347043, Val Loss: 0.007535053417086601\n",
      "Epoch 538, Loss: 0.0011111291823908687, Val Loss: 0.00753326341509819\n",
      "Epoch 539, Loss: 0.0011067205341532826, Val Loss: 0.007532015908509493\n",
      "Epoch 540, Loss: 0.0011023026891052723, Val Loss: 0.00752996327355504\n",
      "Epoch 541, Loss: 0.0010979108046740294, Val Loss: 0.007528114132583141\n",
      "Epoch 542, Loss: 0.001093542668968439, Val Loss: 0.007526739500463009\n",
      "Epoch 543, Loss: 0.0010891971178352833, Val Loss: 0.007525377441197634\n",
      "Epoch 544, Loss: 0.001084877410903573, Val Loss: 0.0075242542661726475\n",
      "Epoch 545, Loss: 0.0010806097416207194, Val Loss: 0.007522509433329105\n",
      "Epoch 546, Loss: 0.0010763559257611632, Val Loss: 0.007520642131567001\n",
      "Epoch 547, Loss: 0.0010721138678491116, Val Loss: 0.0075187417678534985\n",
      "Epoch 548, Loss: 0.0010678861290216446, Val Loss: 0.007517330814152956\n",
      "Epoch 549, Loss: 0.0010636963415890932, Val Loss: 0.007515755016356707\n",
      "Epoch 550, Loss: 0.001059544156305492, Val Loss: 0.007514303084462881\n",
      "Epoch 551, Loss: 0.001055401866324246, Val Loss: 0.007512771058827639\n",
      "Epoch 552, Loss: 0.001051322789862752, Val Loss: 0.007511574309319258\n",
      "Epoch 553, Loss: 0.001047226949594915, Val Loss: 0.007509715389460325\n",
      "Epoch 554, Loss: 0.0010431723203510046, Val Loss: 0.0075083584524691105\n",
      "Epoch 555, Loss: 0.0010391115210950375, Val Loss: 0.007506759371608496\n",
      "Epoch 556, Loss: 0.00103505898732692, Val Loss: 0.007505356799811125\n",
      "Epoch 557, Loss: 0.001031027059070766, Val Loss: 0.0075040110386908054\n",
      "Epoch 558, Loss: 0.0010270734783262014, Val Loss: 0.007502909284085035\n",
      "Epoch 559, Loss: 0.0010231136111542583, Val Loss: 0.007501445710659027\n",
      "Epoch 560, Loss: 0.0010191902983933687, Val Loss: 0.007500245701521635\n",
      "Epoch 561, Loss: 0.001015298767015338, Val Loss: 0.007499275263398886\n",
      "Epoch 562, Loss: 0.001011412707157433, Val Loss: 0.007497969549149275\n",
      "Epoch 563, Loss: 0.001007548184134066, Val Loss: 0.007496556267142296\n",
      "Epoch 564, Loss: 0.0010036686435341835, Val Loss: 0.0074956682510674\n",
      "Epoch 565, Loss: 0.0009998417226597667, Val Loss: 0.007494453340768814\n",
      "Epoch 566, Loss: 0.0009960340103134513, Val Loss: 0.007493301760405302\n",
      "Epoch 567, Loss: 0.000992264598608017, Val Loss: 0.007491810712963343\n",
      "Epoch 568, Loss: 0.0009884861065074801, Val Loss: 0.007490646559745073\n",
      "Epoch 569, Loss: 0.0009847409091889858, Val Loss: 0.007489294745028019\n",
      "Epoch 570, Loss: 0.0009810362244024873, Val Loss: 0.007488610688596964\n",
      "Epoch 571, Loss: 0.00097730103880167, Val Loss: 0.007487645838409662\n",
      "Epoch 572, Loss: 0.0009736258652992547, Val Loss: 0.007486676797270775\n",
      "Epoch 573, Loss: 0.000969969667494297, Val Loss: 0.007485840003937483\n",
      "Epoch 574, Loss: 0.0009663226082921028, Val Loss: 0.007484672591090202\n",
      "Epoch 575, Loss: 0.0009626869577914476, Val Loss: 0.007483913563191891\n",
      "Epoch 576, Loss: 0.0009590685367584229, Val Loss: 0.007483632769435644\n",
      "Epoch 577, Loss: 0.0009554498246870935, Val Loss: 0.007482414599508047\n",
      "Epoch 578, Loss: 0.0009518803562968969, Val Loss: 0.007481427863240242\n",
      "Epoch 579, Loss: 0.000948344066273421, Val Loss: 0.0074804057367146015\n",
      "Epoch 580, Loss: 0.0009448155760765076, Val Loss: 0.007479398511350155\n",
      "Epoch 581, Loss: 0.0009412778890691698, Val Loss: 0.007478474173694849\n",
      "Epoch 582, Loss: 0.0009377614478580654, Val Loss: 0.007477715611457825\n",
      "Epoch 583, Loss: 0.0009342363919131458, Val Loss: 0.0074769919738173485\n",
      "Epoch 584, Loss: 0.0009307723958045244, Val Loss: 0.007476495578885078\n",
      "Epoch 585, Loss: 0.000927319226320833, Val Loss: 0.007476104889065027\n",
      "Epoch 586, Loss: 0.0009238584316335618, Val Loss: 0.007474979851394892\n",
      "Epoch 587, Loss: 0.0009204103262163699, Val Loss: 0.007474436890333891\n",
      "Epoch 588, Loss: 0.0009170289267785847, Val Loss: 0.007473862264305353\n",
      "Epoch 589, Loss: 0.0009136147564277053, Val Loss: 0.0074733709916472435\n",
      "Epoch 590, Loss: 0.0009101983741857111, Val Loss: 0.007472585886716843\n",
      "Epoch 591, Loss: 0.0009068244835361838, Val Loss: 0.0074718487448990345\n",
      "Epoch 592, Loss: 0.0009034633985720575, Val Loss: 0.0074713630601763725\n",
      "Epoch 593, Loss: 0.000900143466424197, Val Loss: 0.007470791693776846\n",
      "Epoch 594, Loss: 0.000896864861715585, Val Loss: 0.007470228243619204\n",
      "Epoch 595, Loss: 0.0008935804944485426, Val Loss: 0.007469482254236937\n",
      "Epoch 596, Loss: 0.0008903411217033863, Val Loss: 0.007468886207789183\n",
      "Epoch 597, Loss: 0.0008870906312949955, Val Loss: 0.007467974442988634\n",
      "Epoch 598, Loss: 0.0008838800713419914, Val Loss: 0.007467458490282297\n",
      "Epoch 599, Loss: 0.0008806823752820492, Val Loss: 0.007466825656592846\n",
      "Epoch 600, Loss: 0.0008774943999014795, Val Loss: 0.007466317154467106\n",
      "Epoch 601, Loss: 0.0008743507205508649, Val Loss: 0.007465687580406666\n",
      "Epoch 602, Loss: 0.0008712080889381468, Val Loss: 0.007465258706361055\n",
      "Epoch 603, Loss: 0.0008680653991177678, Val Loss: 0.007464692462235689\n",
      "Epoch 604, Loss: 0.0008649636874906719, Val Loss: 0.007464011199772358\n",
      "Epoch 605, Loss: 0.000861882755998522, Val Loss: 0.0074634552001953125\n",
      "Epoch 606, Loss: 0.0008588257478550076, Val Loss: 0.007462985347956419\n",
      "Epoch 607, Loss: 0.00085577426943928, Val Loss: 0.007462575100362301\n",
      "Epoch 608, Loss: 0.000852721743285656, Val Loss: 0.007462131790816784\n",
      "Epoch 609, Loss: 0.0008497073431499302, Val Loss: 0.007461863104254007\n",
      "Epoch 610, Loss: 0.0008466828730888665, Val Loss: 0.007461569271981716\n",
      "Epoch 611, Loss: 0.0008436720818281174, Val Loss: 0.007461047265678644\n",
      "Epoch 612, Loss: 0.0008406842825934291, Val Loss: 0.007460596971213818\n",
      "Epoch 613, Loss: 0.0008377258782275021, Val Loss: 0.007459658198058605\n",
      "Epoch 614, Loss: 0.0008347680559381843, Val Loss: 0.007459377404302359\n",
      "Epoch 615, Loss: 0.0008318284526467323, Val Loss: 0.007459133863449097\n",
      "Epoch 616, Loss: 0.0008289046818390489, Val Loss: 0.007458377163857222\n",
      "Epoch 617, Loss: 0.0008260117610916495, Val Loss: 0.007457981817424297\n",
      "Epoch 618, Loss: 0.0008230978855863214, Val Loss: 0.0074574267491698265\n",
      "Epoch 619, Loss: 0.0008202494354918599, Val Loss: 0.007456887513399124\n",
      "Epoch 620, Loss: 0.0008174016838893294, Val Loss: 0.00745687959715724\n",
      "Epoch 621, Loss: 0.0008145741303451359, Val Loss: 0.007457289844751358\n",
      "Epoch 622, Loss: 0.000811716599855572, Val Loss: 0.007457013241946697\n",
      "Epoch 623, Loss: 0.0008089171606115997, Val Loss: 0.007456259336322546\n",
      "Epoch 624, Loss: 0.0008061202825047076, Val Loss: 0.0074557228945195675\n",
      "Epoch 625, Loss: 0.0008033646154217422, Val Loss: 0.007455453276634216\n",
      "Epoch 626, Loss: 0.0008006002171896398, Val Loss: 0.007455148734152317\n",
      "Epoch 627, Loss: 0.0007978132925927639, Val Loss: 0.007454412989318371\n",
      "Epoch 628, Loss: 0.0007950662984512746, Val Loss: 0.00745485071092844\n",
      "Epoch 629, Loss: 0.0007923339726403356, Val Loss: 0.00745491124689579\n",
      "Epoch 630, Loss: 0.0007896299357526004, Val Loss: 0.007454162463545799\n",
      "Epoch 631, Loss: 0.0007869522669352591, Val Loss: 0.007454133592545986\n",
      "Epoch 632, Loss: 0.0007842559716664255, Val Loss: 0.007454220671206713\n",
      "Epoch 633, Loss: 0.00078157102689147, Val Loss: 0.007454075384885073\n",
      "Epoch 634, Loss: 0.0007789020892232656, Val Loss: 0.0074536181055009365\n",
      "Epoch 635, Loss: 0.0007762906025163829, Val Loss: 0.0074538420885801315\n",
      "Epoch 636, Loss: 0.0007736373227089643, Val Loss: 0.007453310303390026\n",
      "Epoch 637, Loss: 0.0007710282225161791, Val Loss: 0.0074534169398248196\n",
      "Epoch 638, Loss: 0.0007684182492084801, Val Loss: 0.007452907040715218\n",
      "Epoch 639, Loss: 0.000765795586630702, Val Loss: 0.0074531021527945995\n",
      "Epoch 640, Loss: 0.0007631994085386395, Val Loss: 0.007453547790646553\n",
      "Epoch 641, Loss: 0.0007606237777508795, Val Loss: 0.007454241625964642\n",
      "Epoch 642, Loss: 0.0007580611272715032, Val Loss: 0.007454133592545986\n",
      "Epoch 643, Loss: 0.0007555197225883603, Val Loss: 0.007454387377947569\n",
      "Epoch 644, Loss: 0.0007529921713285148, Val Loss: 0.007454499136656523\n",
      "Epoch 645, Loss: 0.0007504639215767384, Val Loss: 0.007455049082636833\n",
      "Epoch 646, Loss: 0.0007479492924176157, Val Loss: 0.007455692160874605\n",
      "Epoch 647, Loss: 0.000745421857573092, Val Loss: 0.007455591578036547\n",
      "Epoch 648, Loss: 0.0007429400575347245, Val Loss: 0.007456174585968256\n",
      "Epoch 649, Loss: 0.0007404902135021985, Val Loss: 0.007456607185304165\n",
      "Epoch 650, Loss: 0.0007380538154393435, Val Loss: 0.007456740364432335\n"
     ]
    }
   ],
   "source": [
    "from flashrag.prompt import PromptTemplate\n",
    "import pickle\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "config_taqa = set_config(dataset_name='taqa', split = split)\n",
    "template_taqa= PromptTemplate(\n",
    "        config = config_taqa,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "config_know= set_config(dataset_name='triviaqa', split = split)\n",
    "template_know= PromptTemplate(\n",
    "        config = config_know,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "\n",
    "\n",
    "loading_hidden = True\n",
    "if not loading_hidden:\n",
    "    taqa_hidden_states, triviaqa_hidden_states = load_datasets(config_taqa, config_triviaqa, template_taqa, template_triviaqa)\n",
    "else:\n",
    "    file_path_know = '/cs/student/projects2/dsml/cdiezmar/hidden_states/knowledge-aware.pkl'\n",
    "    file_path_taqa = '/cs/student/projects2/dsml/cdiezmar/hidden_states/taqa.pkl'\n",
    "\n",
    "    with open(file_path_know, 'rb') as file_know:\n",
    "        know_hidden_states = pickle.load(file_know)\n",
    "        print('Length triviaqa: ', len(know_hidden_states))\n",
    "        know_hidden_states_tensor= torch.tensor(know_hidden_states, dtype=torch.float32)\n",
    "    with open(file_path_taqa, 'rb') as file_taqa:\n",
    "        taqa_hidden_states = pickle.load(file_taqa)\n",
    "        print('Length taqa: ', len(taqa_hidden_states))\n",
    "        taqa_hidden_states_tensor= torch.tensor(taqa_hidden_states, dtype=torch.float32)\n",
    "train_inputs, train_labels, val_inputs, val_labels = prepare_data(taqa_hidden_states_tensor, know_hidden_states_tensor)\n",
    "\n",
    "input_size = train_inputs.size(1)\n",
    "num_classes = 2  # Knowledge-sensitive or not\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "epochs = 650\n",
    "lr = 1e-4\n",
    "\n",
    "# Train GRU Classifier\n",
    "gru_model = train_gru_classifier_cuda(train_inputs, train_labels, val_inputs, val_labels, input_size, hidden_size, num_layers, num_classes, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `question` in template\n",
      "Find `question` in template\n",
      "Length triviaqa:  20000\n",
      "Length taqa:  10148\n",
      "Epoch 1, Loss: 0.6883650422096252, Val Loss: 0.6679486632347107\n",
      "Epoch 2, Loss: 0.6677824854850769, Val Loss: 0.6489018797874451\n",
      "Epoch 3, Loss: 0.6487072706222534, Val Loss: 0.6313925385475159\n",
      "Epoch 4, Loss: 0.6311742663383484, Val Loss: 0.6153821349143982\n",
      "Epoch 5, Loss: 0.6151313781738281, Val Loss: 0.6006302833557129\n",
      "Epoch 6, Loss: 0.6003705263137817, Val Loss: 0.5867671966552734\n",
      "Epoch 7, Loss: 0.5865219235420227, Val Loss: 0.5733804702758789\n",
      "Epoch 8, Loss: 0.573111891746521, Val Loss: 0.5602161288261414\n",
      "Epoch 9, Loss: 0.5598524212837219, Val Loss: 0.547027587890625\n",
      "Epoch 10, Loss: 0.546565592288971, Val Loss: 0.5335932970046997\n",
      "Epoch 11, Loss: 0.5330953001976013, Val Loss: 0.5199558138847351\n",
      "Epoch 12, Loss: 0.5194850564002991, Val Loss: 0.5063032507896423\n",
      "Epoch 13, Loss: 0.5058802962303162, Val Loss: 0.49255552887916565\n",
      "Epoch 14, Loss: 0.4921559691429138, Val Loss: 0.47869130969047546\n",
      "Epoch 15, Loss: 0.4782881736755371, Val Loss: 0.4646693766117096\n",
      "Epoch 16, Loss: 0.4642658233642578, Val Loss: 0.45057448744773865\n",
      "Epoch 17, Loss: 0.4501846730709076, Val Loss: 0.43644654750823975\n",
      "Epoch 18, Loss: 0.436083048582077, Val Loss: 0.42242953181266785\n",
      "Epoch 19, Loss: 0.4220922291278839, Val Loss: 0.40867117047309875\n",
      "Epoch 20, Loss: 0.40835464000701904, Val Loss: 0.39524033665657043\n",
      "Epoch 21, Loss: 0.39494380354881287, Val Loss: 0.3819620907306671\n",
      "Epoch 22, Loss: 0.38172364234924316, Val Loss: 0.3687432110309601\n",
      "Epoch 23, Loss: 0.3686050772666931, Val Loss: 0.35555940866470337\n",
      "Epoch 24, Loss: 0.35553035140037537, Val Loss: 0.34251266717910767\n",
      "Epoch 25, Loss: 0.3425907790660858, Val Loss: 0.329667329788208\n",
      "Epoch 26, Loss: 0.329850971698761, Val Loss: 0.3170235753059387\n",
      "Epoch 27, Loss: 0.31730765104293823, Val Loss: 0.30463218688964844\n",
      "Epoch 28, Loss: 0.3050144910812378, Val Loss: 0.2925172448158264\n",
      "Epoch 29, Loss: 0.2930017411708832, Val Loss: 0.2806725800037384\n",
      "Epoch 30, Loss: 0.2812677025794983, Val Loss: 0.2691327631473541\n",
      "Epoch 31, Loss: 0.26982516050338745, Val Loss: 0.2579334080219269\n",
      "Epoch 32, Loss: 0.258705198764801, Val Loss: 0.24706804752349854\n",
      "Epoch 33, Loss: 0.24790602922439575, Val Loss: 0.23652812838554382\n",
      "Epoch 34, Loss: 0.23742708563804626, Val Loss: 0.22635219991207123\n",
      "Epoch 35, Loss: 0.2273063063621521, Val Loss: 0.21653787791728973\n",
      "Epoch 36, Loss: 0.21754035353660583, Val Loss: 0.20709264278411865\n",
      "Epoch 37, Loss: 0.20814192295074463, Val Loss: 0.19800463318824768\n",
      "Epoch 38, Loss: 0.19908732175827026, Val Loss: 0.18928378820419312\n",
      "Epoch 39, Loss: 0.19038017094135284, Val Loss: 0.18093597888946533\n",
      "Epoch 40, Loss: 0.18203160166740417, Val Loss: 0.17294996976852417\n",
      "Epoch 41, Loss: 0.1740257889032364, Val Loss: 0.1653476357460022\n",
      "Epoch 42, Loss: 0.1663728505373001, Val Loss: 0.15814745426177979\n",
      "Epoch 43, Loss: 0.15909817814826965, Val Loss: 0.15134181082248688\n",
      "Epoch 44, Loss: 0.15220457315444946, Val Loss: 0.14490193128585815\n",
      "Epoch 45, Loss: 0.1456698775291443, Val Loss: 0.13878145813941956\n",
      "Epoch 46, Loss: 0.13945011794567108, Val Loss: 0.13294737040996552\n",
      "Epoch 47, Loss: 0.1335141956806183, Val Loss: 0.1273876130580902\n",
      "Epoch 48, Loss: 0.12785141170024872, Val Loss: 0.12208018451929092\n",
      "Epoch 49, Loss: 0.12243649363517761, Val Loss: 0.1170041412115097\n",
      "Epoch 50, Loss: 0.11724360287189484, Val Loss: 0.11216616630554199\n",
      "Epoch 51, Loss: 0.11227833479642868, Val Loss: 0.10753745585680008\n",
      "Epoch 52, Loss: 0.10751698911190033, Val Loss: 0.10311087220907211\n",
      "Epoch 53, Loss: 0.10294780135154724, Val Loss: 0.0988643541932106\n",
      "Epoch 54, Loss: 0.098544642329216, Val Loss: 0.09479835629463196\n",
      "Epoch 55, Loss: 0.09432269632816315, Val Loss: 0.09088702499866486\n",
      "Epoch 56, Loss: 0.09025890380144119, Val Loss: 0.08713127672672272\n",
      "Epoch 57, Loss: 0.0863531231880188, Val Loss: 0.0835210531949997\n",
      "Epoch 58, Loss: 0.08260021358728409, Val Loss: 0.08004467934370041\n",
      "Epoch 59, Loss: 0.07899722456932068, Val Loss: 0.07670729607343674\n",
      "Epoch 60, Loss: 0.0755433440208435, Val Loss: 0.07349533587694168\n",
      "Epoch 61, Loss: 0.07222367823123932, Val Loss: 0.07040759176015854\n",
      "Epoch 62, Loss: 0.06904382258653641, Val Loss: 0.06743765622377396\n",
      "Epoch 63, Loss: 0.06599429249763489, Val Loss: 0.06458145380020142\n",
      "Epoch 64, Loss: 0.06306993216276169, Val Loss: 0.06185213476419449\n",
      "Epoch 65, Loss: 0.0602819062769413, Val Loss: 0.05923662334680557\n",
      "Epoch 66, Loss: 0.0576254241168499, Val Loss: 0.05673936381936073\n",
      "Epoch 67, Loss: 0.05511009320616722, Val Loss: 0.054350562393665314\n",
      "Epoch 68, Loss: 0.052730102092027664, Val Loss: 0.05209709331393242\n",
      "Epoch 69, Loss: 0.05050808563828468, Val Loss: 0.04997045546770096\n",
      "Epoch 70, Loss: 0.04842161387205124, Val Loss: 0.04798656702041626\n",
      "Epoch 71, Loss: 0.04648059234023094, Val Loss: 0.04614090919494629\n",
      "Epoch 72, Loss: 0.044678326696157455, Val Loss: 0.0444304496049881\n",
      "Epoch 73, Loss: 0.04300902411341667, Val Loss: 0.042837515473365784\n",
      "Epoch 74, Loss: 0.04145358130335808, Val Loss: 0.04135534539818764\n",
      "Epoch 75, Loss: 0.04000108316540718, Val Loss: 0.039970315992832184\n",
      "Epoch 76, Loss: 0.03863484039902687, Val Loss: 0.038673222064971924\n",
      "Epoch 77, Loss: 0.03734057396650314, Val Loss: 0.03745606914162636\n",
      "Epoch 78, Loss: 0.03611104562878609, Val Loss: 0.03630813583731651\n",
      "Epoch 79, Loss: 0.03493066132068634, Val Loss: 0.03522833064198494\n",
      "Epoch 80, Loss: 0.033799588680267334, Val Loss: 0.03420441225171089\n",
      "Epoch 81, Loss: 0.032706212252378464, Val Loss: 0.03323636204004288\n",
      "Epoch 82, Loss: 0.031652819365262985, Val Loss: 0.03231621906161308\n",
      "Epoch 83, Loss: 0.03063535876572132, Val Loss: 0.03143941983580589\n",
      "Epoch 84, Loss: 0.029652228578925133, Val Loss: 0.03060547448694706\n",
      "Epoch 85, Loss: 0.0287043247371912, Val Loss: 0.02981831505894661\n",
      "Epoch 86, Loss: 0.027799086645245552, Val Loss: 0.029064223170280457\n",
      "Epoch 87, Loss: 0.02692563459277153, Val Loss: 0.028350597247481346\n",
      "Epoch 88, Loss: 0.026094378903508186, Val Loss: 0.02767287753522396\n",
      "Epoch 89, Loss: 0.025298258289694786, Val Loss: 0.02703002095222473\n",
      "Epoch 90, Loss: 0.024538271129131317, Val Loss: 0.02642384171485901\n",
      "Epoch 91, Loss: 0.023817604407668114, Val Loss: 0.025846604257822037\n",
      "Epoch 92, Loss: 0.023127857595682144, Val Loss: 0.02530187927186489\n",
      "Epoch 93, Loss: 0.022472530603408813, Val Loss: 0.024784022942185402\n",
      "Epoch 94, Loss: 0.021847158670425415, Val Loss: 0.02429461106657982\n",
      "Epoch 95, Loss: 0.021251255646348, Val Loss: 0.023827945813536644\n",
      "Epoch 96, Loss: 0.02068142406642437, Val Loss: 0.0233836118131876\n",
      "Epoch 97, Loss: 0.020136836916208267, Val Loss: 0.02295888029038906\n",
      "Epoch 98, Loss: 0.019614098593592644, Val Loss: 0.02255317009985447\n",
      "Epoch 99, Loss: 0.019114632159471512, Val Loss: 0.02216387912631035\n",
      "Epoch 100, Loss: 0.018634695559740067, Val Loss: 0.021788110956549644\n",
      "Epoch 101, Loss: 0.018173258751630783, Val Loss: 0.02142365090548992\n",
      "Epoch 102, Loss: 0.017727423459291458, Val Loss: 0.021074380725622177\n",
      "Epoch 103, Loss: 0.017299113795161247, Val Loss: 0.02073676697909832\n",
      "Epoch 104, Loss: 0.016884414479136467, Val Loss: 0.020411569625139236\n",
      "Epoch 105, Loss: 0.016485409811139107, Val Loss: 0.020097609609365463\n",
      "Epoch 106, Loss: 0.01609896682202816, Val Loss: 0.019796088337898254\n",
      "Epoch 107, Loss: 0.0157272070646286, Val Loss: 0.019504057243466377\n",
      "Epoch 108, Loss: 0.015368381515145302, Val Loss: 0.019222741946578026\n",
      "Epoch 109, Loss: 0.015020867809653282, Val Loss: 0.018950246274471283\n",
      "Epoch 110, Loss: 0.014685584232211113, Val Loss: 0.018686385825276375\n",
      "Epoch 111, Loss: 0.014361650682985783, Val Loss: 0.01843138039112091\n",
      "Epoch 112, Loss: 0.014048662036657333, Val Loss: 0.01818503998219967\n",
      "Epoch 113, Loss: 0.01374544482678175, Val Loss: 0.017944369465112686\n",
      "Epoch 114, Loss: 0.01345102395862341, Val Loss: 0.017712581902742386\n",
      "Epoch 115, Loss: 0.013166333548724651, Val Loss: 0.017490042373538017\n",
      "Epoch 116, Loss: 0.012890747748315334, Val Loss: 0.017273690551519394\n",
      "Epoch 117, Loss: 0.012623876333236694, Val Loss: 0.01706404983997345\n",
      "Epoch 118, Loss: 0.01236498262733221, Val Loss: 0.016863077878952026\n",
      "Epoch 119, Loss: 0.012114283628761768, Val Loss: 0.016668397933244705\n",
      "Epoch 120, Loss: 0.01187150739133358, Val Loss: 0.01648174598813057\n",
      "Epoch 121, Loss: 0.011636229231953621, Val Loss: 0.016299955546855927\n",
      "Epoch 122, Loss: 0.011407791636884212, Val Loss: 0.01612553372979164\n",
      "Epoch 123, Loss: 0.01118774339556694, Val Loss: 0.015955766662955284\n",
      "Epoch 124, Loss: 0.010972416028380394, Val Loss: 0.015790710225701332\n",
      "Epoch 125, Loss: 0.010764573700726032, Val Loss: 0.01563090831041336\n",
      "Epoch 126, Loss: 0.010561507195234299, Val Loss: 0.015476414002478123\n",
      "Epoch 127, Loss: 0.010365435853600502, Val Loss: 0.015327290631830692\n",
      "Epoch 128, Loss: 0.010174587368965149, Val Loss: 0.015182425267994404\n",
      "Epoch 129, Loss: 0.009988208301365376, Val Loss: 0.015042299404740334\n",
      "Epoch 130, Loss: 0.009807053953409195, Val Loss: 0.014906326308846474\n",
      "Epoch 131, Loss: 0.009631062857806683, Val Loss: 0.014774026349186897\n",
      "Epoch 132, Loss: 0.009459580294787884, Val Loss: 0.014645978808403015\n",
      "Epoch 133, Loss: 0.009291784837841988, Val Loss: 0.014521812088787556\n",
      "Epoch 134, Loss: 0.009128970094025135, Val Loss: 0.014401106163859367\n",
      "Epoch 135, Loss: 0.00896990206092596, Val Loss: 0.014284628443419933\n",
      "Epoch 136, Loss: 0.008815375156700611, Val Loss: 0.014170512557029724\n",
      "Epoch 137, Loss: 0.008663796819746494, Val Loss: 0.01405867375433445\n",
      "Epoch 138, Loss: 0.008516667410731316, Val Loss: 0.013950689695775509\n",
      "Epoch 139, Loss: 0.008372930809855461, Val Loss: 0.013845308683812618\n",
      "Epoch 140, Loss: 0.008231810294091702, Val Loss: 0.013743112795054913\n",
      "Epoch 141, Loss: 0.00809563510119915, Val Loss: 0.013644656166434288\n",
      "Epoch 142, Loss: 0.007962879724800587, Val Loss: 0.01354936696588993\n",
      "Epoch 143, Loss: 0.007832815870642662, Val Loss: 0.013455581851303577\n",
      "Epoch 144, Loss: 0.0077055469155311584, Val Loss: 0.013364479877054691\n",
      "Epoch 145, Loss: 0.00758159626275301, Val Loss: 0.01327584870159626\n",
      "Epoch 146, Loss: 0.007460894528776407, Val Loss: 0.013189708814024925\n",
      "Epoch 147, Loss: 0.007342925760895014, Val Loss: 0.013105767779052258\n",
      "Epoch 148, Loss: 0.007228301838040352, Val Loss: 0.01302482932806015\n",
      "Epoch 149, Loss: 0.0071161650121212006, Val Loss: 0.012944348156452179\n",
      "Epoch 150, Loss: 0.007006378844380379, Val Loss: 0.012866889126598835\n",
      "Epoch 151, Loss: 0.0068999649956822395, Val Loss: 0.012790679931640625\n",
      "Epoch 152, Loss: 0.006795342545956373, Val Loss: 0.012716008350253105\n",
      "Epoch 153, Loss: 0.006693687289953232, Val Loss: 0.012641791254281998\n",
      "Epoch 154, Loss: 0.006593926344066858, Val Loss: 0.01256934180855751\n",
      "Epoch 155, Loss: 0.006496914196759462, Val Loss: 0.012498697265982628\n",
      "Epoch 156, Loss: 0.006401598919183016, Val Loss: 0.012429027818143368\n",
      "Epoch 157, Loss: 0.00630868598818779, Val Loss: 0.01236118283122778\n",
      "Epoch 158, Loss: 0.0062177954241633415, Val Loss: 0.012293234467506409\n",
      "Epoch 159, Loss: 0.006127945613116026, Val Loss: 0.012227159924805164\n",
      "Epoch 160, Loss: 0.006040430627763271, Val Loss: 0.012163576669991016\n",
      "Epoch 161, Loss: 0.00595515314489603, Val Loss: 0.012100727297365665\n",
      "Epoch 162, Loss: 0.005871260538697243, Val Loss: 0.012041907757520676\n",
      "Epoch 163, Loss: 0.005790341179817915, Val Loss: 0.011983992531895638\n",
      "Epoch 164, Loss: 0.005710913334041834, Val Loss: 0.011928042396903038\n",
      "Epoch 165, Loss: 0.005633407738059759, Val Loss: 0.011874545365571976\n",
      "Epoch 166, Loss: 0.005557679571211338, Val Loss: 0.011821613647043705\n",
      "Epoch 167, Loss: 0.005483632441610098, Val Loss: 0.01176962535828352\n",
      "Epoch 168, Loss: 0.005411153193563223, Val Loss: 0.011717620305716991\n",
      "Epoch 169, Loss: 0.005340007599443197, Val Loss: 0.011667916551232338\n",
      "Epoch 170, Loss: 0.005270478315651417, Val Loss: 0.011620082892477512\n",
      "Epoch 171, Loss: 0.005202217027544975, Val Loss: 0.011573691852390766\n",
      "Epoch 172, Loss: 0.005135178565979004, Val Loss: 0.011528088711202145\n",
      "Epoch 173, Loss: 0.0050699468702077866, Val Loss: 0.011482324451208115\n",
      "Epoch 174, Loss: 0.005005774088203907, Val Loss: 0.011438547633588314\n",
      "Epoch 175, Loss: 0.00494338059797883, Val Loss: 0.011395466513931751\n",
      "Epoch 176, Loss: 0.004882156848907471, Val Loss: 0.011351825669407845\n",
      "Epoch 177, Loss: 0.00482164416462183, Val Loss: 0.01131106074899435\n",
      "Epoch 178, Loss: 0.004762830212712288, Val Loss: 0.011268464848399162\n",
      "Epoch 179, Loss: 0.004704566672444344, Val Loss: 0.011227338574826717\n",
      "Epoch 180, Loss: 0.004647335968911648, Val Loss: 0.011188157834112644\n",
      "Epoch 181, Loss: 0.004590938333421946, Val Loss: 0.011148165911436081\n",
      "Epoch 182, Loss: 0.0045350901782512665, Val Loss: 0.011108563281595707\n",
      "Epoch 183, Loss: 0.004480597097426653, Val Loss: 0.01107070967555046\n",
      "Epoch 184, Loss: 0.0044270665384829044, Val Loss: 0.01103329285979271\n",
      "Epoch 185, Loss: 0.00437399884685874, Val Loss: 0.01099699642509222\n",
      "Epoch 186, Loss: 0.004322716034948826, Val Loss: 0.010960998944938183\n",
      "Epoch 187, Loss: 0.004271945916116238, Val Loss: 0.010925959795713425\n",
      "Epoch 188, Loss: 0.00422265101224184, Val Loss: 0.010891861282289028\n",
      "Epoch 189, Loss: 0.004174389876425266, Val Loss: 0.010858699679374695\n",
      "Epoch 190, Loss: 0.00412679323926568, Val Loss: 0.01082589477300644\n",
      "Epoch 191, Loss: 0.004080205224454403, Val Loss: 0.010793258436024189\n",
      "Epoch 192, Loss: 0.004034309182316065, Val Loss: 0.010762457735836506\n",
      "Epoch 193, Loss: 0.003989393822848797, Val Loss: 0.01073251198977232\n",
      "Epoch 194, Loss: 0.003945081029087305, Val Loss: 0.01070273108780384\n",
      "Epoch 195, Loss: 0.003902079537510872, Val Loss: 0.010673534125089645\n",
      "Epoch 196, Loss: 0.0038595437072217464, Val Loss: 0.010643916204571724\n",
      "Epoch 197, Loss: 0.00381722254678607, Val Loss: 0.01061562541872263\n",
      "Epoch 198, Loss: 0.0037759533151984215, Val Loss: 0.010587859898805618\n",
      "Epoch 199, Loss: 0.003735325299203396, Val Loss: 0.010560296475887299\n",
      "Epoch 200, Loss: 0.0036953503731638193, Val Loss: 0.010533916763961315\n",
      "Epoch 201, Loss: 0.00365625973790884, Val Loss: 0.010507827624678612\n",
      "Epoch 202, Loss: 0.0036179267335683107, Val Loss: 0.010481469333171844\n",
      "Epoch 203, Loss: 0.00357968476600945, Val Loss: 0.010455542244017124\n",
      "Epoch 204, Loss: 0.0035423540975898504, Val Loss: 0.010431336238980293\n",
      "Epoch 205, Loss: 0.003505786182358861, Val Loss: 0.010406602174043655\n",
      "Epoch 206, Loss: 0.0034697463270276785, Val Loss: 0.010382486507296562\n",
      "Epoch 207, Loss: 0.0034342824947088957, Val Loss: 0.010358752682805061\n",
      "Epoch 208, Loss: 0.0033991599921137094, Val Loss: 0.010334927588701248\n",
      "Epoch 209, Loss: 0.003364777425304055, Val Loss: 0.010312949307262897\n",
      "Epoch 210, Loss: 0.003331049345433712, Val Loss: 0.01028978731483221\n",
      "Epoch 211, Loss: 0.0032978923991322517, Val Loss: 0.01026657409965992\n",
      "Epoch 212, Loss: 0.003265100298449397, Val Loss: 0.010245280340313911\n",
      "Epoch 213, Loss: 0.0032327016815543175, Val Loss: 0.01022343710064888\n",
      "Epoch 214, Loss: 0.0032012455631047487, Val Loss: 0.010202470235526562\n",
      "Epoch 215, Loss: 0.003170149400830269, Val Loss: 0.010181249119341373\n",
      "Epoch 216, Loss: 0.0031395405530929565, Val Loss: 0.0101607795804739\n",
      "Epoch 217, Loss: 0.0031095719896256924, Val Loss: 0.010140474885702133\n",
      "Epoch 218, Loss: 0.00308014964684844, Val Loss: 0.010119860991835594\n",
      "Epoch 219, Loss: 0.003051032545045018, Val Loss: 0.010100510902702808\n",
      "Epoch 220, Loss: 0.0030223848298192024, Val Loss: 0.010081603191792965\n",
      "Epoch 221, Loss: 0.0029944246634840965, Val Loss: 0.010062513872981071\n",
      "Epoch 222, Loss: 0.002966694999486208, Val Loss: 0.010043502785265446\n",
      "Epoch 223, Loss: 0.0029396992176771164, Val Loss: 0.01002519577741623\n",
      "Epoch 224, Loss: 0.002912800759077072, Val Loss: 0.010008024983108044\n",
      "Epoch 225, Loss: 0.002886297181248665, Val Loss: 0.009990918450057507\n",
      "Epoch 226, Loss: 0.002859933767467737, Val Loss: 0.009973869659006596\n",
      "Epoch 227, Loss: 0.0028342092409729958, Val Loss: 0.009956574067473412\n",
      "Epoch 228, Loss: 0.002808694262057543, Val Loss: 0.009939770214259624\n",
      "Epoch 229, Loss: 0.0027836160734295845, Val Loss: 0.009924178943037987\n",
      "Epoch 230, Loss: 0.0027590396348387003, Val Loss: 0.009907913394272327\n",
      "Epoch 231, Loss: 0.002734279027208686, Val Loss: 0.00989216472953558\n",
      "Epoch 232, Loss: 0.0027100304141640663, Val Loss: 0.00987850222736597\n",
      "Epoch 233, Loss: 0.002686152933165431, Val Loss: 0.009863140992820263\n",
      "Epoch 234, Loss: 0.002662565791979432, Val Loss: 0.009848217479884624\n",
      "Epoch 235, Loss: 0.0026392547879368067, Val Loss: 0.009834332391619682\n",
      "Epoch 236, Loss: 0.0026163896545767784, Val Loss: 0.009819680824875832\n",
      "Epoch 237, Loss: 0.0025934556033462286, Val Loss: 0.009805632755160332\n",
      "Epoch 238, Loss: 0.0025711674243211746, Val Loss: 0.009792542085051537\n",
      "Epoch 239, Loss: 0.002548940246924758, Val Loss: 0.009779095649719238\n",
      "Epoch 240, Loss: 0.002527054166421294, Val Loss: 0.009765543043613434\n",
      "Epoch 241, Loss: 0.0025053760036826134, Val Loss: 0.009752790443599224\n",
      "Epoch 242, Loss: 0.00248391623608768, Val Loss: 0.009739656001329422\n",
      "Epoch 243, Loss: 0.002462791744619608, Val Loss: 0.009727171622216702\n",
      "Epoch 244, Loss: 0.0024416991509497166, Val Loss: 0.009714731946587563\n",
      "Epoch 245, Loss: 0.002421175129711628, Val Loss: 0.009702883660793304\n",
      "Epoch 246, Loss: 0.0024007069878280163, Val Loss: 0.009689928032457829\n",
      "Epoch 247, Loss: 0.0023802861105650663, Val Loss: 0.0096781887114048\n",
      "Epoch 248, Loss: 0.0023603071458637714, Val Loss: 0.009666814468801022\n",
      "Epoch 249, Loss: 0.0023405437823385, Val Loss: 0.009655022993683815\n",
      "Epoch 250, Loss: 0.0023208956699818373, Val Loss: 0.009643594734370708\n",
      "Epoch 251, Loss: 0.0023014184553176165, Val Loss: 0.009631934575736523\n",
      "Epoch 252, Loss: 0.002282179659232497, Val Loss: 0.009620466269552708\n",
      "Epoch 253, Loss: 0.002263037720695138, Val Loss: 0.009610069915652275\n",
      "Epoch 254, Loss: 0.0022441777400672436, Val Loss: 0.009598716162145138\n",
      "Epoch 255, Loss: 0.0022253424394875765, Val Loss: 0.009588689543306828\n",
      "Epoch 256, Loss: 0.0022068903781473637, Val Loss: 0.00957915186882019\n",
      "Epoch 257, Loss: 0.0021884122397750616, Val Loss: 0.009568415582180023\n",
      "Epoch 258, Loss: 0.0021701359655708075, Val Loss: 0.009558064863085747\n",
      "Epoch 259, Loss: 0.0021517151035368443, Val Loss: 0.009548205882310867\n",
      "Epoch 260, Loss: 0.0021334111224859953, Val Loss: 0.009538266807794571\n",
      "Epoch 261, Loss: 0.002115050796419382, Val Loss: 0.009528763592243195\n",
      "Epoch 262, Loss: 0.002096457639709115, Val Loss: 0.009519538842141628\n",
      "Epoch 263, Loss: 0.002078055404126644, Val Loss: 0.009509624913334846\n",
      "Epoch 264, Loss: 0.0020596259273588657, Val Loss: 0.00949989166110754\n",
      "Epoch 265, Loss: 0.0020412509329617023, Val Loss: 0.009490545839071274\n",
      "Epoch 266, Loss: 0.0020234202966094017, Val Loss: 0.009481501765549183\n",
      "Epoch 267, Loss: 0.002006051829084754, Val Loss: 0.009472481906414032\n",
      "Epoch 268, Loss: 0.0019890661351382732, Val Loss: 0.009462440386414528\n",
      "Epoch 269, Loss: 0.00197241292335093, Val Loss: 0.009454120881855488\n",
      "Epoch 270, Loss: 0.0019563622772693634, Val Loss: 0.009445246309041977\n",
      "Epoch 271, Loss: 0.0019405143102630973, Val Loss: 0.009435935877263546\n",
      "Epoch 272, Loss: 0.0019250103505328298, Val Loss: 0.00942742545157671\n",
      "Epoch 273, Loss: 0.0019099985947832465, Val Loss: 0.00941876508295536\n",
      "Epoch 274, Loss: 0.0018950178055092692, Val Loss: 0.009410860948264599\n",
      "Epoch 275, Loss: 0.0018802061676979065, Val Loss: 0.009402163326740265\n",
      "Epoch 276, Loss: 0.0018656591419130564, Val Loss: 0.009394099935889244\n",
      "Epoch 277, Loss: 0.0018513939576223493, Val Loss: 0.009385130368173122\n",
      "Epoch 278, Loss: 0.0018371891928836703, Val Loss: 0.00937733892351389\n",
      "Epoch 279, Loss: 0.001823294791392982, Val Loss: 0.00936928205192089\n",
      "Epoch 280, Loss: 0.0018096447456628084, Val Loss: 0.009361091069877148\n",
      "Epoch 281, Loss: 0.0017962047131732106, Val Loss: 0.009353360161185265\n",
      "Epoch 282, Loss: 0.001782745122909546, Val Loss: 0.009346239268779755\n",
      "Epoch 283, Loss: 0.001769757247529924, Val Loss: 0.009338174015283585\n",
      "Epoch 284, Loss: 0.0017567077884450555, Val Loss: 0.00933056976646185\n",
      "Epoch 285, Loss: 0.0017439468065276742, Val Loss: 0.009323128499090672\n",
      "Epoch 286, Loss: 0.0017313116695731878, Val Loss: 0.009315336123108864\n",
      "Epoch 287, Loss: 0.0017187888734042645, Val Loss: 0.009307908825576305\n",
      "Epoch 288, Loss: 0.0017066268483176827, Val Loss: 0.009301047772169113\n",
      "Epoch 289, Loss: 0.001694642473012209, Val Loss: 0.009294040501117706\n",
      "Epoch 290, Loss: 0.0016826163046061993, Val Loss: 0.009287391789257526\n",
      "Epoch 291, Loss: 0.001670769415795803, Val Loss: 0.009281104430556297\n",
      "Epoch 292, Loss: 0.0016593312611803412, Val Loss: 0.009274182841181755\n",
      "Epoch 293, Loss: 0.0016479382757097483, Val Loss: 0.009267495945096016\n",
      "Epoch 294, Loss: 0.0016366109484806657, Val Loss: 0.00926082395017147\n",
      "Epoch 295, Loss: 0.0016254544025287032, Val Loss: 0.009254110977053642\n",
      "Epoch 296, Loss: 0.0016145588597282767, Val Loss: 0.009248223155736923\n",
      "Epoch 297, Loss: 0.0016035845037549734, Val Loss: 0.009241241030395031\n",
      "Epoch 298, Loss: 0.0015928129432722926, Val Loss: 0.009235097095370293\n",
      "Epoch 299, Loss: 0.0015822822460904717, Val Loss: 0.009229008108377457\n",
      "Epoch 300, Loss: 0.001571687520481646, Val Loss: 0.009222457185387611\n",
      "Epoch 301, Loss: 0.0015613475115969777, Val Loss: 0.009216800332069397\n",
      "Epoch 302, Loss: 0.0015511586098000407, Val Loss: 0.00921067874878645\n",
      "Epoch 303, Loss: 0.0015409960178658366, Val Loss: 0.009205115027725697\n",
      "Epoch 304, Loss: 0.0015310392482206225, Val Loss: 0.009199075400829315\n",
      "Epoch 305, Loss: 0.0015211879508569837, Val Loss: 0.009194103069603443\n",
      "Epoch 306, Loss: 0.0015113058034330606, Val Loss: 0.00918808113783598\n",
      "Epoch 307, Loss: 0.00150157674215734, Val Loss: 0.00918275024741888\n",
      "Epoch 308, Loss: 0.001492045121267438, Val Loss: 0.00917787291109562\n",
      "Epoch 309, Loss: 0.0014826268889009953, Val Loss: 0.009173164144158363\n",
      "Epoch 310, Loss: 0.0014732687268406153, Val Loss: 0.009168552234768867\n",
      "Epoch 311, Loss: 0.001464041997678578, Val Loss: 0.00916286651045084\n",
      "Epoch 312, Loss: 0.0014547825558111072, Val Loss: 0.009158208966255188\n",
      "Epoch 313, Loss: 0.0014456225326284766, Val Loss: 0.00915340706706047\n",
      "Epoch 314, Loss: 0.0014366669347509742, Val Loss: 0.009148810990154743\n",
      "Epoch 315, Loss: 0.0014276992296800017, Val Loss: 0.009144176729023457\n",
      "Epoch 316, Loss: 0.0014189269859343767, Val Loss: 0.009139647707343102\n",
      "Epoch 317, Loss: 0.0014101280830800533, Val Loss: 0.009134671650826931\n",
      "Epoch 318, Loss: 0.0014015028718858957, Val Loss: 0.009130329824984074\n",
      "Epoch 319, Loss: 0.00139288988430053, Val Loss: 0.009125481359660625\n",
      "Epoch 320, Loss: 0.0013843787601217628, Val Loss: 0.009121411480009556\n",
      "Epoch 321, Loss: 0.0013760320143774152, Val Loss: 0.00911776628345251\n",
      "Epoch 322, Loss: 0.0013676165835931897, Val Loss: 0.009114636108279228\n",
      "Epoch 323, Loss: 0.0013594619231298566, Val Loss: 0.009110359475016594\n",
      "Epoch 324, Loss: 0.001351398997940123, Val Loss: 0.009106246754527092\n",
      "Epoch 325, Loss: 0.001343339099548757, Val Loss: 0.009102237410843372\n",
      "Epoch 326, Loss: 0.001335248234681785, Val Loss: 0.009098413400352001\n",
      "Epoch 327, Loss: 0.0013272695941850543, Val Loss: 0.009095188230276108\n",
      "Epoch 328, Loss: 0.0013193365884944797, Val Loss: 0.009091469459235668\n",
      "Epoch 329, Loss: 0.0013115346664562821, Val Loss: 0.00908727291971445\n",
      "Epoch 330, Loss: 0.0013037429889664054, Val Loss: 0.009083859622478485\n",
      "Epoch 331, Loss: 0.0012959736632183194, Val Loss: 0.009081091731786728\n",
      "Epoch 332, Loss: 0.001288458937779069, Val Loss: 0.009077014401555061\n",
      "Epoch 333, Loss: 0.0012808165047317743, Val Loss: 0.009074710309505463\n",
      "Epoch 334, Loss: 0.0012732386821880937, Val Loss: 0.009071525186300278\n",
      "Epoch 335, Loss: 0.001265844563022256, Val Loss: 0.009068435057997704\n",
      "Epoch 336, Loss: 0.001258445787243545, Val Loss: 0.009066445752978325\n",
      "Epoch 337, Loss: 0.0012511721579357982, Val Loss: 0.009063297882676125\n",
      "Epoch 338, Loss: 0.00124383217189461, Val Loss: 0.00905988272279501\n",
      "Epoch 339, Loss: 0.0012366833398118615, Val Loss: 0.009057597257196903\n",
      "Epoch 340, Loss: 0.0012295650085434318, Val Loss: 0.009054431691765785\n",
      "Epoch 341, Loss: 0.001222403021529317, Val Loss: 0.009051686152815819\n",
      "Epoch 342, Loss: 0.0012154285795986652, Val Loss: 0.009048402309417725\n",
      "Epoch 343, Loss: 0.0012084253830835223, Val Loss: 0.009046000428497791\n",
      "Epoch 344, Loss: 0.0012015114771202207, Val Loss: 0.009043398313224316\n",
      "Epoch 345, Loss: 0.001194561249576509, Val Loss: 0.009040390141308308\n",
      "Epoch 346, Loss: 0.0011877432698383927, Val Loss: 0.009037705138325691\n",
      "Epoch 347, Loss: 0.001180947874672711, Val Loss: 0.009035222232341766\n",
      "Epoch 348, Loss: 0.0011742858914658427, Val Loss: 0.009032933972775936\n",
      "Epoch 349, Loss: 0.0011677485890686512, Val Loss: 0.009030012413859367\n",
      "Epoch 350, Loss: 0.001161073800176382, Val Loss: 0.009027630090713501\n",
      "Epoch 351, Loss: 0.0011546174064278603, Val Loss: 0.009025382809340954\n",
      "Epoch 352, Loss: 0.0011482036206871271, Val Loss: 0.009022785350680351\n",
      "Epoch 353, Loss: 0.0011419065995141864, Val Loss: 0.00902134831994772\n",
      "Epoch 354, Loss: 0.0011356205213814974, Val Loss: 0.00901931431144476\n",
      "Epoch 355, Loss: 0.0011294224532321095, Val Loss: 0.009016982279717922\n",
      "Epoch 356, Loss: 0.0011232871329411864, Val Loss: 0.009015072137117386\n",
      "Epoch 357, Loss: 0.0011172039667144418, Val Loss: 0.009013211354613304\n",
      "Epoch 358, Loss: 0.0011112381471320987, Val Loss: 0.009011199697852135\n",
      "Epoch 359, Loss: 0.0011052553309127688, Val Loss: 0.009008888155221939\n",
      "Epoch 360, Loss: 0.0010993428295478225, Val Loss: 0.00900749396532774\n",
      "Epoch 361, Loss: 0.0010935203172266483, Val Loss: 0.009005837142467499\n",
      "Epoch 362, Loss: 0.0010877668391913176, Val Loss: 0.0090037165209651\n",
      "Epoch 363, Loss: 0.001081977505236864, Val Loss: 0.009001919999718666\n",
      "Epoch 364, Loss: 0.001076266635209322, Val Loss: 0.009000205434858799\n",
      "Epoch 365, Loss: 0.0010705717140808702, Val Loss: 0.008998380973935127\n",
      "Epoch 366, Loss: 0.001065005548298359, Val Loss: 0.008997244760394096\n",
      "Epoch 367, Loss: 0.001059432397596538, Val Loss: 0.008995471522212029\n",
      "Epoch 368, Loss: 0.001053885673172772, Val Loss: 0.008994669653475285\n",
      "Epoch 369, Loss: 0.0010484002996236086, Val Loss: 0.008993195369839668\n",
      "Epoch 370, Loss: 0.0010428698733448982, Val Loss: 0.00899189431220293\n",
      "Epoch 371, Loss: 0.0010374876437708735, Val Loss: 0.008990255184471607\n",
      "Epoch 372, Loss: 0.00103198760189116, Val Loss: 0.008989417925477028\n",
      "Epoch 373, Loss: 0.0010266037425026298, Val Loss: 0.008988258428871632\n",
      "Epoch 374, Loss: 0.0010213416535407305, Val Loss: 0.00898705143481493\n",
      "Epoch 375, Loss: 0.00101603870280087, Val Loss: 0.008985971100628376\n",
      "Epoch 376, Loss: 0.0010107946582138538, Val Loss: 0.008984828367829323\n",
      "Epoch 377, Loss: 0.001005592872388661, Val Loss: 0.008983857929706573\n",
      "Epoch 378, Loss: 0.0010004204232245684, Val Loss: 0.008981984108686447\n",
      "Epoch 379, Loss: 0.0009953136323019862, Val Loss: 0.008981211110949516\n",
      "Epoch 380, Loss: 0.000990209518931806, Val Loss: 0.00898029189556837\n",
      "Epoch 381, Loss: 0.0009851710638031363, Val Loss: 0.008979563601315022\n",
      "Epoch 382, Loss: 0.0009800450643524528, Val Loss: 0.0089782839640975\n",
      "Epoch 383, Loss: 0.0009751446777954698, Val Loss: 0.008976907469332218\n",
      "Epoch 384, Loss: 0.0009701197850517929, Val Loss: 0.008975708857178688\n",
      "Epoch 385, Loss: 0.0009651717846281826, Val Loss: 0.008974622935056686\n",
      "Epoch 386, Loss: 0.0009602968348190188, Val Loss: 0.008973248302936554\n",
      "Epoch 387, Loss: 0.0009554295102134347, Val Loss: 0.00897302757948637\n",
      "Epoch 388, Loss: 0.0009507149225100875, Val Loss: 0.008971884846687317\n",
      "Epoch 389, Loss: 0.0009460400906391442, Val Loss: 0.008971372619271278\n",
      "Epoch 390, Loss: 0.000941327481996268, Val Loss: 0.008970612660050392\n",
      "Epoch 391, Loss: 0.0009366931626573205, Val Loss: 0.00896925013512373\n",
      "Epoch 392, Loss: 0.0009321514517068863, Val Loss: 0.008968569338321686\n",
      "Epoch 393, Loss: 0.0009275614866055548, Val Loss: 0.00896783173084259\n",
      "Epoch 394, Loss: 0.0009230798459611833, Val Loss: 0.008966824971139431\n",
      "Epoch 395, Loss: 0.000918615551199764, Val Loss: 0.008966061286628246\n",
      "Epoch 396, Loss: 0.00091420894023031, Val Loss: 0.008965803310275078\n",
      "Epoch 397, Loss: 0.0009098359732888639, Val Loss: 0.008965336717665195\n",
      "Epoch 398, Loss: 0.0009054675465449691, Val Loss: 0.008963791653513908\n",
      "Epoch 399, Loss: 0.0009011354995891452, Val Loss: 0.008963567204773426\n",
      "Epoch 400, Loss: 0.0008968790643848479, Val Loss: 0.008962469175457954\n",
      "Epoch 401, Loss: 0.0008926598820835352, Val Loss: 0.008961874060332775\n",
      "Epoch 402, Loss: 0.0008885072311386466, Val Loss: 0.008961533196270466\n",
      "Epoch 403, Loss: 0.0008843756513670087, Val Loss: 0.008961275219917297\n",
      "Epoch 404, Loss: 0.0008802704978734255, Val Loss: 0.00896047055721283\n",
      "Epoch 405, Loss: 0.0008762493380345404, Val Loss: 0.008959859609603882\n",
      "Epoch 406, Loss: 0.0008721747435629368, Val Loss: 0.008960389532148838\n",
      "Epoch 407, Loss: 0.0008681631879881024, Val Loss: 0.008960048668086529\n",
      "Epoch 408, Loss: 0.0008641587337478995, Val Loss: 0.008959535509347916\n",
      "Epoch 409, Loss: 0.0008601762820035219, Val Loss: 0.008958513848483562\n",
      "Epoch 410, Loss: 0.0008562321309000254, Val Loss: 0.008958893828094006\n",
      "Epoch 411, Loss: 0.0008523750002495944, Val Loss: 0.00895782746374607\n",
      "Epoch 412, Loss: 0.0008485137368552387, Val Loss: 0.008957396261394024\n",
      "Epoch 413, Loss: 0.0008447704021818936, Val Loss: 0.0089567881077528\n",
      "Epoch 414, Loss: 0.000840984343085438, Val Loss: 0.008957087993621826\n",
      "Epoch 415, Loss: 0.0008372328011319041, Val Loss: 0.0089570926502347\n",
      "Epoch 416, Loss: 0.0008334777667187154, Val Loss: 0.008956817910075188\n",
      "Epoch 417, Loss: 0.0008297908352687955, Val Loss: 0.008956530131399632\n",
      "Epoch 418, Loss: 0.0008261011098511517, Val Loss: 0.008956018835306168\n",
      "Epoch 419, Loss: 0.0008224277989938855, Val Loss: 0.008955879136919975\n",
      "Epoch 420, Loss: 0.0008187451167032123, Val Loss: 0.00895529892295599\n",
      "Epoch 421, Loss: 0.0008151355432346463, Val Loss: 0.008954791352152824\n",
      "Epoch 422, Loss: 0.0008115960517898202, Val Loss: 0.008954884484410286\n",
      "Epoch 423, Loss: 0.0008080574334599078, Val Loss: 0.008954087272286415\n",
      "Epoch 424, Loss: 0.0008045205031521618, Val Loss: 0.008953854441642761\n",
      "Epoch 425, Loss: 0.0008010646561160684, Val Loss: 0.008953935466706753\n",
      "Epoch 426, Loss: 0.0007976082852110267, Val Loss: 0.008953573182225227\n",
      "Epoch 427, Loss: 0.0007941597141325474, Val Loss: 0.00895416084676981\n",
      "Epoch 428, Loss: 0.0007907720282673836, Val Loss: 0.008954044431447983\n",
      "Epoch 429, Loss: 0.0007873369613662362, Val Loss: 0.008953150361776352\n",
      "Epoch 430, Loss: 0.0007839935133233666, Val Loss: 0.00895343255251646\n",
      "Epoch 431, Loss: 0.0007806431967765093, Val Loss: 0.008953828364610672\n",
      "Epoch 432, Loss: 0.0007773599354550242, Val Loss: 0.008953978307545185\n",
      "Epoch 433, Loss: 0.0007740545552223921, Val Loss: 0.00895400159060955\n",
      "Epoch 434, Loss: 0.0007707712356932461, Val Loss: 0.008954311721026897\n",
      "Epoch 435, Loss: 0.000767549907322973, Val Loss: 0.008953863754868507\n",
      "Epoch 436, Loss: 0.0007642831187695265, Val Loss: 0.008953165262937546\n",
      "Epoch 437, Loss: 0.0007610947941429913, Val Loss: 0.0089533356949687\n",
      "Epoch 438, Loss: 0.0007578566437587142, Val Loss: 0.008953447453677654\n",
      "Epoch 439, Loss: 0.0007546365959569812, Val Loss: 0.008953594602644444\n",
      "Epoch 440, Loss: 0.0007514923345297575, Val Loss: 0.008953710086643696\n",
      "Epoch 441, Loss: 0.0007483866647817194, Val Loss: 0.00895328912883997\n",
      "Epoch 442, Loss: 0.0007452727877534926, Val Loss: 0.00895334780216217\n",
      "Epoch 443, Loss: 0.0007421672344207764, Val Loss: 0.008953273296356201\n",
      "Epoch 444, Loss: 0.0007391342660412192, Val Loss: 0.008952995762228966\n",
      "Epoch 445, Loss: 0.0007360331946983933, Val Loss: 0.008953185752034187\n",
      "Epoch 446, Loss: 0.0007329960353672504, Val Loss: 0.00895274430513382\n",
      "Epoch 447, Loss: 0.0007300448487512767, Val Loss: 0.008952781558036804\n",
      "Epoch 448, Loss: 0.0007270756177604198, Val Loss: 0.00895296037197113\n",
      "Epoch 449, Loss: 0.0007240617997013032, Val Loss: 0.008952762000262737\n",
      "Epoch 450, Loss: 0.0007211567717604339, Val Loss: 0.008952314965426922\n",
      "Epoch 451, Loss: 0.0007182697299867868, Val Loss: 0.008952206932008266\n",
      "Epoch 452, Loss: 0.0007153379847295582, Val Loss: 0.00895202811807394\n",
      "Epoch 453, Loss: 0.0007124370895326138, Val Loss: 0.008952382020652294\n",
      "Epoch 454, Loss: 0.0007095722830854356, Val Loss: 0.008952203206717968\n",
      "Epoch 455, Loss: 0.0007066618418321013, Val Loss: 0.008952568285167217\n",
      "Epoch 456, Loss: 0.0007038239273242652, Val Loss: 0.008952179923653603\n",
      "Epoch 457, Loss: 0.0007009623222984374, Val Loss: 0.008951978757977486\n",
      "Epoch 458, Loss: 0.0006981045589782298, Val Loss: 0.008951894007623196\n",
      "Epoch 459, Loss: 0.0006953072734177113, Val Loss: 0.0089527303352952\n",
      "Epoch 460, Loss: 0.0006925095221959054, Val Loss: 0.008952245116233826\n",
      "Epoch 461, Loss: 0.0006897449493408203, Val Loss: 0.008952091448009014\n",
      "Epoch 462, Loss: 0.0006869772914797068, Val Loss: 0.008951826952397823\n",
      "Epoch 463, Loss: 0.0006841851864010096, Val Loss: 0.008952039293944836\n",
      "Epoch 464, Loss: 0.0006814122898504138, Val Loss: 0.008952241390943527\n",
      "Epoch 465, Loss: 0.0006786037120036781, Val Loss: 0.0089517030864954\n",
      "Epoch 466, Loss: 0.0006757897208444774, Val Loss: 0.008951472118496895\n",
      "Epoch 467, Loss: 0.0006729026208631694, Val Loss: 0.008951119147241116\n",
      "Epoch 468, Loss: 0.0006699904915876687, Val Loss: 0.008950711227953434\n",
      "Epoch 469, Loss: 0.0006669543799944222, Val Loss: 0.008949564769864082\n",
      "Epoch 470, Loss: 0.0006638753111474216, Val Loss: 0.008948669768869877\n"
     ]
    }
   ],
   "source": [
    "from flashrag.prompt import PromptTemplate\n",
    "import pickle\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "config_taqa = set_config(dataset_name='taqa', split = split)\n",
    "template_taqa= PromptTemplate(\n",
    "        config = config_taqa,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "config_know= set_config(dataset_name='triviaqa', split = split)\n",
    "template_know= PromptTemplate(\n",
    "        config = config_know,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "\n",
    "\n",
    "loading_hidden = True\n",
    "if not loading_hidden:\n",
    "    taqa_hidden_states, triviaqa_hidden_states = load_datasets(config_taqa, config_triviaqa, template_taqa, template_triviaqa)\n",
    "else:\n",
    "    file_path_know = '/cs/student/projects2/dsml/cdiezmar/hidden_states/knowledge-aware.pkl'\n",
    "    file_path_taqa = '/cs/student/projects2/dsml/cdiezmar/hidden_states/taqa.pkl'\n",
    "\n",
    "    with open(file_path_know, 'rb') as file_know:\n",
    "        know_hidden_states = pickle.load(file_know)\n",
    "        print('Length triviaqa: ', len(know_hidden_states))\n",
    "        know_hidden_states_tensor= torch.tensor(know_hidden_states, dtype=torch.float32)\n",
    "    with open(file_path_taqa, 'rb') as file_taqa:\n",
    "        taqa_hidden_states = pickle.load(file_taqa)\n",
    "        print('Length taqa: ', len(taqa_hidden_states))\n",
    "        taqa_hidden_states_tensor= torch.tensor(taqa_hidden_states, dtype=torch.float32)\n",
    "train_inputs, train_labels, val_inputs, val_labels = prepare_data(taqa_hidden_states_tensor, know_hidden_states_tensor)\n",
    "\n",
    "input_size = train_inputs.size(1)\n",
    "num_classes = 2  # Knowledge-sensitive or not\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "epochs = 470\n",
    "lr = 1e-4\n",
    "\n",
    "# Train RNN Classifier\n",
    "rnn_model = train_rnn_classifier_cuda(train_inputs, train_labels, val_inputs, val_labels, input_size, hidden_size, num_layers, num_classes, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `question` in template\n",
      "Find `question` in template\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length triviaqa:  20000\n",
      "Length taqa:  10148\n",
      "Epoch 1, Loss: 0.7395070195198059, Val Loss: 0.6702927350997925\n",
      "Epoch 2, Loss: 0.6719962358474731, Val Loss: 0.6105952858924866\n",
      "Epoch 3, Loss: 0.6124705672264099, Val Loss: 0.558438777923584\n",
      "Epoch 4, Loss: 0.5605215430259705, Val Loss: 0.5134671926498413\n",
      "Epoch 5, Loss: 0.515680730342865, Val Loss: 0.4747273921966553\n",
      "Epoch 6, Loss: 0.4770130515098572, Val Loss: 0.4411036968231201\n",
      "Epoch 7, Loss: 0.44339942932128906, Val Loss: 0.41137051582336426\n",
      "Epoch 8, Loss: 0.4136374294757843, Val Loss: 0.384230375289917\n",
      "Epoch 9, Loss: 0.3864520490169525, Val Loss: 0.3587872385978699\n",
      "Epoch 10, Loss: 0.3609929084777832, Val Loss: 0.3347596824169159\n",
      "Epoch 11, Loss: 0.3369525969028473, Val Loss: 0.3120802342891693\n",
      "Epoch 12, Loss: 0.3142564296722412, Val Loss: 0.2909301221370697\n",
      "Epoch 13, Loss: 0.2931034564971924, Val Loss: 0.2714200019836426\n",
      "Epoch 14, Loss: 0.27362769842147827, Val Loss: 0.2535693347454071\n",
      "Epoch 15, Loss: 0.25584033131599426, Val Loss: 0.2373189777135849\n",
      "Epoch 16, Loss: 0.23969228565692902, Val Loss: 0.22239328920841217\n",
      "Epoch 17, Loss: 0.22489500045776367, Val Loss: 0.20859207212924957\n",
      "Epoch 18, Loss: 0.2112203687429428, Val Loss: 0.1958439201116562\n",
      "Epoch 19, Loss: 0.19857318699359894, Val Loss: 0.18416811525821686\n",
      "Epoch 20, Loss: 0.18696513772010803, Val Loss: 0.17351582646369934\n",
      "Epoch 21, Loss: 0.17634104192256927, Val Loss: 0.16373597085475922\n",
      "Epoch 22, Loss: 0.16654647886753082, Val Loss: 0.15462720394134521\n",
      "Epoch 23, Loss: 0.1573881059885025, Val Loss: 0.1460895538330078\n",
      "Epoch 24, Loss: 0.14878962934017181, Val Loss: 0.13809750974178314\n",
      "Epoch 25, Loss: 0.1407267302274704, Val Loss: 0.13062912225723267\n",
      "Epoch 26, Loss: 0.13316093385219574, Val Loss: 0.12362533062696457\n",
      "Epoch 27, Loss: 0.12604185938835144, Val Loss: 0.11705546826124191\n",
      "Epoch 28, Loss: 0.11933929473161697, Val Loss: 0.11092530190944672\n",
      "Epoch 29, Loss: 0.11304371803998947, Val Loss: 0.10520967096090317\n",
      "Epoch 30, Loss: 0.1071179211139679, Val Loss: 0.09987601637840271\n",
      "Epoch 31, Loss: 0.1015441045165062, Val Loss: 0.09487631916999817\n",
      "Epoch 32, Loss: 0.09629116952419281, Val Loss: 0.09012211114168167\n",
      "Epoch 33, Loss: 0.09127060323953629, Val Loss: 0.08555003255605698\n",
      "Epoch 34, Loss: 0.08641542494297028, Val Loss: 0.08113562315702438\n",
      "Epoch 35, Loss: 0.08170992881059647, Val Loss: 0.07686173170804977\n",
      "Epoch 36, Loss: 0.07714298367500305, Val Loss: 0.07272502034902573\n",
      "Epoch 37, Loss: 0.07272302359342575, Val Loss: 0.06874033808708191\n",
      "Epoch 38, Loss: 0.06846284866333008, Val Loss: 0.0649118572473526\n",
      "Epoch 39, Loss: 0.06437864899635315, Val Loss: 0.06125083938241005\n",
      "Epoch 40, Loss: 0.06048612669110298, Val Loss: 0.057767558842897415\n",
      "Epoch 41, Loss: 0.056804101914167404, Val Loss: 0.05447329580783844\n",
      "Epoch 42, Loss: 0.05335298180580139, Val Loss: 0.05138382315635681\n",
      "Epoch 43, Loss: 0.050145555287599564, Val Loss: 0.048498839139938354\n",
      "Epoch 44, Loss: 0.047190241515636444, Val Loss: 0.045820001512765884\n",
      "Epoch 45, Loss: 0.0444946326315403, Val Loss: 0.04335026070475578\n",
      "Epoch 46, Loss: 0.04204258322715759, Val Loss: 0.0411037802696228\n",
      "Epoch 47, Loss: 0.03981969133019447, Val Loss: 0.03906752169132233\n",
      "Epoch 48, Loss: 0.0377950556576252, Val Loss: 0.03722449764609337\n",
      "Epoch 49, Loss: 0.035954974591732025, Val Loss: 0.03554331883788109\n",
      "Epoch 50, Loss: 0.03427337482571602, Val Loss: 0.03400713577866554\n",
      "Epoch 51, Loss: 0.03272417187690735, Val Loss: 0.03260160610079765\n",
      "Epoch 52, Loss: 0.03128659725189209, Val Loss: 0.031297717243433\n",
      "Epoch 53, Loss: 0.02992790751159191, Val Loss: 0.030080020427703857\n",
      "Epoch 54, Loss: 0.028628403320908546, Val Loss: 0.028927218168973923\n",
      "Epoch 55, Loss: 0.02736326865851879, Val Loss: 0.027835680171847343\n",
      "Epoch 56, Loss: 0.026130782440304756, Val Loss: 0.02679152600467205\n",
      "Epoch 57, Loss: 0.024925699457526207, Val Loss: 0.025798631832003593\n",
      "Epoch 58, Loss: 0.023753231391310692, Val Loss: 0.02485262230038643\n",
      "Epoch 59, Loss: 0.022619718685746193, Val Loss: 0.023958832025527954\n",
      "Epoch 60, Loss: 0.02153007499873638, Val Loss: 0.02312307618558407\n",
      "Epoch 61, Loss: 0.020493827760219574, Val Loss: 0.022344376891851425\n",
      "Epoch 62, Loss: 0.01951596327126026, Val Loss: 0.021626917645335197\n",
      "Epoch 63, Loss: 0.018599752336740494, Val Loss: 0.020967183634638786\n",
      "Epoch 64, Loss: 0.01774444617331028, Val Loss: 0.020364664494991302\n",
      "Epoch 65, Loss: 0.016952767968177795, Val Loss: 0.01981661282479763\n",
      "Epoch 66, Loss: 0.016221821308135986, Val Loss: 0.01931663416326046\n",
      "Epoch 67, Loss: 0.015545116737484932, Val Loss: 0.018859008327126503\n",
      "Epoch 68, Loss: 0.01492068637162447, Val Loss: 0.01844133622944355\n",
      "Epoch 69, Loss: 0.014341405592858791, Val Loss: 0.01805306412279606\n",
      "Epoch 70, Loss: 0.013800295069813728, Val Loss: 0.01769290119409561\n",
      "Epoch 71, Loss: 0.013294038362801075, Val Loss: 0.017355192452669144\n",
      "Epoch 72, Loss: 0.012818406336009502, Val Loss: 0.017034435644745827\n",
      "Epoch 73, Loss: 0.012368288822472095, Val Loss: 0.01673070713877678\n",
      "Epoch 74, Loss: 0.011941934935748577, Val Loss: 0.016436882317066193\n",
      "Epoch 75, Loss: 0.011534049175679684, Val Loss: 0.01615477167069912\n",
      "Epoch 76, Loss: 0.01114535890519619, Val Loss: 0.015882637351751328\n",
      "Epoch 77, Loss: 0.010773226618766785, Val Loss: 0.015618296340107918\n",
      "Epoch 78, Loss: 0.010416916571557522, Val Loss: 0.015361634083092213\n",
      "Epoch 79, Loss: 0.010074528865516186, Val Loss: 0.015114456415176392\n",
      "Epoch 80, Loss: 0.009747013449668884, Val Loss: 0.014874625951051712\n",
      "Epoch 81, Loss: 0.009433582425117493, Val Loss: 0.014644855633378029\n",
      "Epoch 82, Loss: 0.00913445558398962, Val Loss: 0.014423169195652008\n",
      "Epoch 83, Loss: 0.008849668316543102, Val Loss: 0.01420964952558279\n",
      "Epoch 84, Loss: 0.008577587082982063, Val Loss: 0.014004853554069996\n",
      "Epoch 85, Loss: 0.008318018168210983, Val Loss: 0.013809692114591599\n",
      "Epoch 86, Loss: 0.008071154356002808, Val Loss: 0.01362326554954052\n",
      "Epoch 87, Loss: 0.007835736498236656, Val Loss: 0.013443288393318653\n",
      "Epoch 88, Loss: 0.0076109482906758785, Val Loss: 0.013272902928292751\n",
      "Epoch 89, Loss: 0.007396577391773462, Val Loss: 0.013110056519508362\n",
      "Epoch 90, Loss: 0.0071905567310750484, Val Loss: 0.012954957783222198\n",
      "Epoch 91, Loss: 0.006994106341153383, Val Loss: 0.012806545943021774\n",
      "Epoch 92, Loss: 0.006805813871324062, Val Loss: 0.012665603309869766\n",
      "Epoch 93, Loss: 0.00662578409537673, Val Loss: 0.012531772255897522\n",
      "Epoch 94, Loss: 0.006453456822782755, Val Loss: 0.012402953580021858\n",
      "Epoch 95, Loss: 0.006286823656409979, Val Loss: 0.012279381044209003\n",
      "Epoch 96, Loss: 0.006127105560153723, Val Loss: 0.01216126512736082\n",
      "Epoch 97, Loss: 0.005973050370812416, Val Loss: 0.012047626078128815\n",
      "Epoch 98, Loss: 0.0058248392306268215, Val Loss: 0.01193877775222063\n",
      "Epoch 99, Loss: 0.005681440234184265, Val Loss: 0.01183459535241127\n",
      "Epoch 100, Loss: 0.005542535334825516, Val Loss: 0.0117337079718709\n",
      "Epoch 101, Loss: 0.005408385302871466, Val Loss: 0.011637292802333832\n",
      "Epoch 102, Loss: 0.005279306322336197, Val Loss: 0.011545052751898766\n",
      "Epoch 103, Loss: 0.005154795944690704, Val Loss: 0.01145436055958271\n",
      "Epoch 104, Loss: 0.00503418268635869, Val Loss: 0.011367355473339558\n",
      "Epoch 105, Loss: 0.004917699843645096, Val Loss: 0.01128276064991951\n",
      "Epoch 106, Loss: 0.004805806092917919, Val Loss: 0.01120160985738039\n",
      "Epoch 107, Loss: 0.004697456955909729, Val Loss: 0.011122818104922771\n",
      "Epoch 108, Loss: 0.004592954181134701, Val Loss: 0.011046960018575191\n",
      "Epoch 109, Loss: 0.004492047242820263, Val Loss: 0.010974139906466007\n",
      "Epoch 110, Loss: 0.004394165240228176, Val Loss: 0.0109028285369277\n",
      "Epoch 111, Loss: 0.004299771506339312, Val Loss: 0.01083378866314888\n",
      "Epoch 112, Loss: 0.004207892809063196, Val Loss: 0.010767463594675064\n",
      "Epoch 113, Loss: 0.004119655583053827, Val Loss: 0.010702801868319511\n",
      "Epoch 114, Loss: 0.004033202771097422, Val Loss: 0.01064090896397829\n",
      "Epoch 115, Loss: 0.0039498102851212025, Val Loss: 0.010581069625914097\n",
      "Epoch 116, Loss: 0.0038690785877406597, Val Loss: 0.010522172786295414\n",
      "Epoch 117, Loss: 0.003790981601923704, Val Loss: 0.010464880615472794\n",
      "Epoch 118, Loss: 0.0037149284034967422, Val Loss: 0.010408129543066025\n",
      "Epoch 119, Loss: 0.003641451708972454, Val Loss: 0.010354414582252502\n",
      "Epoch 120, Loss: 0.003569673513993621, Val Loss: 0.010302086360752583\n",
      "Epoch 121, Loss: 0.003500016639009118, Val Loss: 0.010250426828861237\n",
      "Epoch 122, Loss: 0.003432490862905979, Val Loss: 0.010199673473834991\n",
      "Epoch 123, Loss: 0.0033670433331280947, Val Loss: 0.010149609297513962\n",
      "Epoch 124, Loss: 0.0033031876664608717, Val Loss: 0.010102136060595512\n",
      "Epoch 125, Loss: 0.003241233993321657, Val Loss: 0.010054394602775574\n",
      "Epoch 126, Loss: 0.003181047271937132, Val Loss: 0.010008450597524643\n",
      "Epoch 127, Loss: 0.0031221951358020306, Val Loss: 0.009962559677660465\n",
      "Epoch 128, Loss: 0.003065319499000907, Val Loss: 0.009918661788105965\n",
      "Epoch 129, Loss: 0.003009895794093609, Val Loss: 0.0098748579621315\n",
      "Epoch 130, Loss: 0.002955658594146371, Val Loss: 0.009831911884248257\n",
      "Epoch 131, Loss: 0.002903237473219633, Val Loss: 0.009790915064513683\n",
      "Epoch 132, Loss: 0.002852086443454027, Val Loss: 0.009751089848577976\n",
      "Epoch 133, Loss: 0.0028025279752910137, Val Loss: 0.009710967540740967\n",
      "Epoch 134, Loss: 0.002754359506070614, Val Loss: 0.009673032909631729\n",
      "Epoch 135, Loss: 0.002707370324060321, Val Loss: 0.00963554810732603\n",
      "Epoch 136, Loss: 0.002661348320543766, Val Loss: 0.009597946889698505\n",
      "Epoch 137, Loss: 0.002616784768179059, Val Loss: 0.00956252496689558\n",
      "Epoch 138, Loss: 0.0025735695380717516, Val Loss: 0.00952677708119154\n",
      "Epoch 139, Loss: 0.0025314711965620518, Val Loss: 0.009492948651313782\n",
      "Epoch 140, Loss: 0.002490457147359848, Val Loss: 0.00946000311523676\n",
      "Epoch 141, Loss: 0.0024503450840711594, Val Loss: 0.009428191930055618\n",
      "Epoch 142, Loss: 0.002411418594419956, Val Loss: 0.00939694419503212\n",
      "Epoch 143, Loss: 0.0023730250541120768, Val Loss: 0.009365814737975597\n",
      "Epoch 144, Loss: 0.002335559343919158, Val Loss: 0.009335293434560299\n",
      "Epoch 145, Loss: 0.002299201674759388, Val Loss: 0.009306637570261955\n",
      "Epoch 146, Loss: 0.0022638437803834677, Val Loss: 0.00927788857370615\n",
      "Epoch 147, Loss: 0.0022294153459370136, Val Loss: 0.009249246679246426\n",
      "Epoch 148, Loss: 0.002195813460275531, Val Loss: 0.009222595021128654\n",
      "Epoch 149, Loss: 0.002163046272471547, Val Loss: 0.009196295402944088\n",
      "Epoch 150, Loss: 0.00213083578273654, Val Loss: 0.009170129895210266\n",
      "Epoch 151, Loss: 0.002099593635648489, Val Loss: 0.009143907576799393\n",
      "Epoch 152, Loss: 0.0020688483491539955, Val Loss: 0.009119605645537376\n",
      "Epoch 153, Loss: 0.00203891028650105, Val Loss: 0.009094832465052605\n",
      "Epoch 154, Loss: 0.0020097948145121336, Val Loss: 0.009070931002497673\n",
      "Epoch 155, Loss: 0.0019810351077467203, Val Loss: 0.009046792052686214\n",
      "Epoch 156, Loss: 0.0019530035788193345, Val Loss: 0.009023268707096577\n",
      "Epoch 157, Loss: 0.0019255309598520398, Val Loss: 0.009000632911920547\n",
      "Epoch 158, Loss: 0.0018987040966749191, Val Loss: 0.008978651836514473\n",
      "Epoch 159, Loss: 0.0018725150730460882, Val Loss: 0.008958318270742893\n",
      "Epoch 160, Loss: 0.001846710336394608, Val Loss: 0.008937878534197807\n",
      "Epoch 161, Loss: 0.0018217520555481315, Val Loss: 0.008917578496038914\n",
      "Epoch 162, Loss: 0.0017969730542972684, Val Loss: 0.008897677063941956\n",
      "Epoch 163, Loss: 0.0017727705417200923, Val Loss: 0.008878856897354126\n",
      "Epoch 164, Loss: 0.0017489120364189148, Val Loss: 0.008858943358063698\n",
      "Epoch 165, Loss: 0.0017255325801670551, Val Loss: 0.008840641938149929\n",
      "Epoch 166, Loss: 0.001702753477729857, Val Loss: 0.008821903727948666\n",
      "Epoch 167, Loss: 0.0016805218765512109, Val Loss: 0.008803586475551128\n",
      "Epoch 168, Loss: 0.0016585662961006165, Val Loss: 0.008785460144281387\n",
      "Epoch 169, Loss: 0.0016370578669011593, Val Loss: 0.008768446743488312\n",
      "Epoch 170, Loss: 0.0016160011291503906, Val Loss: 0.008751383982598782\n",
      "Epoch 171, Loss: 0.0015953525435179472, Val Loss: 0.008735273964703083\n",
      "Epoch 172, Loss: 0.001575140981003642, Val Loss: 0.008720510639250278\n",
      "Epoch 173, Loss: 0.0015552480472251773, Val Loss: 0.00870542973279953\n",
      "Epoch 174, Loss: 0.0015357694355770946, Val Loss: 0.008689620532095432\n",
      "Epoch 175, Loss: 0.0015166322700679302, Val Loss: 0.008674678392708302\n",
      "Epoch 176, Loss: 0.0014978774124756455, Val Loss: 0.008659963496029377\n",
      "Epoch 177, Loss: 0.0014794605085626245, Val Loss: 0.008645187132060528\n",
      "Epoch 178, Loss: 0.001461404375731945, Val Loss: 0.008630755357444286\n",
      "Epoch 179, Loss: 0.0014436279889196157, Val Loss: 0.008615946397185326\n",
      "Epoch 180, Loss: 0.0014261390315368772, Val Loss: 0.008602875284850597\n",
      "Epoch 181, Loss: 0.001409081625752151, Val Loss: 0.008589684031903744\n",
      "Epoch 182, Loss: 0.001392287202179432, Val Loss: 0.0085773766040802\n",
      "Epoch 183, Loss: 0.0013758100103586912, Val Loss: 0.008564970456063747\n",
      "Epoch 184, Loss: 0.0013597053475677967, Val Loss: 0.008552240207791328\n",
      "Epoch 185, Loss: 0.001343754236586392, Val Loss: 0.008539966307580471\n",
      "Epoch 186, Loss: 0.0013281413121148944, Val Loss: 0.008528520353138447\n",
      "Epoch 187, Loss: 0.0013128707651048899, Val Loss: 0.008516862988471985\n",
      "Epoch 188, Loss: 0.0012977832229807973, Val Loss: 0.00850518699735403\n",
      "Epoch 189, Loss: 0.0012830328196287155, Val Loss: 0.00849444791674614\n",
      "Epoch 190, Loss: 0.0012684990651905537, Val Loss: 0.008481616154313087\n",
      "Epoch 191, Loss: 0.0012543544871732593, Val Loss: 0.008471856825053692\n",
      "Epoch 192, Loss: 0.0012404019944369793, Val Loss: 0.008461248129606247\n",
      "Epoch 193, Loss: 0.0012267077108845115, Val Loss: 0.008450399152934551\n",
      "Epoch 194, Loss: 0.0012133102864027023, Val Loss: 0.008439580909907818\n",
      "Epoch 195, Loss: 0.001200112048536539, Val Loss: 0.008429024368524551\n",
      "Epoch 196, Loss: 0.0011870129965245724, Val Loss: 0.008418982848525047\n",
      "Epoch 197, Loss: 0.0011743535287678242, Val Loss: 0.008408747613430023\n",
      "Epoch 198, Loss: 0.0011616270057857037, Val Loss: 0.008399228565394878\n",
      "Epoch 199, Loss: 0.0011492209741845727, Val Loss: 0.00838973093777895\n",
      "Epoch 200, Loss: 0.0011370222782716155, Val Loss: 0.008379998616874218\n",
      "Epoch 201, Loss: 0.001125006121583283, Val Loss: 0.008371247909963131\n",
      "Epoch 202, Loss: 0.0011131647042930126, Val Loss: 0.008362716995179653\n",
      "Epoch 203, Loss: 0.0011015173513442278, Val Loss: 0.008353508077561855\n",
      "Epoch 204, Loss: 0.001090044854208827, Val Loss: 0.008343922905623913\n",
      "Epoch 205, Loss: 0.001078604836948216, Val Loss: 0.008335031569004059\n",
      "Epoch 206, Loss: 0.0010674147633835673, Val Loss: 0.00832625012844801\n",
      "Epoch 207, Loss: 0.0010563930263742805, Val Loss: 0.008317770436406136\n",
      "Epoch 208, Loss: 0.0010456342715770006, Val Loss: 0.008310052566230297\n",
      "Epoch 209, Loss: 0.0010349428048357368, Val Loss: 0.008301932364702225\n",
      "Epoch 210, Loss: 0.0010243519209325314, Val Loss: 0.00829373486340046\n",
      "Epoch 211, Loss: 0.0010139512596651912, Val Loss: 0.00828696507960558\n",
      "Epoch 212, Loss: 0.0010037318570539355, Val Loss: 0.008279114961624146\n",
      "Epoch 213, Loss: 0.0009936320129781961, Val Loss: 0.008272304199635983\n",
      "Epoch 214, Loss: 0.0009838396217674017, Val Loss: 0.008264648728072643\n",
      "Epoch 215, Loss: 0.0009741550893522799, Val Loss: 0.008256275206804276\n",
      "Epoch 216, Loss: 0.0009646297548897564, Val Loss: 0.008248466067016125\n",
      "Epoch 217, Loss: 0.0009551807888783514, Val Loss: 0.008241013623774052\n",
      "Epoch 218, Loss: 0.0009459831635467708, Val Loss: 0.008233973756432533\n",
      "Epoch 219, Loss: 0.0009369280305691063, Val Loss: 0.008226972073316574\n",
      "Epoch 220, Loss: 0.0009279301739297807, Val Loss: 0.008220315910875797\n",
      "Epoch 221, Loss: 0.0009191475110128522, Val Loss: 0.008214244619011879\n",
      "Epoch 222, Loss: 0.000910480972379446, Val Loss: 0.008207627572119236\n",
      "Epoch 223, Loss: 0.0009018229320645332, Val Loss: 0.008201069198548794\n",
      "Epoch 224, Loss: 0.0008933595381677151, Val Loss: 0.008195332251489162\n",
      "Epoch 225, Loss: 0.0008850344456732273, Val Loss: 0.008188473992049694\n",
      "Epoch 226, Loss: 0.0008768463158048689, Val Loss: 0.008181746117770672\n",
      "Epoch 227, Loss: 0.0008687737281434238, Val Loss: 0.008175074122846127\n",
      "Epoch 228, Loss: 0.0008607751806266606, Val Loss: 0.00816919282078743\n",
      "Epoch 229, Loss: 0.000852887169457972, Val Loss: 0.008162369951605797\n",
      "Epoch 230, Loss: 0.0008452108013443649, Val Loss: 0.008156323805451393\n",
      "Epoch 231, Loss: 0.0008375688339583576, Val Loss: 0.0081507358700037\n",
      "Epoch 232, Loss: 0.0008299968321807683, Val Loss: 0.008145584724843502\n",
      "Epoch 233, Loss: 0.0008225097553804517, Val Loss: 0.008140027523040771\n",
      "Epoch 234, Loss: 0.0008151602232828736, Val Loss: 0.00813437718898058\n",
      "Epoch 235, Loss: 0.0008079271065071225, Val Loss: 0.00812846701592207\n",
      "Epoch 236, Loss: 0.0008008140721358359, Val Loss: 0.008123432286083698\n",
      "Epoch 237, Loss: 0.0007937112241052091, Val Loss: 0.008118994534015656\n",
      "Epoch 238, Loss: 0.0007867466774769127, Val Loss: 0.008114132098853588\n",
      "Epoch 239, Loss: 0.0007799109444022179, Val Loss: 0.008108227513730526\n",
      "Epoch 240, Loss: 0.0007731406367383897, Val Loss: 0.008104131557047367\n",
      "Epoch 241, Loss: 0.0007664714939892292, Val Loss: 0.00810015108436346\n",
      "Epoch 242, Loss: 0.0007598612573929131, Val Loss: 0.008094621822237968\n",
      "Epoch 243, Loss: 0.0007533410098403692, Val Loss: 0.008090265095233917\n",
      "Epoch 244, Loss: 0.0007468611001968384, Val Loss: 0.008085329085588455\n",
      "Epoch 245, Loss: 0.000740512041375041, Val Loss: 0.008080649189651012\n",
      "Epoch 246, Loss: 0.0007342345779761672, Val Loss: 0.008077281527221203\n",
      "Epoch 247, Loss: 0.0007280443096533418, Val Loss: 0.008073139935731888\n",
      "Epoch 248, Loss: 0.0007219493272714317, Val Loss: 0.008068880066275597\n",
      "Epoch 249, Loss: 0.0007159269880503416, Val Loss: 0.008064046502113342\n",
      "Epoch 250, Loss: 0.0007099455688148737, Val Loss: 0.008059978485107422\n",
      "Epoch 251, Loss: 0.0007040277123451233, Val Loss: 0.008054501377046108\n",
      "Epoch 252, Loss: 0.0006981962942518294, Val Loss: 0.008050710894167423\n",
      "Epoch 253, Loss: 0.0006924509070813656, Val Loss: 0.008046536706387997\n",
      "Epoch 254, Loss: 0.0006867740885354578, Val Loss: 0.008042546920478344\n",
      "Epoch 255, Loss: 0.0006811859202571213, Val Loss: 0.008038435131311417\n",
      "Epoch 256, Loss: 0.0006756362854503095, Val Loss: 0.008035266771912575\n",
      "Epoch 257, Loss: 0.0006701835081912577, Val Loss: 0.00803077220916748\n",
      "Epoch 258, Loss: 0.0006647881236858666, Val Loss: 0.008027086034417152\n",
      "Epoch 259, Loss: 0.0006594263250008225, Val Loss: 0.008022632449865341\n",
      "Epoch 260, Loss: 0.0006541504408232868, Val Loss: 0.008018851280212402\n",
      "Epoch 261, Loss: 0.0006489591905847192, Val Loss: 0.008015205152332783\n",
      "Epoch 262, Loss: 0.0006438047275878489, Val Loss: 0.008011032827198505\n",
      "Epoch 263, Loss: 0.0006387695902958512, Val Loss: 0.008007407188415527\n",
      "Epoch 264, Loss: 0.0006337719387374818, Val Loss: 0.00800391472876072\n",
      "Epoch 265, Loss: 0.0006288168951869011, Val Loss: 0.00800023227930069\n",
      "Epoch 266, Loss: 0.0006239780341275036, Val Loss: 0.007995408028364182\n",
      "Epoch 267, Loss: 0.0006191779393702745, Val Loss: 0.007992162369191647\n",
      "Epoch 268, Loss: 0.0006144173676148057, Val Loss: 0.007988391444087029\n",
      "Epoch 269, Loss: 0.0006097821751609445, Val Loss: 0.007984139956533909\n",
      "Epoch 270, Loss: 0.0006051927339285612, Val Loss: 0.007980935275554657\n",
      "Epoch 271, Loss: 0.0006006442126818001, Val Loss: 0.007977786473929882\n",
      "Epoch 272, Loss: 0.0005962518625892699, Val Loss: 0.007975198328495026\n",
      "Epoch 273, Loss: 0.0005918527604080737, Val Loss: 0.00797181110829115\n",
      "Epoch 274, Loss: 0.0005874514463357627, Val Loss: 0.007969296537339687\n",
      "Epoch 275, Loss: 0.0005831490852870047, Val Loss: 0.007965806871652603\n",
      "Epoch 276, Loss: 0.0005789337446913123, Val Loss: 0.007963718846440315\n",
      "Epoch 277, Loss: 0.0005747289396822453, Val Loss: 0.007960451766848564\n",
      "Epoch 278, Loss: 0.0005706357769668102, Val Loss: 0.007958960719406605\n",
      "Epoch 279, Loss: 0.0005665418575517833, Val Loss: 0.007956416346132755\n",
      "Epoch 280, Loss: 0.0005624968907795846, Val Loss: 0.007953462190926075\n",
      "Epoch 281, Loss: 0.0005584759637713432, Val Loss: 0.007950786501169205\n",
      "Epoch 282, Loss: 0.000554514757823199, Val Loss: 0.007947633042931557\n",
      "Epoch 283, Loss: 0.0005505679873749614, Val Loss: 0.00794525258243084\n",
      "Epoch 284, Loss: 0.0005467113223858178, Val Loss: 0.007942345924675465\n",
      "Epoch 285, Loss: 0.0005429031443782151, Val Loss: 0.007939650677144527\n",
      "Epoch 286, Loss: 0.0005391245358623564, Val Loss: 0.007937697693705559\n",
      "Epoch 287, Loss: 0.0005353517481125891, Val Loss: 0.007935131900012493\n",
      "Epoch 288, Loss: 0.0005316766910254955, Val Loss: 0.007932792417705059\n",
      "Epoch 289, Loss: 0.0005280541372485459, Val Loss: 0.007930685766041279\n",
      "Epoch 290, Loss: 0.0005244274507276714, Val Loss: 0.007928946055471897\n",
      "Epoch 291, Loss: 0.0005208990187384188, Val Loss: 0.0079268803820014\n",
      "Epoch 292, Loss: 0.0005173403769731522, Val Loss: 0.007925426587462425\n",
      "Epoch 293, Loss: 0.0005138443666510284, Val Loss: 0.007923664525151253\n",
      "Epoch 294, Loss: 0.0005103941075503826, Val Loss: 0.00792205985635519\n",
      "Epoch 295, Loss: 0.0005069414037279785, Val Loss: 0.007920291274785995\n",
      "Epoch 296, Loss: 0.0005035769427195191, Val Loss: 0.007918880321085453\n",
      "Epoch 297, Loss: 0.0005002331454306841, Val Loss: 0.007916909642517567\n",
      "Epoch 298, Loss: 0.0004968995926901698, Val Loss: 0.007915075868368149\n",
      "Epoch 299, Loss: 0.0004936210461892188, Val Loss: 0.00791332870721817\n",
      "Epoch 300, Loss: 0.0004904068191535771, Val Loss: 0.007911943830549717\n",
      "Epoch 301, Loss: 0.0004872314748354256, Val Loss: 0.007910490036010742\n",
      "Epoch 302, Loss: 0.00048411241732537746, Val Loss: 0.007908191531896591\n",
      "Epoch 303, Loss: 0.00048098305705934763, Val Loss: 0.007906710729002953\n",
      "Epoch 304, Loss: 0.00047789476229809225, Val Loss: 0.007905655540525913\n",
      "Epoch 305, Loss: 0.0004748574865516275, Val Loss: 0.00790470466017723\n",
      "Epoch 306, Loss: 0.00047184276627376676, Val Loss: 0.007902491837739944\n",
      "Epoch 307, Loss: 0.0004688663175329566, Val Loss: 0.007901977747678757\n",
      "Epoch 308, Loss: 0.0004659098922275007, Val Loss: 0.00790105015039444\n",
      "Epoch 309, Loss: 0.00046299301902763546, Val Loss: 0.007899814285337925\n",
      "Epoch 310, Loss: 0.0004601100808940828, Val Loss: 0.007898945361375809\n",
      "Epoch 311, Loss: 0.0004572570906020701, Val Loss: 0.007898136973381042\n",
      "Epoch 312, Loss: 0.0004544206312857568, Val Loss: 0.007897133938968182\n",
      "Epoch 313, Loss: 0.00045163987670093775, Val Loss: 0.007895681075751781\n",
      "Epoch 314, Loss: 0.00044885711395181715, Val Loss: 0.007894427515566349\n",
      "Epoch 315, Loss: 0.00044616442755796015, Val Loss: 0.007893077097833157\n",
      "Epoch 316, Loss: 0.0004434603324625641, Val Loss: 0.007892323657870293\n",
      "Epoch 317, Loss: 0.0004407503583934158, Val Loss: 0.007891211658716202\n",
      "Epoch 318, Loss: 0.0004380721948109567, Val Loss: 0.00788997020572424\n",
      "Epoch 319, Loss: 0.0004354621050879359, Val Loss: 0.007888716645538807\n",
      "Epoch 320, Loss: 0.0004328723007347435, Val Loss: 0.007886895909905434\n",
      "Epoch 321, Loss: 0.0004302802844904363, Val Loss: 0.007886230945587158\n",
      "Epoch 322, Loss: 0.00042773550376296043, Val Loss: 0.00788559764623642\n",
      "Epoch 323, Loss: 0.00042518414556980133, Val Loss: 0.007885146886110306\n",
      "Epoch 324, Loss: 0.0004226930032018572, Val Loss: 0.007884330116212368\n",
      "Epoch 325, Loss: 0.0004202086420264095, Val Loss: 0.007882937788963318\n",
      "Epoch 326, Loss: 0.0004177383380010724, Val Loss: 0.007882615551352501\n",
      "Epoch 327, Loss: 0.0004153422487434, Val Loss: 0.007881914265453815\n",
      "Epoch 328, Loss: 0.0004129031440243125, Val Loss: 0.007881027646362782\n",
      "Epoch 329, Loss: 0.0004105096450075507, Val Loss: 0.007880942896008492\n",
      "Epoch 330, Loss: 0.00040812534280121326, Val Loss: 0.007879480719566345\n",
      "Epoch 331, Loss: 0.0004057881888002157, Val Loss: 0.007879105396568775\n",
      "Epoch 332, Loss: 0.0004034417506773025, Val Loss: 0.00787823274731636\n",
      "Epoch 333, Loss: 0.00040114406147040427, Val Loss: 0.007876616902649403\n",
      "Epoch 334, Loss: 0.0003988558892160654, Val Loss: 0.00787560734897852\n",
      "Epoch 335, Loss: 0.00039659932372160256, Val Loss: 0.007875088602304459\n",
      "Epoch 336, Loss: 0.00039435119833797216, Val Loss: 0.007873830385506153\n",
      "Epoch 337, Loss: 0.0003921320603694767, Val Loss: 0.00787313375622034\n",
      "Epoch 338, Loss: 0.00038992040208540857, Val Loss: 0.007872652262449265\n",
      "Epoch 339, Loss: 0.0003877559211105108, Val Loss: 0.007872297428548336\n",
      "Epoch 340, Loss: 0.00038557409425266087, Val Loss: 0.007871951907873154\n",
      "Epoch 341, Loss: 0.0003834207309409976, Val Loss: 0.007871164940297604\n",
      "Epoch 342, Loss: 0.000381289457436651, Val Loss: 0.007870656438171864\n",
      "Epoch 343, Loss: 0.0003791739873122424, Val Loss: 0.00787015724927187\n",
      "Epoch 344, Loss: 0.00037707117735408247, Val Loss: 0.007869214750826359\n",
      "Epoch 345, Loss: 0.0003750019532162696, Val Loss: 0.007868320681154728\n",
      "Epoch 346, Loss: 0.000372917769709602, Val Loss: 0.007867099717259407\n",
      "Epoch 347, Loss: 0.0003708876029122621, Val Loss: 0.007866683416068554\n",
      "Epoch 348, Loss: 0.0003688719298224896, Val Loss: 0.007866444997489452\n",
      "Epoch 349, Loss: 0.0003668567806016654, Val Loss: 0.007866110652685165\n",
      "Epoch 350, Loss: 0.0003648612182587385, Val Loss: 0.007865550927817822\n",
      "Epoch 351, Loss: 0.0003628927224781364, Val Loss: 0.007865410298109055\n",
      "Epoch 352, Loss: 0.0003609459672588855, Val Loss: 0.007864851504564285\n",
      "Epoch 353, Loss: 0.0003590233391150832, Val Loss: 0.007864914834499359\n",
      "Epoch 354, Loss: 0.0003570953558664769, Val Loss: 0.007865482941269875\n",
      "Epoch 355, Loss: 0.00035520066739991307, Val Loss: 0.007864831015467644\n",
      "Epoch 356, Loss: 0.0003533349372446537, Val Loss: 0.007864371873438358\n",
      "Epoch 357, Loss: 0.0003514454874675721, Val Loss: 0.00786392018198967\n",
      "Epoch 358, Loss: 0.0003495722485240549, Val Loss: 0.007863891310989857\n",
      "Epoch 359, Loss: 0.0003477362624835223, Val Loss: 0.007863831706345081\n",
      "Epoch 360, Loss: 0.0003459005383774638, Val Loss: 0.007862983271479607\n",
      "Epoch 361, Loss: 0.0003440617292653769, Val Loss: 0.007862335070967674\n",
      "Epoch 362, Loss: 0.0003422459994908422, Val Loss: 0.007861814461648464\n",
      "Epoch 363, Loss: 0.0003404873423278332, Val Loss: 0.007862080819904804\n",
      "Epoch 364, Loss: 0.00033869381877593696, Val Loss: 0.007861951366066933\n",
      "Epoch 365, Loss: 0.0003369744517840445, Val Loss: 0.007861093617975712\n"
     ]
    }
   ],
   "source": [
    "from flashrag.prompt import PromptTemplate\n",
    "import pickle\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "config_taqa = set_config(dataset_name='taqa', split = split)\n",
    "template_taqa= PromptTemplate(\n",
    "        config = config_taqa,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "config_know= set_config(dataset_name='triviaqa', split = split)\n",
    "template_know= PromptTemplate(\n",
    "        config = config_know,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "\n",
    "\n",
    "loading_hidden = True\n",
    "if not loading_hidden:\n",
    "    taqa_hidden_states, triviaqa_hidden_states = load_datasets(config_taqa, config_triviaqa, template_taqa, template_triviaqa)\n",
    "else:\n",
    "    file_path_know = '/cs/student/projects2/dsml/cdiezmar/hidden_states/knowledge-aware.pkl'\n",
    "    file_path_taqa = '/cs/student/projects2/dsml/cdiezmar/hidden_states/taqa.pkl'\n",
    "\n",
    "    with open(file_path_know, 'rb') as file_know:\n",
    "        know_hidden_states = pickle.load(file_know)\n",
    "        print('Length triviaqa: ', len(know_hidden_states))\n",
    "        know_hidden_states_tensor= torch.tensor(know_hidden_states, dtype=torch.float32)\n",
    "    with open(file_path_taqa, 'rb') as file_taqa:\n",
    "        taqa_hidden_states = pickle.load(file_taqa)\n",
    "        print('Length taqa: ', len(taqa_hidden_states))\n",
    "        taqa_hidden_states_tensor= torch.tensor(taqa_hidden_states, dtype=torch.float32)\n",
    "train_inputs, train_labels, val_inputs, val_labels = prepare_data(taqa_hidden_states_tensor, know_hidden_states_tensor)\n",
    "\n",
    "input_size = train_inputs.size(1)\n",
    "num_classes = 2  # Knowledge-sensitive or not\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "epochs = 365\n",
    "lr = 1e-4\n",
    "\n",
    "# Train RNN Classifier\n",
    "bidir_rnn_model = train_bidir_rnn_classifier_cuda(train_inputs, train_labels, val_inputs, val_labels, input_size, hidden_size, num_layers, num_classes, epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent-awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To generate intents using self_instruct\n",
    "\n",
    "# !cd self-instruct\n",
    "# !set batch_dir=data\\gpt3_generations && python self_instruct/bootstrap_instructions.py --batch_dir %batch_dir% --num_instructions_to_generate 3000 --seed_tasks_path handwritten_intents.jsonl --engine \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split completed. 2000 training, 500 validation, and 500 testing examples.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "def split_data(input_file, output_dir, train_ratio=0.6667, val_ratio=0.1667, test_ratio=0.1666):\n",
    "    with open(input_file, 'r') as infile:\n",
    "        data = [json.loads(line) for line in infile]\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Calculate the split indices\n",
    "    total = len(data)\n",
    "    train_end = int(total * train_ratio)\n",
    "    val_end = train_end + int(total * val_ratio)\n",
    "\n",
    "    # Split the data\n",
    "    train_data = data[:train_end]\n",
    "    val_data = data[train_end:val_end]\n",
    "    test_data = data[val_end:]\n",
    "\n",
    "    # Write the split data to files\n",
    "    with open(os.path.join(output_dir, 'train.jsonl'), 'w') as train_file:\n",
    "        for item in train_data:\n",
    "            train_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "    with open(os.path.join(output_dir, 'val.jsonl'), 'w') as val_file:\n",
    "        for item in val_data:\n",
    "            val_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "    with open(os.path.join(output_dir, 'test.jsonl'), 'w') as test_file:\n",
    "        for item in test_data:\n",
    "            test_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "    print(f\"Data split completed. {len(train_data)} training, {len(val_data)} validation, and {len(test_data)} testing examples.\")\n",
    "\n",
    "split_data('/cs/student/projects2/dsml/cdiezmar/machine_generated_instructions.jsonl', '/cs/student/projects2/dsml/cdiezmar/dataset/intent_generated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 18000 entries from /cs/student/projects2/dsml/cdiezmar/dataset/knowledge-aware/train.jsonl to /cs/student/projects2/dsml/cdiezmar/new_data/user_queries_train.jsonl\n",
      "Sampled 2000 entries from /cs/student/projects2/dsml/cdiezmar/dataset/knowledge-aware/train.jsonl to /cs/student/projects2/dsml/cdiezmar/new_data/user_queries_val.jsonl\n",
      "Sampled 8000 entries from /cs/student/projects2/dsml/cdiezmar/dataset/taqa/train.jsonl to /cs/student/projects2/dsml/cdiezmar/new_data/factual_knowledge_train.jsonl\n",
      "Sampled 8000 entries from /cs/student/projects2/dsml/cdiezmar/dataset/triviaqa/train.jsonl to /cs/student/projects2/dsml/cdiezmar/new_data/factual_knowledge_train.jsonl\n",
      "Sampled 1500 entries from /cs/student/projects2/dsml/cdiezmar/dataset/taqa/train.jsonl to /cs/student/projects2/dsml/cdiezmar/new_data/factual_knowledge_val.jsonl\n",
      "Sampled 1500 entries from /cs/student/projects2/dsml/cdiezmar/dataset/triviaqa/train.jsonl to /cs/student/projects2/dsml/cdiezmar/new_data/factual_knowledge_val.jsonl\n"
     ]
    }
   ],
   "source": [
    "def sample_data(source_file, output_file, sample_size):\n",
    "    with open(source_file, 'r') as infile:\n",
    "        data = [json.loads(line) for line in infile]\n",
    "\n",
    "    sampled_data = random.sample(data, sample_size)\n",
    "\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for item in sampled_data:\n",
    "            outfile.write(json.dumps(item) + '\\n')\n",
    "\n",
    "    print(f\"Sampled {sample_size} entries from {source_file} to {output_file}\")\n",
    "\n",
    "# Sample user queries\n",
    "sample_data('/cs/student/projects2/dsml/cdiezmar/dataset/knowledge-aware/train.jsonl', '/cs/student/projects2/dsml/cdiezmar/new_data/user_queries_train.jsonl', 18000)\n",
    "sample_data('/cs/student/projects2/dsml/cdiezmar/dataset/knowledge-aware/train.jsonl', '/cs/student/projects2/dsml/cdiezmar/new_data/user_queries_val.jsonl', 2000)\n",
    "\n",
    "# Sample factual knowledge questions\n",
    "sample_data('/cs/student/projects2/dsml/cdiezmar/dataset/taqa/train.jsonl', '/cs/student/projects2/dsml/cdiezmar/new_data/factual_knowledge_train.jsonl', 8000) # Adjust the size as needed\n",
    "sample_data('/cs/student/projects2/dsml/cdiezmar/dataset/triviaqa/train.jsonl', '/cs/student/projects2/dsml/cdiezmar/new_data/factual_knowledge_train.jsonl', 8000) # Adjust the size as needed\n",
    "sample_data('/cs/student/projects2/dsml/cdiezmar/dataset/taqa/train.jsonl', '/cs/student/projects2/dsml/cdiezmar/new_data/factual_knowledge_val.jsonl', 1500)\n",
    "sample_data('/cs/student/projects2/dsml/cdiezmar/dataset/triviaqa/train.jsonl', '/cs/student/projects2/dsml/cdiezmar/new_data/factual_knowledge_val.jsonl', 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def combine_data(no_retrieval_data_file, knowledge_questions_file, retrieval_intents_file, output_file, alternate=True):\n",
    "    # Helper function to read jsonl files\n",
    "    def read_jsonl(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            return [json.loads(line) for line in file]\n",
    "\n",
    "    # Helper function to write jsonl files\n",
    "    def write_jsonl(data, file_path):\n",
    "        with open(file_path, 'w') as file:\n",
    "            for entry in data:\n",
    "                file.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "    # Load data from files\n",
    "    no_retrieval_data = read_jsonl(no_retrieval_data_file)\n",
    "    knowledge_questions = read_jsonl(knowledge_questions_file)\n",
    "    retrieval_intents = read_jsonl(retrieval_intents_file)\n",
    "\n",
    "    # Combine non-retrieval-required data and knowledge questions\n",
    "    combined_queries = no_retrieval_data + knowledge_questions\n",
    "\n",
    "    # Sample data\n",
    "    sample_size = int(len(combined_queries) / 2)\n",
    "    sampled_queries = random.sample(combined_queries, sample_size)\n",
    "    remaining_queries = [query for query in combined_queries if query not in sampled_queries]\n",
    "\n",
    "    # Integrate retrieval intents with sampled queries\n",
    "    integrated_data = []\n",
    "    for i, query_dict in enumerate(sampled_queries):\n",
    "        query = query_dict['question']\n",
    "        intent = random.choice(retrieval_intents)['instruction']\n",
    "        if alternate:\n",
    "            if i % 2 == 0:\n",
    "                integrated_query = f\"{intent} {query}\"\n",
    "            else:\n",
    "                integrated_query = f\"{query} {intent}\"\n",
    "        else:\n",
    "            integrated_query = f\"{intent} {query}\"\n",
    "        integrated_data.append({'question': integrated_query, 'intent': True})\n",
    "\n",
    "    # Add the remaining queries without retrieval intents\n",
    "    for query_dict in remaining_queries:\n",
    "        integrated_data.append({'question': query_dict['question'], 'intent': False})\n",
    "\n",
    "    # Write the combined data to the output file\n",
    "    write_jsonl(integrated_data, output_file)\n",
    "\n",
    "combine_data(\n",
    "            '/cs/student/projects2/dsml/cdiezmar/new_data/no_retrieval_train.jsonl',\n",
    "            '/cs/student/projects2/dsml/cdiezmar/new_data/factual_knowledge_train.jsonl',\n",
    "            '/cs/student/projects2/dsml/cdiezmar/dataset/intent_generated/train.jsonl',\n",
    "            '/cs/student/projects2/dsml/cdiezmar/new_data/intent_train.jsonl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `question` in template\n",
      "Length new_dataset:  26000\n",
      "Epoch 1, Loss: 1.0103098154067993, Val Loss: 0.8892785906791687\n",
      "Epoch 2, Loss: 0.9009081125259399, Val Loss: 0.8991847634315491\n",
      "Epoch 3, Loss: 0.9095547199249268, Val Loss: 0.8831505179405212\n",
      "Epoch 4, Loss: 0.8921182155609131, Val Loss: 0.8400178551673889\n",
      "Epoch 5, Loss: 0.8477131724357605, Val Loss: 0.8062586188316345\n",
      "Epoch 6, Loss: 0.813209593296051, Val Loss: 0.796464741230011\n",
      "Epoch 7, Loss: 0.8032003045082092, Val Loss: 0.7940062284469604\n",
      "Epoch 8, Loss: 0.8004708290100098, Val Loss: 0.7811925411224365\n",
      "Epoch 9, Loss: 0.7868571281433105, Val Loss: 0.7596229910850525\n",
      "Epoch 10, Loss: 0.7639588117599487, Val Loss: 0.740336537361145\n",
      "Epoch 11, Loss: 0.7430750727653503, Val Loss: 0.7303290367126465\n",
      "Epoch 12, Loss: 0.731512188911438, Val Loss: 0.7267224788665771\n",
      "Epoch 13, Loss: 0.7266066074371338, Val Loss: 0.7215574979782104\n",
      "Epoch 14, Loss: 0.7204737067222595, Val Loss: 0.7107037901878357\n",
      "Epoch 15, Loss: 0.7089772820472717, Val Loss: 0.696487307548523\n",
      "Epoch 16, Loss: 0.69440758228302, Val Loss: 0.6838106513023376\n",
      "Epoch 17, Loss: 0.6815821528434753, Val Loss: 0.6752340197563171\n",
      "Epoch 18, Loss: 0.6729114651679993, Val Loss: 0.669222354888916\n",
      "Epoch 19, Loss: 0.6666897535324097, Val Loss: 0.6625341176986694\n",
      "Epoch 20, Loss: 0.6595633029937744, Val Loss: 0.6537137627601624\n",
      "Epoch 21, Loss: 0.6500661969184875, Val Loss: 0.6441065669059753\n",
      "Epoch 22, Loss: 0.6396206617355347, Val Loss: 0.6360804438591003\n",
      "Epoch 23, Loss: 0.6307145357131958, Val Loss: 0.6305826902389526\n",
      "Epoch 24, Loss: 0.6244120001792908, Val Loss: 0.6263389587402344\n",
      "Epoch 25, Loss: 0.6195201277732849, Val Loss: 0.6213362812995911\n",
      "Epoch 26, Loss: 0.6140586733818054, Val Loss: 0.6148452758789062\n",
      "Epoch 27, Loss: 0.6072889566421509, Val Loss: 0.6078665852546692\n",
      "Epoch 28, Loss: 0.6001700758934021, Val Loss: 0.6018549799919128\n",
      "Epoch 29, Loss: 0.5940933227539062, Val Loss: 0.5972554683685303\n",
      "Epoch 30, Loss: 0.5894312262535095, Val Loss: 0.5932797193527222\n",
      "Epoch 31, Loss: 0.5853355526924133, Val Loss: 0.5889236330986023\n",
      "Epoch 32, Loss: 0.5807708501815796, Val Loss: 0.5839802026748657\n",
      "Epoch 33, Loss: 0.5755360722541809, Val Loss: 0.5790567994117737\n",
      "Epoch 34, Loss: 0.5702729225158691, Val Loss: 0.5747857689857483\n",
      "Epoch 35, Loss: 0.5656622052192688, Val Loss: 0.5711457133293152\n",
      "Epoch 36, Loss: 0.561730146408081, Val Loss: 0.5675582885742188\n",
      "Epoch 37, Loss: 0.5579318404197693, Val Loss: 0.56355220079422\n",
      "Epoch 38, Loss: 0.5538091659545898, Val Loss: 0.5592253804206848\n",
      "Epoch 39, Loss: 0.5494506359100342, Val Loss: 0.5550609230995178\n",
      "Epoch 40, Loss: 0.5453107357025146, Val Loss: 0.5513924360275269\n",
      "Epoch 41, Loss: 0.5416840314865112, Val Loss: 0.5481191873550415\n",
      "Epoch 42, Loss: 0.5384297966957092, Val Loss: 0.5449062585830688\n",
      "Epoch 43, Loss: 0.5351850986480713, Val Loss: 0.5415745377540588\n",
      "Epoch 44, Loss: 0.5317609906196594, Val Loss: 0.5382483601570129\n",
      "Epoch 45, Loss: 0.5282924175262451, Val Loss: 0.5351585745811462\n",
      "Epoch 46, Loss: 0.525034487247467, Val Loss: 0.5323650240898132\n",
      "Epoch 47, Loss: 0.5220770835876465, Val Loss: 0.5296984314918518\n",
      "Epoch 48, Loss: 0.5192754864692688, Val Loss: 0.5269519686698914\n",
      "Epoch 49, Loss: 0.5164361000061035, Val Loss: 0.5240862965583801\n",
      "Epoch 50, Loss: 0.5135183334350586, Val Loss: 0.5212354063987732\n",
      "Epoch 51, Loss: 0.5106413960456848, Val Loss: 0.5185403227806091\n",
      "Epoch 52, Loss: 0.5079246163368225, Val Loss: 0.5160139203071594\n",
      "Epoch 53, Loss: 0.5053581595420837, Val Loss: 0.513565719127655\n",
      "Epoch 54, Loss: 0.5028347969055176, Val Loss: 0.5111271142959595\n",
      "Epoch 55, Loss: 0.5002800822257996, Val Loss: 0.508723258972168\n",
      "Epoch 56, Loss: 0.4977252781391144, Val Loss: 0.5064244270324707\n",
      "Epoch 57, Loss: 0.4952549934387207, Val Loss: 0.5042504668235779\n",
      "Epoch 58, Loss: 0.49290695786476135, Val Loss: 0.5021413564682007\n",
      "Epoch 59, Loss: 0.49063640832901, Val Loss: 0.5000202655792236\n",
      "Epoch 60, Loss: 0.4883754253387451, Val Loss: 0.4978700280189514\n",
      "Epoch 61, Loss: 0.486106276512146, Val Loss: 0.4957386255264282\n",
      "Epoch 62, Loss: 0.48386985063552856, Val Loss: 0.49368032813072205\n",
      "Epoch 63, Loss: 0.48170846700668335, Val Loss: 0.49170368909835815\n",
      "Epoch 64, Loss: 0.4796181619167328, Val Loss: 0.48977768421173096\n",
      "Epoch 65, Loss: 0.47755977511405945, Val Loss: 0.487876832485199\n",
      "Epoch 66, Loss: 0.4755057096481323, Val Loss: 0.4860062897205353\n",
      "Epoch 67, Loss: 0.47346585988998413, Val Loss: 0.48418423533439636\n",
      "Epoch 68, Loss: 0.4714677929878235, Val Loss: 0.4824099540710449\n",
      "Epoch 69, Loss: 0.469522088766098, Val Loss: 0.48065871000289917\n",
      "Epoch 70, Loss: 0.4676129221916199, Val Loss: 0.47890639305114746\n",
      "Epoch 71, Loss: 0.46572017669677734, Val Loss: 0.4771527647972107\n",
      "Epoch 72, Loss: 0.4638425409793854, Val Loss: 0.4754195809364319\n",
      "Epoch 73, Loss: 0.4619949162006378, Val Loss: 0.4737260937690735\n",
      "Epoch 74, Loss: 0.4601881206035614, Val Loss: 0.47207552194595337\n",
      "Epoch 75, Loss: 0.4584161937236786, Val Loss: 0.47045889496803284\n",
      "Epoch 76, Loss: 0.45666542649269104, Val Loss: 0.46887123584747314\n",
      "Epoch 77, Loss: 0.45492979884147644, Val Loss: 0.4673151671886444\n",
      "Epoch 78, Loss: 0.4532155990600586, Val Loss: 0.46579232811927795\n",
      "Epoch 79, Loss: 0.45153161883354187, Val Loss: 0.46429499983787537\n",
      "Epoch 80, Loss: 0.4498775601387024, Val Loss: 0.46280959248542786\n",
      "Epoch 81, Loss: 0.44824546575546265, Val Loss: 0.46132704615592957\n",
      "Epoch 82, Loss: 0.44662898778915405, Val Loss: 0.4598503112792969\n",
      "Epoch 83, Loss: 0.44502943754196167, Val Loss: 0.4583892822265625\n",
      "Epoch 84, Loss: 0.44345200061798096, Val Loss: 0.45695170760154724\n",
      "Epoch 85, Loss: 0.44189879298210144, Val Loss: 0.4555392563343048\n",
      "Epoch 86, Loss: 0.4403661787509918, Val Loss: 0.454150527715683\n",
      "Epoch 87, Loss: 0.4388500154018402, Val Loss: 0.4527847170829773\n",
      "Epoch 88, Loss: 0.43734946846961975, Val Loss: 0.4514418840408325\n",
      "Epoch 89, Loss: 0.43586745858192444, Val Loss: 0.45011967420578003\n",
      "Epoch 90, Loss: 0.43440574407577515, Val Loss: 0.44881248474121094\n",
      "Epoch 91, Loss: 0.432962566614151, Val Loss: 0.4475145637989044\n",
      "Epoch 92, Loss: 0.43153509497642517, Val Loss: 0.44622474908828735\n",
      "Epoch 93, Loss: 0.4301222562789917, Val Loss: 0.44494569301605225\n",
      "Epoch 94, Loss: 0.42872539162635803, Val Loss: 0.4436817467212677\n",
      "Epoch 95, Loss: 0.42734581232070923, Val Loss: 0.44243529438972473\n",
      "Epoch 96, Loss: 0.4259827733039856, Val Loss: 0.4412066638469696\n",
      "Epoch 97, Loss: 0.4246342182159424, Val Loss: 0.43999531865119934\n",
      "Epoch 98, Loss: 0.4232991933822632, Val Loss: 0.4388009011745453\n",
      "Epoch 99, Loss: 0.42197805643081665, Val Loss: 0.4376222491264343\n",
      "Epoch 100, Loss: 0.42067185044288635, Val Loss: 0.43645694851875305\n",
      "Epoch 101, Loss: 0.41938045620918274, Val Loss: 0.4353024661540985\n",
      "Epoch 102, Loss: 0.41810256242752075, Val Loss: 0.4341573715209961\n",
      "Epoch 103, Loss: 0.41683757305145264, Val Loss: 0.43302297592163086\n",
      "Epoch 104, Loss: 0.4155851900577545, Val Loss: 0.43190139532089233\n",
      "Epoch 105, Loss: 0.4143461287021637, Val Loss: 0.43079492449760437\n",
      "Epoch 106, Loss: 0.4131203889846802, Val Loss: 0.4297042489051819\n",
      "Epoch 107, Loss: 0.41190701723098755, Val Loss: 0.42862948775291443\n",
      "Epoch 108, Loss: 0.41070565581321716, Val Loss: 0.427569717168808\n",
      "Epoch 109, Loss: 0.4095160961151123, Val Loss: 0.426523894071579\n",
      "Epoch 110, Loss: 0.40833863615989685, Val Loss: 0.42549005150794983\n",
      "Epoch 111, Loss: 0.40717288851737976, Val Loss: 0.4244661331176758\n",
      "Epoch 112, Loss: 0.40601876378059387, Val Loss: 0.42345130443573\n",
      "Epoch 113, Loss: 0.40487566590309143, Val Loss: 0.42244553565979004\n",
      "Epoch 114, Loss: 0.40374353528022766, Val Loss: 0.421450138092041\n",
      "Epoch 115, Loss: 0.40262216329574585, Val Loss: 0.4204665422439575\n",
      "Epoch 116, Loss: 0.4015118479728699, Val Loss: 0.4194955825805664\n",
      "Epoch 117, Loss: 0.40041208267211914, Val Loss: 0.41853755712509155\n",
      "Epoch 118, Loss: 0.399322509765625, Val Loss: 0.4175920784473419\n",
      "Epoch 119, Loss: 0.39824315905570984, Val Loss: 0.41665828227996826\n",
      "Epoch 120, Loss: 0.3971737325191498, Val Loss: 0.4157348573207855\n",
      "Epoch 121, Loss: 0.3961143493652344, Val Loss: 0.41482046246528625\n",
      "Epoch 122, Loss: 0.3950648307800293, Val Loss: 0.41391393542289734\n",
      "Epoch 123, Loss: 0.3940248489379883, Val Loss: 0.41301509737968445\n",
      "Epoch 124, Loss: 0.3929941654205322, Val Loss: 0.41212430596351624\n",
      "Epoch 125, Loss: 0.3919728100299835, Val Loss: 0.4112423062324524\n",
      "Epoch 126, Loss: 0.39096078276634216, Val Loss: 0.4103696048259735\n",
      "Epoch 127, Loss: 0.3899576663970947, Val Loss: 0.4095067083835602\n",
      "Epoch 128, Loss: 0.3889634907245636, Val Loss: 0.40865346789360046\n",
      "Epoch 129, Loss: 0.38797786831855774, Val Loss: 0.4078094959259033\n",
      "Epoch 130, Loss: 0.38700103759765625, Val Loss: 0.40697410702705383\n",
      "Epoch 131, Loss: 0.38603267073631287, Val Loss: 0.4061465561389923\n",
      "Epoch 132, Loss: 0.38507264852523804, Val Loss: 0.40532630681991577\n",
      "Epoch 133, Loss: 0.384120911359787, Val Loss: 0.40451309084892273\n",
      "Epoch 134, Loss: 0.3831772804260254, Val Loss: 0.40370693802833557\n",
      "Epoch 135, Loss: 0.3822415769100189, Val Loss: 0.40290847420692444\n",
      "Epoch 136, Loss: 0.38131392002105713, Val Loss: 0.4021177887916565\n",
      "Epoch 137, Loss: 0.38039395213127136, Val Loss: 0.40133512020111084\n",
      "Epoch 138, Loss: 0.379481703042984, Val Loss: 0.40056052803993225\n",
      "Epoch 139, Loss: 0.37857702374458313, Val Loss: 0.39979350566864014\n",
      "Epoch 140, Loss: 0.3776797652244568, Val Loss: 0.39903372526168823\n",
      "Epoch 141, Loss: 0.3767898976802826, Val Loss: 0.3982809782028198\n",
      "Epoch 142, Loss: 0.3759073317050934, Val Loss: 0.3975345492362976\n",
      "Epoch 143, Loss: 0.37503182888031006, Val Loss: 0.3967946171760559\n",
      "Epoch 144, Loss: 0.3741634488105774, Val Loss: 0.3960612714290619\n",
      "Epoch 145, Loss: 0.37330201268196106, Val Loss: 0.39533451199531555\n",
      "Epoch 146, Loss: 0.37244749069213867, Val Loss: 0.39461496472358704\n",
      "Epoch 147, Loss: 0.37159964442253113, Val Loss: 0.39390212297439575\n",
      "Epoch 148, Loss: 0.3707585334777832, Val Loss: 0.39319631457328796\n",
      "Epoch 149, Loss: 0.36992397904396057, Val Loss: 0.3924970328807831\n",
      "Epoch 150, Loss: 0.3690960109233856, Val Loss: 0.391804039478302\n",
      "Epoch 151, Loss: 0.36827439069747925, Val Loss: 0.3911169767379761\n",
      "Epoch 152, Loss: 0.3674592077732086, Val Loss: 0.39043551683425903\n",
      "Epoch 153, Loss: 0.36665016412734985, Val Loss: 0.38975968956947327\n",
      "Epoch 154, Loss: 0.3658473789691925, Val Loss: 0.38908952474594116\n",
      "Epoch 155, Loss: 0.36505061388015747, Val Loss: 0.3884252607822418\n",
      "Epoch 156, Loss: 0.36425983905792236, Val Loss: 0.38776683807373047\n",
      "Epoch 157, Loss: 0.36347511410713196, Val Loss: 0.3871144652366638\n",
      "Epoch 158, Loss: 0.36269617080688477, Val Loss: 0.38646799325942993\n",
      "Epoch 159, Loss: 0.3619230091571808, Val Loss: 0.38582727313041687\n",
      "Epoch 160, Loss: 0.36115556955337524, Val Loss: 0.38519203662872314\n",
      "Epoch 161, Loss: 0.3603937327861786, Val Loss: 0.38456201553344727\n",
      "Epoch 162, Loss: 0.3596374988555908, Val Loss: 0.3839370310306549\n",
      "Epoch 163, Loss: 0.35888683795928955, Val Loss: 0.38331708312034607\n",
      "Epoch 164, Loss: 0.3581414818763733, Val Loss: 0.3827020823955536\n",
      "Epoch 165, Loss: 0.3574014902114868, Val Loss: 0.3820922374725342\n",
      "Epoch 166, Loss: 0.3566669523715973, Val Loss: 0.38148733973503113\n",
      "Epoch 167, Loss: 0.35593751072883606, Val Loss: 0.38088780641555786\n",
      "Epoch 168, Loss: 0.3552132248878479, Val Loss: 0.3802933096885681\n",
      "Epoch 169, Loss: 0.3544940650463104, Val Loss: 0.37970373034477234\n",
      "Epoch 170, Loss: 0.35378000140190125, Val Loss: 0.3791190981864929\n",
      "Epoch 171, Loss: 0.35307079553604126, Val Loss: 0.3785390257835388\n",
      "Epoch 172, Loss: 0.35236650705337524, Val Loss: 0.377963662147522\n",
      "Epoch 173, Loss: 0.35166725516319275, Val Loss: 0.3773927092552185\n",
      "Epoch 174, Loss: 0.35097265243530273, Val Loss: 0.3768262565135956\n",
      "Epoch 175, Loss: 0.35028281807899475, Val Loss: 0.3762642443180084\n",
      "Epoch 176, Loss: 0.3495977520942688, Val Loss: 0.37570682168006897\n",
      "Epoch 177, Loss: 0.34891724586486816, Val Loss: 0.37515386939048767\n",
      "Epoch 178, Loss: 0.34824132919311523, Val Loss: 0.3746053874492645\n",
      "Epoch 179, Loss: 0.34756991267204285, Val Loss: 0.3740611672401428\n",
      "Epoch 180, Loss: 0.346902996301651, Val Loss: 0.37352120876312256\n",
      "Epoch 181, Loss: 0.3462405502796173, Val Loss: 0.3729853630065918\n",
      "Epoch 182, Loss: 0.34558233618736267, Val Loss: 0.37245362997055054\n",
      "Epoch 183, Loss: 0.3449286222457886, Val Loss: 0.371925950050354\n",
      "Epoch 184, Loss: 0.344279021024704, Val Loss: 0.37140220403671265\n",
      "Epoch 185, Loss: 0.3436337411403656, Val Loss: 0.370882511138916\n",
      "Epoch 186, Loss: 0.3429926335811615, Val Loss: 0.3703668415546417\n",
      "Epoch 187, Loss: 0.3423556089401245, Val Loss: 0.3698550760746002\n",
      "Epoch 188, Loss: 0.34172260761260986, Val Loss: 0.3693472445011139\n",
      "Epoch 189, Loss: 0.3410937488079071, Val Loss: 0.3688432276248932\n",
      "Epoch 190, Loss: 0.3404688537120819, Val Loss: 0.36834290623664856\n",
      "Epoch 191, Loss: 0.3398478925228119, Val Loss: 0.36784639954566956\n",
      "Epoch 192, Loss: 0.33923080563545227, Val Loss: 0.3673533797264099\n",
      "Epoch 193, Loss: 0.3386175334453583, Val Loss: 0.36686405539512634\n",
      "Epoch 194, Loss: 0.3380081355571747, Val Loss: 0.3663782477378845\n",
      "Epoch 195, Loss: 0.3374025523662567, Val Loss: 0.3658961057662964\n",
      "Epoch 196, Loss: 0.3368006944656372, Val Loss: 0.3654174506664276\n",
      "Epoch 197, Loss: 0.336202472448349, Val Loss: 0.36494240164756775\n",
      "Epoch 198, Loss: 0.33560794591903687, Val Loss: 0.36447083950042725\n",
      "Epoch 199, Loss: 0.33501702547073364, Val Loss: 0.36400261521339417\n",
      "Epoch 200, Loss: 0.33442962169647217, Val Loss: 0.3635377585887909\n",
      "Epoch 201, Loss: 0.3338458836078644, Val Loss: 0.36307623982429504\n",
      "Epoch 202, Loss: 0.3332655727863312, Val Loss: 0.3626179099082947\n",
      "Epoch 203, Loss: 0.33268871903419495, Val Loss: 0.36216282844543457\n",
      "Epoch 204, Loss: 0.3321153223514557, Val Loss: 0.3617110252380371\n",
      "Epoch 205, Loss: 0.33154526352882385, Val Loss: 0.36126235127449036\n",
      "Epoch 206, Loss: 0.3309785723686218, Val Loss: 0.3608168959617615\n",
      "Epoch 207, Loss: 0.33041518926620483, Val Loss: 0.36037465929985046\n",
      "Epoch 208, Loss: 0.32985514402389526, Val Loss: 0.35993537306785583\n",
      "Epoch 209, Loss: 0.3292982578277588, Val Loss: 0.3594992160797119\n",
      "Epoch 210, Loss: 0.3287447392940521, Val Loss: 0.35906603932380676\n",
      "Epoch 211, Loss: 0.328194260597229, Val Loss: 0.358635812997818\n",
      "Epoch 212, Loss: 0.32764697074890137, Val Loss: 0.3582085371017456\n",
      "Epoch 213, Loss: 0.3271028399467468, Val Loss: 0.35778409242630005\n",
      "Epoch 214, Loss: 0.3265618681907654, Val Loss: 0.3573627471923828\n",
      "Epoch 215, Loss: 0.32602378726005554, Val Loss: 0.3569440543651581\n",
      "Epoch 216, Loss: 0.3254888653755188, Val Loss: 0.3565283715724945\n",
      "Epoch 217, Loss: 0.32495686411857605, Val Loss: 0.35611531138420105\n",
      "Epoch 218, Loss: 0.32442793250083923, Val Loss: 0.355705201625824\n",
      "Epoch 219, Loss: 0.32390183210372925, Val Loss: 0.3552977442741394\n",
      "Epoch 220, Loss: 0.32337868213653564, Val Loss: 0.3548929691314697\n",
      "Epoch 221, Loss: 0.32285845279693604, Val Loss: 0.3544909656047821\n",
      "Epoch 222, Loss: 0.32234105467796326, Val Loss: 0.35409146547317505\n",
      "Epoch 223, Loss: 0.32182636857032776, Val Loss: 0.35369473695755005\n",
      "Epoch 224, Loss: 0.32131466269493103, Val Loss: 0.35330063104629517\n",
      "Epoch 225, Loss: 0.32080569863319397, Val Loss: 0.3529089689254761\n",
      "Epoch 226, Loss: 0.3202993869781494, Val Loss: 0.3525199890136719\n",
      "Epoch 227, Loss: 0.3197958469390869, Val Loss: 0.35213354229927063\n",
      "Epoch 228, Loss: 0.3192949891090393, Val Loss: 0.3517495393753052\n",
      "Epoch 229, Loss: 0.31879687309265137, Val Loss: 0.35136809945106506\n",
      "Epoch 230, Loss: 0.3183012008666992, Val Loss: 0.35098910331726074\n",
      "Epoch 231, Loss: 0.3178083300590515, Val Loss: 0.35061243176460266\n",
      "Epoch 232, Loss: 0.31731799244880676, Val Loss: 0.35023826360702515\n",
      "Epoch 233, Loss: 0.31683027744293213, Val Loss: 0.34986647963523865\n",
      "Epoch 234, Loss: 0.31634509563446045, Val Loss: 0.34949707984924316\n",
      "Epoch 235, Loss: 0.31586241722106934, Val Loss: 0.34912994503974915\n",
      "Epoch 236, Loss: 0.315382182598114, Val Loss: 0.34876522421836853\n",
      "Epoch 237, Loss: 0.31490442156791687, Val Loss: 0.34840279817581177\n",
      "Epoch 238, Loss: 0.3144291937351227, Val Loss: 0.34804266691207886\n",
      "Epoch 239, Loss: 0.3139564096927643, Val Loss: 0.3476848304271698\n",
      "Epoch 240, Loss: 0.31348592042922974, Val Loss: 0.3473290503025055\n",
      "Epoch 241, Loss: 0.31301793456077576, Val Loss: 0.3469755947589874\n",
      "Epoch 242, Loss: 0.31255224347114563, Val Loss: 0.34662431478500366\n",
      "Epoch 243, Loss: 0.3120889663696289, Val Loss: 0.3462751805782318\n",
      "Epoch 244, Loss: 0.3116280138492584, Val Loss: 0.34592825174331665\n",
      "Epoch 245, Loss: 0.311169296503067, Val Loss: 0.3455835282802582\n",
      "Epoch 246, Loss: 0.31071290373802185, Val Loss: 0.3452407419681549\n",
      "Epoch 247, Loss: 0.31025877594947815, Val Loss: 0.3449001610279083\n",
      "Epoch 248, Loss: 0.30980685353279114, Val Loss: 0.3445616364479065\n",
      "Epoch 249, Loss: 0.3093571662902832, Val Loss: 0.3442251980304718\n",
      "Epoch 250, Loss: 0.30890974402427673, Val Loss: 0.3438907563686371\n",
      "Epoch 251, Loss: 0.3084644675254822, Val Loss: 0.3435583710670471\n",
      "Epoch 252, Loss: 0.30802133679389954, Val Loss: 0.3432278633117676\n",
      "Epoch 253, Loss: 0.3075803816318512, Val Loss: 0.34289953112602234\n",
      "Epoch 254, Loss: 0.30714151263237, Val Loss: 0.34257298707962036\n",
      "Epoch 255, Loss: 0.30670487880706787, Val Loss: 0.3422485291957855\n",
      "Epoch 256, Loss: 0.30627018213272095, Val Loss: 0.3419260084629059\n",
      "Epoch 257, Loss: 0.30583760142326355, Val Loss: 0.34160536527633667\n",
      "Epoch 258, Loss: 0.3054071068763733, Val Loss: 0.3412865698337555\n",
      "Epoch 259, Loss: 0.30497872829437256, Val Loss: 0.34096965193748474\n",
      "Epoch 260, Loss: 0.30455222725868225, Val Loss: 0.3406546413898468\n",
      "Epoch 261, Loss: 0.30412787199020386, Val Loss: 0.3403414785861969\n",
      "Epoch 262, Loss: 0.3037053644657135, Val Loss: 0.3400301933288574\n",
      "Epoch 263, Loss: 0.30328500270843506, Val Loss: 0.3397206962108612\n",
      "Epoch 264, Loss: 0.30286648869514465, Val Loss: 0.33941298723220825\n",
      "Epoch 265, Loss: 0.30244991183280945, Val Loss: 0.33910706639289856\n",
      "Epoch 266, Loss: 0.3020353317260742, Val Loss: 0.3388029932975769\n",
      "Epoch 267, Loss: 0.30162253975868225, Val Loss: 0.3385005593299866\n",
      "Epoch 268, Loss: 0.30121180415153503, Val Loss: 0.3381999433040619\n",
      "Epoch 269, Loss: 0.3008027672767639, Val Loss: 0.3379010558128357\n",
      "Epoch 270, Loss: 0.30039578676223755, Val Loss: 0.3376038670539856\n",
      "Epoch 271, Loss: 0.2999905049800873, Val Loss: 0.3373083174228668\n",
      "Epoch 272, Loss: 0.2995871603488922, Val Loss: 0.3370145559310913\n",
      "Epoch 273, Loss: 0.29918548464775085, Val Loss: 0.33672240376472473\n",
      "Epoch 274, Loss: 0.29878583550453186, Val Loss: 0.33643192052841187\n",
      "Epoch 275, Loss: 0.2983877956867218, Val Loss: 0.3361431658267975\n",
      "Epoch 276, Loss: 0.2979916036128998, Val Loss: 0.33585596084594727\n",
      "Epoch 277, Loss: 0.297597199678421, Val Loss: 0.33557042479515076\n",
      "Epoch 278, Loss: 0.29720452427864075, Val Loss: 0.3352864384651184\n",
      "Epoch 279, Loss: 0.2968135476112366, Val Loss: 0.3350040018558502\n",
      "Epoch 280, Loss: 0.29642435908317566, Val Loss: 0.3347232937812805\n",
      "Epoch 281, Loss: 0.29603686928749084, Val Loss: 0.3344440460205078\n",
      "Epoch 282, Loss: 0.29565104842185974, Val Loss: 0.33416637778282166\n",
      "Epoch 283, Loss: 0.29526692628860474, Val Loss: 0.33389025926589966\n",
      "Epoch 284, Loss: 0.29488444328308105, Val Loss: 0.3336156904697418\n",
      "Epoch 285, Loss: 0.29450371861457825, Val Loss: 0.33334267139434814\n",
      "Epoch 286, Loss: 0.29412445425987244, Val Loss: 0.3330710828304291\n",
      "Epoch 287, Loss: 0.2937469780445099, Val Loss: 0.3328009843826294\n",
      "Epoch 288, Loss: 0.2933710515499115, Val Loss: 0.33253246545791626\n",
      "Epoch 289, Loss: 0.2929967939853668, Val Loss: 0.33226528763771057\n",
      "Epoch 290, Loss: 0.2926240563392639, Val Loss: 0.33199965953826904\n",
      "Epoch 291, Loss: 0.29225292801856995, Val Loss: 0.33173543214797974\n",
      "Epoch 292, Loss: 0.2918834388256073, Val Loss: 0.3314726948738098\n",
      "Epoch 293, Loss: 0.2915154695510864, Val Loss: 0.3312113881111145\n",
      "Epoch 294, Loss: 0.29114899039268494, Val Loss: 0.3309514820575714\n",
      "Epoch 295, Loss: 0.2907841205596924, Val Loss: 0.33069294691085815\n",
      "Epoch 296, Loss: 0.2904207110404968, Val Loss: 0.3304358720779419\n",
      "Epoch 297, Loss: 0.2900589406490326, Val Loss: 0.3301801383495331\n",
      "Epoch 298, Loss: 0.2896985113620758, Val Loss: 0.3299257457256317\n",
      "Epoch 299, Loss: 0.28933975100517273, Val Loss: 0.32967281341552734\n",
      "Epoch 300, Loss: 0.2889823913574219, Val Loss: 0.32942116260528564\n",
      "Epoch 301, Loss: 0.288626492023468, Val Loss: 0.32917091250419617\n",
      "Epoch 302, Loss: 0.28827202320098877, Val Loss: 0.32892200350761414\n",
      "Epoch 303, Loss: 0.2879190444946289, Val Loss: 0.32867431640625\n",
      "Epoch 304, Loss: 0.28756749629974365, Val Loss: 0.3284280300140381\n",
      "Epoch 305, Loss: 0.287217378616333, Val Loss: 0.32818299531936646\n",
      "Epoch 306, Loss: 0.286868691444397, Val Loss: 0.32793930172920227\n",
      "Epoch 307, Loss: 0.28652140498161316, Val Loss: 0.32769691944122314\n",
      "Epoch 308, Loss: 0.28617551922798157, Val Loss: 0.3274557590484619\n",
      "Epoch 309, Loss: 0.2858310043811798, Val Loss: 0.32721588015556335\n",
      "Epoch 310, Loss: 0.28548792004585266, Val Loss: 0.3269772529602051\n",
      "Epoch 311, Loss: 0.28514620661735535, Val Loss: 0.3267399072647095\n",
      "Epoch 312, Loss: 0.2848058342933655, Val Loss: 0.3265037536621094\n",
      "Epoch 313, Loss: 0.28446680307388306, Val Loss: 0.32626888155937195\n",
      "Epoch 314, Loss: 0.28412914276123047, Val Loss: 0.3260352611541748\n",
      "Epoch 315, Loss: 0.28379279375076294, Val Loss: 0.3258028030395508\n",
      "Epoch 316, Loss: 0.28345778584480286, Val Loss: 0.32557156682014465\n",
      "Epoch 317, Loss: 0.28312408924102783, Val Loss: 0.32534152269363403\n",
      "Epoch 318, Loss: 0.28279173374176025, Val Loss: 0.3251126706600189\n",
      "Epoch 319, Loss: 0.2824605107307434, Val Loss: 0.32488498091697693\n",
      "Epoch 320, Loss: 0.28213074803352356, Val Loss: 0.32465848326683044\n",
      "Epoch 321, Loss: 0.2818022072315216, Val Loss: 0.3244331181049347\n",
      "Epoch 322, Loss: 0.2814749479293823, Val Loss: 0.32420897483825684\n",
      "Epoch 323, Loss: 0.2811489701271057, Val Loss: 0.3239859342575073\n",
      "Epoch 324, Loss: 0.280824214220047, Val Loss: 0.32376405596733093\n",
      "Epoch 325, Loss: 0.28050071001052856, Val Loss: 0.32354333996772766\n",
      "Epoch 326, Loss: 0.2801785171031952, Val Loss: 0.32332366704940796\n",
      "Epoch 327, Loss: 0.27985745668411255, Val Loss: 0.32310518622398376\n",
      "Epoch 328, Loss: 0.2795376181602478, Val Loss: 0.3228878378868103\n",
      "Epoch 329, Loss: 0.2792190909385681, Val Loss: 0.32267147302627563\n",
      "Epoch 330, Loss: 0.27890169620513916, Val Loss: 0.3224563002586365\n",
      "Epoch 331, Loss: 0.2785854637622833, Val Loss: 0.32224223017692566\n",
      "Epoch 332, Loss: 0.2782704532146454, Val Loss: 0.3220292031764984\n",
      "Epoch 333, Loss: 0.27795666456222534, Val Loss: 0.3218172788619995\n",
      "Epoch 334, Loss: 0.27764397859573364, Val Loss: 0.3216063380241394\n",
      "Epoch 335, Loss: 0.2773325443267822, Val Loss: 0.32139647006988525\n",
      "Epoch 336, Loss: 0.27702221274375916, Val Loss: 0.32118773460388184\n",
      "Epoch 337, Loss: 0.27671298384666443, Val Loss: 0.320980042219162\n",
      "Epoch 338, Loss: 0.2764050364494324, Val Loss: 0.32077333331108093\n",
      "Epoch 339, Loss: 0.2760981023311615, Val Loss: 0.32056763768196106\n",
      "Epoch 340, Loss: 0.27579236030578613, Val Loss: 0.3203630745410919\n",
      "Epoch 341, Loss: 0.2754877805709839, Val Loss: 0.3201594650745392\n",
      "Epoch 342, Loss: 0.27518415451049805, Val Loss: 0.31995683908462524\n",
      "Epoch 343, Loss: 0.27488183975219727, Val Loss: 0.3197551965713501\n",
      "Epoch 344, Loss: 0.2745805084705353, Val Loss: 0.3195545971393585\n",
      "Epoch 345, Loss: 0.2742803394794464, Val Loss: 0.3193550407886505\n",
      "Epoch 346, Loss: 0.2739811837673187, Val Loss: 0.3191564679145813\n",
      "Epoch 347, Loss: 0.2736831307411194, Val Loss: 0.3189588487148285\n",
      "Epoch 348, Loss: 0.2733861207962036, Val Loss: 0.3187621533870697\n",
      "Epoch 349, Loss: 0.2730901837348938, Val Loss: 0.3185665011405945\n",
      "Epoch 350, Loss: 0.2727954089641571, Val Loss: 0.3183717131614685\n",
      "Epoch 351, Loss: 0.2725015878677368, Val Loss: 0.3181779086589813\n",
      "Epoch 352, Loss: 0.2722088396549225, Val Loss: 0.3179851174354553\n",
      "Epoch 353, Loss: 0.2719171643257141, Val Loss: 0.31779322028160095\n",
      "Epoch 354, Loss: 0.2716265022754669, Val Loss: 0.31760233640670776\n",
      "Epoch 355, Loss: 0.2713368535041809, Val Loss: 0.31741228699684143\n",
      "Epoch 356, Loss: 0.2710481882095337, Val Loss: 0.3172231614589691\n",
      "Epoch 357, Loss: 0.27076059579849243, Val Loss: 0.3170350193977356\n",
      "Epoch 358, Loss: 0.2704739570617676, Val Loss: 0.3168478012084961\n",
      "Epoch 359, Loss: 0.2701883614063263, Val Loss: 0.3166614770889282\n",
      "Epoch 360, Loss: 0.2699038088321686, Val Loss: 0.3164759576320648\n",
      "Epoch 361, Loss: 0.26962023973464966, Val Loss: 0.3162914514541626\n",
      "Epoch 362, Loss: 0.26933759450912476, Val Loss: 0.3161078691482544\n",
      "Epoch 363, Loss: 0.26905596256256104, Val Loss: 0.31592506170272827\n",
      "Epoch 364, Loss: 0.2687753140926361, Val Loss: 0.31574317812919617\n",
      "Epoch 365, Loss: 0.2684955894947052, Val Loss: 0.3155621886253357\n",
      "Epoch 366, Loss: 0.2682168781757355, Val Loss: 0.3153819739818573\n",
      "Epoch 367, Loss: 0.26793909072875977, Val Loss: 0.3152027428150177\n",
      "Epoch 368, Loss: 0.26766231656074524, Val Loss: 0.31502437591552734\n",
      "Epoch 369, Loss: 0.26738637685775757, Val Loss: 0.31484681367874146\n",
      "Epoch 370, Loss: 0.2671114504337311, Val Loss: 0.31467005610466003\n",
      "Epoch 371, Loss: 0.26683735847473145, Val Loss: 0.31449419260025024\n",
      "Epoch 372, Loss: 0.26656433939933777, Val Loss: 0.3143192231655121\n",
      "Epoch 373, Loss: 0.26629218459129333, Val Loss: 0.3141450881958008\n",
      "Epoch 374, Loss: 0.26602086424827576, Val Loss: 0.3139716386795044\n",
      "Epoch 375, Loss: 0.26575061678886414, Val Loss: 0.3137991428375244\n",
      "Epoch 376, Loss: 0.26548120379447937, Val Loss: 0.3136274218559265\n",
      "Epoch 377, Loss: 0.2652127146720886, Val Loss: 0.31345653533935547\n",
      "Epoch 378, Loss: 0.26494503021240234, Val Loss: 0.3132864832878113\n",
      "Epoch 379, Loss: 0.26467829942703247, Val Loss: 0.3131171762943268\n",
      "Epoch 380, Loss: 0.26441246271133423, Val Loss: 0.31294870376586914\n",
      "Epoch 381, Loss: 0.2641475200653076, Val Loss: 0.31278109550476074\n",
      "Epoch 382, Loss: 0.26388344168663025, Val Loss: 0.31261417269706726\n",
      "Epoch 383, Loss: 0.26362019777297974, Val Loss: 0.312448114156723\n",
      "Epoch 384, Loss: 0.26335790753364563, Val Loss: 0.3122827708721161\n",
      "Epoch 385, Loss: 0.26309633255004883, Val Loss: 0.312118262052536\n",
      "Epoch 386, Loss: 0.26283571124076843, Val Loss: 0.31195446848869324\n",
      "Epoch 387, Loss: 0.2625758945941925, Val Loss: 0.31179147958755493\n",
      "Epoch 388, Loss: 0.2623170018196106, Val Loss: 0.31162920594215393\n",
      "Epoch 389, Loss: 0.26205891370773315, Val Loss: 0.31146782636642456\n",
      "Epoch 390, Loss: 0.26180168986320496, Val Loss: 0.31130707263946533\n",
      "Epoch 391, Loss: 0.261545330286026, Val Loss: 0.31114721298217773\n",
      "Epoch 392, Loss: 0.2612895965576172, Val Loss: 0.3109878599643707\n",
      "Epoch 393, Loss: 0.2610348165035248, Val Loss: 0.3108294606208801\n",
      "Epoch 394, Loss: 0.26078084111213684, Val Loss: 0.31067174673080444\n",
      "Epoch 395, Loss: 0.26052772998809814, Val Loss: 0.31051474809646606\n",
      "Epoch 396, Loss: 0.2602754235267639, Val Loss: 0.310358464717865\n",
      "Epoch 397, Loss: 0.2600238621234894, Val Loss: 0.310202956199646\n",
      "Epoch 398, Loss: 0.2597731053829193, Val Loss: 0.31004810333251953\n",
      "Epoch 399, Loss: 0.25952327251434326, Val Loss: 0.3098940849304199\n",
      "Epoch 400, Loss: 0.25927412509918213, Val Loss: 0.3097406327724457\n",
      "Epoch 401, Loss: 0.2590256929397583, Val Loss: 0.3095880150794983\n",
      "Epoch 402, Loss: 0.2587781846523285, Val Loss: 0.3094361126422882\n",
      "Epoch 403, Loss: 0.258531391620636, Val Loss: 0.3092847764492035\n",
      "Epoch 404, Loss: 0.2582853436470032, Val Loss: 0.30913427472114563\n",
      "Epoch 405, Loss: 0.25804010033607483, Val Loss: 0.3089844286441803\n",
      "Epoch 406, Loss: 0.25779563188552856, Val Loss: 0.3088352084159851\n",
      "Epoch 407, Loss: 0.257551908493042, Val Loss: 0.3086867928504944\n",
      "Epoch 408, Loss: 0.2573089897632599, Val Loss: 0.3085389733314514\n",
      "Epoch 409, Loss: 0.2570666968822479, Val Loss: 0.308391809463501\n",
      "Epoch 410, Loss: 0.2568252682685852, Val Loss: 0.308245450258255\n",
      "Epoch 411, Loss: 0.2565845847129822, Val Loss: 0.3080996870994568\n",
      "Epoch 412, Loss: 0.2563445568084717, Val Loss: 0.3079545199871063\n",
      "Epoch 413, Loss: 0.25610536336898804, Val Loss: 0.3078101575374603\n",
      "Epoch 414, Loss: 0.2558669447898865, Val Loss: 0.3076664209365845\n",
      "Epoch 415, Loss: 0.25562915205955505, Val Loss: 0.307523250579834\n",
      "Epoch 416, Loss: 0.2553921639919281, Val Loss: 0.3073808550834656\n",
      "Epoch 417, Loss: 0.2551558017730713, Val Loss: 0.30723902583122253\n",
      "Epoch 418, Loss: 0.25492021441459656, Val Loss: 0.3070979118347168\n",
      "Epoch 419, Loss: 0.25468528270721436, Val Loss: 0.3069573640823364\n",
      "Epoch 420, Loss: 0.254451185464859, Val Loss: 0.30681753158569336\n",
      "Epoch 421, Loss: 0.25421765446662903, Val Loss: 0.30667826533317566\n",
      "Epoch 422, Loss: 0.25398489832878113, Val Loss: 0.30653974413871765\n",
      "Epoch 423, Loss: 0.25375282764434814, Val Loss: 0.3064017593860626\n",
      "Epoch 424, Loss: 0.25352147221565247, Val Loss: 0.30626440048217773\n",
      "Epoch 425, Loss: 0.2532908022403717, Val Loss: 0.30612775683403015\n",
      "Epoch 426, Loss: 0.25306087732315063, Val Loss: 0.30599159002304077\n",
      "Epoch 427, Loss: 0.2528315484523773, Val Loss: 0.3058561682701111\n",
      "Epoch 428, Loss: 0.2526029050350189, Val Loss: 0.30572137236595154\n",
      "Epoch 429, Loss: 0.2523750066757202, Val Loss: 0.30558714270591736\n",
      "Epoch 430, Loss: 0.2521476745605469, Val Loss: 0.30545347929000854\n",
      "Epoch 431, Loss: 0.251921147108078, Val Loss: 0.30532050132751465\n",
      "Epoch 432, Loss: 0.2516952455043793, Val Loss: 0.30518805980682373\n",
      "Epoch 433, Loss: 0.2514699697494507, Val Loss: 0.3050561845302582\n",
      "Epoch 434, Loss: 0.2512454092502594, Val Loss: 0.30492496490478516\n",
      "Epoch 435, Loss: 0.2510213851928711, Val Loss: 0.30479443073272705\n",
      "Epoch 436, Loss: 0.25079819560050964, Val Loss: 0.30466428399086\n",
      "Epoch 437, Loss: 0.25057554244995117, Val Loss: 0.3045348823070526\n",
      "Epoch 438, Loss: 0.2503535747528076, Val Loss: 0.3044060170650482\n",
      "Epoch 439, Loss: 0.2501322627067566, Val Loss: 0.3042777180671692\n",
      "Epoch 440, Loss: 0.24991151690483093, Val Loss: 0.3041500747203827\n",
      "Epoch 441, Loss: 0.24969151616096497, Val Loss: 0.3040229082107544\n",
      "Epoch 442, Loss: 0.24947209656238556, Val Loss: 0.30389633774757385\n",
      "Epoch 443, Loss: 0.2492532879114151, Val Loss: 0.30377036333084106\n",
      "Epoch 444, Loss: 0.2490350753068924, Val Loss: 0.30364495515823364\n",
      "Epoch 445, Loss: 0.2488175630569458, Val Loss: 0.303520143032074\n",
      "Epoch 446, Loss: 0.24860067665576935, Val Loss: 0.3033957779407501\n",
      "Epoch 447, Loss: 0.24838435649871826, Val Loss: 0.3032720685005188\n",
      "Epoch 448, Loss: 0.2481686919927597, Val Loss: 0.30314889550209045\n",
      "Epoch 449, Loss: 0.2479536086320877, Val Loss: 0.3030262291431427\n",
      "Epoch 450, Loss: 0.24773916602134705, Val Loss: 0.3029041588306427\n",
      "Epoch 451, Loss: 0.24752534925937653, Val Loss: 0.3027826249599457\n",
      "Epoch 452, Loss: 0.24731215834617615, Val Loss: 0.302661657333374\n",
      "Epoch 453, Loss: 0.24709947407245636, Val Loss: 0.30254119634628296\n",
      "Epoch 454, Loss: 0.24688740074634552, Val Loss: 0.3024212718009949\n",
      "Epoch 455, Loss: 0.2466760277748108, Val Loss: 0.30230191349983215\n",
      "Epoch 456, Loss: 0.2464650720357895, Val Loss: 0.3021831512451172\n",
      "Epoch 457, Loss: 0.24625490605831146, Val Loss: 0.30206480622291565\n",
      "Epoch 458, Loss: 0.2460452765226364, Val Loss: 0.30194705724716187\n",
      "Epoch 459, Loss: 0.24583609402179718, Val Loss: 0.3018297851085663\n",
      "Epoch 460, Loss: 0.24562761187553406, Val Loss: 0.3017130494117737\n",
      "Epoch 461, Loss: 0.2454196810722351, Val Loss: 0.30159687995910645\n",
      "Epoch 462, Loss: 0.2452123612165451, Val Loss: 0.3014812171459198\n",
      "Epoch 463, Loss: 0.2450055181980133, Val Loss: 0.3013659715652466\n",
      "Epoch 464, Loss: 0.24479936063289642, Val Loss: 0.3012513518333435\n",
      "Epoch 465, Loss: 0.24459366500377655, Val Loss: 0.301137238740921\n",
      "Epoch 466, Loss: 0.24438856542110443, Val Loss: 0.30102360248565674\n",
      "Epoch 467, Loss: 0.24418410658836365, Val Loss: 0.30091044306755066\n",
      "Epoch 468, Loss: 0.24398010969161987, Val Loss: 0.30079787969589233\n",
      "Epoch 469, Loss: 0.24377678334712982, Val Loss: 0.30068567395210266\n",
      "Epoch 470, Loss: 0.2435738891363144, Val Loss: 0.30057406425476074\n",
      "Epoch 471, Loss: 0.2433716058731079, Val Loss: 0.300462931394577\n",
      "Epoch 472, Loss: 0.2431698590517044, Val Loss: 0.3003523647785187\n",
      "Epoch 473, Loss: 0.24296864867210388, Val Loss: 0.300242155790329\n",
      "Epoch 474, Loss: 0.24276801943778992, Val Loss: 0.30013254284858704\n",
      "Epoch 475, Loss: 0.24256791174411774, Val Loss: 0.3000233471393585\n",
      "Epoch 476, Loss: 0.24236831068992615, Val Loss: 0.2999146580696106\n",
      "Epoch 477, Loss: 0.24216929078102112, Val Loss: 0.2998064458370209\n",
      "Epoch 478, Loss: 0.2419707477092743, Val Loss: 0.29969874024391174\n",
      "Epoch 479, Loss: 0.24177277088165283, Val Loss: 0.2995914816856384\n",
      "Epoch 480, Loss: 0.24157534539699554, Val Loss: 0.2994846999645233\n",
      "Epoch 481, Loss: 0.24137838184833527, Val Loss: 0.2993784248828888\n",
      "Epoch 482, Loss: 0.24118205904960632, Val Loss: 0.2992725968360901\n",
      "Epoch 483, Loss: 0.2409861832857132, Val Loss: 0.29916730523109436\n",
      "Epoch 484, Loss: 0.24079082906246185, Val Loss: 0.2990624010562897\n",
      "Epoch 485, Loss: 0.24059590697288513, Val Loss: 0.2989580035209656\n",
      "Epoch 486, Loss: 0.24040165543556213, Val Loss: 0.2988540530204773\n",
      "Epoch 487, Loss: 0.24020777642726898, Val Loss: 0.29875054955482483\n",
      "Epoch 488, Loss: 0.24001450836658478, Val Loss: 0.2986474931240082\n",
      "Epoch 489, Loss: 0.239821657538414, Val Loss: 0.29854488372802734\n",
      "Epoch 490, Loss: 0.23962941765785217, Val Loss: 0.2984428405761719\n",
      "Epoch 491, Loss: 0.23943762481212616, Val Loss: 0.2983410954475403\n",
      "Epoch 492, Loss: 0.23924633860588074, Val Loss: 0.29823988676071167\n",
      "Epoch 493, Loss: 0.2390555441379547, Val Loss: 0.2981390655040741\n",
      "Epoch 494, Loss: 0.2388652116060257, Val Loss: 0.2980387806892395\n",
      "Epoch 495, Loss: 0.23867546021938324, Val Loss: 0.29793885350227356\n",
      "Epoch 496, Loss: 0.23848609626293182, Val Loss: 0.2978394329547882\n",
      "Epoch 497, Loss: 0.23829728364944458, Val Loss: 0.2977403700351715\n",
      "Epoch 498, Loss: 0.23810891807079315, Val Loss: 0.2976418733596802\n",
      "Epoch 499, Loss: 0.23792104423046112, Val Loss: 0.2975437045097351\n",
      "Epoch 500, Loss: 0.23773372173309326, Val Loss: 0.29744601249694824\n"
     ]
    }
   ],
   "source": [
    "from flashrag.prompt import PromptTemplate\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "# Set configuration for the new dataset\n",
    "config_new_dataset = set_config(dataset_name='final_intent', split=split)\n",
    "template_new_dataset = PromptTemplate(\n",
    "    config=config_new_dataset,\n",
    "    system_prompt=\"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "    user_prompt=\"Question: {question}\"\n",
    ")\n",
    "\n",
    "# Loading hidden states\n",
    "loading_hidden = True\n",
    "if not loading_hidden:\n",
    "    new_hidden_states = load_datasets(config_new_dataset, template_new_dataset)\n",
    "else:\n",
    "    file_path_new_dataset = '/cs/student/projects2/dsml/cdiezmar/hidden_states/final_intent.pkl'\n",
    "\n",
    "    with open(file_path_new_dataset, 'rb') as file_new:\n",
    "        new_hidden_states = pickle.load(file_new)\n",
    "        print('Length new_dataset: ', len(new_hidden_states))\n",
    "        new_hidden_states_tensor = torch.tensor(new_hidden_states, dtype=torch.float32)\n",
    "\n",
    "# Prepare data\n",
    "train_inputs, train_labels, val_inputs, val_labels = prepare_data_intent(new_hidden_states_tensor)\n",
    "\n",
    "# Define model parameters\n",
    "input_size = train_inputs.size(1)\n",
    "num_classes = 2  # Time-sensitive or not\n",
    "epochs = 500\n",
    "lr = 1e-4\n",
    "\n",
    "# Train the model\n",
    "model = train_mlp_classifier_cuda(train_inputs, train_labels, val_inputs, val_labels, input_size, num_classes, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `question` in template\n",
      "Length new_dataset:  26000\n",
      "Epoch 1, Loss: 0.6934804320335388, Val Loss: 0.6935790777206421\n",
      "Epoch 2, Loss: 0.6934183239936829, Val Loss: 0.6935204863548279\n",
      "Epoch 3, Loss: 0.6933579444885254, Val Loss: 0.6934630274772644\n",
      "Epoch 4, Loss: 0.6932995319366455, Val Loss: 0.6934062242507935\n",
      "Epoch 5, Loss: 0.6932418346405029, Val Loss: 0.6933478713035583\n",
      "Epoch 6, Loss: 0.693183183670044, Val Loss: 0.6932879686355591\n",
      "Epoch 7, Loss: 0.6931222081184387, Val Loss: 0.6932252049446106\n",
      "Epoch 8, Loss: 0.6930578947067261, Val Loss: 0.6931588053703308\n",
      "Epoch 9, Loss: 0.6929892897605896, Val Loss: 0.6930885910987854\n",
      "Epoch 10, Loss: 0.6929159164428711, Val Loss: 0.6930136680603027\n",
      "Epoch 11, Loss: 0.692837655544281, Val Loss: 0.6929340362548828\n",
      "Epoch 12, Loss: 0.6927538514137268, Val Loss: 0.6928495764732361\n",
      "Epoch 13, Loss: 0.6926649808883667, Val Loss: 0.6927599906921387\n",
      "Epoch 14, Loss: 0.6925706267356873, Val Loss: 0.6926655173301697\n",
      "Epoch 15, Loss: 0.6924707293510437, Val Loss: 0.6925655007362366\n",
      "Epoch 16, Loss: 0.6923643946647644, Val Loss: 0.692458987236023\n",
      "Epoch 17, Loss: 0.6922508478164673, Val Loss: 0.6923449039459229\n",
      "Epoch 18, Loss: 0.6921290159225464, Val Loss: 0.6922234892845154\n",
      "Epoch 19, Loss: 0.6919984817504883, Val Loss: 0.6920943260192871\n",
      "Epoch 20, Loss: 0.6918589472770691, Val Loss: 0.6919561624526978\n",
      "Epoch 21, Loss: 0.6917091608047485, Val Loss: 0.6918074488639832\n",
      "Epoch 22, Loss: 0.6915481090545654, Val Loss: 0.6916471719741821\n",
      "Epoch 23, Loss: 0.6913747191429138, Val Loss: 0.6914738416671753\n",
      "Epoch 24, Loss: 0.6911879777908325, Val Loss: 0.6912867426872253\n",
      "Epoch 25, Loss: 0.6909865140914917, Val Loss: 0.6910854578018188\n",
      "Epoch 26, Loss: 0.6907692551612854, Val Loss: 0.6908687949180603\n",
      "Epoch 27, Loss: 0.6905346512794495, Val Loss: 0.6906351447105408\n",
      "Epoch 28, Loss: 0.6902813911437988, Val Loss: 0.6903828978538513\n",
      "Epoch 29, Loss: 0.6900082230567932, Val Loss: 0.6901106238365173\n",
      "Epoch 30, Loss: 0.6897141337394714, Val Loss: 0.6898162364959717\n",
      "Epoch 31, Loss: 0.6893969178199768, Val Loss: 0.6894996166229248\n",
      "Epoch 32, Loss: 0.6890559196472168, Val Loss: 0.6891597509384155\n",
      "Epoch 33, Loss: 0.6886891722679138, Val Loss: 0.688795804977417\n",
      "Epoch 34, Loss: 0.6882961988449097, Val Loss: 0.6884067058563232\n",
      "Epoch 35, Loss: 0.6878752708435059, Val Loss: 0.6879902482032776\n",
      "Epoch 36, Loss: 0.6874246001243591, Val Loss: 0.6875438690185547\n",
      "Epoch 37, Loss: 0.6869418621063232, Val Loss: 0.6870653629302979\n",
      "Epoch 38, Loss: 0.6864244937896729, Val Loss: 0.6865534782409668\n",
      "Epoch 39, Loss: 0.685871422290802, Val Loss: 0.6860065460205078\n",
      "Epoch 40, Loss: 0.685279905796051, Val Loss: 0.6854231357574463\n",
      "Epoch 41, Loss: 0.6846480369567871, Val Loss: 0.6848016977310181\n",
      "Epoch 42, Loss: 0.6839732527732849, Val Loss: 0.6841391921043396\n",
      "Epoch 43, Loss: 0.6832529902458191, Val Loss: 0.6834301948547363\n",
      "Epoch 44, Loss: 0.6824812889099121, Val Loss: 0.6826751232147217\n",
      "Epoch 45, Loss: 0.6816582679748535, Val Loss: 0.681869387626648\n",
      "Epoch 46, Loss: 0.6807799935340881, Val Loss: 0.6810101270675659\n",
      "Epoch 47, Loss: 0.679844856262207, Val Loss: 0.6800940036773682\n",
      "Epoch 48, Loss: 0.6788503527641296, Val Loss: 0.6791176795959473\n",
      "Epoch 49, Loss: 0.6777917742729187, Val Loss: 0.6780803203582764\n",
      "Epoch 50, Loss: 0.6766680479049683, Val Loss: 0.6769806146621704\n",
      "Epoch 51, Loss: 0.675476610660553, Val Loss: 0.6758162379264832\n",
      "Epoch 52, Loss: 0.6742121577262878, Val Loss: 0.6745839715003967\n",
      "Epoch 53, Loss: 0.6728699803352356, Val Loss: 0.6732820868492126\n",
      "Epoch 54, Loss: 0.6714485883712769, Val Loss: 0.6719046235084534\n",
      "Epoch 55, Loss: 0.6699411869049072, Val Loss: 0.6704496145248413\n",
      "Epoch 56, Loss: 0.6683467626571655, Val Loss: 0.6689132452011108\n",
      "Epoch 57, Loss: 0.6666592359542847, Val Loss: 0.6672897338867188\n",
      "Epoch 58, Loss: 0.6648750305175781, Val Loss: 0.6655703783035278\n",
      "Epoch 59, Loss: 0.6629867553710938, Val Loss: 0.6637510657310486\n",
      "Epoch 60, Loss: 0.6609915494918823, Val Loss: 0.6618242859840393\n",
      "Epoch 61, Loss: 0.6588805317878723, Val Loss: 0.6597862839698792\n",
      "Epoch 62, Loss: 0.6566491723060608, Val Loss: 0.65762859582901\n",
      "Epoch 63, Loss: 0.654288113117218, Val Loss: 0.6553515791893005\n",
      "Epoch 64, Loss: 0.651793360710144, Val Loss: 0.6529539823532104\n",
      "Epoch 65, Loss: 0.6491642594337463, Val Loss: 0.6504355669021606\n",
      "Epoch 66, Loss: 0.6464042067527771, Val Loss: 0.6477845907211304\n",
      "Epoch 67, Loss: 0.6435003876686096, Val Loss: 0.6450019478797913\n",
      "Epoch 68, Loss: 0.6404500007629395, Val Loss: 0.6420920491218567\n",
      "Epoch 69, Loss: 0.6372506022453308, Val Loss: 0.6390584707260132\n",
      "Epoch 70, Loss: 0.6339066624641418, Val Loss: 0.6358940601348877\n",
      "Epoch 71, Loss: 0.6304116249084473, Val Loss: 0.6326074004173279\n",
      "Epoch 72, Loss: 0.6267787218093872, Val Loss: 0.629207968711853\n",
      "Epoch 73, Loss: 0.6230150461196899, Val Loss: 0.6256868243217468\n",
      "Epoch 74, Loss: 0.6191120743751526, Val Loss: 0.6220448017120361\n",
      "Epoch 75, Loss: 0.6150678396224976, Val Loss: 0.6182762384414673\n",
      "Epoch 76, Loss: 0.610880970954895, Val Loss: 0.6143779754638672\n",
      "Epoch 77, Loss: 0.6065472364425659, Val Loss: 0.6103567481040955\n",
      "Epoch 78, Loss: 0.602065920829773, Val Loss: 0.6062144637107849\n",
      "Epoch 79, Loss: 0.5974364876747131, Val Loss: 0.6019531488418579\n",
      "Epoch 80, Loss: 0.5926615595817566, Val Loss: 0.5975586175918579\n",
      "Epoch 81, Loss: 0.5877365469932556, Val Loss: 0.5930333733558655\n",
      "Epoch 82, Loss: 0.5826621651649475, Val Loss: 0.5883894562721252\n",
      "Epoch 83, Loss: 0.5774410963058472, Val Loss: 0.5836310982704163\n",
      "Epoch 84, Loss: 0.5720828771591187, Val Loss: 0.5787652134895325\n",
      "Epoch 85, Loss: 0.5665906071662903, Val Loss: 0.5738056302070618\n",
      "Epoch 86, Loss: 0.5609795451164246, Val Loss: 0.568764865398407\n",
      "Epoch 87, Loss: 0.5552547574043274, Val Loss: 0.5636471509933472\n",
      "Epoch 88, Loss: 0.5494171380996704, Val Loss: 0.5584607124328613\n",
      "Epoch 89, Loss: 0.5434756875038147, Val Loss: 0.553219199180603\n",
      "Epoch 90, Loss: 0.5374487042427063, Val Loss: 0.5479148030281067\n",
      "Epoch 91, Loss: 0.5313405394554138, Val Loss: 0.5425562262535095\n",
      "Epoch 92, Loss: 0.5251516103744507, Val Loss: 0.5371496081352234\n",
      "Epoch 93, Loss: 0.5188941359519958, Val Loss: 0.5317162871360779\n",
      "Epoch 94, Loss: 0.5125764012336731, Val Loss: 0.5262587070465088\n",
      "Epoch 95, Loss: 0.5061989426612854, Val Loss: 0.5207999348640442\n",
      "Epoch 96, Loss: 0.49977195262908936, Val Loss: 0.5153265595436096\n",
      "Epoch 97, Loss: 0.49331486225128174, Val Loss: 0.5099090337753296\n",
      "Epoch 98, Loss: 0.4868432283401489, Val Loss: 0.5044686198234558\n",
      "Epoch 99, Loss: 0.4803764522075653, Val Loss: 0.49919626116752625\n",
      "Epoch 100, Loss: 0.47393107414245605, Val Loss: 0.49381375312805176\n",
      "Epoch 101, Loss: 0.46751004457473755, Val Loss: 0.4886516034603119\n",
      "Epoch 102, Loss: 0.4610978364944458, Val Loss: 0.48342689871788025\n",
      "Epoch 103, Loss: 0.4547152519226074, Val Loss: 0.4783267378807068\n",
      "Epoch 104, Loss: 0.4483805000782013, Val Loss: 0.47340333461761475\n",
      "Epoch 105, Loss: 0.44209083914756775, Val Loss: 0.46837317943573\n",
      "Epoch 106, Loss: 0.43583694100379944, Val Loss: 0.46360060572624207\n",
      "Epoch 107, Loss: 0.42963191866874695, Val Loss: 0.45880764722824097\n",
      "Epoch 108, Loss: 0.42347726225852966, Val Loss: 0.45409709215164185\n",
      "Epoch 109, Loss: 0.4173763394355774, Val Loss: 0.4495737850666046\n",
      "Epoch 110, Loss: 0.41132593154907227, Val Loss: 0.4449309706687927\n",
      "Epoch 111, Loss: 0.4053177237510681, Val Loss: 0.4405892789363861\n",
      "Epoch 112, Loss: 0.39935195446014404, Val Loss: 0.4361381530761719\n",
      "Epoch 113, Loss: 0.39342987537384033, Val Loss: 0.43187278509140015\n",
      "Epoch 114, Loss: 0.38755297660827637, Val Loss: 0.42765912413597107\n",
      "Epoch 115, Loss: 0.38172823190689087, Val Loss: 0.4234948754310608\n",
      "Epoch 116, Loss: 0.37595686316490173, Val Loss: 0.4195275902748108\n",
      "Epoch 117, Loss: 0.37024861574172974, Val Loss: 0.4154522716999054\n",
      "Epoch 118, Loss: 0.36460360884666443, Val Loss: 0.41177523136138916\n",
      "Epoch 119, Loss: 0.3590242266654968, Val Loss: 0.4077323079109192\n",
      "Epoch 120, Loss: 0.35353362560272217, Val Loss: 0.4044727385044098\n",
      "Epoch 121, Loss: 0.3481085002422333, Val Loss: 0.4004446566104889\n",
      "Epoch 122, Loss: 0.3427208662033081, Val Loss: 0.39718952775001526\n",
      "Epoch 123, Loss: 0.3373797833919525, Val Loss: 0.3937755227088928\n",
      "Epoch 124, Loss: 0.3321857154369354, Val Loss: 0.3902111351490021\n",
      "Epoch 125, Loss: 0.3271101117134094, Val Loss: 0.3874110281467438\n",
      "Epoch 126, Loss: 0.3220469653606415, Val Loss: 0.3838815689086914\n",
      "Epoch 127, Loss: 0.31701532006263733, Val Loss: 0.38090386986732483\n",
      "Epoch 128, Loss: 0.3120725154876709, Val Loss: 0.37820127606391907\n",
      "Epoch 129, Loss: 0.30720779299736023, Val Loss: 0.374844491481781\n",
      "Epoch 130, Loss: 0.3023689091205597, Val Loss: 0.3724064528942108\n",
      "Epoch 131, Loss: 0.2975335121154785, Val Loss: 0.3694312274456024\n",
      "Epoch 132, Loss: 0.2927543520927429, Val Loss: 0.3665708601474762\n",
      "Epoch 133, Loss: 0.28804999589920044, Val Loss: 0.36436229944229126\n",
      "Epoch 134, Loss: 0.2833898365497589, Val Loss: 0.36126741766929626\n",
      "Epoch 135, Loss: 0.27875083684921265, Val Loss: 0.35905739665031433\n",
      "Epoch 136, Loss: 0.2741301655769348, Val Loss: 0.3564293384552002\n",
      "Epoch 137, Loss: 0.26955944299697876, Val Loss: 0.35383322834968567\n",
      "Epoch 138, Loss: 0.26504918932914734, Val Loss: 0.3518016040325165\n",
      "Epoch 139, Loss: 0.26058173179626465, Val Loss: 0.3490038514137268\n",
      "Epoch 140, Loss: 0.2561521828174591, Val Loss: 0.3471406102180481\n",
      "Epoch 141, Loss: 0.25174954533576965, Val Loss: 0.3444964587688446\n",
      "Epoch 142, Loss: 0.24738506972789764, Val Loss: 0.3425532579421997\n",
      "Epoch 143, Loss: 0.24306581914424896, Val Loss: 0.34024450182914734\n",
      "Epoch 144, Loss: 0.23879963159561157, Val Loss: 0.33801913261413574\n",
      "Epoch 145, Loss: 0.23458221554756165, Val Loss: 0.33612924814224243\n",
      "Epoch 146, Loss: 0.23040445148944855, Val Loss: 0.33355337381362915\n",
      "Epoch 147, Loss: 0.22627462446689606, Val Loss: 0.33218348026275635\n",
      "Epoch 148, Loss: 0.22218884527683258, Val Loss: 0.32928407192230225\n",
      "Epoch 149, Loss: 0.2181643843650818, Val Loss: 0.3282512128353119\n",
      "Epoch 150, Loss: 0.21415351331233978, Val Loss: 0.32530903816223145\n",
      "Epoch 151, Loss: 0.21015916764736176, Val Loss: 0.32394707202911377\n",
      "Epoch 152, Loss: 0.20620177686214447, Val Loss: 0.32183146476745605\n",
      "Epoch 153, Loss: 0.20234346389770508, Val Loss: 0.3197330832481384\n",
      "Epoch 154, Loss: 0.19858182966709137, Val Loss: 0.31860318779945374\n",
      "Epoch 155, Loss: 0.19487525522708893, Val Loss: 0.3160400688648224\n",
      "Epoch 156, Loss: 0.19119545817375183, Val Loss: 0.3151090145111084\n",
      "Epoch 157, Loss: 0.18752795457839966, Val Loss: 0.3129200041294098\n",
      "Epoch 158, Loss: 0.18391349911689758, Val Loss: 0.31137508153915405\n",
      "Epoch 159, Loss: 0.18036788702011108, Val Loss: 0.3101116716861725\n",
      "Epoch 160, Loss: 0.17688575387001038, Val Loss: 0.30797073245048523\n",
      "Epoch 161, Loss: 0.1734527200460434, Val Loss: 0.3072049617767334\n",
      "Epoch 162, Loss: 0.17004945874214172, Val Loss: 0.30518168210983276\n",
      "Epoch 163, Loss: 0.16668464243412018, Val Loss: 0.3041281998157501\n",
      "Epoch 164, Loss: 0.16336730122566223, Val Loss: 0.30274373292922974\n",
      "Epoch 165, Loss: 0.16010817885398865, Val Loss: 0.30117252469062805\n",
      "Epoch 166, Loss: 0.15690460801124573, Val Loss: 0.3003651797771454\n",
      "Epoch 167, Loss: 0.15375101566314697, Val Loss: 0.2985074818134308\n",
      "Epoch 168, Loss: 0.1506471186876297, Val Loss: 0.29794594645500183\n",
      "Epoch 169, Loss: 0.14758820831775665, Val Loss: 0.29604867100715637\n",
      "Epoch 170, Loss: 0.144579216837883, Val Loss: 0.2955804467201233\n",
      "Epoch 171, Loss: 0.14160196483135223, Val Loss: 0.2936803698539734\n",
      "Epoch 172, Loss: 0.138668954372406, Val Loss: 0.29315224289894104\n",
      "Epoch 173, Loss: 0.13576897978782654, Val Loss: 0.29149213433265686\n",
      "Epoch 174, Loss: 0.1329209953546524, Val Loss: 0.29078155755996704\n",
      "Epoch 175, Loss: 0.1301286220550537, Val Loss: 0.289584219455719\n",
      "Epoch 176, Loss: 0.12738783657550812, Val Loss: 0.28853899240493774\n",
      "Epoch 177, Loss: 0.12469232082366943, Val Loss: 0.28782325983047485\n",
      "Epoch 178, Loss: 0.12204508483409882, Val Loss: 0.28645098209381104\n",
      "Epoch 179, Loss: 0.11944643408060074, Val Loss: 0.28621360659599304\n",
      "Epoch 180, Loss: 0.11690186709165573, Val Loss: 0.28456827998161316\n",
      "Epoch 181, Loss: 0.11442172527313232, Val Loss: 0.28474631905555725\n",
      "Epoch 182, Loss: 0.11200228333473206, Val Loss: 0.2829211950302124\n",
      "Epoch 183, Loss: 0.10965517163276672, Val Loss: 0.28335076570510864\n",
      "Epoch 184, Loss: 0.10731673985719681, Val Loss: 0.28163713216781616\n",
      "Epoch 185, Loss: 0.1050003170967102, Val Loss: 0.2816494107246399\n",
      "Epoch 186, Loss: 0.10270172357559204, Val Loss: 0.28079408407211304\n",
      "Epoch 187, Loss: 0.10050474852323532, Val Loss: 0.2800275385379791\n",
      "Epoch 188, Loss: 0.0984077900648117, Val Loss: 0.2801765203475952\n",
      "Epoch 189, Loss: 0.09635619819164276, Val Loss: 0.2788769006729126\n",
      "Epoch 190, Loss: 0.09431497752666473, Val Loss: 0.2790592908859253\n",
      "Epoch 191, Loss: 0.09228703379631042, Val Loss: 0.2783108949661255\n",
      "Epoch 192, Loss: 0.09033096581697464, Val Loss: 0.27782294154167175\n",
      "Epoch 193, Loss: 0.08845590054988861, Val Loss: 0.278043657541275\n",
      "Epoch 194, Loss: 0.08662140369415283, Val Loss: 0.2771590054035187\n",
      "Epoch 195, Loss: 0.08480791002511978, Val Loss: 0.27733495831489563\n",
      "Epoch 196, Loss: 0.08302363753318787, Val Loss: 0.27716323733329773\n",
      "Epoch 197, Loss: 0.0813024491071701, Val Loss: 0.2766644358634949\n",
      "Epoch 198, Loss: 0.07963836938142776, Val Loss: 0.27705609798431396\n",
      "Epoch 199, Loss: 0.07799702137708664, Val Loss: 0.2765067219734192\n",
      "Epoch 200, Loss: 0.07638038694858551, Val Loss: 0.27643999457359314\n",
      "Epoch 201, Loss: 0.07481332868337631, Val Loss: 0.27664655447006226\n",
      "Epoch 202, Loss: 0.07329770177602768, Val Loss: 0.276149183511734\n",
      "Epoch 203, Loss: 0.07180825620889664, Val Loss: 0.27636682987213135\n",
      "Epoch 204, Loss: 0.07034372538328171, Val Loss: 0.27634260058403015\n",
      "Epoch 205, Loss: 0.06892017275094986, Val Loss: 0.2760949432849884\n",
      "Epoch 206, Loss: 0.06753768026828766, Val Loss: 0.27641987800598145\n",
      "Epoch 207, Loss: 0.06617845594882965, Val Loss: 0.27633893489837646\n",
      "Epoch 208, Loss: 0.06484735012054443, Val Loss: 0.27628177404403687\n",
      "Epoch 209, Loss: 0.06355004757642746, Val Loss: 0.2765830159187317\n",
      "Epoch 210, Loss: 0.06228290870785713, Val Loss: 0.2764478027820587\n"
     ]
    }
   ],
   "source": [
    "from flashrag.prompt import PromptTemplate\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "# Set configuration for the new dataset\n",
    "config_new_dataset = set_config(dataset_name='final_intent', split=split)\n",
    "template_new_dataset = PromptTemplate(\n",
    "    config=config_new_dataset,\n",
    "    system_prompt=\"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "    user_prompt=\"Question: {question}\"\n",
    ")\n",
    "\n",
    "# Loading hidden states\n",
    "loading_hidden = True\n",
    "if not loading_hidden:\n",
    "    new_hidden_states = load_datasets(config_new_dataset, template_new_dataset)\n",
    "else:\n",
    "    file_path_new_dataset = '/cs/student/projects2/dsml/cdiezmar/hidden_states/final_intent.pkl'\n",
    "\n",
    "    with open(file_path_new_dataset, 'rb') as file_new:\n",
    "        new_hidden_states = pickle.load(file_new)\n",
    "        print('Length new_dataset: ', len(new_hidden_states))\n",
    "        new_hidden_states_tensor = torch.tensor(new_hidden_states, dtype=torch.float32)\n",
    "\n",
    "# Prepare data\n",
    "train_inputs, train_labels, val_inputs, val_labels = prepare_data_intent(new_hidden_states_tensor)\n",
    "\n",
    "# Define model parameters\n",
    "input_size = train_inputs.size(1)\n",
    "num_classes = 2  # Time-sensitive or not\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "epochs = 210\n",
    "lr = 1e-4\n",
    "\n",
    "# Train LSTM Classifier\n",
    "lstm_model = train_lstm_classifier_cuda(train_inputs, train_labels, val_inputs, val_labels, input_size, hidden_size, num_layers, num_classes, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `question` in template\n",
      "Length new_dataset:  26000\n",
      "Epoch 1, Loss: 0.6931670904159546, Val Loss: 0.6928743720054626\n",
      "Epoch 2, Loss: 0.6928393840789795, Val Loss: 0.6925790905952454\n",
      "Epoch 3, Loss: 0.6925147175788879, Val Loss: 0.6922849416732788\n",
      "Epoch 4, Loss: 0.6921913623809814, Val Loss: 0.6919901967048645\n",
      "Epoch 5, Loss: 0.6918661594390869, Val Loss: 0.6916937232017517\n",
      "Epoch 6, Loss: 0.6915377974510193, Val Loss: 0.6913948655128479\n",
      "Epoch 7, Loss: 0.6912051439285278, Val Loss: 0.6910916566848755\n",
      "Epoch 8, Loss: 0.6908667683601379, Val Loss: 0.6907833814620972\n",
      "Epoch 9, Loss: 0.6905233263969421, Val Loss: 0.6904674172401428\n",
      "Epoch 10, Loss: 0.6901729106903076, Val Loss: 0.690142035484314\n",
      "Epoch 11, Loss: 0.6898141503334045, Val Loss: 0.689804196357727\n",
      "Epoch 12, Loss: 0.6894446015357971, Val Loss: 0.6894522905349731\n",
      "Epoch 13, Loss: 0.6890630722045898, Val Loss: 0.6890841722488403\n",
      "Epoch 14, Loss: 0.6886666417121887, Val Loss: 0.6887001395225525\n",
      "Epoch 15, Loss: 0.6882541179656982, Val Loss: 0.6883006691932678\n",
      "Epoch 16, Loss: 0.6878247857093811, Val Loss: 0.68788743019104\n",
      "Epoch 17, Loss: 0.6873793601989746, Val Loss: 0.6874602437019348\n",
      "Epoch 18, Loss: 0.6869187355041504, Val Loss: 0.6870178580284119\n",
      "Epoch 19, Loss: 0.6864423751831055, Val Loss: 0.6865591406822205\n",
      "Epoch 20, Loss: 0.6859493255615234, Val Loss: 0.6860826015472412\n",
      "Epoch 21, Loss: 0.6854377388954163, Val Loss: 0.68558669090271\n",
      "Epoch 22, Loss: 0.6849052906036377, Val Loss: 0.6850693821907043\n",
      "Epoch 23, Loss: 0.6843494176864624, Val Loss: 0.6845279932022095\n",
      "Epoch 24, Loss: 0.683767557144165, Val Loss: 0.6839637756347656\n",
      "Epoch 25, Loss: 0.6831613183021545, Val Loss: 0.683373212814331\n",
      "Epoch 26, Loss: 0.6825277209281921, Val Loss: 0.6827561855316162\n",
      "Epoch 27, Loss: 0.6818665266036987, Val Loss: 0.6821120381355286\n",
      "Epoch 28, Loss: 0.68117755651474, Val Loss: 0.6814400553703308\n",
      "Epoch 29, Loss: 0.6804595589637756, Val Loss: 0.6807405352592468\n",
      "Epoch 30, Loss: 0.6797130107879639, Val Loss: 0.6800126433372498\n",
      "Epoch 31, Loss: 0.6789359450340271, Val Loss: 0.6792578101158142\n",
      "Epoch 32, Loss: 0.6781280040740967, Val Loss: 0.6784748435020447\n",
      "Epoch 33, Loss: 0.6772878170013428, Val Loss: 0.6776599287986755\n",
      "Epoch 34, Loss: 0.6764121651649475, Val Loss: 0.6768112182617188\n",
      "Epoch 35, Loss: 0.675497829914093, Val Loss: 0.6759275197982788\n",
      "Epoch 36, Loss: 0.6745442748069763, Val Loss: 0.6750078797340393\n",
      "Epoch 37, Loss: 0.6735494136810303, Val Loss: 0.6740487813949585\n",
      "Epoch 38, Loss: 0.672512412071228, Val Loss: 0.6730480790138245\n",
      "Epoch 39, Loss: 0.6714314818382263, Val Loss: 0.6720035076141357\n",
      "Epoch 40, Loss: 0.6703048348426819, Val Loss: 0.6709120273590088\n",
      "Epoch 41, Loss: 0.669127881526947, Val Loss: 0.6697763204574585\n",
      "Epoch 42, Loss: 0.6679033041000366, Val Loss: 0.6685931086540222\n",
      "Epoch 43, Loss: 0.6666268706321716, Val Loss: 0.6673600077629089\n",
      "Epoch 44, Loss: 0.6652963757514954, Val Loss: 0.6660767197608948\n",
      "Epoch 45, Loss: 0.663913369178772, Val Loss: 0.664740264415741\n",
      "Epoch 46, Loss: 0.6624755859375, Val Loss: 0.6633487939834595\n",
      "Epoch 47, Loss: 0.6609817147254944, Val Loss: 0.661903440952301\n",
      "Epoch 48, Loss: 0.6594318747520447, Val Loss: 0.6603987216949463\n",
      "Epoch 49, Loss: 0.6578189134597778, Val Loss: 0.6588358283042908\n",
      "Epoch 50, Loss: 0.6561461687088013, Val Loss: 0.6572151780128479\n",
      "Epoch 51, Loss: 0.6544123291969299, Val Loss: 0.6555374264717102\n",
      "Epoch 52, Loss: 0.652614951133728, Val Loss: 0.6538061499595642\n",
      "Epoch 53, Loss: 0.6507555246353149, Val Loss: 0.6520165205001831\n",
      "Epoch 54, Loss: 0.6488291025161743, Val Loss: 0.6501681208610535\n",
      "Epoch 55, Loss: 0.6468361020088196, Val Loss: 0.6482570171356201\n",
      "Epoch 56, Loss: 0.6447718143463135, Val Loss: 0.6462843418121338\n",
      "Epoch 57, Loss: 0.6426385641098022, Val Loss: 0.6442524790763855\n",
      "Epoch 58, Loss: 0.6404382586479187, Val Loss: 0.6421537399291992\n",
      "Epoch 59, Loss: 0.6381629109382629, Val Loss: 0.6399885416030884\n",
      "Epoch 60, Loss: 0.6358136534690857, Val Loss: 0.637755811214447\n",
      "Epoch 61, Loss: 0.6333884596824646, Val Loss: 0.6354570984840393\n",
      "Epoch 62, Loss: 0.6308879852294922, Val Loss: 0.6330885291099548\n",
      "Epoch 63, Loss: 0.628307044506073, Val Loss: 0.6306504011154175\n",
      "Epoch 64, Loss: 0.6256465315818787, Val Loss: 0.6281426548957825\n",
      "Epoch 65, Loss: 0.6229052543640137, Val Loss: 0.6255671977996826\n",
      "Epoch 66, Loss: 0.6200830340385437, Val Loss: 0.6229188442230225\n",
      "Epoch 67, Loss: 0.6171767711639404, Val Loss: 0.6201926469802856\n",
      "Epoch 68, Loss: 0.6141839027404785, Val Loss: 0.6174030900001526\n",
      "Epoch 69, Loss: 0.6111194491386414, Val Loss: 0.6145414710044861\n",
      "Epoch 70, Loss: 0.607974648475647, Val Loss: 0.611608624458313\n",
      "Epoch 71, Loss: 0.6047508716583252, Val Loss: 0.6086142659187317\n",
      "Epoch 72, Loss: 0.6014593839645386, Val Loss: 0.6055586934089661\n",
      "Epoch 73, Loss: 0.5980995893478394, Val Loss: 0.6024425625801086\n",
      "Epoch 74, Loss: 0.594670832157135, Val Loss: 0.599278450012207\n",
      "Epoch 75, Loss: 0.5911827683448792, Val Loss: 0.5960572361946106\n",
      "Epoch 76, Loss: 0.5876224637031555, Val Loss: 0.592795193195343\n",
      "Epoch 77, Loss: 0.5840068459510803, Val Loss: 0.5894908308982849\n",
      "Epoch 78, Loss: 0.5803343653678894, Val Loss: 0.5861378908157349\n",
      "Epoch 79, Loss: 0.5765987038612366, Val Loss: 0.5827370285987854\n",
      "Epoch 80, Loss: 0.5728031396865845, Val Loss: 0.5792942047119141\n",
      "Epoch 81, Loss: 0.5689552426338196, Val Loss: 0.5758072137832642\n",
      "Epoch 82, Loss: 0.565056324005127, Val Loss: 0.5722693204879761\n",
      "Epoch 83, Loss: 0.5610965490341187, Val Loss: 0.5686878561973572\n",
      "Epoch 84, Loss: 0.5570838451385498, Val Loss: 0.5650611519813538\n",
      "Epoch 85, Loss: 0.553015410900116, Val Loss: 0.5614020824432373\n",
      "Epoch 86, Loss: 0.5489043593406677, Val Loss: 0.5577077865600586\n",
      "Epoch 87, Loss: 0.5447444319725037, Val Loss: 0.5539941191673279\n",
      "Epoch 88, Loss: 0.5405526161193848, Val Loss: 0.5502535104751587\n",
      "Epoch 89, Loss: 0.5363209247589111, Val Loss: 0.5464943647384644\n",
      "Epoch 90, Loss: 0.5320577025413513, Val Loss: 0.5427209138870239\n",
      "Epoch 91, Loss: 0.5277677178382874, Val Loss: 0.5389270782470703\n",
      "Epoch 92, Loss: 0.5234439373016357, Val Loss: 0.5351166725158691\n",
      "Epoch 93, Loss: 0.5190896987915039, Val Loss: 0.531303346157074\n",
      "Epoch 94, Loss: 0.5147163271903992, Val Loss: 0.5274825096130371\n",
      "Epoch 95, Loss: 0.5103180408477783, Val Loss: 0.5236592888832092\n",
      "Epoch 96, Loss: 0.5059004426002502, Val Loss: 0.5198420882225037\n",
      "Epoch 97, Loss: 0.5014683604240417, Val Loss: 0.516026496887207\n",
      "Epoch 98, Loss: 0.4970238208770752, Val Loss: 0.512224018573761\n",
      "Epoch 99, Loss: 0.4925753176212311, Val Loss: 0.5084298253059387\n",
      "Epoch 100, Loss: 0.48811811208724976, Val Loss: 0.5046524405479431\n",
      "Epoch 101, Loss: 0.4836669862270355, Val Loss: 0.5008846521377563\n",
      "Epoch 102, Loss: 0.47921010851860046, Val Loss: 0.4971350431442261\n",
      "Epoch 103, Loss: 0.47476357221603394, Val Loss: 0.4934036135673523\n",
      "Epoch 104, Loss: 0.47032108902931213, Val Loss: 0.4897001385688782\n",
      "Epoch 105, Loss: 0.46589234471321106, Val Loss: 0.48601943254470825\n",
      "Epoch 106, Loss: 0.4614681303501129, Val Loss: 0.482363760471344\n",
      "Epoch 107, Loss: 0.4570561945438385, Val Loss: 0.4787406921386719\n",
      "Epoch 108, Loss: 0.4526636600494385, Val Loss: 0.47514957189559937\n",
      "Epoch 109, Loss: 0.4482922852039337, Val Loss: 0.47158437967300415\n",
      "Epoch 110, Loss: 0.4439312815666199, Val Loss: 0.46805745363235474\n",
      "Epoch 111, Loss: 0.43959659337997437, Val Loss: 0.46455472707748413\n",
      "Epoch 112, Loss: 0.4352720379829407, Val Loss: 0.4610918164253235\n",
      "Epoch 113, Loss: 0.4309782087802887, Val Loss: 0.45766621828079224\n",
      "Epoch 114, Loss: 0.426706999540329, Val Loss: 0.4542772173881531\n",
      "Epoch 115, Loss: 0.42246097326278687, Val Loss: 0.45093321800231934\n",
      "Epoch 116, Loss: 0.41824623942375183, Val Loss: 0.44762614369392395\n",
      "Epoch 117, Loss: 0.4140629768371582, Val Loss: 0.44436749815940857\n",
      "Epoch 118, Loss: 0.4099120497703552, Val Loss: 0.44116249680519104\n",
      "Epoch 119, Loss: 0.4057949483394623, Val Loss: 0.43800392746925354\n",
      "Epoch 120, Loss: 0.4017123281955719, Val Loss: 0.43489378690719604\n",
      "Epoch 121, Loss: 0.3976653516292572, Val Loss: 0.4318167269229889\n",
      "Epoch 122, Loss: 0.393645703792572, Val Loss: 0.4287757873535156\n",
      "Epoch 123, Loss: 0.38965290784835815, Val Loss: 0.42577555775642395\n",
      "Epoch 124, Loss: 0.3856956958770752, Val Loss: 0.42280638217926025\n",
      "Epoch 125, Loss: 0.38176727294921875, Val Loss: 0.41987356543540955\n",
      "Epoch 126, Loss: 0.37786760926246643, Val Loss: 0.41698288917541504\n",
      "Epoch 127, Loss: 0.3739989399909973, Val Loss: 0.4141272008419037\n",
      "Epoch 128, Loss: 0.37015804648399353, Val Loss: 0.4113108515739441\n",
      "Epoch 129, Loss: 0.3663441240787506, Val Loss: 0.40854573249816895\n",
      "Epoch 130, Loss: 0.36256179213523865, Val Loss: 0.40583211183547974\n",
      "Epoch 131, Loss: 0.3588119149208069, Val Loss: 0.40316280722618103\n",
      "Epoch 132, Loss: 0.355087012052536, Val Loss: 0.400543212890625\n",
      "Epoch 133, Loss: 0.35138994455337524, Val Loss: 0.39797091484069824\n",
      "Epoch 134, Loss: 0.34772175550460815, Val Loss: 0.39544668793678284\n",
      "Epoch 135, Loss: 0.34407806396484375, Val Loss: 0.39296019077301025\n",
      "Epoch 136, Loss: 0.34046342968940735, Val Loss: 0.39051544666290283\n",
      "Epoch 137, Loss: 0.3368716537952423, Val Loss: 0.388107568025589\n",
      "Epoch 138, Loss: 0.33330509066581726, Val Loss: 0.38573554158210754\n",
      "Epoch 139, Loss: 0.32976165413856506, Val Loss: 0.38340556621551514\n",
      "Epoch 140, Loss: 0.32624396681785583, Val Loss: 0.3811059296131134\n",
      "Epoch 141, Loss: 0.32274389266967773, Val Loss: 0.3788415193557739\n",
      "Epoch 142, Loss: 0.3192627429962158, Val Loss: 0.37661653757095337\n",
      "Epoch 143, Loss: 0.31580957770347595, Val Loss: 0.37442874908447266\n",
      "Epoch 144, Loss: 0.31237828731536865, Val Loss: 0.37227436900138855\n",
      "Epoch 145, Loss: 0.3089640140533447, Val Loss: 0.3701535165309906\n",
      "Epoch 146, Loss: 0.30556660890579224, Val Loss: 0.3680807948112488\n",
      "Epoch 147, Loss: 0.3021870255470276, Val Loss: 0.36604174971580505\n",
      "Epoch 148, Loss: 0.2988246977329254, Val Loss: 0.3640320897102356\n",
      "Epoch 149, Loss: 0.29548054933547974, Val Loss: 0.36206337809562683\n",
      "Epoch 150, Loss: 0.2921559810638428, Val Loss: 0.360127717256546\n",
      "Epoch 151, Loss: 0.288850337266922, Val Loss: 0.35820770263671875\n",
      "Epoch 152, Loss: 0.28556370735168457, Val Loss: 0.35630738735198975\n",
      "Epoch 153, Loss: 0.28229087591171265, Val Loss: 0.35443732142448425\n",
      "Epoch 154, Loss: 0.279035747051239, Val Loss: 0.3526002764701843\n",
      "Epoch 155, Loss: 0.27580025792121887, Val Loss: 0.3508075773715973\n",
      "Epoch 156, Loss: 0.2725861668586731, Val Loss: 0.3490664064884186\n",
      "Epoch 157, Loss: 0.26939234137535095, Val Loss: 0.3473626375198364\n",
      "Epoch 158, Loss: 0.2662241458892822, Val Loss: 0.34569355845451355\n",
      "Epoch 159, Loss: 0.26307570934295654, Val Loss: 0.3440558910369873\n",
      "Epoch 160, Loss: 0.25995349884033203, Val Loss: 0.342446506023407\n",
      "Epoch 161, Loss: 0.25685346126556396, Val Loss: 0.34085896611213684\n",
      "Epoch 162, Loss: 0.2537689805030823, Val Loss: 0.3393059968948364\n",
      "Epoch 163, Loss: 0.2507067918777466, Val Loss: 0.33778101205825806\n",
      "Epoch 164, Loss: 0.24766352772712708, Val Loss: 0.33627691864967346\n",
      "Epoch 165, Loss: 0.24463388323783875, Val Loss: 0.33480265736579895\n",
      "Epoch 166, Loss: 0.241624116897583, Val Loss: 0.33335861563682556\n",
      "Epoch 167, Loss: 0.23863163590431213, Val Loss: 0.33192622661590576\n",
      "Epoch 168, Loss: 0.23565605282783508, Val Loss: 0.3305310308933258\n",
      "Epoch 169, Loss: 0.23270313441753387, Val Loss: 0.329149067401886\n",
      "Epoch 170, Loss: 0.2297631800174713, Val Loss: 0.32779446244239807\n",
      "Epoch 171, Loss: 0.22684018313884735, Val Loss: 0.3264722228050232\n",
      "Epoch 172, Loss: 0.2239394634962082, Val Loss: 0.3251596987247467\n",
      "Epoch 173, Loss: 0.221059188246727, Val Loss: 0.3238976001739502\n",
      "Epoch 174, Loss: 0.21819987893104553, Val Loss: 0.32263872027397156\n",
      "Epoch 175, Loss: 0.21535536646842957, Val Loss: 0.321416974067688\n",
      "Epoch 176, Loss: 0.21253423392772675, Val Loss: 0.320220947265625\n",
      "Epoch 177, Loss: 0.20973479747772217, Val Loss: 0.3190431296825409\n",
      "Epoch 178, Loss: 0.20695431530475616, Val Loss: 0.31790104508399963\n",
      "Epoch 179, Loss: 0.20419593155384064, Val Loss: 0.3167760670185089\n",
      "Epoch 180, Loss: 0.2014566957950592, Val Loss: 0.31568148732185364\n",
      "Epoch 181, Loss: 0.1987360566854477, Val Loss: 0.31461891531944275\n",
      "Epoch 182, Loss: 0.19603756070137024, Val Loss: 0.31356897950172424\n",
      "Epoch 183, Loss: 0.19336114823818207, Val Loss: 0.31254857778549194\n",
      "Epoch 184, Loss: 0.1907007247209549, Val Loss: 0.3115449547767639\n",
      "Epoch 185, Loss: 0.18806564807891846, Val Loss: 0.3105742633342743\n",
      "Epoch 186, Loss: 0.18545253574848175, Val Loss: 0.3096196949481964\n",
      "Epoch 187, Loss: 0.18285848200321198, Val Loss: 0.3086931109428406\n",
      "Epoch 188, Loss: 0.18028399348258972, Val Loss: 0.3077954947948456\n",
      "Epoch 189, Loss: 0.1777365505695343, Val Loss: 0.3068993389606476\n",
      "Epoch 190, Loss: 0.17520833015441895, Val Loss: 0.3060522973537445\n",
      "Epoch 191, Loss: 0.17270506918430328, Val Loss: 0.3051890730857849\n",
      "Epoch 192, Loss: 0.17022038996219635, Val Loss: 0.3043901324272156\n",
      "Epoch 193, Loss: 0.16776148974895477, Val Loss: 0.30355018377304077\n",
      "Epoch 194, Loss: 0.165325328707695, Val Loss: 0.3028053343296051\n",
      "Epoch 195, Loss: 0.16291306912899017, Val Loss: 0.30199065804481506\n",
      "Epoch 196, Loss: 0.16052773594856262, Val Loss: 0.3013201951980591\n",
      "Epoch 197, Loss: 0.1581699252128601, Val Loss: 0.3005003333091736\n",
      "Epoch 198, Loss: 0.1558457911014557, Val Loss: 0.2999255955219269\n",
      "Epoch 199, Loss: 0.15354977548122406, Val Loss: 0.29909154772758484\n",
      "Epoch 200, Loss: 0.1512647122144699, Val Loss: 0.29849231243133545\n",
      "Epoch 201, Loss: 0.1489848494529724, Val Loss: 0.2977740168571472\n",
      "Epoch 202, Loss: 0.14673849940299988, Val Loss: 0.29711246490478516\n",
      "Epoch 203, Loss: 0.144540935754776, Val Loss: 0.2965921461582184\n",
      "Epoch 204, Loss: 0.1423727124929428, Val Loss: 0.29588037729263306\n",
      "Epoch 205, Loss: 0.14021258056163788, Val Loss: 0.29534420371055603\n",
      "Epoch 206, Loss: 0.13805915415287018, Val Loss: 0.29473981261253357\n",
      "Epoch 207, Loss: 0.13593770563602448, Val Loss: 0.29414495825767517\n",
      "Epoch 208, Loss: 0.13385361433029175, Val Loss: 0.2936883866786957\n",
      "Epoch 209, Loss: 0.13179603219032288, Val Loss: 0.29305052757263184\n",
      "Epoch 210, Loss: 0.1297474205493927, Val Loss: 0.2925899028778076\n",
      "Epoch 211, Loss: 0.12771737575531006, Val Loss: 0.2920461893081665\n",
      "Epoch 212, Loss: 0.12571559846401215, Val Loss: 0.29155462980270386\n",
      "Epoch 213, Loss: 0.12375279515981674, Val Loss: 0.2911711037158966\n",
      "Epoch 214, Loss: 0.12181097269058228, Val Loss: 0.2906801104545593\n",
      "Epoch 215, Loss: 0.11988375335931778, Val Loss: 0.2903304398059845\n",
      "Epoch 216, Loss: 0.11797413975000381, Val Loss: 0.2899530827999115\n",
      "Epoch 217, Loss: 0.11609561741352081, Val Loss: 0.2895936071872711\n",
      "Epoch 218, Loss: 0.11424414068460464, Val Loss: 0.2893295884132385\n",
      "Epoch 219, Loss: 0.11241329461336136, Val Loss: 0.28896084427833557\n",
      "Epoch 220, Loss: 0.11059942096471786, Val Loss: 0.28871193528175354\n",
      "Epoch 221, Loss: 0.1088043600320816, Val Loss: 0.28837984800338745\n",
      "Epoch 222, Loss: 0.10703536123037338, Val Loss: 0.2880832254886627\n",
      "Epoch 223, Loss: 0.10529862344264984, Val Loss: 0.28781071305274963\n",
      "Epoch 224, Loss: 0.10358869284391403, Val Loss: 0.28748735785484314\n",
      "Epoch 225, Loss: 0.10190198570489883, Val Loss: 0.2872638404369354\n",
      "Epoch 226, Loss: 0.10024327039718628, Val Loss: 0.286963552236557\n",
      "Epoch 227, Loss: 0.098604217171669, Val Loss: 0.28676408529281616\n",
      "Epoch 228, Loss: 0.09698717296123505, Val Loss: 0.28649574518203735\n",
      "Epoch 229, Loss: 0.09539570659399033, Val Loss: 0.2862989604473114\n",
      "Epoch 230, Loss: 0.09382312744855881, Val Loss: 0.2860509753227234\n",
      "Epoch 231, Loss: 0.09227582812309265, Val Loss: 0.28586700558662415\n",
      "Epoch 232, Loss: 0.09074962139129639, Val Loss: 0.28565818071365356\n",
      "Epoch 233, Loss: 0.08924601227045059, Val Loss: 0.285496324300766\n",
      "Epoch 234, Loss: 0.08776228874921799, Val Loss: 0.28531911969184875\n",
      "Epoch 235, Loss: 0.08629677444696426, Val Loss: 0.28516536951065063\n",
      "Epoch 236, Loss: 0.08485544472932816, Val Loss: 0.285035640001297\n",
      "Epoch 237, Loss: 0.08343398571014404, Val Loss: 0.2849026620388031\n",
      "Epoch 238, Loss: 0.08203703910112381, Val Loss: 0.2847948968410492\n",
      "Epoch 239, Loss: 0.08065976202487946, Val Loss: 0.28469181060791016\n",
      "Epoch 240, Loss: 0.07930402457714081, Val Loss: 0.2845984101295471\n",
      "Epoch 241, Loss: 0.07796745747327805, Val Loss: 0.2845146656036377\n",
      "Epoch 242, Loss: 0.0766516700387001, Val Loss: 0.2844637930393219\n",
      "Epoch 243, Loss: 0.07535786926746368, Val Loss: 0.28438839316368103\n",
      "Epoch 244, Loss: 0.07408523559570312, Val Loss: 0.2843579947948456\n",
      "Epoch 245, Loss: 0.07283046841621399, Val Loss: 0.28429868817329407\n",
      "Epoch 246, Loss: 0.07159779220819473, Val Loss: 0.2843018174171448\n",
      "Epoch 247, Loss: 0.07038505375385284, Val Loss: 0.2842590808868408\n",
      "Epoch 248, Loss: 0.06919257342815399, Val Loss: 0.2843165397644043\n",
      "Epoch 249, Loss: 0.06801988929510117, Val Loss: 0.28425517678260803\n",
      "Epoch 250, Loss: 0.06687059998512268, Val Loss: 0.2844218909740448\n"
     ]
    }
   ],
   "source": [
    "from flashrag.prompt import PromptTemplate\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "# Set configuration for the new dataset\n",
    "config_new_dataset = set_config(dataset_name='final_intent', split=split)\n",
    "template_new_dataset = PromptTemplate(\n",
    "    config=config_new_dataset,\n",
    "    system_prompt=\"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "    user_prompt=\"Question: {question}\"\n",
    ")\n",
    "\n",
    "# Loading hidden states\n",
    "loading_hidden = True\n",
    "if not loading_hidden:\n",
    "    new_hidden_states = load_datasets(config_new_dataset, template_new_dataset)\n",
    "else:\n",
    "    file_path_new_dataset = '/cs/student/projects2/dsml/cdiezmar/hidden_states/final_intent.pkl'\n",
    "\n",
    "    with open(file_path_new_dataset, 'rb') as file_new:\n",
    "        new_hidden_states = pickle.load(file_new)\n",
    "        print('Length new_dataset: ', len(new_hidden_states))\n",
    "        new_hidden_states_tensor = torch.tensor(new_hidden_states, dtype=torch.float32)\n",
    "\n",
    "# Prepare data\n",
    "train_inputs, train_labels, val_inputs, val_labels = prepare_data_intent(new_hidden_states_tensor)\n",
    "\n",
    "# Define model parameters\n",
    "input_size = train_inputs.size(1)\n",
    "num_classes = 2  # Intent-sensitive or not\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "epochs = 250\n",
    "lr = 5e-5\n",
    "\n",
    "# Train GRU Classifier\n",
    "gru_model = train_gru_classifier_cuda(train_inputs, train_labels, val_inputs, val_labels, input_size, hidden_size, num_layers, num_classes, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `question` in template\n",
      "Length new_dataset:  26000\n",
      "Epoch 1, Loss: 0.6955057382583618, Val Loss: 0.6942139863967896\n",
      "Epoch 2, Loss: 0.6949785947799683, Val Loss: 0.6937206387519836\n",
      "Epoch 3, Loss: 0.6944542527198792, Val Loss: 0.6932360529899597\n",
      "Epoch 4, Loss: 0.6939404010772705, Val Loss: 0.6927513480186462\n",
      "Epoch 5, Loss: 0.6934249997138977, Val Loss: 0.6922734379768372\n",
      "Epoch 6, Loss: 0.6929158568382263, Val Loss: 0.6918005347251892\n",
      "Epoch 7, Loss: 0.6924132704734802, Val Loss: 0.6913344860076904\n",
      "Epoch 8, Loss: 0.6919152736663818, Val Loss: 0.690870463848114\n",
      "Epoch 9, Loss: 0.6914184093475342, Val Loss: 0.690411388874054\n",
      "Epoch 10, Loss: 0.69092857837677, Val Loss: 0.6899517178535461\n",
      "Epoch 11, Loss: 0.6904371380805969, Val Loss: 0.6895014643669128\n",
      "Epoch 12, Loss: 0.6899539232254028, Val Loss: 0.6890527606010437\n",
      "Epoch 13, Loss: 0.6894721388816833, Val Loss: 0.688607394695282\n",
      "Epoch 14, Loss: 0.6889947056770325, Val Loss: 0.6881648302078247\n",
      "Epoch 15, Loss: 0.6885178089141846, Val Loss: 0.6877236366271973\n",
      "Epoch 16, Loss: 0.6880416870117188, Val Loss: 0.687282145023346\n",
      "Epoch 17, Loss: 0.6875683069229126, Val Loss: 0.6868413686752319\n",
      "Epoch 18, Loss: 0.6870949864387512, Val Loss: 0.6864033937454224\n",
      "Epoch 19, Loss: 0.6866223812103271, Val Loss: 0.685969889163971\n",
      "Epoch 20, Loss: 0.6861546039581299, Val Loss: 0.6855370998382568\n",
      "Epoch 21, Loss: 0.6856865286827087, Val Loss: 0.6850987672805786\n",
      "Epoch 22, Loss: 0.6852148175239563, Val Loss: 0.684662938117981\n",
      "Epoch 23, Loss: 0.6847456097602844, Val Loss: 0.6842238903045654\n",
      "Epoch 24, Loss: 0.6842728853225708, Val Loss: 0.683788001537323\n",
      "Epoch 25, Loss: 0.6838036179542542, Val Loss: 0.683352530002594\n",
      "Epoch 26, Loss: 0.683334469795227, Val Loss: 0.6829164624214172\n",
      "Epoch 27, Loss: 0.6828653812408447, Val Loss: 0.6824759244918823\n",
      "Epoch 28, Loss: 0.6823915243148804, Val Loss: 0.6820343732833862\n",
      "Epoch 29, Loss: 0.681918203830719, Val Loss: 0.6815918684005737\n",
      "Epoch 30, Loss: 0.6814432740211487, Val Loss: 0.6811482310295105\n",
      "Epoch 31, Loss: 0.6809666752815247, Val Loss: 0.6807055473327637\n",
      "Epoch 32, Loss: 0.6804929971694946, Val Loss: 0.6802600622177124\n",
      "Epoch 33, Loss: 0.6800148487091064, Val Loss: 0.6798114776611328\n",
      "Epoch 34, Loss: 0.6795338988304138, Val Loss: 0.6793603301048279\n",
      "Epoch 35, Loss: 0.6790517568588257, Val Loss: 0.6789063215255737\n",
      "Epoch 36, Loss: 0.6785659790039062, Val Loss: 0.6784507632255554\n",
      "Epoch 37, Loss: 0.6780797243118286, Val Loss: 0.6779943108558655\n",
      "Epoch 38, Loss: 0.6775913238525391, Val Loss: 0.677534282207489\n",
      "Epoch 39, Loss: 0.6770998239517212, Val Loss: 0.6770753264427185\n",
      "Epoch 40, Loss: 0.6766069531440735, Val Loss: 0.6766068935394287\n",
      "Epoch 41, Loss: 0.6761081218719482, Val Loss: 0.6761394739151001\n",
      "Epoch 42, Loss: 0.6756094098091125, Val Loss: 0.6756660342216492\n",
      "Epoch 43, Loss: 0.6751037836074829, Val Loss: 0.6751884818077087\n",
      "Epoch 44, Loss: 0.674595832824707, Val Loss: 0.6747098565101624\n",
      "Epoch 45, Loss: 0.6740856170654297, Val Loss: 0.6742274165153503\n",
      "Epoch 46, Loss: 0.6735719442367554, Val Loss: 0.6737411022186279\n",
      "Epoch 47, Loss: 0.673055112361908, Val Loss: 0.6732525825500488\n",
      "Epoch 48, Loss: 0.6725356578826904, Val Loss: 0.6727573275566101\n",
      "Epoch 49, Loss: 0.6720095276832581, Val Loss: 0.6722589135169983\n",
      "Epoch 50, Loss: 0.6714812517166138, Val Loss: 0.6717573404312134\n",
      "Epoch 51, Loss: 0.670949399471283, Val Loss: 0.6712532043457031\n",
      "Epoch 52, Loss: 0.6704153418540955, Val Loss: 0.6707425117492676\n",
      "Epoch 53, Loss: 0.6698755621910095, Val Loss: 0.6702320575714111\n",
      "Epoch 54, Loss: 0.6693328619003296, Val Loss: 0.6697185635566711\n",
      "Epoch 55, Loss: 0.6687875390052795, Val Loss: 0.6691977381706238\n",
      "Epoch 56, Loss: 0.6682336330413818, Val Loss: 0.6686795949935913\n",
      "Epoch 57, Loss: 0.6676846742630005, Val Loss: 0.6681544184684753\n",
      "Epoch 58, Loss: 0.667125403881073, Val Loss: 0.6676313877105713\n",
      "Epoch 59, Loss: 0.6665670871734619, Val Loss: 0.6671042442321777\n",
      "Epoch 60, Loss: 0.6660030484199524, Val Loss: 0.6665735840797424\n",
      "Epoch 61, Loss: 0.665433943271637, Val Loss: 0.6660380959510803\n",
      "Epoch 62, Loss: 0.6648602485656738, Val Loss: 0.6655051112174988\n",
      "Epoch 63, Loss: 0.6642887592315674, Val Loss: 0.6649710536003113\n",
      "Epoch 64, Loss: 0.6637133955955505, Val Loss: 0.664432168006897\n",
      "Epoch 65, Loss: 0.6631346940994263, Val Loss: 0.6638913750648499\n",
      "Epoch 66, Loss: 0.662551999092102, Val Loss: 0.6633444428443909\n",
      "Epoch 67, Loss: 0.6619646549224854, Val Loss: 0.6628034114837646\n",
      "Epoch 68, Loss: 0.6613814234733582, Val Loss: 0.6622543334960938\n",
      "Epoch 69, Loss: 0.6607913374900818, Val Loss: 0.661701500415802\n",
      "Epoch 70, Loss: 0.660198450088501, Val Loss: 0.6611467599868774\n",
      "Epoch 71, Loss: 0.6596028208732605, Val Loss: 0.660586953163147\n",
      "Epoch 72, Loss: 0.6590043902397156, Val Loss: 0.6600291132926941\n",
      "Epoch 73, Loss: 0.6584062576293945, Val Loss: 0.6594632863998413\n",
      "Epoch 74, Loss: 0.6578007936477661, Val Loss: 0.658887505531311\n",
      "Epoch 75, Loss: 0.657187283039093, Val Loss: 0.6583122611045837\n",
      "Epoch 76, Loss: 0.6565726399421692, Val Loss: 0.6577356457710266\n",
      "Epoch 77, Loss: 0.6559557914733887, Val Loss: 0.6571468114852905\n",
      "Epoch 78, Loss: 0.6553260087966919, Val Loss: 0.656559407711029\n",
      "Epoch 79, Loss: 0.6546976566314697, Val Loss: 0.6559610962867737\n",
      "Epoch 80, Loss: 0.6540587544441223, Val Loss: 0.655357301235199\n",
      "Epoch 81, Loss: 0.6534130573272705, Val Loss: 0.6547473669052124\n",
      "Epoch 82, Loss: 0.6527595520019531, Val Loss: 0.6541351079940796\n",
      "Epoch 83, Loss: 0.6521037220954895, Val Loss: 0.653504490852356\n",
      "Epoch 84, Loss: 0.6514284610748291, Val Loss: 0.6528798341751099\n",
      "Epoch 85, Loss: 0.6507565975189209, Val Loss: 0.6522431373596191\n",
      "Epoch 86, Loss: 0.6500735282897949, Val Loss: 0.6515976190567017\n",
      "Epoch 87, Loss: 0.6493800282478333, Val Loss: 0.650947093963623\n",
      "Epoch 88, Loss: 0.6486802101135254, Val Loss: 0.6502928733825684\n",
      "Epoch 89, Loss: 0.6479745507240295, Val Loss: 0.6496275663375854\n",
      "Epoch 90, Loss: 0.6472575068473816, Val Loss: 0.648955225944519\n",
      "Epoch 91, Loss: 0.6465319395065308, Val Loss: 0.6482812762260437\n",
      "Epoch 92, Loss: 0.6458033919334412, Val Loss: 0.6475978493690491\n",
      "Epoch 93, Loss: 0.6450627446174622, Val Loss: 0.6469166874885559\n",
      "Epoch 94, Loss: 0.6443243622779846, Val Loss: 0.6462212204933167\n",
      "Epoch 95, Loss: 0.6435689926147461, Val Loss: 0.6455237865447998\n",
      "Epoch 96, Loss: 0.6428094506263733, Val Loss: 0.6448228359222412\n",
      "Epoch 97, Loss: 0.6420489549636841, Val Loss: 0.6441145539283752\n",
      "Epoch 98, Loss: 0.6412789225578308, Val Loss: 0.6434040665626526\n",
      "Epoch 99, Loss: 0.6405059099197388, Val Loss: 0.6426835656166077\n",
      "Epoch 100, Loss: 0.6397212743759155, Val Loss: 0.6419660449028015\n",
      "Epoch 101, Loss: 0.63894122838974, Val Loss: 0.6412326097488403\n",
      "Epoch 102, Loss: 0.6381469368934631, Val Loss: 0.6405035853385925\n",
      "Epoch 103, Loss: 0.6373571157455444, Val Loss: 0.6397613883018494\n",
      "Epoch 104, Loss: 0.636554479598999, Val Loss: 0.6390264630317688\n",
      "Epoch 105, Loss: 0.6357589960098267, Val Loss: 0.6382714509963989\n",
      "Epoch 106, Loss: 0.6349450945854187, Val Loss: 0.637522280216217\n",
      "Epoch 107, Loss: 0.6341365575790405, Val Loss: 0.6367675065994263\n",
      "Epoch 108, Loss: 0.6333231329917908, Val Loss: 0.6360057592391968\n",
      "Epoch 109, Loss: 0.6325017809867859, Val Loss: 0.6352487206459045\n",
      "Epoch 110, Loss: 0.6316866278648376, Val Loss: 0.6344740986824036\n",
      "Epoch 111, Loss: 0.630853533744812, Val Loss: 0.6337097883224487\n",
      "Epoch 112, Loss: 0.6300317049026489, Val Loss: 0.6329282522201538\n",
      "Epoch 113, Loss: 0.6291919946670532, Val Loss: 0.6321538090705872\n",
      "Epoch 114, Loss: 0.6283583045005798, Val Loss: 0.6313661336898804\n",
      "Epoch 115, Loss: 0.6275127530097961, Val Loss: 0.6305774450302124\n",
      "Epoch 116, Loss: 0.6266666650772095, Val Loss: 0.6297886371612549\n",
      "Epoch 117, Loss: 0.625816822052002, Val Loss: 0.6289938688278198\n",
      "Epoch 118, Loss: 0.6249649524688721, Val Loss: 0.6281901001930237\n",
      "Epoch 119, Loss: 0.6241006255149841, Val Loss: 0.6273878216743469\n",
      "Epoch 120, Loss: 0.623238742351532, Val Loss: 0.6265819668769836\n",
      "Epoch 121, Loss: 0.6223725080490112, Val Loss: 0.625771701335907\n",
      "Epoch 122, Loss: 0.6215002536773682, Val Loss: 0.6249565482139587\n",
      "Epoch 123, Loss: 0.6206236481666565, Val Loss: 0.6241363883018494\n",
      "Epoch 124, Loss: 0.6197397112846375, Val Loss: 0.6233079433441162\n",
      "Epoch 125, Loss: 0.6188464164733887, Val Loss: 0.6224819421768188\n",
      "Epoch 126, Loss: 0.617955207824707, Val Loss: 0.6216471195220947\n",
      "Epoch 127, Loss: 0.6170534491539001, Val Loss: 0.6208099722862244\n",
      "Epoch 128, Loss: 0.6161508560180664, Val Loss: 0.6199641227722168\n",
      "Epoch 129, Loss: 0.615236759185791, Val Loss: 0.6191221475601196\n",
      "Epoch 130, Loss: 0.614324688911438, Val Loss: 0.6182692646980286\n",
      "Epoch 131, Loss: 0.6133987307548523, Val Loss: 0.6174193620681763\n",
      "Epoch 132, Loss: 0.6124764084815979, Val Loss: 0.6165561676025391\n",
      "Epoch 133, Loss: 0.6115402579307556, Val Loss: 0.615702211856842\n",
      "Epoch 134, Loss: 0.6106129288673401, Val Loss: 0.6148255467414856\n",
      "Epoch 135, Loss: 0.6096586585044861, Val Loss: 0.6139547228813171\n",
      "Epoch 136, Loss: 0.6087114810943604, Val Loss: 0.6130806803703308\n",
      "Epoch 137, Loss: 0.6077605485916138, Val Loss: 0.6122024059295654\n",
      "Epoch 138, Loss: 0.6068049669265747, Val Loss: 0.6113193035125732\n",
      "Epoch 139, Loss: 0.6058427691459656, Val Loss: 0.610417902469635\n",
      "Epoch 140, Loss: 0.6048611998558044, Val Loss: 0.609531044960022\n",
      "Epoch 141, Loss: 0.6038934588432312, Val Loss: 0.6086328625679016\n",
      "Epoch 142, Loss: 0.6029146909713745, Val Loss: 0.6077297329902649\n",
      "Epoch 143, Loss: 0.6019303202629089, Val Loss: 0.6068160533905029\n",
      "Epoch 144, Loss: 0.6009340882301331, Val Loss: 0.6059040427207947\n",
      "Epoch 145, Loss: 0.599940299987793, Val Loss: 0.6049840450286865\n",
      "Epoch 146, Loss: 0.5989406704902649, Val Loss: 0.6040582060813904\n",
      "Epoch 147, Loss: 0.5979324579238892, Val Loss: 0.6031262278556824\n",
      "Epoch 148, Loss: 0.5969198942184448, Val Loss: 0.6021984219551086\n",
      "Epoch 149, Loss: 0.5959073305130005, Val Loss: 0.6012609004974365\n",
      "Epoch 150, Loss: 0.5948900580406189, Val Loss: 0.6003162860870361\n",
      "Epoch 151, Loss: 0.5938636660575867, Val Loss: 0.5993691682815552\n",
      "Epoch 152, Loss: 0.5928343534469604, Val Loss: 0.598425030708313\n",
      "Epoch 153, Loss: 0.591806948184967, Val Loss: 0.5974704027175903\n",
      "Epoch 154, Loss: 0.5907679200172424, Val Loss: 0.5965189933776855\n",
      "Epoch 155, Loss: 0.5897337198257446, Val Loss: 0.5955594778060913\n",
      "Epoch 156, Loss: 0.5886890888214111, Val Loss: 0.5946065187454224\n",
      "Epoch 157, Loss: 0.587645947933197, Val Loss: 0.5936440825462341\n",
      "Epoch 158, Loss: 0.5865963697433472, Val Loss: 0.5926834940910339\n",
      "Epoch 159, Loss: 0.5855457782745361, Val Loss: 0.5917194485664368\n",
      "Epoch 160, Loss: 0.584493100643158, Val Loss: 0.5907523036003113\n",
      "Epoch 161, Loss: 0.5834358334541321, Val Loss: 0.5897843837738037\n",
      "Epoch 162, Loss: 0.5823752284049988, Val Loss: 0.5888142585754395\n",
      "Epoch 163, Loss: 0.5813135504722595, Val Loss: 0.5878391861915588\n",
      "Epoch 164, Loss: 0.5802476406097412, Val Loss: 0.5868706703186035\n",
      "Epoch 165, Loss: 0.5791851878166199, Val Loss: 0.5858851671218872\n",
      "Epoch 166, Loss: 0.5781058669090271, Val Loss: 0.5849115252494812\n",
      "Epoch 167, Loss: 0.577037513256073, Val Loss: 0.5839327573776245\n",
      "Epoch 168, Loss: 0.5759657025337219, Val Loss: 0.5829546451568604\n",
      "Epoch 169, Loss: 0.5748922824859619, Val Loss: 0.5819665789604187\n",
      "Epoch 170, Loss: 0.5738087892532349, Val Loss: 0.5809727907180786\n",
      "Epoch 171, Loss: 0.5727206468582153, Val Loss: 0.5799895524978638\n",
      "Epoch 172, Loss: 0.5716428160667419, Val Loss: 0.579003632068634\n",
      "Epoch 173, Loss: 0.5705597996711731, Val Loss: 0.5780021548271179\n",
      "Epoch 174, Loss: 0.5694617629051208, Val Loss: 0.5770118832588196\n",
      "Epoch 175, Loss: 0.568373441696167, Val Loss: 0.5760148167610168\n",
      "Epoch 176, Loss: 0.5672807097434998, Val Loss: 0.5750171542167664\n",
      "Epoch 177, Loss: 0.5661832094192505, Val Loss: 0.5740187168121338\n",
      "Epoch 178, Loss: 0.5650850534439087, Val Loss: 0.5730098485946655\n",
      "Epoch 179, Loss: 0.5639781951904297, Val Loss: 0.5720120668411255\n",
      "Epoch 180, Loss: 0.562881350517273, Val Loss: 0.5710034966468811\n",
      "Epoch 181, Loss: 0.5617725849151611, Val Loss: 0.5700032711029053\n",
      "Epoch 182, Loss: 0.5606712102890015, Val Loss: 0.5689901113510132\n",
      "Epoch 183, Loss: 0.5595564246177673, Val Loss: 0.567986786365509\n",
      "Epoch 184, Loss: 0.5584502220153809, Val Loss: 0.5669744610786438\n",
      "Epoch 185, Loss: 0.5573360323905945, Val Loss: 0.5659611821174622\n",
      "Epoch 186, Loss: 0.5562172532081604, Val Loss: 0.5649513006210327\n",
      "Epoch 187, Loss: 0.5551033020019531, Val Loss: 0.5639364719390869\n",
      "Epoch 188, Loss: 0.553980827331543, Val Loss: 0.5629278421401978\n",
      "Epoch 189, Loss: 0.55286705493927, Val Loss: 0.5619056224822998\n",
      "Epoch 190, Loss: 0.551738440990448, Val Loss: 0.5608831644058228\n",
      "Epoch 191, Loss: 0.5506100654602051, Val Loss: 0.5598703026771545\n",
      "Epoch 192, Loss: 0.5494892001152039, Val Loss: 0.5588588714599609\n",
      "Epoch 193, Loss: 0.5483669638633728, Val Loss: 0.5578327178955078\n",
      "Epoch 194, Loss: 0.5472339987754822, Val Loss: 0.5568153262138367\n",
      "Epoch 195, Loss: 0.5461073517799377, Val Loss: 0.555794358253479\n",
      "Epoch 196, Loss: 0.5449748635292053, Val Loss: 0.5547807216644287\n",
      "Epoch 197, Loss: 0.5438507199287415, Val Loss: 0.5537567138671875\n",
      "Epoch 198, Loss: 0.5427142381668091, Val Loss: 0.5527375936508179\n",
      "Epoch 199, Loss: 0.5415847897529602, Val Loss: 0.5517188310623169\n",
      "Epoch 200, Loss: 0.5404518842697144, Val Loss: 0.550703763961792\n",
      "Epoch 201, Loss: 0.5393220782279968, Val Loss: 0.5496750473976135\n",
      "Epoch 202, Loss: 0.5381810665130615, Val Loss: 0.5486574172973633\n",
      "Epoch 203, Loss: 0.5370489358901978, Val Loss: 0.5476343035697937\n",
      "Epoch 204, Loss: 0.5359085202217102, Val Loss: 0.5466117858886719\n",
      "Epoch 205, Loss: 0.5347690582275391, Val Loss: 0.545600175857544\n",
      "Epoch 206, Loss: 0.5336344838142395, Val Loss: 0.5445758104324341\n",
      "Epoch 207, Loss: 0.5324897766113281, Val Loss: 0.5435602068901062\n",
      "Epoch 208, Loss: 0.531351625919342, Val Loss: 0.5425359010696411\n",
      "Epoch 209, Loss: 0.5302042961120605, Val Loss: 0.5415151715278625\n",
      "Epoch 210, Loss: 0.5290570855140686, Val Loss: 0.5404947400093079\n",
      "Epoch 211, Loss: 0.5279079675674438, Val Loss: 0.53947514295578\n",
      "Epoch 212, Loss: 0.5267606973648071, Val Loss: 0.5384490489959717\n",
      "Epoch 213, Loss: 0.525606632232666, Val Loss: 0.5374197959899902\n",
      "Epoch 214, Loss: 0.524445116519928, Val Loss: 0.5364027619361877\n",
      "Epoch 215, Loss: 0.5232948660850525, Val Loss: 0.5353753566741943\n",
      "Epoch 216, Loss: 0.5221344828605652, Val Loss: 0.5343567132949829\n",
      "Epoch 217, Loss: 0.5209762454032898, Val Loss: 0.5333232879638672\n",
      "Epoch 218, Loss: 0.5198033452033997, Val Loss: 0.5323053598403931\n",
      "Epoch 219, Loss: 0.518643856048584, Val Loss: 0.5312834978103638\n",
      "Epoch 220, Loss: 0.5174797177314758, Val Loss: 0.5302569270133972\n",
      "Epoch 221, Loss: 0.5163066387176514, Val Loss: 0.5292412638664246\n",
      "Epoch 222, Loss: 0.5151417851448059, Val Loss: 0.5282257795333862\n",
      "Epoch 223, Loss: 0.5139718055725098, Val Loss: 0.5272204875946045\n",
      "Epoch 224, Loss: 0.5128098130226135, Val Loss: 0.5262080430984497\n",
      "Epoch 225, Loss: 0.511637806892395, Val Loss: 0.525206983089447\n",
      "Epoch 226, Loss: 0.51047682762146, Val Loss: 0.5242125391960144\n",
      "Epoch 227, Loss: 0.5093198418617249, Val Loss: 0.523221492767334\n",
      "Epoch 228, Loss: 0.5081604719161987, Val Loss: 0.5222327709197998\n",
      "Epoch 229, Loss: 0.5070130825042725, Val Loss: 0.5212482213973999\n",
      "Epoch 230, Loss: 0.5058711767196655, Val Loss: 0.5202656388282776\n",
      "Epoch 231, Loss: 0.5047348737716675, Val Loss: 0.5192849040031433\n",
      "Epoch 232, Loss: 0.5036025643348694, Val Loss: 0.5183083415031433\n",
      "Epoch 233, Loss: 0.5024822354316711, Val Loss: 0.5173314809799194\n",
      "Epoch 234, Loss: 0.5013608336448669, Val Loss: 0.5163499116897583\n",
      "Epoch 235, Loss: 0.5002374649047852, Val Loss: 0.5153724551200867\n",
      "Epoch 236, Loss: 0.49911823868751526, Val Loss: 0.5144026279449463\n",
      "Epoch 237, Loss: 0.4980059266090393, Val Loss: 0.5134311318397522\n",
      "Epoch 238, Loss: 0.49689170718193054, Val Loss: 0.5124629735946655\n",
      "Epoch 239, Loss: 0.49577412009239197, Val Loss: 0.5114974975585938\n",
      "Epoch 240, Loss: 0.4946599006652832, Val Loss: 0.510536253452301\n",
      "Epoch 241, Loss: 0.4935472309589386, Val Loss: 0.5095778107643127\n",
      "Epoch 242, Loss: 0.49243709444999695, Val Loss: 0.508638322353363\n",
      "Epoch 243, Loss: 0.4913400113582611, Val Loss: 0.5076911449432373\n",
      "Epoch 244, Loss: 0.49023306369781494, Val Loss: 0.5067519545555115\n",
      "Epoch 245, Loss: 0.48913630843162537, Val Loss: 0.5058124661445618\n",
      "Epoch 246, Loss: 0.48803260922431946, Val Loss: 0.5048883557319641\n",
      "Epoch 247, Loss: 0.4869464933872223, Val Loss: 0.5039591789245605\n",
      "Epoch 248, Loss: 0.4858560562133789, Val Loss: 0.503034234046936\n",
      "Epoch 249, Loss: 0.48476845026016235, Val Loss: 0.5021136403083801\n",
      "Epoch 250, Loss: 0.48368674516677856, Val Loss: 0.5011962652206421\n",
      "Epoch 251, Loss: 0.48260968923568726, Val Loss: 0.5002721548080444\n",
      "Epoch 252, Loss: 0.4815264046192169, Val Loss: 0.49936142563819885\n",
      "Epoch 253, Loss: 0.4804573059082031, Val Loss: 0.49843791127204895\n",
      "Epoch 254, Loss: 0.4793745279312134, Val Loss: 0.49753233790397644\n",
      "Epoch 255, Loss: 0.47831276059150696, Val Loss: 0.49662742018699646\n",
      "Epoch 256, Loss: 0.4772505462169647, Val Loss: 0.49572083353996277\n",
      "Epoch 257, Loss: 0.47618362307548523, Val Loss: 0.49481141567230225\n",
      "Epoch 258, Loss: 0.4751172959804535, Val Loss: 0.4938996732234955\n",
      "Epoch 259, Loss: 0.474049836397171, Val Loss: 0.4930090308189392\n",
      "Epoch 260, Loss: 0.4730020761489868, Val Loss: 0.49210312962532043\n",
      "Epoch 261, Loss: 0.4719395935535431, Val Loss: 0.49121275544166565\n",
      "Epoch 262, Loss: 0.4708884358406067, Val Loss: 0.49032118916511536\n",
      "Epoch 263, Loss: 0.46983739733695984, Val Loss: 0.48943501710891724\n",
      "Epoch 264, Loss: 0.46879154443740845, Val Loss: 0.4885469973087311\n",
      "Epoch 265, Loss: 0.46773990988731384, Val Loss: 0.4876652657985687\n",
      "Epoch 266, Loss: 0.4666966497898102, Val Loss: 0.4867851734161377\n",
      "Epoch 267, Loss: 0.4656563103199005, Val Loss: 0.48590943217277527\n",
      "Epoch 268, Loss: 0.46461427211761475, Val Loss: 0.48503977060317993\n",
      "Epoch 269, Loss: 0.4635797142982483, Val Loss: 0.4841679632663727\n",
      "Epoch 270, Loss: 0.4625413715839386, Val Loss: 0.4833008348941803\n",
      "Epoch 271, Loss: 0.4615062475204468, Val Loss: 0.4824390709400177\n",
      "Epoch 272, Loss: 0.46047690510749817, Val Loss: 0.48158109188079834\n",
      "Epoch 273, Loss: 0.4594481587409973, Val Loss: 0.48072293400764465\n",
      "Epoch 274, Loss: 0.45841866731643677, Val Loss: 0.4798645079135895\n",
      "Epoch 275, Loss: 0.45739296078681946, Val Loss: 0.479013055562973\n",
      "Epoch 276, Loss: 0.4563739597797394, Val Loss: 0.4781554341316223\n",
      "Epoch 277, Loss: 0.4553467631340027, Val Loss: 0.47731417417526245\n",
      "Epoch 278, Loss: 0.454336553812027, Val Loss: 0.4764683246612549\n",
      "Epoch 279, Loss: 0.45331788063049316, Val Loss: 0.4756275713443756\n",
      "Epoch 280, Loss: 0.45230746269226074, Val Loss: 0.4747907817363739\n",
      "Epoch 281, Loss: 0.45129796862602234, Val Loss: 0.47395920753479004\n",
      "Epoch 282, Loss: 0.45029231905937195, Val Loss: 0.4731288552284241\n",
      "Epoch 283, Loss: 0.44928568601608276, Val Loss: 0.4723094701766968\n",
      "Epoch 284, Loss: 0.448287695646286, Val Loss: 0.471488356590271\n",
      "Epoch 285, Loss: 0.44728872179985046, Val Loss: 0.4706745445728302\n",
      "Epoch 286, Loss: 0.44629740715026855, Val Loss: 0.4698612093925476\n",
      "Epoch 287, Loss: 0.4453009068965912, Val Loss: 0.469052255153656\n",
      "Epoch 288, Loss: 0.4443097412586212, Val Loss: 0.468240350484848\n",
      "Epoch 289, Loss: 0.44331932067871094, Val Loss: 0.4674450159072876\n",
      "Epoch 290, Loss: 0.44234755635261536, Val Loss: 0.4666425585746765\n",
      "Epoch 291, Loss: 0.4413679838180542, Val Loss: 0.465846985578537\n",
      "Epoch 292, Loss: 0.4403938055038452, Val Loss: 0.4650507867336273\n",
      "Epoch 293, Loss: 0.43942105770111084, Val Loss: 0.46425673365592957\n",
      "Epoch 294, Loss: 0.43844908475875854, Val Loss: 0.463473379611969\n",
      "Epoch 295, Loss: 0.43748944997787476, Val Loss: 0.46268460154533386\n",
      "Epoch 296, Loss: 0.4365239143371582, Val Loss: 0.4619028866291046\n",
      "Epoch 297, Loss: 0.4355670213699341, Val Loss: 0.4611191153526306\n",
      "Epoch 298, Loss: 0.4346029758453369, Val Loss: 0.4603388011455536\n",
      "Epoch 299, Loss: 0.43364378809928894, Val Loss: 0.4595732092857361\n",
      "Epoch 300, Loss: 0.4326953589916229, Val Loss: 0.45880258083343506\n",
      "Epoch 301, Loss: 0.43174466490745544, Val Loss: 0.45803073048591614\n",
      "Epoch 302, Loss: 0.43078431487083435, Val Loss: 0.4572603106498718\n",
      "Epoch 303, Loss: 0.4298316240310669, Val Loss: 0.45650723576545715\n",
      "Epoch 304, Loss: 0.428891122341156, Val Loss: 0.45574650168418884\n",
      "Epoch 305, Loss: 0.42794474959373474, Val Loss: 0.454997181892395\n",
      "Epoch 306, Loss: 0.4270051419734955, Val Loss: 0.4542500674724579\n",
      "Epoch 307, Loss: 0.4260673522949219, Val Loss: 0.4535018801689148\n",
      "Epoch 308, Loss: 0.42513540387153625, Val Loss: 0.452760249376297\n",
      "Epoch 309, Loss: 0.42421039938926697, Val Loss: 0.452029287815094\n",
      "Epoch 310, Loss: 0.42328935861587524, Val Loss: 0.4512956738471985\n",
      "Epoch 311, Loss: 0.4223649203777313, Val Loss: 0.4505707621574402\n",
      "Epoch 312, Loss: 0.4214535653591156, Val Loss: 0.449846476316452\n",
      "Epoch 313, Loss: 0.42054104804992676, Val Loss: 0.449134886264801\n",
      "Epoch 314, Loss: 0.41963931918144226, Val Loss: 0.4484213590621948\n",
      "Epoch 315, Loss: 0.41873595118522644, Val Loss: 0.44771116971969604\n",
      "Epoch 316, Loss: 0.4178287088871002, Val Loss: 0.4469984173774719\n",
      "Epoch 317, Loss: 0.41692227125167847, Val Loss: 0.44629961252212524\n",
      "Epoch 318, Loss: 0.41603147983551025, Val Loss: 0.44559940695762634\n",
      "Epoch 319, Loss: 0.4151388704776764, Val Loss: 0.4449019730091095\n",
      "Epoch 320, Loss: 0.41424620151519775, Val Loss: 0.44420987367630005\n",
      "Epoch 321, Loss: 0.41335079073905945, Val Loss: 0.44351357221603394\n",
      "Epoch 322, Loss: 0.4124574661254883, Val Loss: 0.44282612204551697\n",
      "Epoch 323, Loss: 0.4115736782550812, Val Loss: 0.44214919209480286\n",
      "Epoch 324, Loss: 0.41069743037223816, Val Loss: 0.4414670169353485\n",
      "Epoch 325, Loss: 0.4098157584667206, Val Loss: 0.4407851994037628\n",
      "Epoch 326, Loss: 0.40893179178237915, Val Loss: 0.4401121735572815\n",
      "Epoch 327, Loss: 0.4080550968647003, Val Loss: 0.4394456744194031\n",
      "Epoch 328, Loss: 0.4071884751319885, Val Loss: 0.43877437710762024\n",
      "Epoch 329, Loss: 0.40631577372550964, Val Loss: 0.4381141662597656\n",
      "Epoch 330, Loss: 0.40544846653938293, Val Loss: 0.437455952167511\n",
      "Epoch 331, Loss: 0.4045836329460144, Val Loss: 0.4367951452732086\n",
      "Epoch 332, Loss: 0.4037182033061981, Val Loss: 0.43614181876182556\n",
      "Epoch 333, Loss: 0.4028605818748474, Val Loss: 0.4354938268661499\n",
      "Epoch 334, Loss: 0.402008056640625, Val Loss: 0.4348413944244385\n",
      "Epoch 335, Loss: 0.4011476933956146, Val Loss: 0.43419983983039856\n",
      "Epoch 336, Loss: 0.400295227766037, Val Loss: 0.4335634708404541\n",
      "Epoch 337, Loss: 0.399446964263916, Val Loss: 0.4329196810722351\n",
      "Epoch 338, Loss: 0.39859384298324585, Val Loss: 0.4322865605354309\n",
      "Epoch 339, Loss: 0.39775046706199646, Val Loss: 0.4316565990447998\n",
      "Epoch 340, Loss: 0.39690935611724854, Val Loss: 0.4310237467288971\n",
      "Epoch 341, Loss: 0.3960678279399872, Val Loss: 0.4303957521915436\n",
      "Epoch 342, Loss: 0.39522606134414673, Val Loss: 0.42977291345596313\n",
      "Epoch 343, Loss: 0.3943904936313629, Val Loss: 0.4291535019874573\n",
      "Epoch 344, Loss: 0.39355728030204773, Val Loss: 0.4285356402397156\n",
      "Epoch 345, Loss: 0.3927256166934967, Val Loss: 0.42792195081710815\n",
      "Epoch 346, Loss: 0.39189478754997253, Val Loss: 0.4273092448711395\n",
      "Epoch 347, Loss: 0.39106476306915283, Val Loss: 0.42669954895973206\n",
      "Epoch 348, Loss: 0.3902397155761719, Val Loss: 0.4260929524898529\n",
      "Epoch 349, Loss: 0.3894166350364685, Val Loss: 0.42549124360084534\n",
      "Epoch 350, Loss: 0.3885965645313263, Val Loss: 0.4248906970024109\n",
      "Epoch 351, Loss: 0.38777610659599304, Val Loss: 0.42429476976394653\n",
      "Epoch 352, Loss: 0.3869602382183075, Val Loss: 0.4236966073513031\n",
      "Epoch 353, Loss: 0.3861437141895294, Val Loss: 0.4231027066707611\n",
      "Epoch 354, Loss: 0.3853304982185364, Val Loss: 0.4225154519081116\n",
      "Epoch 355, Loss: 0.3845207989215851, Val Loss: 0.421924352645874\n",
      "Epoch 356, Loss: 0.3837085962295532, Val Loss: 0.4213405251502991\n",
      "Epoch 357, Loss: 0.38290491700172424, Val Loss: 0.42076003551483154\n",
      "Epoch 358, Loss: 0.3821031451225281, Val Loss: 0.42018046975135803\n",
      "Epoch 359, Loss: 0.38129597902297974, Val Loss: 0.41960200667381287\n",
      "Epoch 360, Loss: 0.38049599528312683, Val Loss: 0.41902682185173035\n",
      "Epoch 361, Loss: 0.3796960711479187, Val Loss: 0.4184502363204956\n",
      "Epoch 362, Loss: 0.3788931965827942, Val Loss: 0.417874276638031\n",
      "Epoch 363, Loss: 0.3780963122844696, Val Loss: 0.41731587052345276\n",
      "Epoch 364, Loss: 0.3773113489151001, Val Loss: 0.4167473316192627\n",
      "Epoch 365, Loss: 0.3765178918838501, Val Loss: 0.41617995500564575\n",
      "Epoch 366, Loss: 0.37572354078292847, Val Loss: 0.41561979055404663\n",
      "Epoch 367, Loss: 0.37493571639060974, Val Loss: 0.41506215929985046\n",
      "Epoch 368, Loss: 0.374148428440094, Val Loss: 0.4145044982433319\n",
      "Epoch 369, Loss: 0.37336283922195435, Val Loss: 0.41394752264022827\n",
      "Epoch 370, Loss: 0.3725743591785431, Val Loss: 0.4133991301059723\n",
      "Epoch 371, Loss: 0.37179723381996155, Val Loss: 0.41284602880477905\n",
      "Epoch 372, Loss: 0.3710120916366577, Val Loss: 0.4123009443283081\n",
      "Epoch 373, Loss: 0.3702358305454254, Val Loss: 0.4117559790611267\n",
      "Epoch 374, Loss: 0.3694566786289215, Val Loss: 0.4112153947353363\n",
      "Epoch 375, Loss: 0.36867716908454895, Val Loss: 0.4106713533401489\n",
      "Epoch 376, Loss: 0.3678993582725525, Val Loss: 0.4101335406303406\n",
      "Epoch 377, Loss: 0.3671244978904724, Val Loss: 0.4095902740955353\n",
      "Epoch 378, Loss: 0.36634954810142517, Val Loss: 0.40905827283859253\n",
      "Epoch 379, Loss: 0.3655775189399719, Val Loss: 0.4085251986980438\n",
      "Epoch 380, Loss: 0.36480218172073364, Val Loss: 0.40799233317375183\n",
      "Epoch 381, Loss: 0.3640317916870117, Val Loss: 0.4074587821960449\n",
      "Epoch 382, Loss: 0.3632548749446869, Val Loss: 0.40693315863609314\n",
      "Epoch 383, Loss: 0.36248883605003357, Val Loss: 0.4063991606235504\n",
      "Epoch 384, Loss: 0.36170825362205505, Val Loss: 0.40586981177330017\n",
      "Epoch 385, Loss: 0.3609393537044525, Val Loss: 0.40534424781799316\n",
      "Epoch 386, Loss: 0.3601718544960022, Val Loss: 0.404808908700943\n",
      "Epoch 387, Loss: 0.35940104722976685, Val Loss: 0.4042789936065674\n",
      "Epoch 388, Loss: 0.35863640904426575, Val Loss: 0.4037381112575531\n",
      "Epoch 389, Loss: 0.35786300897598267, Val Loss: 0.40320199728012085\n",
      "Epoch 390, Loss: 0.357098788022995, Val Loss: 0.40266892313957214\n",
      "Epoch 391, Loss: 0.3563336730003357, Val Loss: 0.4021349251270294\n",
      "Epoch 392, Loss: 0.35557353496551514, Val Loss: 0.40160807967185974\n",
      "Epoch 393, Loss: 0.35480841994285583, Val Loss: 0.4010731279850006\n",
      "Epoch 394, Loss: 0.3540489673614502, Val Loss: 0.4005495607852936\n",
      "Epoch 395, Loss: 0.3532879054546356, Val Loss: 0.40003713965415955\n",
      "Epoch 396, Loss: 0.35253751277923584, Val Loss: 0.3995310068130493\n",
      "Epoch 397, Loss: 0.3517899513244629, Val Loss: 0.3990238606929779\n",
      "Epoch 398, Loss: 0.3510422110557556, Val Loss: 0.3985244035720825\n",
      "Epoch 399, Loss: 0.3502980172634125, Val Loss: 0.3980303406715393\n",
      "Epoch 400, Loss: 0.34955912828445435, Val Loss: 0.3975387215614319\n",
      "Epoch 401, Loss: 0.34882381558418274, Val Loss: 0.3970533609390259\n",
      "Epoch 402, Loss: 0.3480943441390991, Val Loss: 0.3965723514556885\n",
      "Epoch 403, Loss: 0.3473673462867737, Val Loss: 0.3960874676704407\n",
      "Epoch 404, Loss: 0.34663307666778564, Val Loss: 0.3956061601638794\n",
      "Epoch 405, Loss: 0.34589964151382446, Val Loss: 0.39513254165649414\n",
      "Epoch 406, Loss: 0.3451715111732483, Val Loss: 0.3946572542190552\n",
      "Epoch 407, Loss: 0.3444487154483795, Val Loss: 0.3941830098628998\n",
      "Epoch 408, Loss: 0.3437231779098511, Val Loss: 0.3937157094478607\n",
      "Epoch 409, Loss: 0.3430008292198181, Val Loss: 0.39323610067367554\n",
      "Epoch 410, Loss: 0.34226906299591064, Val Loss: 0.39277079701423645\n",
      "Epoch 411, Loss: 0.341544508934021, Val Loss: 0.39230695366859436\n",
      "Epoch 412, Loss: 0.34082522988319397, Val Loss: 0.3918347656726837\n",
      "Epoch 413, Loss: 0.3400985598564148, Val Loss: 0.3913784921169281\n",
      "Epoch 414, Loss: 0.3393785059452057, Val Loss: 0.3909229636192322\n",
      "Epoch 415, Loss: 0.3386627435684204, Val Loss: 0.3904634118080139\n",
      "Epoch 416, Loss: 0.3379398286342621, Val Loss: 0.3900087773799896\n",
      "Epoch 417, Loss: 0.33722469210624695, Val Loss: 0.38955265283584595\n",
      "Epoch 418, Loss: 0.3365039527416229, Val Loss: 0.38909950852394104\n",
      "Epoch 419, Loss: 0.3357914984226227, Val Loss: 0.388651579618454\n",
      "Epoch 420, Loss: 0.33507996797561646, Val Loss: 0.3882007300853729\n",
      "Epoch 421, Loss: 0.33436596393585205, Val Loss: 0.38774874806404114\n",
      "Epoch 422, Loss: 0.3336499333381653, Val Loss: 0.3873141407966614\n",
      "Epoch 423, Loss: 0.3329492509365082, Val Loss: 0.38687121868133545\n",
      "Epoch 424, Loss: 0.3322400748729706, Val Loss: 0.38643020391464233\n",
      "Epoch 425, Loss: 0.33153006434440613, Val Loss: 0.38599246740341187\n",
      "Epoch 426, Loss: 0.3308229446411133, Val Loss: 0.38555192947387695\n",
      "Epoch 427, Loss: 0.3301125764846802, Val Loss: 0.3851093649864197\n",
      "Epoch 428, Loss: 0.32940709590911865, Val Loss: 0.3846740126609802\n",
      "Epoch 429, Loss: 0.32870393991470337, Val Loss: 0.38424092531204224\n",
      "Epoch 430, Loss: 0.3280034065246582, Val Loss: 0.38380998373031616\n",
      "Epoch 431, Loss: 0.32729777693748474, Val Loss: 0.38338062167167664\n",
      "Epoch 432, Loss: 0.32659685611724854, Val Loss: 0.3829551339149475\n",
      "Epoch 433, Loss: 0.3258940577507019, Val Loss: 0.38253048062324524\n",
      "Epoch 434, Loss: 0.3251913785934448, Val Loss: 0.3821048438549042\n",
      "Epoch 435, Loss: 0.3244948387145996, Val Loss: 0.3816784918308258\n",
      "Epoch 436, Loss: 0.3237977623939514, Val Loss: 0.38125666975975037\n",
      "Epoch 437, Loss: 0.3231011927127838, Val Loss: 0.38083839416503906\n",
      "Epoch 438, Loss: 0.32240602374076843, Val Loss: 0.3804124593734741\n",
      "Epoch 439, Loss: 0.32170718908309937, Val Loss: 0.37999919056892395\n",
      "Epoch 440, Loss: 0.32101452350616455, Val Loss: 0.3795805275440216\n",
      "Epoch 441, Loss: 0.32031795382499695, Val Loss: 0.3791690170764923\n",
      "Epoch 442, Loss: 0.3196233808994293, Val Loss: 0.37875470519065857\n",
      "Epoch 443, Loss: 0.3189341425895691, Val Loss: 0.37834712862968445\n",
      "Epoch 444, Loss: 0.3182438313961029, Val Loss: 0.377937912940979\n",
      "Epoch 445, Loss: 0.317553848028183, Val Loss: 0.37752866744995117\n",
      "Epoch 446, Loss: 0.3168618083000183, Val Loss: 0.37711721658706665\n",
      "Epoch 447, Loss: 0.31617432832717896, Val Loss: 0.37671568989753723\n",
      "Epoch 448, Loss: 0.31548431515693665, Val Loss: 0.37630966305732727\n",
      "Epoch 449, Loss: 0.31479766964912415, Val Loss: 0.3759070932865143\n",
      "Epoch 450, Loss: 0.3141126334667206, Val Loss: 0.3755108714103699\n",
      "Epoch 451, Loss: 0.31342610716819763, Val Loss: 0.3751153349876404\n",
      "Epoch 452, Loss: 0.31274282932281494, Val Loss: 0.37471890449523926\n",
      "Epoch 453, Loss: 0.3120565414428711, Val Loss: 0.3743201792240143\n",
      "Epoch 454, Loss: 0.3113744258880615, Val Loss: 0.37392696738243103\n",
      "Epoch 455, Loss: 0.31069087982177734, Val Loss: 0.3735390901565552\n",
      "Epoch 456, Loss: 0.31000909209251404, Val Loss: 0.3731481432914734\n",
      "Epoch 457, Loss: 0.3093250095844269, Val Loss: 0.3727637529373169\n",
      "Epoch 458, Loss: 0.3086475431919098, Val Loss: 0.3723777234554291\n",
      "Epoch 459, Loss: 0.3079691529273987, Val Loss: 0.3719986379146576\n",
      "Epoch 460, Loss: 0.3072860836982727, Val Loss: 0.3716191053390503\n",
      "Epoch 461, Loss: 0.3066099286079407, Val Loss: 0.3712441921234131\n",
      "Epoch 462, Loss: 0.30593571066856384, Val Loss: 0.37086793780326843\n",
      "Epoch 463, Loss: 0.30525970458984375, Val Loss: 0.37049251794815063\n",
      "Epoch 464, Loss: 0.304580956697464, Val Loss: 0.37011992931365967\n",
      "Epoch 465, Loss: 0.30390664935112, Val Loss: 0.36974820494651794\n",
      "Epoch 466, Loss: 0.30323538184165955, Val Loss: 0.36938539147377014\n",
      "Epoch 467, Loss: 0.30256345868110657, Val Loss: 0.36901748180389404\n",
      "Epoch 468, Loss: 0.3018917441368103, Val Loss: 0.368650883436203\n",
      "Epoch 469, Loss: 0.3012193739414215, Val Loss: 0.36829036474227905\n",
      "Epoch 470, Loss: 0.3005507290363312, Val Loss: 0.36792609095573425\n",
      "Epoch 471, Loss: 0.2998826801776886, Val Loss: 0.36756059527397156\n",
      "Epoch 472, Loss: 0.299213171005249, Val Loss: 0.3671954274177551\n",
      "Epoch 473, Loss: 0.29854539036750793, Val Loss: 0.36683395504951477\n",
      "Epoch 474, Loss: 0.29788076877593994, Val Loss: 0.3664727807044983\n",
      "Epoch 475, Loss: 0.29721343517303467, Val Loss: 0.36611634492874146\n",
      "Epoch 476, Loss: 0.29654887318611145, Val Loss: 0.36575642228126526\n",
      "Epoch 477, Loss: 0.29588156938552856, Val Loss: 0.36539801955223083\n",
      "Epoch 478, Loss: 0.2952176034450531, Val Loss: 0.36503925919532776\n",
      "Epoch 479, Loss: 0.2945537269115448, Val Loss: 0.36467865109443665\n",
      "Epoch 480, Loss: 0.2938903868198395, Val Loss: 0.3643258512020111\n",
      "Epoch 481, Loss: 0.2932291030883789, Val Loss: 0.36398011445999146\n",
      "Epoch 482, Loss: 0.2925693094730377, Val Loss: 0.36362916231155396\n",
      "Epoch 483, Loss: 0.291909396648407, Val Loss: 0.3632751703262329\n",
      "Epoch 484, Loss: 0.29124706983566284, Val Loss: 0.3629201352596283\n",
      "Epoch 485, Loss: 0.2905878722667694, Val Loss: 0.3625768721103668\n",
      "Epoch 486, Loss: 0.2899278402328491, Val Loss: 0.36222851276397705\n",
      "Epoch 487, Loss: 0.28927043080329895, Val Loss: 0.3618837893009186\n",
      "Epoch 488, Loss: 0.288611501455307, Val Loss: 0.36154401302337646\n",
      "Epoch 489, Loss: 0.2879585921764374, Val Loss: 0.36120495200157166\n",
      "Epoch 490, Loss: 0.287300169467926, Val Loss: 0.36086133122444153\n",
      "Epoch 491, Loss: 0.28664642572402954, Val Loss: 0.36052069067955017\n",
      "Epoch 492, Loss: 0.2859898507595062, Val Loss: 0.36018848419189453\n",
      "Epoch 493, Loss: 0.2853371500968933, Val Loss: 0.35985293984413147\n",
      "Epoch 494, Loss: 0.2846846878528595, Val Loss: 0.3595181703567505\n",
      "Epoch 495, Loss: 0.284034788608551, Val Loss: 0.35918325185775757\n",
      "Epoch 496, Loss: 0.2833791673183441, Val Loss: 0.35885632038116455\n",
      "Epoch 497, Loss: 0.28272682428359985, Val Loss: 0.35852062702178955\n",
      "Epoch 498, Loss: 0.28207436203956604, Val Loss: 0.35819971561431885\n",
      "Epoch 499, Loss: 0.2814256250858307, Val Loss: 0.35786980390548706\n",
      "Epoch 500, Loss: 0.2807701826095581, Val Loss: 0.357543408870697\n",
      "Epoch 501, Loss: 0.2801262438297272, Val Loss: 0.3572213351726532\n",
      "Epoch 502, Loss: 0.27947697043418884, Val Loss: 0.3568992614746094\n",
      "Epoch 503, Loss: 0.27883008122444153, Val Loss: 0.35658347606658936\n",
      "Epoch 504, Loss: 0.27818554639816284, Val Loss: 0.3562588691711426\n",
      "Epoch 505, Loss: 0.2775358557701111, Val Loss: 0.3559437096118927\n",
      "Epoch 506, Loss: 0.27688875794410706, Val Loss: 0.35562968254089355\n",
      "Epoch 507, Loss: 0.276242196559906, Val Loss: 0.3553188741207123\n",
      "Epoch 508, Loss: 0.2755981385707855, Val Loss: 0.3550010621547699\n",
      "Epoch 509, Loss: 0.27495187520980835, Val Loss: 0.3546847999095917\n",
      "Epoch 510, Loss: 0.2743077576160431, Val Loss: 0.35437607765197754\n",
      "Epoch 511, Loss: 0.2736661732196808, Val Loss: 0.35406970977783203\n",
      "Epoch 512, Loss: 0.27302324771881104, Val Loss: 0.3537602424621582\n",
      "Epoch 513, Loss: 0.27238336205482483, Val Loss: 0.3534539043903351\n",
      "Epoch 514, Loss: 0.2717398405075073, Val Loss: 0.35314780473709106\n",
      "Epoch 515, Loss: 0.2710973024368286, Val Loss: 0.3528487980365753\n",
      "Epoch 516, Loss: 0.2704574167728424, Val Loss: 0.35254353284835815\n",
      "Epoch 517, Loss: 0.2698174715042114, Val Loss: 0.3522418737411499\n",
      "Epoch 518, Loss: 0.26917606592178345, Val Loss: 0.35193970799446106\n",
      "Epoch 519, Loss: 0.2685389518737793, Val Loss: 0.3516395092010498\n",
      "Epoch 520, Loss: 0.2678995728492737, Val Loss: 0.35134077072143555\n",
      "Epoch 521, Loss: 0.2672651410102844, Val Loss: 0.3510449230670929\n",
      "Epoch 522, Loss: 0.266627699136734, Val Loss: 0.3507499396800995\n",
      "Epoch 523, Loss: 0.2659875750541687, Val Loss: 0.3504513204097748\n",
      "Epoch 524, Loss: 0.26535385847091675, Val Loss: 0.35016483068466187\n",
      "Epoch 525, Loss: 0.26471903920173645, Val Loss: 0.3498762845993042\n",
      "Epoch 526, Loss: 0.2640838325023651, Val Loss: 0.3495851159095764\n",
      "Epoch 527, Loss: 0.2634500563144684, Val Loss: 0.3492922782897949\n",
      "Epoch 528, Loss: 0.26281777024269104, Val Loss: 0.34900209307670593\n",
      "Epoch 529, Loss: 0.2621823251247406, Val Loss: 0.34871259331703186\n",
      "Epoch 530, Loss: 0.26154807209968567, Val Loss: 0.34842368960380554\n",
      "Epoch 531, Loss: 0.2609197199344635, Val Loss: 0.3481464982032776\n",
      "Epoch 532, Loss: 0.2602919936180115, Val Loss: 0.34785589575767517\n",
      "Epoch 533, Loss: 0.25965583324432373, Val Loss: 0.34757140278816223\n",
      "Epoch 534, Loss: 0.2590261697769165, Val Loss: 0.3472864627838135\n",
      "Epoch 535, Loss: 0.2583954632282257, Val Loss: 0.3470156788825989\n",
      "Epoch 536, Loss: 0.25777000188827515, Val Loss: 0.3467367887496948\n",
      "Epoch 537, Loss: 0.2571382224559784, Val Loss: 0.34645360708236694\n",
      "Epoch 538, Loss: 0.25651198625564575, Val Loss: 0.34617355465888977\n",
      "Epoch 539, Loss: 0.2558833956718445, Val Loss: 0.3458971083164215\n",
      "Epoch 540, Loss: 0.255257248878479, Val Loss: 0.34563249349594116\n",
      "Epoch 541, Loss: 0.2546318769454956, Val Loss: 0.3453514575958252\n",
      "Epoch 542, Loss: 0.2540067732334137, Val Loss: 0.34507614374160767\n",
      "Epoch 543, Loss: 0.2533802390098572, Val Loss: 0.34480389952659607\n",
      "Epoch 544, Loss: 0.2527553141117096, Val Loss: 0.3445296585559845\n",
      "Epoch 545, Loss: 0.25213006138801575, Val Loss: 0.3442641496658325\n",
      "Epoch 546, Loss: 0.25150713324546814, Val Loss: 0.34398719668388367\n",
      "Epoch 547, Loss: 0.2508859932422638, Val Loss: 0.3437235653400421\n",
      "Epoch 548, Loss: 0.25026166439056396, Val Loss: 0.3434619605541229\n",
      "Epoch 549, Loss: 0.24964231252670288, Val Loss: 0.3431965410709381\n",
      "Epoch 550, Loss: 0.24901634454727173, Val Loss: 0.3429272472858429\n",
      "Epoch 551, Loss: 0.248396635055542, Val Loss: 0.34266844391822815\n",
      "Epoch 552, Loss: 0.2477772831916809, Val Loss: 0.34240788221359253\n",
      "Epoch 553, Loss: 0.2471597045660019, Val Loss: 0.34214648604393005\n",
      "Epoch 554, Loss: 0.24654148519039154, Val Loss: 0.34188058972358704\n",
      "Epoch 555, Loss: 0.2459215670824051, Val Loss: 0.34163162112236023\n",
      "Epoch 556, Loss: 0.2453039586544037, Val Loss: 0.3413817584514618\n",
      "Epoch 557, Loss: 0.24468491971492767, Val Loss: 0.3411206007003784\n",
      "Epoch 558, Loss: 0.24406994879245758, Val Loss: 0.3408668041229248\n",
      "Epoch 559, Loss: 0.2434566617012024, Val Loss: 0.34060877561569214\n",
      "Epoch 560, Loss: 0.242839053273201, Val Loss: 0.34035348892211914\n",
      "Epoch 561, Loss: 0.24222280085086823, Val Loss: 0.3401063084602356\n",
      "Epoch 562, Loss: 0.241609126329422, Val Loss: 0.339861124753952\n",
      "Epoch 563, Loss: 0.24099816381931305, Val Loss: 0.3396141529083252\n",
      "Epoch 564, Loss: 0.2403872013092041, Val Loss: 0.3393722474575043\n",
      "Epoch 565, Loss: 0.23977510631084442, Val Loss: 0.3391258716583252\n",
      "Epoch 566, Loss: 0.23916153609752655, Val Loss: 0.33887991309165955\n",
      "Epoch 567, Loss: 0.23855158686637878, Val Loss: 0.3386387228965759\n",
      "Epoch 568, Loss: 0.23794302344322205, Val Loss: 0.3383869230747223\n",
      "Epoch 569, Loss: 0.2373308688402176, Val Loss: 0.3381512761116028\n",
      "Epoch 570, Loss: 0.23672248423099518, Val Loss: 0.33791738748550415\n",
      "Epoch 571, Loss: 0.23611347377300262, Val Loss: 0.337681382894516\n",
      "Epoch 572, Loss: 0.23550958931446075, Val Loss: 0.3374423384666443\n",
      "Epoch 573, Loss: 0.23490436375141144, Val Loss: 0.3372039198875427\n",
      "Epoch 574, Loss: 0.2342945784330368, Val Loss: 0.33696600794792175\n",
      "Epoch 575, Loss: 0.2336878627538681, Val Loss: 0.3367418050765991\n",
      "Epoch 576, Loss: 0.2330864816904068, Val Loss: 0.33651307225227356\n",
      "Epoch 577, Loss: 0.2324826568365097, Val Loss: 0.33627957105636597\n",
      "Epoch 578, Loss: 0.23187965154647827, Val Loss: 0.33605220913887024\n",
      "Epoch 579, Loss: 0.23127296566963196, Val Loss: 0.3358232080936432\n",
      "Epoch 580, Loss: 0.23067204654216766, Val Loss: 0.33559271693229675\n",
      "Epoch 581, Loss: 0.2300693690776825, Val Loss: 0.3353731036186218\n",
      "Epoch 582, Loss: 0.22947220504283905, Val Loss: 0.3351476490497589\n",
      "Epoch 583, Loss: 0.22887185215950012, Val Loss: 0.33492904901504517\n",
      "Epoch 584, Loss: 0.2282709926366806, Val Loss: 0.3347083032131195\n",
      "Epoch 585, Loss: 0.22766929864883423, Val Loss: 0.3344896733760834\n",
      "Epoch 586, Loss: 0.2270767241716385, Val Loss: 0.33426839113235474\n",
      "Epoch 587, Loss: 0.2264755666255951, Val Loss: 0.33405476808547974\n",
      "Epoch 588, Loss: 0.22588226199150085, Val Loss: 0.333836168050766\n",
      "Epoch 589, Loss: 0.2252865880727768, Val Loss: 0.33361998200416565\n",
      "Epoch 590, Loss: 0.22468766570091248, Val Loss: 0.333406925201416\n",
      "Epoch 591, Loss: 0.2240944802761078, Val Loss: 0.3331911563873291\n",
      "Epoch 592, Loss: 0.2234978973865509, Val Loss: 0.3329804837703705\n",
      "Epoch 593, Loss: 0.22290557622909546, Val Loss: 0.33277562260627747\n",
      "Epoch 594, Loss: 0.22231361269950867, Val Loss: 0.3325602114200592\n",
      "Epoch 595, Loss: 0.22172248363494873, Val Loss: 0.33235394954681396\n",
      "Epoch 596, Loss: 0.22112923860549927, Val Loss: 0.33215492963790894\n",
      "Epoch 597, Loss: 0.2205384075641632, Val Loss: 0.33195117115974426\n",
      "Epoch 598, Loss: 0.21994900703430176, Val Loss: 0.33174681663513184\n",
      "Epoch 599, Loss: 0.21936260163784027, Val Loss: 0.33153438568115234\n",
      "Epoch 600, Loss: 0.21877354383468628, Val Loss: 0.3313281238079071\n",
      "Epoch 601, Loss: 0.21818362176418304, Val Loss: 0.331130713224411\n",
      "Epoch 602, Loss: 0.2175966501235962, Val Loss: 0.3309305012226105\n",
      "Epoch 603, Loss: 0.2170124650001526, Val Loss: 0.33072933554649353\n",
      "Epoch 604, Loss: 0.21642670035362244, Val Loss: 0.3305377662181854\n",
      "Epoch 605, Loss: 0.2158382534980774, Val Loss: 0.3303406238555908\n",
      "Epoch 606, Loss: 0.21525590121746063, Val Loss: 0.3301449716091156\n",
      "Epoch 607, Loss: 0.21467308700084686, Val Loss: 0.3299526274204254\n",
      "Epoch 608, Loss: 0.21408823132514954, Val Loss: 0.3297581374645233\n",
      "Epoch 609, Loss: 0.21351134777069092, Val Loss: 0.329568088054657\n",
      "Epoch 610, Loss: 0.212924987077713, Val Loss: 0.32937395572662354\n",
      "Epoch 611, Loss: 0.21234330534934998, Val Loss: 0.3291870653629303\n",
      "Epoch 612, Loss: 0.21176652610301971, Val Loss: 0.3290048837661743\n",
      "Epoch 613, Loss: 0.2111901193857193, Val Loss: 0.32880815863609314\n",
      "Epoch 614, Loss: 0.21060797572135925, Val Loss: 0.32862693071365356\n",
      "Epoch 615, Loss: 0.21003009378910065, Val Loss: 0.3284367620944977\n",
      "Epoch 616, Loss: 0.20945504307746887, Val Loss: 0.32824811339378357\n",
      "Epoch 617, Loss: 0.20887859165668488, Val Loss: 0.3280668556690216\n",
      "Epoch 618, Loss: 0.20830294489860535, Val Loss: 0.3278888463973999\n",
      "Epoch 619, Loss: 0.2077314853668213, Val Loss: 0.327705979347229\n",
      "Epoch 620, Loss: 0.20715489983558655, Val Loss: 0.327526718378067\n",
      "Epoch 621, Loss: 0.2065826654434204, Val Loss: 0.3273495137691498\n",
      "Epoch 622, Loss: 0.20600931346416473, Val Loss: 0.32716265320777893\n",
      "Epoch 623, Loss: 0.20543932914733887, Val Loss: 0.3269902765750885\n",
      "Epoch 624, Loss: 0.20486854016780853, Val Loss: 0.32682058215141296\n",
      "Epoch 625, Loss: 0.20429715514183044, Val Loss: 0.32664814591407776\n",
      "Epoch 626, Loss: 0.20372918248176575, Val Loss: 0.3264716565608978\n",
      "Epoch 627, Loss: 0.20315878093242645, Val Loss: 0.32629066705703735\n",
      "Epoch 628, Loss: 0.2025933414697647, Val Loss: 0.32611268758773804\n",
      "Epoch 629, Loss: 0.2020266205072403, Val Loss: 0.32594361901283264\n",
      "Epoch 630, Loss: 0.20145967602729797, Val Loss: 0.3257865905761719\n",
      "Epoch 631, Loss: 0.2008943259716034, Val Loss: 0.3256138265132904\n",
      "Epoch 632, Loss: 0.2003306746482849, Val Loss: 0.32544007897377014\n",
      "Epoch 633, Loss: 0.19976450502872467, Val Loss: 0.32527562975883484\n",
      "Epoch 634, Loss: 0.1992020308971405, Val Loss: 0.3251132667064667\n",
      "Epoch 635, Loss: 0.19864214956760406, Val Loss: 0.3249468207359314\n",
      "Epoch 636, Loss: 0.1980808675289154, Val Loss: 0.3247811496257782\n",
      "Epoch 637, Loss: 0.19751939177513123, Val Loss: 0.3246219754219055\n",
      "Epoch 638, Loss: 0.196958988904953, Val Loss: 0.32446256279945374\n",
      "Epoch 639, Loss: 0.19639819860458374, Val Loss: 0.3243013024330139\n",
      "Epoch 640, Loss: 0.195842444896698, Val Loss: 0.32414042949676514\n",
      "Epoch 641, Loss: 0.19528408348560333, Val Loss: 0.32398152351379395\n",
      "Epoch 642, Loss: 0.19473060965538025, Val Loss: 0.3238164782524109\n",
      "Epoch 643, Loss: 0.19417108595371246, Val Loss: 0.32366296648979187\n",
      "Epoch 644, Loss: 0.19361859560012817, Val Loss: 0.3235073983669281\n",
      "Epoch 645, Loss: 0.19306351244449615, Val Loss: 0.32334864139556885\n",
      "Epoch 646, Loss: 0.1925099790096283, Val Loss: 0.32319703698158264\n",
      "Epoch 647, Loss: 0.19195780158042908, Val Loss: 0.32303953170776367\n",
      "Epoch 648, Loss: 0.1914072185754776, Val Loss: 0.32288891077041626\n",
      "Epoch 649, Loss: 0.190854012966156, Val Loss: 0.3227359652519226\n",
      "Epoch 650, Loss: 0.19030338525772095, Val Loss: 0.32258713245391846\n",
      "Epoch 651, Loss: 0.18975263833999634, Val Loss: 0.3224371671676636\n",
      "Epoch 652, Loss: 0.18920482695102692, Val Loss: 0.3222918212413788\n",
      "Epoch 653, Loss: 0.1886560618877411, Val Loss: 0.32215166091918945\n",
      "Epoch 654, Loss: 0.18811312317848206, Val Loss: 0.3220010995864868\n",
      "Epoch 655, Loss: 0.18756559491157532, Val Loss: 0.3218574523925781\n",
      "Epoch 656, Loss: 0.18701958656311035, Val Loss: 0.32170984148979187\n",
      "Epoch 657, Loss: 0.18647702038288116, Val Loss: 0.32156747579574585\n",
      "Epoch 658, Loss: 0.18593378365039825, Val Loss: 0.3214283287525177\n",
      "Epoch 659, Loss: 0.18538802862167358, Val Loss: 0.32128891348838806\n",
      "Epoch 660, Loss: 0.1848481148481369, Val Loss: 0.32115277647972107\n",
      "Epoch 661, Loss: 0.18430574238300323, Val Loss: 0.32100456953048706\n",
      "Epoch 662, Loss: 0.18376567959785461, Val Loss: 0.3208613097667694\n",
      "Epoch 663, Loss: 0.18322831392288208, Val Loss: 0.3207242488861084\n",
      "Epoch 664, Loss: 0.18269114196300507, Val Loss: 0.3205936849117279\n",
      "Epoch 665, Loss: 0.18215282261371613, Val Loss: 0.32045993208885193\n",
      "Epoch 666, Loss: 0.18161612749099731, Val Loss: 0.3203202486038208\n",
      "Epoch 667, Loss: 0.18107953667640686, Val Loss: 0.3201799690723419\n",
      "Epoch 668, Loss: 0.18054048717021942, Val Loss: 0.32005202770233154\n",
      "Epoch 669, Loss: 0.1800045669078827, Val Loss: 0.3199140429496765\n",
      "Epoch 670, Loss: 0.17947494983673096, Val Loss: 0.3197881579399109\n",
      "Epoch 671, Loss: 0.17894627153873444, Val Loss: 0.31965532898902893\n",
      "Epoch 672, Loss: 0.17841359972953796, Val Loss: 0.3195352852344513\n",
      "Epoch 673, Loss: 0.17788207530975342, Val Loss: 0.31940150260925293\n",
      "Epoch 674, Loss: 0.17735260725021362, Val Loss: 0.31927230954170227\n",
      "Epoch 675, Loss: 0.17682592570781708, Val Loss: 0.31915631890296936\n",
      "Epoch 676, Loss: 0.17629337310791016, Val Loss: 0.3190280497074127\n",
      "Epoch 677, Loss: 0.17577122151851654, Val Loss: 0.31890007853507996\n",
      "Epoch 678, Loss: 0.17524176836013794, Val Loss: 0.3187757432460785\n",
      "Epoch 679, Loss: 0.1747133880853653, Val Loss: 0.31864699721336365\n",
      "Epoch 680, Loss: 0.1741892397403717, Val Loss: 0.31852084398269653\n",
      "Epoch 681, Loss: 0.17366743087768555, Val Loss: 0.3184058368206024\n",
      "Epoch 682, Loss: 0.17314442992210388, Val Loss: 0.31828412413597107\n",
      "Epoch 683, Loss: 0.17262043058872223, Val Loss: 0.3181654214859009\n",
      "Epoch 684, Loss: 0.17210474610328674, Val Loss: 0.3180564343929291\n",
      "Epoch 685, Loss: 0.17158213257789612, Val Loss: 0.31793490052223206\n",
      "Epoch 686, Loss: 0.17106187343597412, Val Loss: 0.31781527400016785\n",
      "Epoch 687, Loss: 0.17054420709609985, Val Loss: 0.3176996409893036\n",
      "Epoch 688, Loss: 0.17002570629119873, Val Loss: 0.31759342551231384\n",
      "Epoch 689, Loss: 0.16950790584087372, Val Loss: 0.31747621297836304\n",
      "Epoch 690, Loss: 0.16899490356445312, Val Loss: 0.3173595368862152\n",
      "Epoch 691, Loss: 0.16848024725914001, Val Loss: 0.31725165247917175\n",
      "Epoch 692, Loss: 0.16796518862247467, Val Loss: 0.3171323835849762\n",
      "Epoch 693, Loss: 0.16745255887508392, Val Loss: 0.31702446937561035\n",
      "Epoch 694, Loss: 0.16693930327892303, Val Loss: 0.31691792607307434\n",
      "Epoch 695, Loss: 0.16642862558364868, Val Loss: 0.3168115019798279\n",
      "Epoch 696, Loss: 0.1659153699874878, Val Loss: 0.31670019030570984\n",
      "Epoch 697, Loss: 0.1654052734375, Val Loss: 0.31659770011901855\n",
      "Epoch 698, Loss: 0.16489900648593903, Val Loss: 0.31649404764175415\n",
      "Epoch 699, Loss: 0.16439108550548553, Val Loss: 0.3163958191871643\n",
      "Epoch 700, Loss: 0.1638864129781723, Val Loss: 0.3162861168384552\n",
      "Epoch 701, Loss: 0.1633768230676651, Val Loss: 0.3161885440349579\n",
      "Epoch 702, Loss: 0.16287480294704437, Val Loss: 0.31609439849853516\n",
      "Epoch 703, Loss: 0.16236647963523865, Val Loss: 0.315984308719635\n",
      "Epoch 704, Loss: 0.16186489164829254, Val Loss: 0.3158862590789795\n",
      "Epoch 705, Loss: 0.16136133670806885, Val Loss: 0.31577935814857483\n",
      "Epoch 706, Loss: 0.16085857152938843, Val Loss: 0.31568393111228943\n",
      "Epoch 707, Loss: 0.16035588085651398, Val Loss: 0.31558769941329956\n",
      "Epoch 708, Loss: 0.15985915064811707, Val Loss: 0.315492182970047\n",
      "Epoch 709, Loss: 0.15936005115509033, Val Loss: 0.3154033124446869\n",
      "Epoch 710, Loss: 0.15886084735393524, Val Loss: 0.3153134286403656\n",
      "Epoch 711, Loss: 0.1583634614944458, Val Loss: 0.3152145743370056\n",
      "Epoch 712, Loss: 0.15786485373973846, Val Loss: 0.3151244521141052\n",
      "Epoch 713, Loss: 0.15737085044384003, Val Loss: 0.31503334641456604\n",
      "Epoch 714, Loss: 0.1568746566772461, Val Loss: 0.31494584679603577\n",
      "Epoch 715, Loss: 0.15637929737567902, Val Loss: 0.3148546516895294\n",
      "Epoch 716, Loss: 0.15588729083538055, Val Loss: 0.3147727847099304\n",
      "Epoch 717, Loss: 0.15539386868476868, Val Loss: 0.31468671560287476\n",
      "Epoch 718, Loss: 0.15490469336509705, Val Loss: 0.3145914673805237\n",
      "Epoch 719, Loss: 0.1544143259525299, Val Loss: 0.31450924277305603\n",
      "Epoch 720, Loss: 0.15392325818538666, Val Loss: 0.3144284188747406\n",
      "Epoch 721, Loss: 0.15343578159809113, Val Loss: 0.31434381008148193\n",
      "Epoch 722, Loss: 0.1529449075460434, Val Loss: 0.31425145268440247\n",
      "Epoch 723, Loss: 0.15245793759822845, Val Loss: 0.31417906284332275\n",
      "Epoch 724, Loss: 0.15196864306926727, Val Loss: 0.314106285572052\n",
      "Epoch 725, Loss: 0.1514851450920105, Val Loss: 0.31402039527893066\n",
      "Epoch 726, Loss: 0.15100054442882538, Val Loss: 0.3139389157295227\n",
      "Epoch 727, Loss: 0.15051855146884918, Val Loss: 0.3138664960861206\n",
      "Epoch 728, Loss: 0.15003478527069092, Val Loss: 0.31379181146621704\n",
      "Epoch 729, Loss: 0.14954873919487, Val Loss: 0.31369835138320923\n",
      "Epoch 730, Loss: 0.14907121658325195, Val Loss: 0.31363141536712646\n",
      "Epoch 731, Loss: 0.1485883891582489, Val Loss: 0.31356364488601685\n",
      "Epoch 732, Loss: 0.14810903370380402, Val Loss: 0.3134945333003998\n",
      "Epoch 733, Loss: 0.14763052761554718, Val Loss: 0.31341466307640076\n",
      "Epoch 734, Loss: 0.14715157449245453, Val Loss: 0.31333866715431213\n",
      "Epoch 735, Loss: 0.14667415618896484, Val Loss: 0.3132723271846771\n",
      "Epoch 736, Loss: 0.14619846642017365, Val Loss: 0.3132082521915436\n",
      "Epoch 737, Loss: 0.1457199603319168, Val Loss: 0.31313958764076233\n",
      "Epoch 738, Loss: 0.14524385333061218, Val Loss: 0.3130720257759094\n",
      "Epoch 739, Loss: 0.14476844668388367, Val Loss: 0.31299543380737305\n",
      "Epoch 740, Loss: 0.14429794251918793, Val Loss: 0.31292882561683655\n",
      "Epoch 741, Loss: 0.14382191002368927, Val Loss: 0.3128538131713867\n",
      "Epoch 742, Loss: 0.1433502435684204, Val Loss: 0.31280454993247986\n",
      "Epoch 743, Loss: 0.14287427067756653, Val Loss: 0.3127439022064209\n",
      "Epoch 744, Loss: 0.14240260422229767, Val Loss: 0.3126908242702484\n",
      "Epoch 745, Loss: 0.14192937314510345, Val Loss: 0.31262990832328796\n",
      "Epoch 746, Loss: 0.141456738114357, Val Loss: 0.31255972385406494\n",
      "Epoch 747, Loss: 0.1409834623336792, Val Loss: 0.3125001788139343\n",
      "Epoch 748, Loss: 0.14051243662834167, Val Loss: 0.3124605715274811\n",
      "Epoch 749, Loss: 0.14004020392894745, Val Loss: 0.3124109208583832\n",
      "Epoch 750, Loss: 0.13956940174102783, Val Loss: 0.31235599517822266\n"
     ]
    }
   ],
   "source": [
    "from flashrag.prompt import PromptTemplate\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "# Set configuration for the new dataset\n",
    "config_new_dataset = set_config(dataset_name='final_intent', split=split)\n",
    "template_new_dataset = PromptTemplate(\n",
    "    config=config_new_dataset,\n",
    "    system_prompt=\"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "    user_prompt=\"Question: {question}\"\n",
    ")\n",
    "\n",
    "# Loading hidden states\n",
    "loading_hidden = True\n",
    "if not loading_hidden:\n",
    "    new_hidden_states = load_datasets(config_new_dataset, template_new_dataset)\n",
    "else:\n",
    "    file_path_new_dataset = '/cs/student/projects2/dsml/cdiezmar/hidden_states/final_intent.pkl'\n",
    "\n",
    "    with open(file_path_new_dataset, 'rb') as file_new:\n",
    "        new_hidden_states = pickle.load(file_new)\n",
    "        print('Length new_dataset: ', len(new_hidden_states))\n",
    "        new_hidden_states_tensor = torch.tensor(new_hidden_states, dtype=torch.float32)\n",
    "\n",
    "# Prepare data\n",
    "train_inputs, train_labels, val_inputs, val_labels = prepare_data_intent(new_hidden_states_tensor)\n",
    "\n",
    "# Define model parameters\n",
    "input_size = train_inputs.size(1)\n",
    "num_classes = 2  # Intent-sensitive or not\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "epochs = 750\n",
    "lr = 1e-5\n",
    "\n",
    "# Train RNN Classifier\n",
    "rnn_model = train_rnn_classifier_cuda(train_inputs, train_labels, val_inputs, val_labels, input_size, hidden_size, num_layers, num_classes, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `question` in template\n",
      "Length new_dataset:  26000\n",
      "Epoch 1, Loss: 0.692796528339386, Val Loss: 0.6934086680412292\n",
      "Epoch 2, Loss: 0.6911830306053162, Val Loss: 0.6919467449188232\n",
      "Epoch 3, Loss: 0.689598023891449, Val Loss: 0.6905089616775513\n",
      "Epoch 4, Loss: 0.6880355477333069, Val Loss: 0.689113438129425\n",
      "Epoch 5, Loss: 0.6865140199661255, Val Loss: 0.6877363920211792\n",
      "Epoch 6, Loss: 0.6850137114524841, Val Loss: 0.6863775253295898\n",
      "Epoch 7, Loss: 0.6835290193557739, Val Loss: 0.6850451231002808\n",
      "Epoch 8, Loss: 0.6820740103721619, Val Loss: 0.6837274432182312\n",
      "Epoch 9, Loss: 0.6806319952011108, Val Loss: 0.6824079155921936\n",
      "Epoch 10, Loss: 0.6791883111000061, Val Loss: 0.6811123490333557\n",
      "Epoch 11, Loss: 0.6777697801589966, Val Loss: 0.6798280477523804\n",
      "Epoch 12, Loss: 0.6763598322868347, Val Loss: 0.6785408854484558\n",
      "Epoch 13, Loss: 0.6749498844146729, Val Loss: 0.6772681474685669\n",
      "Epoch 14, Loss: 0.6735590696334839, Val Loss: 0.6760034561157227\n",
      "Epoch 15, Loss: 0.6721707582473755, Val Loss: 0.6747350096702576\n",
      "Epoch 16, Loss: 0.6707844734191895, Val Loss: 0.6734735369682312\n",
      "Epoch 17, Loss: 0.6694062352180481, Val Loss: 0.672223687171936\n",
      "Epoch 18, Loss: 0.6680338382720947, Val Loss: 0.6709700226783752\n",
      "Epoch 19, Loss: 0.6666625142097473, Val Loss: 0.6697200536727905\n",
      "Epoch 20, Loss: 0.6652964353561401, Val Loss: 0.6684747934341431\n",
      "Epoch 21, Loss: 0.6639369130134583, Val Loss: 0.6672194600105286\n",
      "Epoch 22, Loss: 0.6625665426254272, Val Loss: 0.6659713387489319\n",
      "Epoch 23, Loss: 0.6612012982368469, Val Loss: 0.6647214889526367\n",
      "Epoch 24, Loss: 0.6598351001739502, Val Loss: 0.6634740829467773\n",
      "Epoch 25, Loss: 0.6584717035293579, Val Loss: 0.6622191071510315\n",
      "Epoch 26, Loss: 0.6571017503738403, Val Loss: 0.660959780216217\n",
      "Epoch 27, Loss: 0.6557260155677795, Val Loss: 0.6597014665603638\n",
      "Epoch 28, Loss: 0.6543502807617188, Val Loss: 0.6584328413009644\n",
      "Epoch 29, Loss: 0.6529619097709656, Val Loss: 0.6571647524833679\n",
      "Epoch 30, Loss: 0.6515735983848572, Val Loss: 0.6558966040611267\n",
      "Epoch 31, Loss: 0.6501852869987488, Val Loss: 0.6546116471290588\n",
      "Epoch 32, Loss: 0.6487781405448914, Val Loss: 0.6533288955688477\n",
      "Epoch 33, Loss: 0.6473712921142578, Val Loss: 0.6520402431488037\n",
      "Epoch 34, Loss: 0.6459552645683289, Val Loss: 0.6507432460784912\n",
      "Epoch 35, Loss: 0.6445310711860657, Val Loss: 0.6494438052177429\n",
      "Epoch 36, Loss: 0.6431034207344055, Val Loss: 0.6481401920318604\n",
      "Epoch 37, Loss: 0.6416721940040588, Val Loss: 0.6468232870101929\n",
      "Epoch 38, Loss: 0.6402243971824646, Val Loss: 0.6454980969429016\n",
      "Epoch 39, Loss: 0.638769268989563, Val Loss: 0.6441713571548462\n",
      "Epoch 40, Loss: 0.6373106837272644, Val Loss: 0.6428354978561401\n",
      "Epoch 41, Loss: 0.635844349861145, Val Loss: 0.6414982080459595\n",
      "Epoch 42, Loss: 0.6343770623207092, Val Loss: 0.6401399970054626\n",
      "Epoch 43, Loss: 0.6328856945037842, Val Loss: 0.6387792229652405\n",
      "Epoch 44, Loss: 0.6313951015472412, Val Loss: 0.6374134421348572\n",
      "Epoch 45, Loss: 0.6298984885215759, Val Loss: 0.6360445022583008\n",
      "Epoch 46, Loss: 0.6283957958221436, Val Loss: 0.6346630454063416\n",
      "Epoch 47, Loss: 0.6268872022628784, Val Loss: 0.6332737803459167\n",
      "Epoch 48, Loss: 0.6253681182861328, Val Loss: 0.6318734884262085\n",
      "Epoch 49, Loss: 0.6238371133804321, Val Loss: 0.630474865436554\n",
      "Epoch 50, Loss: 0.6223095059394836, Val Loss: 0.6290629506111145\n",
      "Epoch 51, Loss: 0.6207698583602905, Val Loss: 0.6276624202728271\n",
      "Epoch 52, Loss: 0.6192378401756287, Val Loss: 0.6262410879135132\n",
      "Epoch 53, Loss: 0.6176874041557312, Val Loss: 0.6248211860656738\n",
      "Epoch 54, Loss: 0.6161383390426636, Val Loss: 0.6234018802642822\n",
      "Epoch 55, Loss: 0.6145840883255005, Val Loss: 0.6219768524169922\n",
      "Epoch 56, Loss: 0.6130306720733643, Val Loss: 0.6205489635467529\n",
      "Epoch 57, Loss: 0.6114678382873535, Val Loss: 0.619125247001648\n",
      "Epoch 58, Loss: 0.609912633895874, Val Loss: 0.6176933646202087\n",
      "Epoch 59, Loss: 0.6083480715751648, Val Loss: 0.616266131401062\n",
      "Epoch 60, Loss: 0.6067808270454407, Val Loss: 0.6148374080657959\n",
      "Epoch 61, Loss: 0.6052135825157166, Val Loss: 0.613418459892273\n",
      "Epoch 62, Loss: 0.6036502122879028, Val Loss: 0.6119892597198486\n",
      "Epoch 63, Loss: 0.6020819544792175, Val Loss: 0.6105680465698242\n",
      "Epoch 64, Loss: 0.6005122065544128, Val Loss: 0.6091436743736267\n",
      "Epoch 65, Loss: 0.5989399552345276, Val Loss: 0.6077191829681396\n",
      "Epoch 66, Loss: 0.5973612070083618, Val Loss: 0.6063010692596436\n",
      "Epoch 67, Loss: 0.5957890748977661, Val Loss: 0.6048824191093445\n",
      "Epoch 68, Loss: 0.5942112803459167, Val Loss: 0.6034629940986633\n",
      "Epoch 69, Loss: 0.5926315188407898, Val Loss: 0.6020407676696777\n",
      "Epoch 70, Loss: 0.5910454988479614, Val Loss: 0.6006215214729309\n",
      "Epoch 71, Loss: 0.58946293592453, Val Loss: 0.5992037057876587\n",
      "Epoch 72, Loss: 0.5878749489784241, Val Loss: 0.5977770686149597\n",
      "Epoch 73, Loss: 0.5862793922424316, Val Loss: 0.5963566899299622\n",
      "Epoch 74, Loss: 0.584689736366272, Val Loss: 0.5949204564094543\n",
      "Epoch 75, Loss: 0.5830833315849304, Val Loss: 0.5934955477714539\n",
      "Epoch 76, Loss: 0.5814886093139648, Val Loss: 0.5920601487159729\n",
      "Epoch 77, Loss: 0.5798810124397278, Val Loss: 0.5906275510787964\n",
      "Epoch 78, Loss: 0.5782727003097534, Val Loss: 0.5891823172569275\n",
      "Epoch 79, Loss: 0.5766542553901672, Val Loss: 0.5877349376678467\n",
      "Epoch 80, Loss: 0.5750346779823303, Val Loss: 0.5862877368927002\n",
      "Epoch 81, Loss: 0.5734126567840576, Val Loss: 0.5848342776298523\n",
      "Epoch 82, Loss: 0.5717849731445312, Val Loss: 0.5833838582038879\n",
      "Epoch 83, Loss: 0.5701594948768616, Val Loss: 0.5819180011749268\n",
      "Epoch 84, Loss: 0.5685139894485474, Val Loss: 0.5804572701454163\n",
      "Epoch 85, Loss: 0.5668745040893555, Val Loss: 0.5789926648139954\n",
      "Epoch 86, Loss: 0.565234363079071, Val Loss: 0.5775198936462402\n",
      "Epoch 87, Loss: 0.5635806322097778, Val Loss: 0.5760520696640015\n",
      "Epoch 88, Loss: 0.5619350075721741, Val Loss: 0.5745782256126404\n",
      "Epoch 89, Loss: 0.56027752161026, Val Loss: 0.5731115937232971\n",
      "Epoch 90, Loss: 0.5586255192756653, Val Loss: 0.5716406106948853\n",
      "Epoch 91, Loss: 0.5569660663604736, Val Loss: 0.5701571702957153\n",
      "Epoch 92, Loss: 0.5552938580513, Val Loss: 0.568691074848175\n",
      "Epoch 93, Loss: 0.5536385178565979, Val Loss: 0.5672236680984497\n",
      "Epoch 94, Loss: 0.5519766211509705, Val Loss: 0.5657618641853333\n",
      "Epoch 95, Loss: 0.5503136515617371, Val Loss: 0.5643001198768616\n",
      "Epoch 96, Loss: 0.5486568212509155, Val Loss: 0.5628390908241272\n",
      "Epoch 97, Loss: 0.5469916462898254, Val Loss: 0.5613850951194763\n",
      "Epoch 98, Loss: 0.5453347563743591, Val Loss: 0.5599324107170105\n",
      "Epoch 99, Loss: 0.5436773896217346, Val Loss: 0.5584865808486938\n",
      "Epoch 100, Loss: 0.5420210361480713, Val Loss: 0.5570430159568787\n",
      "Epoch 101, Loss: 0.5403677821159363, Val Loss: 0.5556071996688843\n",
      "Epoch 102, Loss: 0.5387209057807922, Val Loss: 0.5541707277297974\n",
      "Epoch 103, Loss: 0.5370726585388184, Val Loss: 0.5527423620223999\n",
      "Epoch 104, Loss: 0.5354313850402832, Val Loss: 0.5513133406639099\n",
      "Epoch 105, Loss: 0.5337930917739868, Val Loss: 0.5498878955841064\n",
      "Epoch 106, Loss: 0.5321571826934814, Val Loss: 0.5484690070152283\n",
      "Epoch 107, Loss: 0.5305246114730835, Val Loss: 0.5470579266548157\n",
      "Epoch 108, Loss: 0.5288997292518616, Val Loss: 0.5456485748291016\n",
      "Epoch 109, Loss: 0.527283251285553, Val Loss: 0.544234037399292\n",
      "Epoch 110, Loss: 0.5256556868553162, Val Loss: 0.5428328514099121\n",
      "Epoch 111, Loss: 0.5240449905395508, Val Loss: 0.5414339303970337\n",
      "Epoch 112, Loss: 0.5224317908287048, Val Loss: 0.5400396585464478\n",
      "Epoch 113, Loss: 0.5208255648612976, Val Loss: 0.5386572480201721\n",
      "Epoch 114, Loss: 0.519227921962738, Val Loss: 0.537273645401001\n",
      "Epoch 115, Loss: 0.5176241397857666, Val Loss: 0.5359016060829163\n",
      "Epoch 116, Loss: 0.5160335898399353, Val Loss: 0.5345302820205688\n",
      "Epoch 117, Loss: 0.5144379734992981, Val Loss: 0.5331646203994751\n",
      "Epoch 118, Loss: 0.5128486752510071, Val Loss: 0.5318120121955872\n",
      "Epoch 119, Loss: 0.5112664699554443, Val Loss: 0.530457079410553\n",
      "Epoch 120, Loss: 0.5096832513809204, Val Loss: 0.5291087031364441\n",
      "Epoch 121, Loss: 0.5081040859222412, Val Loss: 0.5277677774429321\n",
      "Epoch 122, Loss: 0.5065296292304993, Val Loss: 0.5264372825622559\n",
      "Epoch 123, Loss: 0.5049627423286438, Val Loss: 0.5251125693321228\n",
      "Epoch 124, Loss: 0.5033930540084839, Val Loss: 0.523792028427124\n",
      "Epoch 125, Loss: 0.5018338561058044, Val Loss: 0.5224668979644775\n",
      "Epoch 126, Loss: 0.5002683401107788, Val Loss: 0.5211598873138428\n",
      "Epoch 127, Loss: 0.4987155795097351, Val Loss: 0.5198480486869812\n",
      "Epoch 128, Loss: 0.49715933203697205, Val Loss: 0.5185508131980896\n",
      "Epoch 129, Loss: 0.49562278389930725, Val Loss: 0.517252504825592\n",
      "Epoch 130, Loss: 0.49408066272735596, Val Loss: 0.5159592628479004\n",
      "Epoch 131, Loss: 0.4925406575202942, Val Loss: 0.5146774053573608\n",
      "Epoch 132, Loss: 0.49101054668426514, Val Loss: 0.5133910775184631\n",
      "Epoch 133, Loss: 0.48947715759277344, Val Loss: 0.5121285915374756\n",
      "Epoch 134, Loss: 0.48796015977859497, Val Loss: 0.5108572840690613\n",
      "Epoch 135, Loss: 0.48644018173217773, Val Loss: 0.5095905661582947\n",
      "Epoch 136, Loss: 0.48491764068603516, Val Loss: 0.508338987827301\n",
      "Epoch 137, Loss: 0.48341384530067444, Val Loss: 0.5070919990539551\n",
      "Epoch 138, Loss: 0.48191264271736145, Val Loss: 0.5058509111404419\n",
      "Epoch 139, Loss: 0.48040950298309326, Val Loss: 0.5046188235282898\n",
      "Epoch 140, Loss: 0.47891560196876526, Val Loss: 0.5033888816833496\n",
      "Epoch 141, Loss: 0.4774211347103119, Val Loss: 0.5021685361862183\n",
      "Epoch 142, Loss: 0.4759364128112793, Val Loss: 0.5009610056877136\n",
      "Epoch 143, Loss: 0.4744623303413391, Val Loss: 0.4997541606426239\n",
      "Epoch 144, Loss: 0.4729830324649811, Val Loss: 0.4985446333885193\n",
      "Epoch 145, Loss: 0.4715099036693573, Val Loss: 0.4973459243774414\n",
      "Epoch 146, Loss: 0.47004184126853943, Val Loss: 0.49615657329559326\n",
      "Epoch 147, Loss: 0.4685748815536499, Val Loss: 0.4949836730957031\n",
      "Epoch 148, Loss: 0.4671214818954468, Val Loss: 0.493811696767807\n",
      "Epoch 149, Loss: 0.465667724609375, Val Loss: 0.49263596534729004\n",
      "Epoch 150, Loss: 0.46421322226524353, Val Loss: 0.4914782643318176\n",
      "Epoch 151, Loss: 0.4627726376056671, Val Loss: 0.4903215169906616\n",
      "Epoch 152, Loss: 0.4613310694694519, Val Loss: 0.4891745150089264\n",
      "Epoch 153, Loss: 0.45989492535591125, Val Loss: 0.4880317151546478\n",
      "Epoch 154, Loss: 0.458462655544281, Val Loss: 0.48689574003219604\n",
      "Epoch 155, Loss: 0.4570322036743164, Val Loss: 0.4857654273509979\n",
      "Epoch 156, Loss: 0.45560693740844727, Val Loss: 0.484645813703537\n",
      "Epoch 157, Loss: 0.4541887938976288, Val Loss: 0.4835267961025238\n",
      "Epoch 158, Loss: 0.452776163816452, Val Loss: 0.48241397738456726\n",
      "Epoch 159, Loss: 0.45136478543281555, Val Loss: 0.48130595684051514\n",
      "Epoch 160, Loss: 0.449955016374588, Val Loss: 0.48020878434181213\n",
      "Epoch 161, Loss: 0.4485529959201813, Val Loss: 0.4791181683540344\n",
      "Epoch 162, Loss: 0.44715821743011475, Val Loss: 0.47802385687828064\n",
      "Epoch 163, Loss: 0.4457586109638214, Val Loss: 0.47694119811058044\n",
      "Epoch 164, Loss: 0.44437190890312195, Val Loss: 0.475869745016098\n",
      "Epoch 165, Loss: 0.4429872930049896, Val Loss: 0.4747973084449768\n",
      "Epoch 166, Loss: 0.44159916043281555, Val Loss: 0.47373759746551514\n",
      "Epoch 167, Loss: 0.4402264952659607, Val Loss: 0.4726753234863281\n",
      "Epoch 168, Loss: 0.4388500154018402, Val Loss: 0.4716157913208008\n",
      "Epoch 169, Loss: 0.4374753534793854, Val Loss: 0.47057053446769714\n",
      "Epoch 170, Loss: 0.4361116886138916, Val Loss: 0.4695255756378174\n",
      "Epoch 171, Loss: 0.43474507331848145, Val Loss: 0.4684904217720032\n",
      "Epoch 172, Loss: 0.4333902597427368, Val Loss: 0.46746182441711426\n",
      "Epoch 173, Loss: 0.43203333020210266, Val Loss: 0.4664309322834015\n",
      "Epoch 174, Loss: 0.4306800365447998, Val Loss: 0.46541115641593933\n",
      "Epoch 175, Loss: 0.4293336272239685, Val Loss: 0.4644044041633606\n",
      "Epoch 176, Loss: 0.4279899597167969, Val Loss: 0.4633989930152893\n",
      "Epoch 177, Loss: 0.4266497790813446, Val Loss: 0.4623960554599762\n",
      "Epoch 178, Loss: 0.425315260887146, Val Loss: 0.46139854192733765\n",
      "Epoch 179, Loss: 0.4239816963672638, Val Loss: 0.46040499210357666\n",
      "Epoch 180, Loss: 0.4226502776145935, Val Loss: 0.45942166447639465\n",
      "Epoch 181, Loss: 0.4213254153728485, Val Loss: 0.458450049161911\n",
      "Epoch 182, Loss: 0.4200061559677124, Val Loss: 0.45747634768486023\n",
      "Epoch 183, Loss: 0.4186863899230957, Val Loss: 0.4565126299858093\n",
      "Epoch 184, Loss: 0.41737470030784607, Val Loss: 0.4555501937866211\n",
      "Epoch 185, Loss: 0.4160601794719696, Val Loss: 0.4546007215976715\n",
      "Epoch 186, Loss: 0.41475704312324524, Val Loss: 0.45365193486213684\n",
      "Epoch 187, Loss: 0.41345661878585815, Val Loss: 0.4527099132537842\n",
      "Epoch 188, Loss: 0.4121556580066681, Val Loss: 0.4517713487148285\n",
      "Epoch 189, Loss: 0.41085171699523926, Val Loss: 0.4508417248725891\n",
      "Epoch 190, Loss: 0.40956199169158936, Val Loss: 0.4499225318431854\n",
      "Epoch 191, Loss: 0.40827327966690063, Val Loss: 0.4490050673484802\n",
      "Epoch 192, Loss: 0.4069872200489044, Val Loss: 0.4480925500392914\n",
      "Epoch 193, Loss: 0.40570491552352905, Val Loss: 0.44718819856643677\n",
      "Epoch 194, Loss: 0.4044284224510193, Val Loss: 0.446279376745224\n",
      "Epoch 195, Loss: 0.40314748883247375, Val Loss: 0.44538503885269165\n",
      "Epoch 196, Loss: 0.40187355875968933, Val Loss: 0.4445047378540039\n",
      "Epoch 197, Loss: 0.400613933801651, Val Loss: 0.44361329078674316\n",
      "Epoch 198, Loss: 0.39934420585632324, Val Loss: 0.44273537397384644\n",
      "Epoch 199, Loss: 0.3980802595615387, Val Loss: 0.44186294078826904\n",
      "Epoch 200, Loss: 0.39682576060295105, Val Loss: 0.4409979581832886\n",
      "Epoch 201, Loss: 0.39556992053985596, Val Loss: 0.44013717770576477\n",
      "Epoch 202, Loss: 0.39431649446487427, Val Loss: 0.4392837882041931\n",
      "Epoch 203, Loss: 0.3930731415748596, Val Loss: 0.43843647837638855\n",
      "Epoch 204, Loss: 0.3918225169181824, Val Loss: 0.43759703636169434\n",
      "Epoch 205, Loss: 0.39058148860931396, Val Loss: 0.43675851821899414\n",
      "Epoch 206, Loss: 0.38934147357940674, Val Loss: 0.435926228761673\n",
      "Epoch 207, Loss: 0.388107568025589, Val Loss: 0.43509888648986816\n",
      "Epoch 208, Loss: 0.3868747651576996, Val Loss: 0.43428513407707214\n",
      "Epoch 209, Loss: 0.3856521248817444, Val Loss: 0.433468759059906\n",
      "Epoch 210, Loss: 0.38442331552505493, Val Loss: 0.43266621232032776\n",
      "Epoch 211, Loss: 0.3831973075866699, Val Loss: 0.4318636953830719\n",
      "Epoch 212, Loss: 0.38197609782218933, Val Loss: 0.4310714900493622\n",
      "Epoch 213, Loss: 0.3807626962661743, Val Loss: 0.4302792251110077\n",
      "Epoch 214, Loss: 0.3795433044433594, Val Loss: 0.4294925630092621\n",
      "Epoch 215, Loss: 0.37833625078201294, Val Loss: 0.4287067651748657\n",
      "Epoch 216, Loss: 0.3771245777606964, Val Loss: 0.42793184518814087\n",
      "Epoch 217, Loss: 0.37591975927352905, Val Loss: 0.42716100811958313\n",
      "Epoch 218, Loss: 0.3747161030769348, Val Loss: 0.4263996183872223\n",
      "Epoch 219, Loss: 0.373516708612442, Val Loss: 0.42565152049064636\n",
      "Epoch 220, Loss: 0.37231960892677307, Val Loss: 0.4248928129673004\n",
      "Epoch 221, Loss: 0.37112605571746826, Val Loss: 0.42414984107017517\n",
      "Epoch 222, Loss: 0.36992979049682617, Val Loss: 0.42341017723083496\n",
      "Epoch 223, Loss: 0.3687405586242676, Val Loss: 0.42266780138015747\n",
      "Epoch 224, Loss: 0.36755305528640747, Val Loss: 0.42192500829696655\n",
      "Epoch 225, Loss: 0.3663654625415802, Val Loss: 0.42119789123535156\n",
      "Epoch 226, Loss: 0.36518365144729614, Val Loss: 0.4204671084880829\n",
      "Epoch 227, Loss: 0.3640013337135315, Val Loss: 0.4197508692741394\n",
      "Epoch 228, Loss: 0.36282819509506226, Val Loss: 0.4190410375595093\n",
      "Epoch 229, Loss: 0.3616531193256378, Val Loss: 0.418331116437912\n",
      "Epoch 230, Loss: 0.36047956347465515, Val Loss: 0.4176314175128937\n",
      "Epoch 231, Loss: 0.3593124449253082, Val Loss: 0.4169307351112366\n",
      "Epoch 232, Loss: 0.35813984274864197, Val Loss: 0.4162391424179077\n",
      "Epoch 233, Loss: 0.35697615146636963, Val Loss: 0.41554033756256104\n",
      "Epoch 234, Loss: 0.3558101952075958, Val Loss: 0.4148469865322113\n",
      "Epoch 235, Loss: 0.35465189814567566, Val Loss: 0.4141627848148346\n",
      "Epoch 236, Loss: 0.35349181294441223, Val Loss: 0.413478285074234\n",
      "Epoch 237, Loss: 0.35233283042907715, Val Loss: 0.41280174255371094\n",
      "Epoch 238, Loss: 0.35118070244789124, Val Loss: 0.4121324419975281\n",
      "Epoch 239, Loss: 0.3500254154205322, Val Loss: 0.4114677608013153\n",
      "Epoch 240, Loss: 0.348875492811203, Val Loss: 0.41080552339553833\n",
      "Epoch 241, Loss: 0.34772688150405884, Val Loss: 0.41014745831489563\n",
      "Epoch 242, Loss: 0.34658339619636536, Val Loss: 0.40947744250297546\n",
      "Epoch 243, Loss: 0.34543469548225403, Val Loss: 0.4088256359100342\n",
      "Epoch 244, Loss: 0.34429630637168884, Val Loss: 0.4081713855266571\n",
      "Epoch 245, Loss: 0.3431585133075714, Val Loss: 0.4075244069099426\n",
      "Epoch 246, Loss: 0.3420209586620331, Val Loss: 0.40688246488571167\n",
      "Epoch 247, Loss: 0.3408849537372589, Val Loss: 0.4062454402446747\n",
      "Epoch 248, Loss: 0.33974698185920715, Val Loss: 0.4056176245212555\n",
      "Epoch 249, Loss: 0.3386184573173523, Val Loss: 0.40498802065849304\n",
      "Epoch 250, Loss: 0.33749058842658997, Val Loss: 0.40435895323753357\n",
      "Epoch 251, Loss: 0.3363620936870575, Val Loss: 0.40373244881629944\n",
      "Epoch 252, Loss: 0.33523762226104736, Val Loss: 0.40311312675476074\n",
      "Epoch 253, Loss: 0.3341147005558014, Val Loss: 0.4024934768676758\n",
      "Epoch 254, Loss: 0.332990437746048, Val Loss: 0.40188899636268616\n",
      "Epoch 255, Loss: 0.3318708837032318, Val Loss: 0.40128591656684875\n",
      "Epoch 256, Loss: 0.3307519853115082, Val Loss: 0.4006853699684143\n",
      "Epoch 257, Loss: 0.3296372592449188, Val Loss: 0.40008872747421265\n",
      "Epoch 258, Loss: 0.3285202383995056, Val Loss: 0.3994982838630676\n",
      "Epoch 259, Loss: 0.32740655541419983, Val Loss: 0.3989025950431824\n",
      "Epoch 260, Loss: 0.3262936472892761, Val Loss: 0.398323118686676\n",
      "Epoch 261, Loss: 0.3251870572566986, Val Loss: 0.3977331519126892\n",
      "Epoch 262, Loss: 0.3240759074687958, Val Loss: 0.39715924859046936\n",
      "Epoch 263, Loss: 0.3229682445526123, Val Loss: 0.3965867757797241\n",
      "Epoch 264, Loss: 0.3218657970428467, Val Loss: 0.3960185945034027\n",
      "Epoch 265, Loss: 0.3207601010799408, Val Loss: 0.39545732736587524\n",
      "Epoch 266, Loss: 0.31965333223342896, Val Loss: 0.3949000835418701\n",
      "Epoch 267, Loss: 0.31855788826942444, Val Loss: 0.3943471610546112\n",
      "Epoch 268, Loss: 0.3174571394920349, Val Loss: 0.3937869369983673\n",
      "Epoch 269, Loss: 0.3163580000400543, Val Loss: 0.3932358920574188\n",
      "Epoch 270, Loss: 0.31525957584381104, Val Loss: 0.3926999270915985\n",
      "Epoch 271, Loss: 0.3141680061817169, Val Loss: 0.3921695053577423\n",
      "Epoch 272, Loss: 0.3130735158920288, Val Loss: 0.39164063334465027\n",
      "Epoch 273, Loss: 0.31197965145111084, Val Loss: 0.3911120295524597\n",
      "Epoch 274, Loss: 0.3108871877193451, Val Loss: 0.39057818055152893\n",
      "Epoch 275, Loss: 0.30979594588279724, Val Loss: 0.3900621831417084\n",
      "Epoch 276, Loss: 0.3087109327316284, Val Loss: 0.38954123854637146\n",
      "Epoch 277, Loss: 0.30762359499931335, Val Loss: 0.38902926445007324\n",
      "Epoch 278, Loss: 0.3065376579761505, Val Loss: 0.3885147273540497\n",
      "Epoch 279, Loss: 0.30544769763946533, Val Loss: 0.3880159258842468\n",
      "Epoch 280, Loss: 0.30436477065086365, Val Loss: 0.3875097930431366\n",
      "Epoch 281, Loss: 0.30328240990638733, Val Loss: 0.38700971007347107\n",
      "Epoch 282, Loss: 0.30219846963882446, Val Loss: 0.3865116536617279\n",
      "Epoch 283, Loss: 0.30111563205718994, Val Loss: 0.38602495193481445\n",
      "Epoch 284, Loss: 0.30003759264945984, Val Loss: 0.3855319023132324\n",
      "Epoch 285, Loss: 0.2989591062068939, Val Loss: 0.3850466012954712\n",
      "Epoch 286, Loss: 0.29788026213645935, Val Loss: 0.38457170128822327\n",
      "Epoch 287, Loss: 0.2968025803565979, Val Loss: 0.3840964138507843\n",
      "Epoch 288, Loss: 0.29572588205337524, Val Loss: 0.38361889123916626\n",
      "Epoch 289, Loss: 0.29465222358703613, Val Loss: 0.3831474483013153\n",
      "Epoch 290, Loss: 0.2935757637023926, Val Loss: 0.38267964124679565\n",
      "Epoch 291, Loss: 0.29250743985176086, Val Loss: 0.3822194039821625\n",
      "Epoch 292, Loss: 0.2914368808269501, Val Loss: 0.38176390528678894\n",
      "Epoch 293, Loss: 0.29036352038383484, Val Loss: 0.3813103437423706\n",
      "Epoch 294, Loss: 0.2892948389053345, Val Loss: 0.38084670901298523\n",
      "Epoch 295, Loss: 0.28822341561317444, Val Loss: 0.3804011940956116\n",
      "Epoch 296, Loss: 0.2871544361114502, Val Loss: 0.3799513280391693\n",
      "Epoch 297, Loss: 0.2860885560512543, Val Loss: 0.37950611114501953\n",
      "Epoch 298, Loss: 0.28502586483955383, Val Loss: 0.37906941771507263\n",
      "Epoch 299, Loss: 0.2839607000350952, Val Loss: 0.3786405026912689\n",
      "Epoch 300, Loss: 0.2828977406024933, Val Loss: 0.3782042860984802\n",
      "Epoch 301, Loss: 0.28183141350746155, Val Loss: 0.37777307629585266\n",
      "Epoch 302, Loss: 0.28076961636543274, Val Loss: 0.3773493766784668\n",
      "Epoch 303, Loss: 0.27970919013023376, Val Loss: 0.3769282102584839\n",
      "Epoch 304, Loss: 0.2786475419998169, Val Loss: 0.37650778889656067\n",
      "Epoch 305, Loss: 0.2775890827178955, Val Loss: 0.3760935664176941\n",
      "Epoch 306, Loss: 0.2765320837497711, Val Loss: 0.3756900131702423\n",
      "Epoch 307, Loss: 0.27547454833984375, Val Loss: 0.37527576088905334\n",
      "Epoch 308, Loss: 0.2744181156158447, Val Loss: 0.3748559057712555\n",
      "Epoch 309, Loss: 0.27336224913597107, Val Loss: 0.37445348501205444\n",
      "Epoch 310, Loss: 0.2723076641559601, Val Loss: 0.3740510642528534\n",
      "Epoch 311, Loss: 0.27125367522239685, Val Loss: 0.37366369366645813\n",
      "Epoch 312, Loss: 0.27019983530044556, Val Loss: 0.3732769191265106\n",
      "Epoch 313, Loss: 0.2691473364830017, Val Loss: 0.37288805842399597\n",
      "Epoch 314, Loss: 0.26809588074684143, Val Loss: 0.3725029230117798\n",
      "Epoch 315, Loss: 0.2670446038246155, Val Loss: 0.37211912870407104\n",
      "Epoch 316, Loss: 0.2659962475299835, Val Loss: 0.37172770500183105\n",
      "Epoch 317, Loss: 0.26494863629341125, Val Loss: 0.3713458478450775\n",
      "Epoch 318, Loss: 0.26390156149864197, Val Loss: 0.37097305059432983\n",
      "Epoch 319, Loss: 0.26285499334335327, Val Loss: 0.3705999255180359\n",
      "Epoch 320, Loss: 0.261808305978775, Val Loss: 0.3702351748943329\n",
      "Epoch 321, Loss: 0.26076170802116394, Val Loss: 0.3698789179325104\n",
      "Epoch 322, Loss: 0.25971561670303345, Val Loss: 0.3695206940174103\n",
      "Epoch 323, Loss: 0.25867190957069397, Val Loss: 0.36914944648742676\n",
      "Epoch 324, Loss: 0.25763118267059326, Val Loss: 0.36878424882888794\n",
      "Epoch 325, Loss: 0.25659000873565674, Val Loss: 0.3684302568435669\n",
      "Epoch 326, Loss: 0.2555491626262665, Val Loss: 0.3680835962295532\n",
      "Epoch 327, Loss: 0.2545103430747986, Val Loss: 0.3677467107772827\n",
      "Epoch 328, Loss: 0.25346964597702026, Val Loss: 0.3674030900001526\n",
      "Epoch 329, Loss: 0.252431720495224, Val Loss: 0.36706581711769104\n",
      "Epoch 330, Loss: 0.2513958811759949, Val Loss: 0.36672186851501465\n",
      "Epoch 331, Loss: 0.25035831332206726, Val Loss: 0.36638256907463074\n",
      "Epoch 332, Loss: 0.2493249773979187, Val Loss: 0.36604106426239014\n",
      "Epoch 333, Loss: 0.24829134345054626, Val Loss: 0.3657272756099701\n",
      "Epoch 334, Loss: 0.24725577235221863, Val Loss: 0.36540865898132324\n",
      "Epoch 335, Loss: 0.24622152745723724, Val Loss: 0.3650946021080017\n",
      "Epoch 336, Loss: 0.24519184231758118, Val Loss: 0.36477622389793396\n",
      "Epoch 337, Loss: 0.2441622018814087, Val Loss: 0.364451140165329\n",
      "Epoch 338, Loss: 0.2431323528289795, Val Loss: 0.36412882804870605\n",
      "Epoch 339, Loss: 0.2421073317527771, Val Loss: 0.36381691694259644\n",
      "Epoch 340, Loss: 0.2410811483860016, Val Loss: 0.36351487040519714\n",
      "Epoch 341, Loss: 0.2400481402873993, Val Loss: 0.36322522163391113\n",
      "Epoch 342, Loss: 0.2390250861644745, Val Loss: 0.36293405294418335\n",
      "Epoch 343, Loss: 0.2380024641752243, Val Loss: 0.3626329004764557\n",
      "Epoch 344, Loss: 0.23697592318058014, Val Loss: 0.3623373806476593\n",
      "Epoch 345, Loss: 0.2359553873538971, Val Loss: 0.36204779148101807\n",
      "Epoch 346, Loss: 0.2349373698234558, Val Loss: 0.36174270510673523\n",
      "Epoch 347, Loss: 0.2339145541191101, Val Loss: 0.361472487449646\n",
      "Epoch 348, Loss: 0.23289619386196136, Val Loss: 0.3611876368522644\n",
      "Epoch 349, Loss: 0.23188084363937378, Val Loss: 0.3609250485897064\n",
      "Epoch 350, Loss: 0.23086577653884888, Val Loss: 0.360654354095459\n",
      "Epoch 351, Loss: 0.22984632849693298, Val Loss: 0.36036133766174316\n",
      "Epoch 352, Loss: 0.22883230447769165, Val Loss: 0.360081285238266\n",
      "Epoch 353, Loss: 0.22781828045845032, Val Loss: 0.3598153591156006\n",
      "Epoch 354, Loss: 0.22680920362472534, Val Loss: 0.35956716537475586\n",
      "Epoch 355, Loss: 0.22579899430274963, Val Loss: 0.3593193292617798\n",
      "Epoch 356, Loss: 0.22478783130645752, Val Loss: 0.3590664863586426\n",
      "Epoch 357, Loss: 0.2237861454486847, Val Loss: 0.3588104248046875\n",
      "Epoch 358, Loss: 0.22277918457984924, Val Loss: 0.35854360461235046\n",
      "Epoch 359, Loss: 0.22176891565322876, Val Loss: 0.3582996428012848\n",
      "Epoch 360, Loss: 0.22076526284217834, Val Loss: 0.35806581377983093\n",
      "Epoch 361, Loss: 0.21976275742053986, Val Loss: 0.35782864689826965\n",
      "Epoch 362, Loss: 0.21876084804534912, Val Loss: 0.3576020896434784\n",
      "Epoch 363, Loss: 0.21776050329208374, Val Loss: 0.35736727714538574\n",
      "Epoch 364, Loss: 0.21676203608512878, Val Loss: 0.35713091492652893\n",
      "Epoch 365, Loss: 0.21576744318008423, Val Loss: 0.3568967282772064\n",
      "Epoch 366, Loss: 0.21477118134498596, Val Loss: 0.3566742241382599\n",
      "Epoch 367, Loss: 0.21377748250961304, Val Loss: 0.356460839509964\n",
      "Epoch 368, Loss: 0.2127816528081894, Val Loss: 0.3562551736831665\n",
      "Epoch 369, Loss: 0.21178823709487915, Val Loss: 0.3560338318347931\n",
      "Epoch 370, Loss: 0.21080094575881958, Val Loss: 0.35582539439201355\n",
      "Epoch 371, Loss: 0.20981113612651825, Val Loss: 0.3556135296821594\n",
      "Epoch 372, Loss: 0.2088276743888855, Val Loss: 0.3554094731807709\n",
      "Epoch 373, Loss: 0.20784294605255127, Val Loss: 0.3551999032497406\n",
      "Epoch 374, Loss: 0.20685431361198425, Val Loss: 0.35501283407211304\n",
      "Epoch 375, Loss: 0.20587407052516937, Val Loss: 0.3548167943954468\n",
      "Epoch 376, Loss: 0.20489269495010376, Val Loss: 0.3546285629272461\n",
      "Epoch 377, Loss: 0.20391152799129486, Val Loss: 0.3544273376464844\n",
      "Epoch 378, Loss: 0.2029334008693695, Val Loss: 0.3542414605617523\n",
      "Epoch 379, Loss: 0.20195496082305908, Val Loss: 0.3540613353252411\n",
      "Epoch 380, Loss: 0.20098094642162323, Val Loss: 0.35388508439064026\n",
      "Epoch 381, Loss: 0.20000697672367096, Val Loss: 0.35371261835098267\n",
      "Epoch 382, Loss: 0.1990390419960022, Val Loss: 0.35353946685791016\n",
      "Epoch 383, Loss: 0.1980651170015335, Val Loss: 0.3533678352832794\n",
      "Epoch 384, Loss: 0.19709716737270355, Val Loss: 0.3531888425350189\n",
      "Epoch 385, Loss: 0.19612787663936615, Val Loss: 0.35303130745887756\n",
      "Epoch 386, Loss: 0.19516336917877197, Val Loss: 0.3528602421283722\n",
      "Epoch 387, Loss: 0.19420208036899567, Val Loss: 0.35270437598228455\n",
      "Epoch 388, Loss: 0.19323761761188507, Val Loss: 0.3525635600090027\n",
      "Epoch 389, Loss: 0.19227911531925201, Val Loss: 0.352412611246109\n",
      "Epoch 390, Loss: 0.19132079184055328, Val Loss: 0.3522574305534363\n",
      "Epoch 391, Loss: 0.190365269780159, Val Loss: 0.3520997166633606\n",
      "Epoch 392, Loss: 0.18940825760364532, Val Loss: 0.35194823145866394\n",
      "Epoch 393, Loss: 0.1884547919034958, Val Loss: 0.35179638862609863\n",
      "Epoch 394, Loss: 0.18750445544719696, Val Loss: 0.3516634702682495\n",
      "Epoch 395, Loss: 0.18655498325824738, Val Loss: 0.3515467345714569\n",
      "Epoch 396, Loss: 0.18560966849327087, Val Loss: 0.3514203727245331\n",
      "Epoch 397, Loss: 0.18466049432754517, Val Loss: 0.3512669503688812\n",
      "Epoch 398, Loss: 0.18371663987636566, Val Loss: 0.35112857818603516\n",
      "Epoch 399, Loss: 0.18277393281459808, Val Loss: 0.35100582242012024\n",
      "Epoch 400, Loss: 0.18183408677577972, Val Loss: 0.3508872389793396\n",
      "Epoch 401, Loss: 0.18089792132377625, Val Loss: 0.3507727384567261\n",
      "Epoch 402, Loss: 0.17996205389499664, Val Loss: 0.35065820813179016\n",
      "Epoch 403, Loss: 0.1790289580821991, Val Loss: 0.3505440354347229\n",
      "Epoch 404, Loss: 0.17809374630451202, Val Loss: 0.3504156172275543\n",
      "Epoch 405, Loss: 0.17716512084007263, Val Loss: 0.3502945601940155\n",
      "Epoch 406, Loss: 0.17623433470726013, Val Loss: 0.3502104878425598\n",
      "Epoch 407, Loss: 0.17530611157417297, Val Loss: 0.3501184284687042\n",
      "Epoch 408, Loss: 0.17438292503356934, Val Loss: 0.3500227928161621\n",
      "Epoch 409, Loss: 0.17346134781837463, Val Loss: 0.34992778301239014\n",
      "Epoch 410, Loss: 0.17254215478897095, Val Loss: 0.3498125970363617\n",
      "Epoch 411, Loss: 0.17162396013736725, Val Loss: 0.3496999740600586\n",
      "Epoch 412, Loss: 0.17070898413658142, Val Loss: 0.3496202528476715\n",
      "Epoch 413, Loss: 0.16979417204856873, Val Loss: 0.3495516777038574\n",
      "Epoch 414, Loss: 0.16888220608234406, Val Loss: 0.3494650721549988\n",
      "Epoch 415, Loss: 0.16796767711639404, Val Loss: 0.34937700629234314\n",
      "Epoch 416, Loss: 0.16705843806266785, Val Loss: 0.3492862284183502\n",
      "Epoch 417, Loss: 0.16615703701972961, Val Loss: 0.3492090702056885\n",
      "Epoch 418, Loss: 0.1652555912733078, Val Loss: 0.34914401173591614\n",
      "Epoch 419, Loss: 0.16435196995735168, Val Loss: 0.3490713834762573\n",
      "Epoch 420, Loss: 0.16345295310020447, Val Loss: 0.34901073575019836\n",
      "Epoch 421, Loss: 0.16255511343479156, Val Loss: 0.3489486277103424\n",
      "Epoch 422, Loss: 0.16166037321090698, Val Loss: 0.34887439012527466\n",
      "Epoch 423, Loss: 0.16077002882957458, Val Loss: 0.34879806637763977\n",
      "Epoch 424, Loss: 0.15987659990787506, Val Loss: 0.34874945878982544\n",
      "Epoch 425, Loss: 0.15898843109607697, Val Loss: 0.34870800375938416\n",
      "Epoch 426, Loss: 0.15810266137123108, Val Loss: 0.3486558794975281\n",
      "Epoch 427, Loss: 0.15722240507602692, Val Loss: 0.34859514236450195\n",
      "Epoch 428, Loss: 0.1563403159379959, Val Loss: 0.34854212403297424\n",
      "Epoch 429, Loss: 0.15546029806137085, Val Loss: 0.348503977060318\n",
      "Epoch 430, Loss: 0.1545831263065338, Val Loss: 0.348452627658844\n",
      "Epoch 431, Loss: 0.1537066549062729, Val Loss: 0.3484251797199249\n",
      "Epoch 432, Loss: 0.15283644199371338, Val Loss: 0.34840330481529236\n",
      "Epoch 433, Loss: 0.15196844935417175, Val Loss: 0.34835606813430786\n",
      "Epoch 434, Loss: 0.1511009931564331, Val Loss: 0.3483271896839142\n",
      "Epoch 435, Loss: 0.15024030208587646, Val Loss: 0.34829139709472656\n",
      "Epoch 436, Loss: 0.14937803149223328, Val Loss: 0.34827467799186707\n",
      "Epoch 437, Loss: 0.14851941168308258, Val Loss: 0.34826311469078064\n",
      "Epoch 438, Loss: 0.14766106009483337, Val Loss: 0.3482361435890198\n",
      "Epoch 439, Loss: 0.14680704474449158, Val Loss: 0.34822162985801697\n",
      "Epoch 440, Loss: 0.1459580808877945, Val Loss: 0.3482038378715515\n",
      "Epoch 441, Loss: 0.14510837197303772, Val Loss: 0.34819313883781433\n",
      "Epoch 442, Loss: 0.14426298439502716, Val Loss: 0.34820297360420227\n",
      "Epoch 443, Loss: 0.14341884851455688, Val Loss: 0.34819304943084717\n",
      "Epoch 444, Loss: 0.14257754385471344, Val Loss: 0.34819209575653076\n",
      "Epoch 445, Loss: 0.1417396515607834, Val Loss: 0.34819158911705017\n",
      "Epoch 446, Loss: 0.14090825617313385, Val Loss: 0.34819576144218445\n",
      "Epoch 447, Loss: 0.14007240533828735, Val Loss: 0.3482113480567932\n",
      "Epoch 448, Loss: 0.13924278318881989, Val Loss: 0.348228394985199\n",
      "Epoch 449, Loss: 0.1384156346321106, Val Loss: 0.3482446074485779\n",
      "Epoch 450, Loss: 0.13759467005729675, Val Loss: 0.3482399880886078\n"
     ]
    }
   ],
   "source": [
    "from flashrag.prompt import PromptTemplate\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "# Set configuration for the new dataset\n",
    "config_new_dataset = set_config(dataset_name='final_intent', split=split)\n",
    "template_new_dataset = PromptTemplate(\n",
    "    config=config_new_dataset,\n",
    "    system_prompt=\"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "    user_prompt=\"Question: {question}\"\n",
    ")\n",
    "\n",
    "# Loading hidden states\n",
    "loading_hidden = True\n",
    "if not loading_hidden:\n",
    "    new_hidden_states = load_datasets(config_new_dataset, template_new_dataset)\n",
    "else:\n",
    "    file_path_new_dataset = '/cs/student/projects2/dsml/cdiezmar/hidden_states/final_intent.pkl'\n",
    "\n",
    "    with open(file_path_new_dataset, 'rb') as file_new:\n",
    "        new_hidden_states = pickle.load(file_new)\n",
    "        print('Length new_dataset: ', len(new_hidden_states))\n",
    "        new_hidden_states_tensor = torch.tensor(new_hidden_states, dtype=torch.float32)\n",
    "\n",
    "# Prepare data\n",
    "train_inputs, train_labels, val_inputs, val_labels = prepare_data_intent(new_hidden_states_tensor)\n",
    "\n",
    "# Define model parameters\n",
    "input_size = train_inputs.size(1)\n",
    "num_classes = 2  # Intent-sensitive or not\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "epochs = 450\n",
    "lr = 1e-5\n",
    "\n",
    "# Train BI-RNN Classifier\n",
    "bidir_rnn_model = train_bidir_rnn_classifier_cuda(train_inputs, train_labels, val_inputs, val_labels, input_size, hidden_size, num_layers, num_classes, epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution - Saving classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming `mlp_model` is your trained model\n",
    "model_folder = \"/cs/student/projects2/dsml/cdiezmar/mlp_models/\"\n",
    "classifier_name = \"intent_aware\"\n",
    "extension = \".pth\"\n",
    "model_path = model_folder + classifier_name + extension\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /cs/student/projects2/dsml/cdiezmar/lstm_models/intent_aware.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming `mlp_model` is your trained model\n",
    "model_folder = \"/cs/student/projects2/dsml/cdiezmar/lstm_models/\"\n",
    "classifier_name = \"intent_aware\"\n",
    "extension = \".pth\"\n",
    "model_path = model_folder + classifier_name + extension\n",
    "torch.save(lstm_model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /cs/student/projects2/dsml/cdiezmar/gru_models/intent_aware.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming `mlp_model` is your trained model\n",
    "model_folder = \"/cs/student/projects2/dsml/cdiezmar/gru_models/\"\n",
    "classifier_name = \"intent_aware\"\n",
    "extension = \".pth\"\n",
    "model_path = model_folder + classifier_name + extension\n",
    "torch.save(gru_model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /cs/student/projects2/dsml/cdiezmar/bidir_rnn_models/intent_aware.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming `mlp_model` is your trained model\n",
    "model_folder = \"/cs/student/projects2/dsml/cdiezmar/bidir_rnn_models/\"\n",
    "classifier_name = \"intent_aware\"\n",
    "extension = \".pth\"\n",
    "model_path = model_folder + classifier_name + extension\n",
    "torch.save(bidir_rnn_model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Testing (distribution: 50% retreival , 50% non-retrieval classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def sample_entries(input_file, output_file, sample_size=2400):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        entries = [json.loads(line) for line in f]\n",
    "\n",
    "    # Filter entries based on param_knowledge_answerable value\n",
    "    answerable_entries = [entry for entry in entries if entry['param_knowledge_answerable'] == 1]\n",
    "    non_answerable_entries = [entry for entry in entries if entry['param_knowledge_answerable'] == 0]\n",
    "\n",
    "    # Sample half from each group\n",
    "    sample_size_per_group = sample_size // 2\n",
    "    sampled_answerable = random.sample(answerable_entries, sample_size_per_group)\n",
    "    sampled_non_answerable = random.sample(non_answerable_entries, sample_size_per_group)\n",
    "\n",
    "    # Combine sampled entries\n",
    "    sampled_entries = sampled_answerable + sampled_non_answerable\n",
    "    random.shuffle(sampled_entries)\n",
    "    # Write sampled entries to a new .jsonl file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for entry in sampled_entries:\n",
    "            f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "# Usage\n",
    "input_file = '/cs/student/projects2/dsml/cdiezmar/dataset/retrievalqa/train.jsonl'\n",
    "output_file = '/cs/student/projects2/dsml/cdiezmar/dataset/retrievalqa_2400/train.jsonl'\n",
    "sample_entries(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution - Pipelines evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline TriGate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flashrag.evaluator import Evaluator\n",
    "from flashrag.dataset.utils import split_dataset, merge_dataset\n",
    "from flashrag.pipeline import SequentialPipeline\n",
    "from flashrag.utils import get_dataset\n",
    "# from flashrag.utils import get_retriever, get_generator, get_refiner, get_judger\n",
    "from flashrag.prompt import PromptTemplate\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def run_new_judger(classifiers, dataset, split, do_eval=True, pred_process_fun=None):\n",
    "    # Use the judger function to determine whether retrieval is needed for each question\n",
    "    # judge_result is a list of boolean values, where each element indicates whether retrieval is needed (True) or not (False)\n",
    "    judge_result = judger(dataset, classifiers)\n",
    "    print()\n",
    "    print('Judge result: ', judge_result)\n",
    "    print()\n",
    "\n",
    "    # Set up the configuration using the dataset name and split (train/test/validation)\n",
    "    config = set_config(dataset_name=dataset, split=split)\n",
    "\n",
    "    # Create a prompt template for the model with a specific system and user prompt\n",
    "    template = PromptTemplate(\n",
    "        config=config,\n",
    "        system_prompt=\"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt=\"Question: {question}\"\n",
    "    )\n",
    "\n",
    "    # Load the dataset split (train/test) using the configuration\n",
    "    all_split = get_dataset(config)\n",
    "    dataset = all_split[split]\n",
    "\n",
    "    # Update the dataset with the judge_result field (indicates if retrieval is needed for each question)\n",
    "    dataset.update_output('judge_result', judge_result)\n",
    "\n",
    "    # Split the dataset into two parts: those requiring retrieval and those that do not\n",
    "    pos_dataset, neg_dataset = split_dataset(dataset, judge_result)\n",
    "\n",
    "    # Initialize a pipeline for processing the dataset\n",
    "    pipeline = SequentialPipeline(config)\n",
    "\n",
    "    # Print the number of questions that need and do not need retrieval\n",
    "    print()\n",
    "    print('Questions that NEED retrieval:', len(pos_dataset))\n",
    "    print('Questions that do NOT need retrieval:', len(neg_dataset))\n",
    "    print()\n",
    "\n",
    "    # Run internet retrieval on the dataset that requires retrieval (pos_dataset)\n",
    "    pos_dataset = pipeline.run_internet_retrieval(pos_dataset, do_eval=False)\n",
    "\n",
    "    # Set the prompt template for the questions that do not need retrieval (neg_dataset)\n",
    "    pipeline.prompt_template = template\n",
    "    neg_dataset = pipeline.naive_run(neg_dataset, do_eval=False)\n",
    "\n",
    "    # Merge the positive and negative datasets back into the original dataset format based on the judge_result\n",
    "    dataset = merge_dataset(pos_dataset, neg_dataset, judge_result)\n",
    "\n",
    "    # Evaluate the final dataset if needed (do_eval=True) and apply the prediction processing function if provided\n",
    "    dataset = pipeline.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "run_new_judger(['intent_aware', 'time_aware', 'knowledge_aware'], dataset ='retrievalqa_200', split = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- with retrievalqa_200 (length: 200)\n",
    "\n",
    "{'em': 0.425, 'f1': 0.48292377899877886, 'sub_em': 0.525, 'precision': 0.48489542160737814, 'recall': 0.5108333333333335}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline - TriGate + VanillaFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrag.dataset.utils import split_dataset, merge_dataset\n",
    "from flashrag.pipeline import SequentialPipeline\n",
    "from flashrag.utils import get_dataset\n",
    "# from flashrag.utils import get_retriever, get_generator, get_refiner, get_judger\n",
    "from flashrag.prompt import PromptTemplate\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def run_new_judger_with_vanilla(classifiers, dataset, split, do_eval=True, pred_process_fun=None):\n",
    "    # Use the vanilla filter-based judger to decide which questions need retrieval\n",
    "    judge_result = judger_with_vanilla_filter(dataset, classifiers)\n",
    "    print()\n",
    "    print('Judge result: ', judge_result)\n",
    "    print()\n",
    "    \n",
    "    # Set configuration for dataset, split, and system/user prompts\n",
    "    config = set_config(dataset_name=dataset, split=split)\n",
    "    \n",
    "    template = PromptTemplate(\n",
    "        config=config,\n",
    "        system_prompt=\"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt=\"Question: {question}\"\n",
    "    )\n",
    "\n",
    "    # Load dataset and add the judge result (retrieval decision) to the dataset\n",
    "    all_split = get_dataset(config)\n",
    "    dataset = all_split[split]\n",
    "    dataset.update_output('judge_result', judge_result)\n",
    "\n",
    "    # Split dataset into two parts: those needing retrieval and those that don't\n",
    "    pos_dataset, neg_dataset = split_dataset(dataset, judge_result)\n",
    "    pipeline = SequentialPipeline(config)\n",
    "\n",
    "    print()\n",
    "    print('Questions that NEED retrieval:', len(pos_dataset))\n",
    "    print('Questions that do NOT need retrieval:', len(neg_dataset))\n",
    "    print()\n",
    "\n",
    "    # Process the positive dataset (questions needing retrieval) using internet retrieval\n",
    "    pos_dataset = pipeline.run_internet_retrieval(pos_dataset, do_eval=False)\n",
    "    \n",
    "    # Process the negative dataset (questions not needing retrieval) using naive pipeline\n",
    "    pipeline.prompt_template = template\n",
    "    neg_dataset = pipeline.naive_run(neg_dataset, do_eval=False)\n",
    "\n",
    "    # Merge both the processed datasets (retrieval-based and naive) back together\n",
    "    dataset = merge_dataset(pos_dataset, neg_dataset, judge_result)\n",
    "\n",
    "    # Evaluate the final dataset\n",
    "    dataset = pipeline.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)\n",
    "\n",
    "    # Return the final processed and evaluated dataset\n",
    "    return dataset\n",
    "# run_new_judger_with_vanilla(['intent_aware', 'time_aware', 'knowledge_aware'], dataset ='retrievalqa_200', split = 'train')\n",
    "run_new_judger_with_vanilla(['intent_aware', 'time_aware'], dataset ='retrievalqa_200', split = 'train')\n",
    "# run_new_judger_with_vanilla(['time_aware'], dataset ='retrievalqa_200', split = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_new_judger_with_vanilla(['time_aware', 'intent_aware'], dataset ='small_retrievalqa', split = 'train')\n",
    "\n",
    "-  with retrievalqa_200 (length: 200):\n",
    "\n",
    "{'em': 0.4, 'f1': 0.46396703296703296, 'sub_em': 0.51, 'precision': 0.4667979797979798, 'recall': 0.4866666666666667}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline - TriGate + SKR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrag.dataset.utils import split_dataset, merge_dataset\n",
    "from flashrag.pipeline import SequentialPipeline\n",
    "from flashrag.utils import get_dataset\n",
    "# from flashrag.utils import get_retriever, get_generator, get_refiner, get_judger\n",
    "from flashrag.prompt import PromptTemplate\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def run_new_judger_with_skr(classifiers, dataset, split, do_eval=True, pred_process_fun=None):\n",
    "    # judge_result: list of bool element, representing whether to use retrieval\n",
    "    judge_result = judger_with_skr(dataset, classifiers)\n",
    "    print()\n",
    "    print('Judge result: ',judge_result)\n",
    "    print()\n",
    "    config= set_config(dataset_name=dataset, split = split)\n",
    "\n",
    "    template= PromptTemplate(\n",
    "        config = config,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "\n",
    "    all_split = get_dataset(config)\n",
    "    dataset = all_split[split]\n",
    "    dataset.update_output('judge_result', judge_result)\n",
    "\n",
    "    # split dataset based on judge_result\n",
    "    pos_dataset, neg_dataset = split_dataset(dataset, judge_result)\n",
    "    pipeline = SequentialPipeline(config)\n",
    "\n",
    "    print()\n",
    "    print('Questions that NEED retrieval',len(pos_dataset))\n",
    "    print('Questions that does NOT need retrieval',len(neg_dataset))\n",
    "    print()\n",
    "\n",
    "    pos_dataset = pipeline.run_internet_retrieval(pos_dataset, do_eval=False)\n",
    "    pipeline.prompt_template = template\n",
    "    neg_dataset = pipeline.naive_run(neg_dataset, do_eval=False)\n",
    "\n",
    "    # merge datasets into original format\n",
    "    dataset = merge_dataset(pos_dataset, neg_dataset, judge_result)\n",
    "\n",
    "    dataset = pipeline.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "run_new_judger_with_skr(['intent_aware', 'time_aware', 'knowledge_aware'], dataset ='retrievalqa_200', split = 'train')\n",
    "# run_new_judger_with_skr([], dataset ='retrievalqa_200', split = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_new_judger_with_skr(['intent_aware', 'time_aware', 'knowledge_aware'], dataset ='retrievalqa_200', split = 'train')\n",
    "\n",
    "-  with retrievalqa_200 (length: 200):\n",
    "\n",
    "{'em': 0.405, 'f1': 0.4567231559290382, 'sub_em': 0.495, 'precision': 0.456, 'recall': 0.4733333333333334}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline - No RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrag.config import Config\n",
    "from flashrag.pipeline import SequentialPipeline\n",
    "from flashrag.utils import get_dataset\n",
    "\n",
    "def zero_shot(dataset:str, split:str):\n",
    "    # save_note = 'zero-shot'\n",
    "    config= set_config(dataset_name=dataset, split = split)\n",
    "\n",
    "    all_split = get_dataset(config)\n",
    "    test_data = all_split[split]\n",
    "\n",
    "    from flashrag.prompt import PromptTemplate\n",
    "    template = PromptTemplate(\n",
    "        config = config,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "    pred_process_fun = lambda x: x.split(\"\\n\")[0]\n",
    "    pipeline = SequentialPipeline(config, template)\n",
    "    result = pipeline.naive_run(test_data)\n",
    "    # result = pipeline.naive_run(test_data,pred_process_fun = pred_process_fun)\n",
    "\n",
    "zero_shot(dataset='retrievalqa_200', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero_shot(dataset='retrievalqa_200', split='train')\n",
    "\n",
    "-  with retrievalqa_200 (length: 200):\n",
    "\n",
    "{'em': 0.285, 'f1': 0.31938034188034187, 'sub_em': 0.39, 'precision': 0.31866666666666665, 'recall': 0.33229166666666665}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline - Always RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrag.config import Config\n",
    "from flashrag.pipeline import SequentialPipeline\n",
    "from flashrag.utils import get_dataset\n",
    "from flashrag.prompt import PromptTemplate\n",
    "def sequential(dataset:str, split:str):\n",
    "    \"\"\"\n",
    "    Reference:\n",
    "        Zhihong Shao et al. \"Enhancing Retrieval-Augmented Large Language Models with Iterative\n",
    "                            Retrieval-Generation Synergy\"\n",
    "        in EMNLP Findings 2023.\n",
    "\n",
    "        Zhangyin Feng et al. \"Retrieval-Generation Synergy Augmented Large Language Models\"\n",
    "        in EMNLP Findings 2023. \n",
    "    \"\"\"\n",
    "\n",
    "    config= set_config(dataset_name=dataset, split = split)\n",
    "\n",
    "    all_split = get_dataset(config)\n",
    "    test_data = all_split[split]\n",
    "    pipeline = SequentialPipeline(config)\n",
    "    result = pipeline.run_internet_retrieval(test_data)\n",
    "    return result\n",
    "\n",
    "sequential(dataset='retrievalqa_200', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequential(dataset='retrievalqa_200', split='train')\n",
    "\n",
    "-  with retrievalqa_200 (length: 200):\n",
    "\n",
    "{'em': 0.41, 'f1': 0.4919845661584791, 'sub_em': 0.535, 'precision': 0.4942689393939394, 'recall': 0.5183333333333333}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipepline - TriGate LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrag.dataset.utils import split_dataset, merge_dataset\n",
    "from flashrag.pipeline import SequentialPipeline\n",
    "from flashrag.utils import get_dataset\n",
    "# from flashrag.utils import get_retriever, get_generator, get_refiner, get_judger\n",
    "from flashrag.prompt import PromptTemplate\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def run_judger_lstm(classifiers, dataset, split, do_eval=True, pred_process_fun=None):\n",
    "    # judge_result: list of bool element, representing whether to use retrieval\n",
    "    judge_result = judger_lstm(dataset, classifiers)\n",
    "    print()\n",
    "    print('Judge result: ',judge_result)\n",
    "    print()\n",
    "    config= set_config(dataset_name=dataset, split = split)\n",
    "    template= PromptTemplate(\n",
    "        config = config,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "\n",
    "    all_split = get_dataset(config)\n",
    "    dataset = all_split[split]\n",
    "    dataset.update_output('judge_result', judge_result)\n",
    "\n",
    "    # split dataset based on judge_result\n",
    "    pos_dataset, neg_dataset = split_dataset(dataset, judge_result)\n",
    "    pipeline = SequentialPipeline(config)\n",
    "\n",
    "    print()\n",
    "    print('Questions that NEED retrieval',len(pos_dataset))\n",
    "    print('Questions that does NOT need retrieval',len(neg_dataset))\n",
    "    print()\n",
    "\n",
    "    pos_dataset = pipeline.run_internet_retrieval(pos_dataset, do_eval=False)\n",
    "    pipeline.prompt_template = template\n",
    "    neg_dataset = pipeline.naive_run(neg_dataset, do_eval=False)\n",
    "\n",
    "    # merge datasets into original format\n",
    "    dataset = merge_dataset(pos_dataset, neg_dataset, judge_result)\n",
    "\n",
    "    dataset = pipeline.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "run_judger_lstm(['intent_aware', 'time_aware', 'knowledge_aware'], dataset ='retrievalqa_200', split = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_judger_lstm(['intent_aware', 'time_aware', 'knowledge_aware'], dataset ='retrievalqa_200', split = 'train')\n",
    "\n",
    "- with retrievalqa_200 (length: 200)\n",
    "\n",
    "{'em': 0.435, 'f1': 0.48687301587301585, 'sub_em': 0.515, 'precision': 0.4915, 'recall': 0.4933333333333334}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline -  TriGate GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length small_dataset:  200\n",
      "\n",
      "Judge result:  [False  True False False  True False False  True  True False False  True\n",
      " False  True  True  True False False False False False  True  True False\n",
      " False  True  True False  True False False False  True False  True  True\n",
      " False False  True  True False False False False False False  True False\n",
      "  True False  True False  True False  True False  True False False False\n",
      " False  True False  True False False False  True False False  True False\n",
      " False False False False  True  True False  True  True False  True False\n",
      " False False False False False False False False False  True False  True\n",
      " False  True False  True  True  True False False False  True  True False\n",
      "  True  True False False False False False False False  True False  True\n",
      " False False  True  True  True False  True  True False  True False False\n",
      " False False  True False False False  True False False False False False\n",
      "  True False False  True  True False False  True False False  True False\n",
      "  True False False False False False False False  True False  True False\n",
      " False  True  True False  True False  True False False False False  True\n",
      "  True  True  True False  True False False False False  True False False\n",
      " False  True False  True False  True False False]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_299335/4248982447.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  hidden_states_tensor= torch.tensor(hidden_states, dtype=torch.float32)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `question` in template\n",
      "Find `reference` in template\n",
      "Find `question` in template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 10:45:54,292\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 10:45:54 config.py:1130] Casting torch.float32 to torch.float16.\n",
      "INFO 08-08 10:45:54 config.py:1151] Downcasting torch.float32 to torch.float16.\n",
      "INFO 08-08 10:45:54 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/cs/student/projects2/dsml/cdiezmar/models/llama3-8B-instruct', speculative_config=None, tokenizer='/cs/student/projects2/dsml/cdiezmar/models/llama3-8B-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/cs/student/projects2/dsml/cdiezmar/models/llama3-8B-instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 10:45:57 model_runner.py:146] Loading model weights took 14.9634 GB\n",
      "INFO 08-08 10:46:01 gpu_executor.py:83] # GPU blocks: 1656, # CPU blocks: 2048\n",
      "INFO 08-08 10:46:02 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-08 10:46:02 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-08 10:46:06 model_runner.py:924] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions that NEED retrieval 126\n",
      "Questions that does NOT need retrieval 74\n",
      "[[{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Michael Douglas and Noah Jupe in Franklin (Image credit: Apple TV). Portraying <b>Benjamin</b> Franklin is Hollywood legend Michael Douglas. Douglas is an Oscar-winning actor for his role as Gordon Gekko in Wall Street, as well as an Emmy winner for playing Liberace in the TV limited series Behind the Candelabra.Many will know him from his more recent work in Marvel&#39;s Ant-Man franchise and from his ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Cumberbatch at the 2014 San Diego Comic-Con. Actor Benedict Cumberbatch has performed in many films, television series, theatre productions, and recorded lines for various radio programs, narrations and video games. He first performed for the New Shakespeare Company at Open Air Theatre, Regent&#39;s Park for two seasons. He later portrayed George Tesman in Richard Eyre&#39;s revival of Hedda Gabler ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'At a vacation resort, 11-year-old Sophie (Frankie Corio) spends <b>time</b> with her loving and idealistic father (Paul Mescal) – a last holiday together that she&#39;s still coming to grips with 20 years later in Charlotte Wells’ emotional drama. Frankie Corio, Kayleigh Coleman, Charlotte Wells, Paul Mescal, Celia Rowlson-Hall. Drama.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Giant</b> logo used before re-branding in 2020. Some stores still use this logo as of 2024. The <b>Giant</b> Company (formerly known as <b>Giant</b> <b>Food</b> Stores) is an American regional supermarket chain that operates in Pennsylvania, Maryland, Virginia, and West Virginia under the <b>Giant</b> and Martin&#39;s brands. It is a subsidiary of Ahold Delhaize, and headquartered in Carlisle, Pennsylvania.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'What <b>Country</b> Owns <b>Giant</b> <b>Food</b>? <b>Giant</b> <b>Food</b> is a major grocery store chain that operates throughout the Mid-Atlantic region of the United States. The company was founded in 1936 by David Javitch and Nathan Hochman and has been headquartered in Landover, Maryland since its inception. It is currently owned by Dutch retail <b>giant</b> Ahold Delhaize, which ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Couscous (Arabic: كُسْكُس, romanized: kuskus) is a traditional North African dish of small steamed granules of rolled semolina that is often served with a stew spooned on top. Pearl millet, sorghum, bulgur, and other cereals are sometimes cooked in a similar way in other regions, and the resulting dishes are also sometimes called couscous.: 18', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Thing</b> <b>We</b> <b>Love</b>. The <b>Thing</b> <b>We</b> <b>Love</b> is a 1918 American silent drama film starring Wallace Reid, Kathlyn Williams, and Tully Marshall, produced by Jesse Lasky, distributed by Paramount Pictures, and directed by Lou Tellegen. This marked Tellegen&#39;s second foray into directing as he usually was a leading man in front of the camera like Reid.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Thing We Love</b>: Directed by Lou Tellegen. With Wallace Reid, Kathlyn Williams, Tully Marshall, Mayme Kelso. Just prior to America&#39;s declaration of war, Margaret Kenwood of the Kenwood Manufacturing Company determines that the plant should produce munitions to support the Allies. Rodney Sheridan, her sweetheart and a vice president of the company, remains unimpressed with Margaret&#39;s ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Just prior to America&#39;s declaration of war, Margaret Kenwood of the Kenwood Manufacturing Company determines that the plant should produce munitions to support the Allies. Rodney Sheridan, her sweetheart and a vice president of the company, remains unimpressed with Margaret&#39;s patriotism until he begins to suspect that the plant&#39;s president is involved with a group of German spies.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The main difference between <b>home country</b> and host <b>country</b> is that the <b>home</b> <b>country</b> refers to the <b>country</b> where a person was born while the host <b>country</b> refers to the <b>country</b> where a person resides.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>country</b> a person comes from.... Click for English pronunciations, examples sentences, video.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Key Difference – <b>Home</b> vs Host <b>Country</b>. Host <b>country</b> and <b>home</b> <b>country</b> are terms with opposite meanings in a business context. <b>Home</b> <b>country</b> refers to the <b>country</b> where a company’s headquarters is located, while host <b>country</b> refers to foreign <b>countries</b> where the company invests. This is the main difference between <b>home</b> <b>country</b> and host <b>country</b>.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '&quot;<b>That</b>&#39;s All <b>Right</b>&quot; is a song written and originally performed by the American blues singer Arthur Crudup and recorded in 1946. ... Arthur Crudup was credited as the <b>composer</b> on the label of Presley&#39;s single, but despite legal battles into the 1970s, reportedly never received royalties. An out-of-court settlement was supposed to pay Crudup an ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The meaning behind “<b>That’s</b> All <b>Right</b>” goes beyond its catchy melody and toe-tapping rhythm. At its core, the song speaks to the acceptance of a failed relationship and the determination to move on. It portrays the resilience and optimism of a person who understands that sometimes, ending a relationship is necessary for personal growth and ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The Story Behind The Song: Revisiting Elvis’ first single ‘<b>That’s</b> All <b>Right</b>’. Tyler Golsen @TylerGolsen. Tue 5 July 2022 17:15, UK. On July 5th, 1954, Elvis Presley was stuck. He was in Sun Studios, the same place where he had cut his first official acetate record nearly a year before, but he had no record contract, no fan base, and no ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Lesley Garrett <b>CBE</b> is Britain’s best known soprano, regularly appearing in <b>opera</b>, music theatre, concert, on television and CD. ... Recent albums are Travelling Light, The <b>Singer</b>, So Deep is the Night and Amazing Grace. ... Lesley <b>was awarded</b> <b>a CBE</b> in the <b>2002</b> New Year’s Honours List for Services to Music and is a Fellow of the Royal ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'She has also sung <b>opera</b> and pop classics with Bryan Ferry, ... <b>In 2002</b> Garrett was appointed <b>a CBE</b> for her services to music. She was also <b>awarded</b> with a BASCA Gold Badge <b>Award</b> in October 2010, ... The <b>Singer</b> (<b>2002</b>) So Deep is the Night (2003) When I Fall in Love (2007) – UK No. 11;', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The 64-year-old has travelled around the world singing <b>opera</b> and pop classics, eventually being <b>awarded</b> <b>a CBE</b> for her services to music <b>in 2002</b>. ... But why does the successful <b>singer</b> and TV star ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'When Emile Mosseri stepped in as the <b>composer</b> <b>of Homecoming</b>, he had gigantic shoes to fill.The first season of the Amazon Prime series, based on the popular Gimlet podcast, was “scored” by the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'omecoming&quot; by Bruce DaweDawe here dramatises the <b>homecoming</b> of Australian veter. ns&#39; bodies from Vietnam. This is clearly an anti-war poem, reproducing in the seventies the sentiments of t. e First World War poets.In 25 lines of broken verse presented in one demanding stanza, Dawe recounts how &quot;they are bringing&quot; home the bodies &quot;in deep freeze ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Hugh Hagood Hardy, CM (February 26, 1937 – January 1, 1997) was a Canadian <b>composer</b>, pianist, and vibraphonist.He played mainly jazz and easy listening music. He is best known for the 1975 single, &quot;The <b>Homecoming</b>&quot; from his album of the same name, and for his soundtrack to the Anne of Green Gables and Anne of Avonlea films.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'We reveal the quietest and busiest times at the <b>gym</b> later in the report. 2023/24 <b>GYM</b> USAGE. Key Findings. 16% of people in the UK are currently a member of a <b>gym</b> (up <b>2</b>% year-on-year) A further 16% are planning to join a <b>gym</b> in the next calendar year, which is down <b>3</b>% compared to last year.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'If the times are not already in 24-hour time, convert them to 24-hour time. AM hours are the same in both 12-hour and 24-hour time. For <b>PM</b> hours, add 12 to the number to convert it to 24-hour time. For example, 1:<b>00</b> <b>PM</b> would be 13:<b>00</b> in 24-hour time. Determine whether the number of minutes is larger in the starting time or the ending time.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The second <b>workout</b> of the <b>2022</b> <b>CrossFit</b> Open has been revealed. On March <b>3</b>, <b>2022</b>, in Ohio, two separate male and female duels happened, and kicked off the <b>CrossFit</b> Open <b>Workout</b> 22.<b>2</b>. The athletes who faced off are Justin Medeiros vs Saxon Panchik, and Laura Horvath vs Emma Lawson. The video submissions for the <b>CrossFit</b> Open <b>Workout</b> 22.<b>2</b> are due until Monday, March 7, <b>2022</b>, 8:<b>00</b> <b>p.m</b>. EST.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Genre</b>(s) Fantasy MUD: Mode(s) Multiplayer: A screenshot from MUD1. Multi-User Dungeon, or MUD (referred to as MUD1, to distinguish it from its successor, MUD2, and the MUD <b>genre</b> in general), is the first MUD. History. MUD was created in 1978 by Roy Trubshaw and Richard Bartle at the University of Essex on a DEC PDP-10.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'MUD1 <b>(1978</b>) 5 comments. Multi-User Dungeon, or MUD (referred to as MUD1, to distinguish it from its successor, MUD2, and the MUD <b>genre</b> in general) is the first MUD and the oldest virtual world in existence. It was created in 1978 by Roy Trubshaw at Essex University on a DEC PDP-10 in the UK, using the MACRO-10 assembly language.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'To truly understand MUD1 (the game which spawned the entire <b>genre</b> of Multi-User Dungeons), we need to go back to 1976 and the release of the first text adventure game: “Colossal Cave Adventure” (or simply ADVENT as it was known), developed by William Crowther.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>John</b> Edward <b>Robinson</b> (<b>born</b> December 27, 1943) is an American convicted serial killer, kidnapper, rapist, and forger. He was found guilty and received the death penalty in 2003 for three murders committed in Kansas.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>John</b> Graham <b>Robinson</b> was <b>born</b> on the 11th of November 1903 in Liverpool. His first professional appearance came in his home <b>city</b> in 1929, a...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Here’s what you need to know: 1. <b>Robinson</b> Was an Eagle Scout Who Once Sang for the English Queen But Claimed His Childhood Was Abusive. <b>John</b> Edward <b>Robinson</b> Sr. <b>John</b> Edward <b>Robinson</b> was <b>born</b> in ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Patience</b> Agbabi FRSL (born 1965) is a British poet and performer who emphasizes the spoken word. ... and offers readers a diverse sampling of the <b>author</b>&#39;s views of life in a variety of places.&quot; Carol Rumens has said: &quot;Agbabi characteristically makes poetry an opportunity for conversation with the past, ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Patience</b> Agbabi is a poet, performer and workshop facilitator. She was born in London in 1965 to Nigerian parents and spent her teenage years living in North Wales. She was educated at Oxford University and has appeared at numerous diverse venues in the UK and abroad. R.A.W., her groundbreaking debut collection of poetry, was published in 1995 ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Few people know who said &quot;<b>Patience</b> is a virtue&quot; but the first publishing appears to come from the poem, &quot;Piers Plowman&quot; in the 14th century. ... Like many of the famous sayings we recite today, the original <b>author</b> <b>of &quot;Patience</b> is a virtue&quot; is hard to pin down. Some date it back to Cato to the Elder in the third or fourth century.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Their images are now available to the public in the US, after <b>Disney&#39;s</b> <b>copyright</b> <b>expired</b>. It means creatives like cartoonists can now rework and use the earliest versions <b>of Mickey</b> <b>and Minnie</b>. In ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The original story of <b>Winnie</b>-<b>the-Pooh</b>, created by English author A. A. Milne and illustrator E. H. Shepard, entered the public domain in 2022. The move effectively ended <b>Disney&#39;s</b> exclusive use of the character and led to a low-budget horror movie <b>Winnie</b> <b>the Pooh</b>: Blood and Honey.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Winnie</b> <b>the Pooh</b>, which became another beloved <b>Disney</b> character, actually entered the public domain in 2022 after the <b>copyright</b> on A.A. Milne’s original stories about the bear <b>expired</b>. Last year ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Alain Laurier</b> ( French pronunciation: [alɛ̃ loʁje]) (12 September 1944 – 25 December 2023) was a French football manager and player.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Alain LAURIER</b>. Team France. Games Participations 1. First Olympic Games Mexico City 1968. Year of Birth 1944. Olympic Results.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Alain Laurier</b>, Plays forShenyang Haishi, Al Wasl, Cercle Dijon Football, FC Istres, Grenoble Foot 38, AS Poissy, SM Caen, US Le Mans, Angers SCO, Paris FC, Stade Reims, Paris Joinville', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Washington CNN —. An estimated 305,000 people initially <b>received</b> federal <b>student</b> <b>loan</b> <b>bills</b> with the wrong amount – <b>many</b> with charges higher than they should be – when payments resumed this ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'More than 28 million federal <b>student</b> <b>loan</b> <b>borrowers</b> returned to repayment this month after a pandemic-related relief program put their monthly <b>bills</b> on pause for nearly four years. David Degner ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The Education Department said less than 1% of <b>student</b>-<b>loan</b> <b>borrowers</b> were affected by payment issues as <b>bills</b> started becoming due. ... 305,000 <b>borrowers</b> had <b>received</b> <b>inaccurate</b> monthly payments ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Republican</b> <b>presidential</b> primary debates are underway — without Donald Trump. But for the other <b>candidates</b>, just getting on the <b>debate</b> stage is an existential fight for their campaigns.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Republican</b> <b>presidential</b> <b>candidate</b> and former biotech executive Vivek Ramaswamy gestures at the first <b>Republican</b> <b>candidates</b>&#39; <b>debate</b> of the 2024 U.S. <b>presidential</b> campaign in Milwaukee, Wisconsin, U ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Republican</b> <b>presidential</b> <b>candidates</b> faced off tonight in Miami in the third primary <b>debate</b> of the 2024 cycle. Follow here for the latest live news updates and analysis.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Dominick Bellizzi</b>; <b>Occupation</b>: Jockey: Born: c. 1912 New York: Died: May 17, 1934 (aged 21) Resting place: Holy Sepulchre Cemetery, New Rochelle, New York: ... <b>Dominick Bellizzi</b> (c. 1912 – 17 May 1934) was an American jockey who died at age 21 as a result of a horse racing accident. He was known as &quot;The Duke&quot;.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Dominick</b> <b>Bellizzi</b> was an American jockey who died at age 21 as a result of a horse racing accident. He was known as &quot;The Duke&quot;. Introduction <b>Dominick</b> <b>Bellizzi</b>; References ( , , . ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'In the Education &amp; Training industry, <b>Dominick</b> <b>Bellizzi</b> has 6,907 colleagues in 476 companies located in 34 countries. 3,626 executive movements have been recorded in the last 12 months. Learn more about Education &amp; Training.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Kuala Lumpur</b> was the founding <b>capital</b> of the Federation of Malaya and its successor, Malaysia. The city remained the seat of the executive and judicial branches of the Malaysian federal government until these were relocated to Putrajaya in early 1999. [10] However, some sections of the political bodies still remain in <b>Kuala Lumpur</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Kuala Lumpur</b>, <b>capital</b> of Malaysia. The city is located in west-central Peninsular (West) Malaysia, midway along the west coast tin and rubber belt and about 25 miles (40 km) east of its ocean port, Port Kelang, on the Strait of Malacca. It is the country’s largest urban area and its cultural, commercial, and transportation centre.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Kuala</b> <b>Lumpur</b>, <b>the capital</b> of Malaysia, is located in west-central Peninsular Malaysia; its ocean port, Port Kelang, sits about 25 miles (40 km) to the west on the Strait of Malacca. <b>Kuala</b> <b>Lumpur</b>, which lies astride the confluence of the Kelang and Gombak rivers, is Malaysia&#39;s largest urban area as well as its cultural, commercial, and transportation centre. The city&#39;s commercial quarter, known ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'You can choose to <b>attend</b> the weekly <b>book</b> <b>club</b> <b>meeting</b> either in person at the Edinburgh Jesuit Centre at Sacred Heart Church at 11.15am (in the Ogilvie Room) or online using the Zoom app at 7.00pm - 8.00pm each week: Zoom Login Information. Join us on Zoom by following the link below. The link will be the same each week!', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'A <b>book</b> for prayer, reflection and conversation about our common future. To mark the Ignatian Year, the global leader of the Jesuits, Fr Arturo Sosa SJ, has written a thought provoking <b>book</b> - Walking with Ignatius - which he hopes will promote and encourage an open conversation, creativity and discernment among Jesuits and Ignatian partners in ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The lighthouse and Chesapeake Bay descriptions and the culture brought this <b>book</b> to life. The characters were strong but still displayed a Christ-like gentleness that was wonderful to read. This a great 5-star <b>book</b> to add to your Christian <b>book</b> <b>club</b> reading list! buy on amazon. Star Rating: ☆.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '1500 m. Stephen Cram, CBE (born 14 October 1960) is a British retired track and field <b>athlete</b>. Along with fellow Britons Sebastian Coe and Steve Ovett, he was one of the world&#39;s dominant middle distance runners during the 1980s. <b>Nicknamed</b> &quot;<b>The Jarrow</b> <b>Arrow</b>&quot;, after his home town, Cram set world records in the 1,500 m, 2,000 m, and the mile ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Cram always acknowledges the role Hedley and <b>the Jarrow</b> &amp; Hebburn Athletics Club would play in his career. Becoming known as <b>the “Jarrow</b> <b>Arrow</b>” he first hit the big time, or at least its ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'As <b>the Jarrow</b> <b>Arrow</b> celebrates his landmark birthday, Steve Smythe picks out his best 60 performances For those who remember Steve Cram as the world’s greatest teenage 1500m and mile runner, it ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Sea foam</b>. <b>Sea foam</b>, ocean <b>foam</b>, beach <b>foam</b>, or spume is a type of <b>foam</b> created by the agitation of seawater, particularly when it contains higher concentrations of dissolved organic matter (including proteins, lignins, and lipids) derived from sources such as the offshore breakdown of algal blooms. [1] These compounds can act as surfactants or ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'A <b>foaming</b> ocean is a fascinating phenomenon. <b>Sea</b> <b>foam</b> is an important ecological contributor, but that doesn&#39;t mean its always safe.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Sea</b> <b>foam</b> forms when dissolved organic matter in the ocean is churned up. <b>Sea</b> <b>Foam</b> at Ocean Beach in San Francisco. If you scoop up some water from the ocean in a clear glass and look at it closely, you&#39;ll see that it&#39;s chock full of tiny particles. Seawater contains dissolved salts, proteins, fats, dead algae, detergents and other pollutants ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>The Techniques of Democracy</b>. <b>The Techniques of Democracy</b> is a book written by Alfred Bingham. It was published in 1943 by New York City publishers Duell, Sloan and Pearce. In this book, Bingham argues against both dogmatic individualism and dogmatic socialism. [1] Categories: 1942 non-fiction <b>books</b>. Duell, Sloan and Pearce <b>books</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Tytler was born in the Old Town of Edinburgh, the eldest son of Ann Craig of Costerton (1722–1783) and her husband William Tytler of Woodhouselee (<b>author</b> of Inquiry into the Evidence against Mary Queen of Scots). He was educated at Edinburgh High School and Kensington Academy in London (1763/64), and then studied law at the University of Edinburgh, qualifying as an advocate in 1770.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'In the year of elections, read Margaret Atwood, Mary Beard, Lea Ypi, Elif Shafak and more on what <b>democracy</b> means - and why it matters. In 2024, nearly half the world will take part in a national election, with billions heading to the polls. It&#39;s a thrilling, unprecedented opportunity for change - yet <b>democracy</b> is also under threat.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Mark Bomback (T he Wolverine, War for the Planet of the Apes ); 8. James L. Brooks ( Terms of Endearment, Broadcast News); 9. Allison Burnett ( Autumn in New York, Underworld Awakening ); 10 ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The term “<b>screenwriter</b>” has expanded in recent years to include writers for television, gaming, and all screen-based media emerging in the 21st century, such as Extended Reality or XR. Traditionally, “<b>screenwriter</b>” meant “<b>writer</b> of movies.” Today, I am focusing on the 50 greatest screenwriters in film.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Screenwriter</b>, novelist, playwright, non-fiction <b>author</b>. Born in Highland Park, Illinois, USA, began his career as a novelist in 1957. Started writing screenplays in 1965 with &quot;Masquerade&quot;. A two-<b>time</b> Academy Award Winner, he is one of the most successful screenwriters and script doctors in Hollywood.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>RESISTANCE</b>: The Underground War Against Hitler, 1939-1945, by Halik Kochanski For continental Europeans World War II was a vastly different experience than it was for the people of the British ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Buy <b>Resistance: The</b> Underground War in Europe,<b></b> 1939-1945 1 by Kochanski, Halik (ISBN: 9780241004289) from Amazon&#39;s Book Store. Everyday low prices and free delivery on eligible orders.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Resistance: The</b> Underground War Against Hitler,<b></b> 1939-1945 Hardcover – May 24, 2022 by Halik Kochanski (<b>Author</b>) 4.3 88 ratings See all formats and editions WINNER OF THE 2023 WOLFSON HISTORY PRIZE New Yorker • Best <b>Books</b> of 2022 “This is the most comprehensive and best account <b>of resistance</b> I have read.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The Manchester Food and Drink <b>Festival</b> is a well established, nationally acclaimed event. Conceived and developed by Phil Jones in 1998, the event originated as a means of showing the rest of the nation that there was more to Manchester than meat pies and gravy! In the 24 festivals that have taken <b>place</b> over that time, Manchester’s dining ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The Manchester Food and Drink <b>Festival</b> will return for its 25th anniversary this September, with a host of events planned as it takes over Cathedral Gardens.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The event will now <b>take</b> <b>place</b> between Thursday 22nd – Sunday 25th September and then again from Thursday 29th September – Sunday 2nd October. If that wasn’t all, the <b>festival</b> will be bringing tons of events, offers and more to local restaurants, too, with a special focus on some of the chefs and restaurants that have been a part of MFDF ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Fall and Rise of Reginald Perrin</b> is a British sitcom starring Leonard Rossiter in the title role. Three series were produced from 1976 to 1979, based on a series of novels written by David Nobbs. Nobbs adapted the screenplay for the first series from the first novel. Some of its subplots were considered too dark or risqué for television ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Fall And Rise of Reginald Perrin</b> ... <b>Reginald</b> Iolanthe <b>Perrin</b> had <b>worked</b> in the same boring job with Sunshine Desserts for 20 years. Every day he left his boring Norbiton home, took the same boring train journey, arrived at his boring office, and was greeted by his boring secretary Joan (Sue Nicholls) – a middle-aged bundle of simmering ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '9. <b>What was the name</b> of the cat who chased Pixie and Dixie? 10. What starts with ‘T’, ends with ‘T’ and has ‘T’ in it? ANSWERS: 1. Sunshine Desserts; 2. Brian Talbot (Ipswich and ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Robert <b>FitzRoy</b> circa 1850. The <b>Shipping</b> Forecast was established by Vice-Admiral Robert <b>FitzRoy</b>, the first professional weather forecaster, captain of HMS Beagle and founder of the Met Office. In October 1859, the steam clipper Royal Charter was wrecked in a strong storm off Anglesey; 450 people lost their lives.In response to this loss, <b>FitzRoy</b> introduced a warning service for <b>shipping</b> in ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'February 4th <b>2002</b> marks the end of an era for the <b>Shipping</b> Forecast. Sea <b>area</b> Finisterre changes its <b>name</b> to <b>FitzRoy</b> and boundaries between neighbouring areas are to be realigned.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'For decades, his region was <b>known</b> as Finisterre, after a peninsula on the west coast of Galicia, Spain. But in <b>2002</b> the Met Office decided to rename it, to prevent confusion with an <b>area</b> of that <b>name</b> in the French/Spanish <b>shipping</b> forecast: today it’s <b>Fitzroy</b>. 13.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Korea</b>’s <b>first</b> ever <b>grand</b> <b>prix</b> was always going to have a place in the history books. But taking nearly three hours to complete, it also became the longest world championship <b>race</b> since 1960. Read on for more stats and facts from the Korean <b>Grand</b> <b>Prix</b>. Fernando Alonso’s Korean <b>Grand</b> <b>Prix</b> win has the hallmarks of … Continue reading <b>Korea</b>’s <b>first</b> <b>Grand Prix</b> was the longest for 50 <b>years</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'After the 2012 Australian <b>Grand</b> <b>Prix</b>, organisers of the <b>race</b> in <b>Korea</b> announced that they had reached a new deal with Formula One Management that would save $20.5 million (₩23 billion) in costs. Kang Hyo-seok, director of <b>race</b> organisation for the Korean <b>Grand Prix</b>, admitted that the <b>race</b> was still &#39;too expensive&#39; for <b>Korea</b>, anticipating an estimated loss of $26 million (₩29 billion) in 2012.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Korea</b> International Circuit History. The Korean circuit was scheduled to be ready in July 2010, but several issues delayed the inauguration. Heavy rainfalls postponed soil improvement and lack of funding from the Korean Government made things more difficult. Hence, the Koran <b>Grand</b> <b>Prix</b> saw some unfinished facilities in 2010.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Life. <b>Paul</b> was a Greek -speaking Jew from Asia Minor. His birthplace, Tarsus, was a major city in eastern Cilicia, a region that had been made part of the Roman province of Syria by the time of <b>Paul</b>’s adulthood. Two of the main cities of Syria, Damascus and Antioch, played a prominent part in his life and letters.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Paul</b> (Koinē Greek: Παῦλος, romanized: Paûlos), also named Saul of Tarsus (Aramaic: ܫܐܘܠ, romanized: Šāʾūl), commonly known as <b>Paul</b> the Apostle and <b>Saint Paul</b>, was a Christian apostle (c. 5 – c. 64/65 AD) who spread the teachings of Jesus in the first-century world. For his contributions towards the New Testament, he is generally regarded as one of the most important figures ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Introduction <b>Saint Paul</b> ©. <b>Saint Paul</b> is undoubtedly one of the most important figures in the history of the Western world. Just a quick look at the headlines of his life are enough to understand ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Frank Cornan</b>. Francis <b>Cornan</b> (5 May 1880 – 31 May 1971) was an English professional footballer born in Sunderland, who played as an inside left or left half. He made 165 appearances in the Football League playing for Barnsley (in three separate spells), Birmingham and Aston Villa. He died in Halifax, West Yorkshire, aged 91.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Francis <b>Cornan</b> was a Football player born on 1880-05-05, in Sunderland, England. Played as Midfielder.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'A biographical history of <b>Frank</b> <b>Cornan</b>, Aston Villa Midfielder, 1908-09', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Radik Zhaparov</b> (born February 29, 1984) is a Kazakh ski jumper who has competed since 2003. At the 2006 Winter Olympics in Turin, he finished 11th in the team large hill and 26th in the individual normal hill events.At the FIS Nordic World Ski Championships, <b>Zhaparov</b> has finished 11th in team events three times (2005: large, normal; 2007: large) and 24th in the individual normal hill (2007 ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Radik ZHAPAROV</b>. Team Kazakhstan. Ski Jumping. Games Participations 1. First Olympic Games Turin 2006. Year of Birth 1984. Olympic Results.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Radik</b> <b>ZHAPAROV</b>. Shvsm Dinamo. KAZ Kazakhstan. FIS Code 3787; Birthdate 1984; Age 40; Status Not active; Gender Male; Marital Status – ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Wilmington</b> insurrection of 1898, also known as the <b>Wilmington</b> massacre of 1898 or the <b>Wilmington</b> coup of 1898, [6] was a coup d&#39;état and a massacre which was carried out by white supremacists <b>in Wilmington</b>, North Carolina, United States, on Thursday, November 10, 1898. [7] The white press <b>in Wilmington</b> originally described the event as a ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'A violent mob, whipped into a frenzy by politicians, tearing apart a town to overthrow the elected government. Following state elections in 1898, white supremacists moved into the US port of ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'This illustration of the 1898 <b>Wilmington</b> massacre typifies how publications of the time promoted misleading characterizations of the <b>incident</b> as a &#39;race riot&#39; or a Black insurrection.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Pope <b>Francis</b> ( Latin: Franciscus; Italian: Francesco; Spanish: Francisco; born Jorge Mario Bergoglio; [b] 17 December 1936) is head of the Catholic Church and sovereign of the Vatican City State. He is the first pope to be a member of the Society of Jesus (Jesuits), the first from the Americas and the Southern Hemisphere, and the first born or raised outside Europe since the 8th-century papacy ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Francis</b> ushered in a new era of leadership of the Roman Catholic Church when he was elected pope in 2013. As the first pope from the Western Hemisphere, the first from South America, and the first from the Jesuit order, <b>Francis</b> has brought many reforms to the church and a reputation for humility. His significant achievements include the papal encyclical Laudato si’ (2015), which addressed ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Jorge Mario Bergoglio was elected the 266th pope of the Roman Catholic Church in March 2013, becoming Pope <b>Francis</b>. He is the first pope from the Americas.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'This episode is described in Sand’s autobiographical novel, A winter in Majorca, which gave Coetzee one of the main settings for The <b>Pole</b>. In one of the many layers of this book, there is even a debate about how to interpret Chopin. “I hold Chopin in the highest regard as a <b>composer</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'In “The <b>Pole</b>,” the new novel by the South African writer J. M. Coetzee, Arrau has another fan in the character of Beatriz, a fortysomething socialite. But what does she know? The wife of a ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>Pole</b> (TV Series 2021) cast and crew credits, including actors, actresses, directors, writers and more.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The Arcade <b>Awards</b>, also known as the Arkie <b>Awards</b>, was one of the first <b>video</b> <b>game</b> <b>awards</b>, dating back to the golden age of arcade <b>video</b> <b>games</b> and lasting up until the <b>video</b> <b>game</b> crash of 1983. It was held since 1980 (<b>for games</b> released in 1979 and earlier) and were announced annually by Electronic <b>Games</b> magazine since 1981, covering several platform categories. [68]', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Game</b> <b>Awards</b> 2022 are a wrap, and Elden Ring took home the biggest <b>award</b> of the night: <b>Game</b> <b>of the Year</b>. God of War Ragnarok also did pretty well for itself, taking home a handful of <b>awards</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Cameron Monaghan, Star Wars Jedi: Survivor. Idris Elba, Cyberpunk 2077: Phantom Liberty. Melanie Liburd, Alan Wake 2. Neil Newbon, Baldur&#39;s Gate 3. Yuri Lowenthal, Marvel&#39;s Spider-Man 2.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Sofia Anker-Kofoed</b> (born 28 November 1994) is a Swedish footballer. She last played as an attacker for FC Rosengård in the Damallsvenskan .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'National; FIFA World Cup; Olympics; UEFA European Championship; CONMEBOL Copa America; Gold Cup; AFC Asian Cup', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'View <b>Sofia</b> <b>Anker Kofoed</b> (Borgeby) player profile page on Livescore.in. Explore player stats (appearances, goals, cards) and player transfer history.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Krapf was a 3-<b>time</b> <b>state</b> <b>wrestling</b> <b>champion</b> <b>for Tatnall</b>, winning at 180 pounds in 1966 and 1967 and heavyweight in 1968. He was also a 4-<b>time</b> national prep <b>school</b> champ. Ninety-four of Krapf’s ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>All</b> of the wrestlers who won the IHSAA <b>state</b> tournament <b>three</b> times.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Naaktgeboren <b>became</b> just the third Linn-Mar wrestler to win multiple <b>state</b> titles, joining four-timer Jay Borschel and <b>three-time</b> titlist Matt McDonough. It is the <b>school</b>’s 15th overall <b>state</b> ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Born</b> <b>Lucille</b> Fay <b>LeSueur</b>, of French-Huguenot, English, Dutch, and Irish ancestry in San Antonio, Texas, she was the second of the two children of Thomas E. <b>LeSueur</b> (<b>born</b> January 2, 1867, in Tennessee; died January 1, 1938), a construction worker, and Anna Bell Johnson (died August 15, 1958), later <b>known</b> as Anna Cassin.Crawford&#39;s mother was likely under 20 when her first two children were <b>born</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Joan Crawford got her <b>name</b> from a contest. Crawford&#39;s birth <b>name</b> was <b>Lucille</b> Fay <b>LeSueur</b>, but she also went by the <b>name</b> Billie Cassin — after her stepfather who was called Billie — before her career took off. According to The Guardian, the bosses at MGM prompted her to change her <b>name</b> when they offered her a contract, as they said her ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Joan Crawford. <b>Actress</b>: What Ever Happened to Baby Jane?. Joan Crawford was <b>born</b> <b>Lucille</b> Fay <b>LeSueur</b> on March 23, 1906, in San Antonio, Texas, to Anna Belle (Johnson) and Thomas E. <b>LeSueur</b>, a laundry laborer. By the time she was <b>born</b>, her parents had separated, and by the time she was a teenager, she&#39;d had three stepfathers. It wasn&#39;t an easy life; Crawford worked a variety of menial jobs. She...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Luan Viana</b> <b>Patrocínio</b> (born 14 January 1996), sometimes known simply as <b>Luan</b>, is a Brazilian professional footballer who plays as a forward for Mixto. Club career [ edit ] Born in São Paulo , <b>Luan Viana</b> graduated from Portuguesa &#39;s youth setup, and made his first-team debut on 13 February 2013, coming on as a late substitute in a 2–0 win at São José , for the Campeonato Paulista Série ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Luan</b> Last name <b>Viana</b> <b>Patrocínio</b> Nationality Brazil Date of birth 14 January 1996 Age 28 Country of birth Brazil Place of birth São Paulo Position Attacker Height 184 cm Weight 79 kg Foot Right. Career Domestic Leagues; Domestic Cups; ... Data provided by Opta <b>Sports</b>. Articles provided by OMNISPORT.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Full name: <b>Luan</b> <b>Viana</b> <b>Patrocinio</b> Date of birth/Age: Jan 14, 1996 (28) Place of birth: São Paulo Height: 1,86 m Citizenship: Brazil Position: Attack - Centre-Forward Foot: right Current club: Porto Velho EC Joined: Feb 16, 2024 Contract expires:- Stats of <b>Luan</b> <b>Viana</b> . View full stats. National team career ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Federal prosecutors on Tuesday <b>hit</b> <b>scandal-plagued</b> Rep. George Santos (R-N.Y.) <b>with 10</b> <b>new</b> <b>criminal</b> <b>charges</b>, <b>including</b> for charging at least $44,800 to an unaware campaign donor who had texted him ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Those initial <b>charges</b> included seven counts of <b>wire</b> <b>fraud</b>, three counts of money laundering, one count of <b>theft</b> of public funds and two counts of making materially false statements to the <b>US</b> House ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>new</b> <b>charges</b> in the so-called superseding indictment are: one count of conspiracy to commit offenses against the United States, two counts of <b>wire</b> <b>fraud</b>, two counts of making materially false ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Kuujjuarapik</b> (also spelled Kuujjuaraapik; Inuktitut: ᑰᔾᔪᐊᕌᐱᒃ little great river [5]) is the southernmost northern village ( Inuit community) at the mouth of the Great Whale River ( French: Grande Rivière de la Baleine) on the coast of Hudson Bay in Nunavik, Quebec, Canada. Almost 1,000 people, mostly Cree, live in the adjacent ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Kuujjuarapik</b> (Inuktitut: ᑰᔾᔪᐊᕌᐱᒃ) and neighbouring Whapmagoostui are twin villages with a total of about 1800 people (2021) in Nunavik in the far north of Quebec. <b>Kuujjuarapik</b> sits at the mouth of the Grande-Baleine River on the coast of Hudson Bay. ... Locally-run general store that sells everything from coats to <b>country</b> food ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The land Kuujjuaraapik . KUUJJUARAAPIK means “small river [].”This was the first village that missionaries visited to spread the word of God. One missionary in particular, Reverend W. G. Walton [], saw numerous Inuit each year to baptize and name the infants.Myself, I was born January 1st 1914 and he baptized me in 1917; I still have my baptismal certificate, although I’m sixty-seven now.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Globle will test your knowledge of geography. The goal of the game is to find the mystery <b>country</b> on the world map. After each guess, you will see on the map the <b>country</b> you have chosen and the hotter the color, the closer you are to the hidden <b>country</b>. You have an unlimited number of guesses, so use the color hints and find the target <b>country</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Every day, there is a new Mystery <b>Country</b>. Your goal is to guess which <b>country</b> it is using the fewest number of guesses. Each incorrect guess will appear on the <b>globe</b> with a colour indicating how close it is to the Mystery <b>Country</b>. The hotter the colour, the closer you are to the answer.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Pale color means that the target <b>country</b> is far from your choice. 3. Click, drag and move the <b>globe</b> to get a closer view of the map. 4. When you select the correct <b>country</b>, it will be colored in dark red. 5. In this game, your number of guesses is unlimited, so play with pleasure and improve your geographical skills!', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Alexis</b> <b>Carra</b>. Position: FW. 178cm (5-10) Born: April 27, 1990 in Villefranche-sur-Saône, France. Citizenship: France. Become a Stathead &amp; surf this site ad-free.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Alexis Carra</b> is a 34-year old football player aus France, (* Jan 1, 1990 in Villefranche, France). <b>Carra</b> is Jul 1, 2012 without a club since. He plays in the position Right Winger. His market value is -.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Alexis</b> <b>Carra</b> is a 34-year-old Football ex-player. Born in France on 1990-01-01, he played as Forward. Weights 76 kg and is 180 cm tall.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Erin Hunter is the team of writers behind <b>Warrior</b> Cats. Find out more about the authors and their stories.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Erin Hunter is a collective pseudonym used by the authors Victoria Holmes, Kate Cary, Cherith Baldry, Clarissa Hutton, Inbali Iserles, Tui T. Sutherland, and Rosie Best in the writing of several children&#39;s fantasy novel series which focus on animals and their adventures. Notable works include the <b>Warriors</b>, Seekers, Survivors, Bravelands, and ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Warriors</b> (also known as <b>Warrior</b> Cats) is a series of novels based on the adventures and drama of multiple Clans of feral cats. The series is primarily set in fictional forests. Published by HarperCollins, the series is written by authors Kate Cary and Cherith Baldry, as well as others, under the collective pseudonym Erin Hunter. The concept and plot of the pilot series were developed by series ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Acting Secret Service <b>Director</b> Ronald Rowe said President Biden, the presidential candidates, and their running <b>mates</b> would receive Secret Service counter-sniper coverage at all of their events.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Ashley Etienne is the former communications <b>director</b> for Vice President Kamala Harris and joined CBS News to discuss Harris&#39; decision to have Minnesota Gov. Tim Walz be her running <b>mate</b>. Yahoo ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>director</b> of a Republican group that has for years campaigned against former President Donald Trump drew a damning distinction Sunday between Trump’s 2024 running <b>mate</b>, JD Vance, and two of the Democrats thought to be top picks to become Kamala Harris’ vice presidential candidate.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Airplane! (alternatively titled Flying High!) is a 1980 American disaster comedy film written and directed by Jim Abrahams and brothers David and Jerry Zucker in their directorial debuts, and produced by Jon Davison.It stars Robert Hays and Julie Hagerty and features Leslie Nielsen, Robert Stack, Lloyd Bridges, Peter Graves, Kareem Abdul-Jabbar, and Lorna Patterson.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '&quot;The <b>Pilot</b>&quot; is the first episode of the tenth series of the British science fiction television series Doctor Who. It was written by Steven Moffat and broadcast on 15 April 2017 on BBC One . &quot;The <b>Pilot</b>&quot; received mostly positive reviews, with praise on the introduction of Pearl Mackie , and how the episode served both as a soft reboot and as a series premiere.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'In The <b>Director</b>’s Disorder: <b>Pilot</b> Episode, you’ll play the part of Coal Westwood, retired actor, as you attempt to survive the whims of a deadly killer. To live through the night, Coal will need to avoid antagonizing the <b>Director</b> by following the script or risk his wrath. This is the role of a lifetime. Features: - Fully Voiced Dialogue', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Gap</b>: With Freen Sarocha Chankimha, Rebecca Patricia &#39;Becky&#39; Armstrong, Tassawan Seneewongse, Ratchanon Kanpiang. Mon is an idol of Sam, and when they meet again at the office, she is surprised by her icy exterior. They are different in class and age, with a <b>gap</b> of eight years between them.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Gap</b> Band was an American R&amp;B and funk band that rose to fame during the 1970s and 1980s. The band consisted of three brothers: Charlie , Ronnie, and Robert Wilson, along with other members; it was named after streets (Greenwood, Archer, and Pine) [1] [2] in the historic Greenwood neighborhood in the brothers&#39; hometown of Tulsa, Oklahoma .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The drama also aims to explore a range of distinct <b>genres</b>, including that humor. It is based on the “<b>GAP</b>: ... The <b>GAP</b>, The Series Episodes, releases every Saturday. The <b>GAP</b>, The Series Episodes, will drop on GMM One at around 10.30 pm in Thailand. While International fans can stream The <b>GAP</b> The Series episodes at 9:30 pm IST, 1 am KST (the ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Giacomo Guardi</b> (13 April 1764 - 3 November 1835) was an Italian painter from Venice. The son of famous Veduta painter Francesco <b>Guardi</b>, he continued his <b>father</b>&#39;s line of work, though without the same level of renown. The majority of his works are quite small views of only minor artistic interest, more akin to postcards than to his <b>father</b>&#39;s ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The son of the famous Venetian vedutista Francesco <b>Guardi</b>, <b>Giacomo Guardi</b> followed in his <b>father</b>’s footsteps in his preference for atmospherically rendered topographical views, although unlike the former he primarily concentrated on small formats. In the nineteenth century, his countless impressive views of Venice attracted the interest of collectors and museums.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The youngest son of the view painter Francesco <b>Guardi</b>, <b>Giacomo</b> <b>Guardi</b> assisted his <b>father</b> in the latter part of the elder artist’s career, contributing to the Venetian vedute for which he was well known. As James Byam Shaw has pointed out, ‘Through his <b>father</b>’s elder sister Cecilia, the wife of Giovanni Battista Tiepolo, he was first ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Belorechensky District</b> ( Russian: Белоре́ченский райо́н) is an administrative <b>district</b> ( raion ), one of the thirty-eight in Krasnodar Krai, Russia. [1] As a municipal division, it is incorporated as <b>Belorechensky</b> Municipal <b>District</b>. [5] It is located in the southern central part of the krai, but is bordered for the main ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Administrative and municipal status Within the framework of administrative divisions, Belorechensk serves as the administrative center <b>of Belorechensky</b> <b>District</b>, even though it is not a part of it. [1] As an administrative division, it is, together with the territory of Yuzhny Rural Okrug (which comprises three rural localities ), incorporated separately as the Town of Belorechensk —an ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Belorechensk Geography. Geographic Information regarding City of Belorechensk. Belorechensk Geographical coordinates. Latitude: 44.7667, Longitude: 39.8667. 44° 46′ 0″ North, 39° 52′ 0″ East. Belorechensk Area. 5,600 hectares. 56.00 km² (21.62 sq mi) Belorechensk Altitude.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'George Raymond Richard <b>Martin</b> (born George Raymond <b>Martin</b>; September 20, 1948), also known by the initials G.R.R.M., is an American <b>author</b>, television writer, and television producer. He is best known as the <b>author</b> of the series of epic fantasy novels A Song of Ice and Fire, which were adapted into the Primetime Emmy Award–winning television series Game of Thrones (2011–2019) and its ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'George R. R. <b>Martin</b> writes fantasy novels and for television. His first novel, Dying of the Light, debuted in 1977, and by the mid-1980s, he was also writing for TV. In 1996, <b>Martin</b> published his ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The World of Ice &amp; Fire: The Untold History of Westeros and the Game of Thrones. by. George R.R. <b>Martin</b>, Elio M. García Jr., Linda Antonsson (<b>Goodreads</b> <b>Author</b>) 4.26 avg rating — 36,796 ratings — published 2014 — 60 editions.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>François</b> <b>Gayot</b> (July 17, 1927 – December 16, 2010) was the Catholic archbishop of the Roman Catholic Archdiocese of Cap-Haïtien. Haiti. Ordained to the priesthood in 1954, <b>Gayot</b> was named bishop of the then Cap-Haïtien Diocese. In 1988 the Diocese was elevated to an archdiocese. Archbishop <b>Gayot</b> retired in 2003 and died in 2010.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Haiti <b>- Religion :</b> The Archbishop <b>Francois Gayot</b> died 17/12/2010 09:20:08. ... <b>Francois</b> <b>Gayot</b> was ordained a priest in the Society of Missionaries of Mary or Monfortain, on February 7, 1954.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '&quot;<b>François</b> <b>Gayot</b> de Pitaval&quot; published on by null. (Lyons, 1673–1743, Lyons),a French legal writer, compiled 20 vols. of Causes célèbres et intéressantes (1734, ff.), which were translated into German between 1747 and 1768 Schiller wrote an introduction ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Maksim</b> <b>Andreyevich</b> <b>Fyodorov</b> (Russian: Максим Андреевич Фёдоров; <b>born</b> 5 April 1989) is a Russian former professional footballer. Career. He made his professional debut in the Russian Second Division in 2006 for FC Krylia Sovetov-SOK Dimitrovgrad. He played in the Russian Football National League for FC Dynamo Bryansk in 2010.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Maksim Fyodorov</b> may refer to: <b>Maksim Fyodorov</b> (footballer, <b>born</b> 1986), Russian footballer (striker) <b>Maksim Fyodorov</b> (footballer, <b>born</b> 1989), Russian footballer (midfielder) This page was last edited on 16 June 2018, at 09:36 (UTC). Text is available under the Creative Commons ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Maksim</b> <b>Andreyevich</b> <b>Fyodorov</b> Midfielder, Football player 1989 – 86 Views. Who is <b>Maksim</b> <b>Andreyevich</b> <b>Fyodorov</b>? ... <b>Born</b> Apr 5, 1989 Rasskazovo Nationality. Russia; Lived in. Rasskazovo; Edit. Submitted on July 23, 2013. Citation Use the citation below to add to a bibliography: Style: MLA Chicago APA', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Nina Dittrich</b> (<b>born</b> 20 November 1990 in Vienna) is an Austrian swimmer, who specialized in freestyle and butterfly events. She is a multiple-time Austrian champion, a five-time national record holder, and also, a current member of Simmering Swimming Club (German: Schwimmverein Schwechat Simmering) in Schwechat. <b>Dittrich</b> is also the daughter of Ulrike Bauer, an Austrian record holder in both ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Host <b>city</b> selection; Statistics Medals by country; Medals by athlete; ... Biographical information Roles: Competed in Olympic Games: Sex: Female: Full name: <b>Nina•Dittrich</b>: Used name: <b>Nina•Dittrich</b>: <b>Born</b>: 20 November 1990 in Wien (Vienna), Wien (AUT) Measurements: 174 cm / 58 kg: Affiliations: SV Schwechat, Schwechat (AUT) ... <b>Nina Dittrich</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Nina</b> <b>Dittrich</b> Swimmer, Olympic athlete 1990 – ... <b>Born</b> Nov 20, 1990 Vienna Also known as. Nintscheck; Parents. Ulrike <b>Dittrich</b>; Siblings. Nikolaus <b>Dittrich</b>; Nationality. Austria; Profession. Swimmer; Lived in. Vienna; Edit. Submitted on July 23, 2013. Citation Use the citation below to add to a bibliography:', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Netherlands Antilles</b> ( <b>Dutch</b>: Nederlandse Antillen, pronounced [ˈneːdərlɑntsə ʔɑnˈtɪlə (n)] ⓘ; Papiamento: Antia Hulandes) [2] was a constituent <b>country</b> of the Kingdom of the <b>Netherlands</b>. The <b>country</b> consisted of several island territories located in the Caribbean Sea. The islands were also informally known as the <b>Dutch</b> <b>Antilles</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'In 2006 the <b>Dutch</b> government and the remaining five islands agreed to dissolve the <b>Netherlands Antilles</b> within the following several years. The event took place on October 10, 2010. None of the islands chose full independence. Curaçao and Sint Maarten became autonomous <b>countries</b> within the kingdom, a status similar to that of Aruba.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'In 1954, the various <b>Dutch</b> island colonies were united under a single <b>country</b> and were named “Netherland <b>Antilles</b>.” On December 15, 1954, with the proclamation of the “Charter for the Kingdom of <b>Netherlands</b>,” the <b>Netherlands Antilles</b> became an autonomous part of the Kingdom of <b>Netherlands</b>.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The heartwarming new memoir from the <b>author</b> of the bestselling <b>Max</b> the Miracle Dog. About the <b>Author</b>. Kerry Irving lives in Keswick, where he runs the Paw Store. His first book, <b>Max</b> the Miracle Dog, was a Sunday Times bestseller. A keen amateur photographer, Kerry enjoys daily walks around the Lake District with his three spaniels, Paddy, Harry ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Buy <b>Max the</b> Miracle <b>Dog: The</b> Heart-warming Tale of a Life-saving Friendship First Edition by Irving, Kerry (ISBN: 9780008353490) from Amazon&#39;s Book Store. Everyday low prices and free delivery on eligible orders.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Max</b> Bennett is the cofounder and CEO of Alby, a start-up that helps companies integrate large language models into their websites to create guided shopping and search experiences.Previously, Bennett was the cofounder and chief product offi\\xadcer of Bluecore, one of the fastest growing companies in the U.S., providing AI technologies to some of the largest companies in the world.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>O</b> <b>dia</b> <b>das</b> <b>calças</b> <b>roladas</b>. <b>O</b> <b>dia</b> <b>das</b> <b>calças</b> <b>roladas</b> is a Capeverdean novel published in 1982 by Germano Almeida . The book was first published on Ilhéu Editora. The story is about an account of a strike that happened on the island of Santo Antão .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Published works His first work was <b>O</b> <b>dia</b> <b>das</b> <b>calças</b> <b>roladas</b> which was about an account of a strike on the island of Santo Antão, it was first written in 1982 and was published in 1983.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Compre <b>o</b> livro <b>O Dia das Calças Roladas</b> de Germano Almeida em Bertrand.pt. portes grátis.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '1,678 likes, 37 comments - georginagio_bae on September 1, <b>2022</b>: &quot;<b>Georgina</b> Rodrigues in Venice <b>Film</b> <b>festival</b> <b>2022</b> @georginagio_bae&quot;.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Of the 16 fests I got into, the biggest were a &quot;City or State International <b>Film</b> <b>Festival</b>.&quot; Got into five of those. The rest were lower-tier fests and were more global. I <b>did</b> not get into any meaningful European festivals, even though I really wanted to get into those. (Poff Shorts, London Shorts, Glasgow)', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Award-winners and contenders from The <b>Indie</b> Gathering International <b>Film Festival</b> (<b>2022</b>) Menu. Movies. Release Calendar Top 250 Movies Most Popular Movies Browse Movies by Genre Top Box Office Showtimes &amp; Tickets Movie News India Movie Spotlight. TV Shows. ... <b>2022</b> Awards. Getting Started ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Bert</b> Meyers was <b>born</b> in Los Angeles on March 20, 1928. The son of Romanian Jewish immigrants, he maintained strong lifelong ties to his Jewish cultural heritage without being religious. Always rebellious and a questioner of authority, Meyers decided to drop out of high school and become a poet. For many years, he worked manual labor jobs including janitor, farm worker, house painter, and ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Bert</b> Meyers was <b>born</b> Bertram Ivan Meyers in Los Angeles on March 20, 1928. The son of Romanian and Polish Jewish immigrants, he maintained strong lifelong ties to his Jewish cultural heritage without being religious. Always rebellious and a questioner of authority, he decided to drop out of high school and become a poet.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Bert</b> Meyers was <b>born</b> in Los Angeles in 1928. The son of Romanian Jewish immigrants, and a high school drop out, Meyers worked at manual labor jobs until finally becoming a master picture framer and gilder. When that work negatively impacted his health, he applied and was admitted to the Claremont Graduate School on the basis of his poetic achievements.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Kondh</b> is a village in Dhrangadhra Taluka in <b>Surendranagar</b> district of Gujarat State, India. Nearby villages are Rajcharadi, Hampar, Navalgadh, Bhechada, Gajanvav, Ratanpar, Rampara, Raygadh, Ravaliyavadar and Narichana . <b>Kondh</b>&#39;s main crops are magafali ( peanuts) and kappas ( cotton ). <b>Kondh</b>&#39;s Postal Index Number code is 363310 and the postal ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Kondh</b> village is located in Dhrangadhra taluka of <b>Surendranagar</b> district in Gujarat, India. It is situated 20km away from sub-district headquarter Dhrangadhra (tehsildar office) and 30km away from district headquarter <b>Surendranagar</b>. As per 2009 stats, Kalyanpur is the gram panchayat of <b>Kondh</b> village. The total geographical area of village is ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'About <b>Kondh</b>. <b>Kondh</b> is a Village in Dhrangadhra Taluka in <b>Surendranagar</b> District of Gujarat State, India. It is located 44 KM towards west from District head quarters <b>Surendranagar</b>. 25 KM from . 167 KM from State capital Gandhinagar. <b>Kondh</b> Pin code is 363310 and postal head office is Dhrangadhra . Dhavana ( 6 KM ) , Jiva ( 8 KM ) , Sapkada ( 8 ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'By 2034, eleven cities will <b>have</b> <b>hosted</b> the Olympic Games more than <b>once</b>: Athens ( 1896 and 2004 Summer <b>Olympics</b> ), Paris ( 1900, 1924 and 2024 Summer <b>Olympics</b> ), London ( 1908, 1948 and 2012 Summer <b>Olympics</b> ), St. Moritz ( 1928 and 1948 <b>Winter</b> <b>Olympics</b> ), Lake Placid ( 1932 and 1980 <b>Winter</b> <b>Olympics</b> ), Los Angeles ( 1932, 1984 and 2028 Summer ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Only</b> <b>Europe</b>, North America, Asia, South America, and Oceania <b>have hosted the</b> Olympic Games with <b>Europe</b> hosting 36 editions. In 2016, Brazil became the first South American <b>country</b> to <b>host</b> the <b>Olympics</b>. Africa has yet to <b>host</b> the games. In 2022, China’s Beijing City will be the first city to <b>host</b> both <b>the Winter</b> and Summer Games.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'What do Paris, Helsinki, Atlanta, Beijing, and Tokyo <b>have</b> in common? They all <b>have</b> <b>hosted</b> the <b>Olympics</b>.As the foremost international sporting event, the Olympic Games <b>have</b> been held in cities around the world—though certain regions, notably Africa, <b>have</b> yet to play <b>host</b>.In years past, the bidding process was highly competitive, as numerous cities campaigned to stage the Summer and <b>Winter</b> Games.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Cracker</b> is a British crime drama <b>series</b> produced by Granada <b>Television</b> for ITV, created and principally written by Jimmy McGovern.Set in Manchester, the <b>series</b> follows a criminal psychologist (or &quot;<b>cracker</b>&quot;), Dr Edward &quot;<b>Fitz</b>&quot; Fitzgerald, played by Robbie Coltrane, who works with the Greater Manchester Police (GMP) to help them solve crimes.. The show consists of three <b>series</b>, originally ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Cracker</b> (<b>TV</b> <b>Series</b> 1993–1996) cast and crew credits, including actors, actresses, directors, writers and more. Menu. Movies. ... <b>Fitz&#39;s</b> Mum 1 episode, 1993 Philippa Howell ... Dr Turner 1 episode, 1993 Barbara Young ... Helen McIlvanney 1 episode, 1995 ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Barbara Flynn. <b>Cracker</b>, Open All Hours, The Vanishing Man. Barbara Flynn (born Barbara Joy McMurray; 5 August 1948) is an English actress. She first came to prominence playing Freda Ashton in the ITV drama <b>series</b> A Family at War (1970–72). She went on to play the milk woman in the BBC comedy Open All Hours (1981–85), Jill Swinburne in The ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'GMC <b>Jimmy</b>. The GMC <b>Jimmy</b> was an SUV marketed by General Motors that spanned four generations and two distinct vehicles: The K5 <b>Jimmy</b> – a mid-size SUV built from 1970 to 1999 and based on the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Motor vehicles <b>produced</b> by country in 2013. This is a list of <b>manufacturers</b> by motor vehicle production, by year, based on Organisation Internationale des Constructeurs d&#39;Automobiles (OICA).. Figures include passenger <b>cars</b>, light commercial vehicles, minibuses, trucks, buses and coaches.OICA defines these entries as follows: Passenger <b>cars</b> are motor vehicles with at least four wheels, used for ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The Best Year for GMC <b>Jimmy</b>. The year 1999 stands out as the best year for the GMC <b>Jimmy</b>. This particular <b>model</b> had a perfect balance of modern features and timeless design. In 1999, GMC introduced several updates to <b>the Jimmy</b>, which improved its overall performance, handling, and comfort. One of the most notable enhancements was the addition ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Rio de Janeiro</b> (Portuguese: [ˈʁi.u d(ʒi) ʒɐˈne(j)ɾu] ⓘ), or simply <b>Rio</b>, is the <b>capital</b> of the state of <b>Rio de Janeiro</b>.It is the second-most-populous city in Brazil (after São Paulo) and the sixth-most-populous city in the Americas.. Founded in 1565 by the Portuguese, the city was initially the seat of the Captaincy of <b>Rio de Janeiro</b>, a domain of the Portuguese Empire.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Rio</b> <b>de</b> <b>Janeiro</b> (<b>Rio</b>) is Brazil’s second-most populated city after Sao Paulo. Its population is 6.5 million, with 12.5 million in the urban area. <b>Rio</b> is located in the State of <b>Rio</b> <b>de</b> <b>Janeiro</b>, on the Atlantic coast in southeast Brazil. <b>Rio</b> was the <b>capital</b> of Brazil under Portuguese colonial rule.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Rio de Janeiro</b>, city and port, <b>capital</b> of the estado (state) of <b>Rio de Janeiro</b>, Brazil.It is located on the Atlantic Ocean, in the southeastern part of the tropical zone of South America, and is widely recognized as one of the world’s most beautiful and interesting urban centres.Although <b>Rio de Janeiro</b> continues to be the preeminent icon of Brazil in the eyes of many in the world, in reality ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>2002 Euro Beach Soccer Cup</b> was the fourth <b>Euro</b> <b>Beach</b> <b>Soccer</b> <b>Cup</b>, one of Europe&#39;s two major <b>beach</b> <b>soccer</b> championships at the time, held in February <b>2002</b>, in Barcelona, Catalonia, Spain . Portugal won the championship, claiming their second successive title and third overall, with hosts Spain finishing second. France beat Italy in the third place playoff to finish third and fourth respectively.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>2002</b> <b>Euro</b> <b>Beach</b> <b>Soccer</b> <b>Cup</b> was the fourth <b>Euro</b> <b>Beach</b> <b>Soccer</b> <b>Cup</b>, one of Europe&#39;s two major <b>beach</b> <b>soccer</b> championships at the time, held in February <b>2002</b>, in Barcelona, Catalonia, Spain. Portugal won the championship, claiming their second successive title and third overall, with hosts Spain finishing second. France beat Italy in the third place playoff to finish third and fourth respectively.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>Euro Beach Soccer Cup</b> ( EBSC ), originally known as the <b>European</b> Pro <b>Beach</b> <b>Soccer</b> Championships until 2004, was a biennial (previously annual) <b>beach</b> <b>soccer</b> competition contested between <b>European</b> men&#39;s national teams, organised by <b>Beach</b> <b>Soccer</b> Worldwide (BSWW). Having started in 1998, the tournament&#39;s prestige has held in being one of the ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Homecoming</b> is an American psychological thriller television series based on the Gimlet Media podcast of the same name. Created by Eli Horowitz and Micah Bloomberg, the series premiered November 2, 2018, on Amazon Prime Video. Horowitz and Bloomberg also serve as writers and executive producers alongside Sam Esmail, Chad Hamilton, Julia Roberts, Alex Blumberg, Matt Lieber, and Chris Giliberti ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The story behind Amazon&#39;s new drama <b>&#39;Homecoming&#39;</b> starring Julia Roberts. Taking journalists round the elaborate set of their soon-to-launch Amazon show <b>Homecoming</b> earlier this year, series ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Homecoming</b> (TV Series 2018–2020) cast and crew credits, including actors, actresses, directors, writers and more.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Medzilaborce District</b> ( okres <b>Medzilaborce</b>) is a <b>district</b> in the Prešov <b>Region</b> of northeastern Slovakia. It is the least populated of Slovakia&#39;s 79 districts. Until 1918, the <b>district</b> was part of the county of Kingdom of Hungary of Zemplín .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Medzilaborce</b> ( Rusyn: Міджілабірцї, Midzhilabirtsyi; Ukrainian: Міжлабірці, Mizhlabirtsi; Hungarian: Mezőlaborc) is a town in northeastern Slovakia close to the border with Poland, located near the towns of Sanok and Bukowsko (in southeastern Małopolska ).', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Medzilaborce</b> <b>District</b> Type: <b>district</b> of Slovakia with 12,000 residents Description: <b>district</b> of Slovakia Location: Prešov <b>Region</b>, Slovakia, Central Europe, Europe View on Open\\xadStreet\\xadMap Latitude 49.2706° or 49° 16&#39; 14&quot; north Longitude 21.902° or 21° 54&#39; 7&quot; east Population 12,000 Elevation 323 metres (1,060 feet) Abbreviation ML Open ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'In The <b>Director</b>’s Disorder: <b>Pilot</b> Episode, you’ll play the part of Coal Westwood, retired actor, as you attempt to survive the whims of a deadly killer. To live through the night, Coal will need to avoid antagonizing the <b>Director</b> by following the script or risk his wrath. This is the role of a lifetime. Features: - Fully Voiced Dialogue', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Find the game and support the creator here: https://store.steampowered.com/app/2073640/The_Directors_Disorder_<b>Pilot</b>_Episode/Links:https://www.buymeacoffee.co...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'In The <b>Director</b>’s Disorder: <b>Pilot</b> Episode, you’ll play the part of Coal Westwood, retired actor, as you attempt to survive the whims of a deadly killer. To live through the night, Coal will need to avoid antagonizing the <b>Director</b> by following the script or risk his wrath. This is the role of a lifetime. Features: - Fully Voiced Dialogue', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Marian</b> <b>University</b> celebrates the most significant events of the academic and <b>religious</b> calendar with special all campus liturgies. Institutional Witness The Catholic Franciscan tradition animates <b>Marian</b> <b>University</b> and forms the intellectual and moral foundations for all <b>university</b> policies, including: student services, human resources, and financial practices.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Marian</b> <b>University</b> 3200 Cold Spring Road Indianapolis, IN 46222-1997 (317) 955-6000 admissions@<b>marian</b>.edu COMadmissions@<b>marian</b>.edu Need More Information ?', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'About <b>Marian</b> <b>University</b>. <b>Marian</b> <b>University</b> grew out of the dedication and vision of Sister Theresa Hackelmeier and the Sisters of Saint Francis, Oldenburg, Indiana, who established a school in Oldenburg, Indiana, in 1851. ... sex, gender, gender identity, sexual orientation, <b>religion</b>, creed, national origin, age or disabilities in the selection ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'PS3603.A465 M35 2005. <b>Make Love! The Bruce Campbell Way</b> is a comedy novel written by actor <b>Bruce</b> <b>Campbell</b>. [1] [2] The novel is written in the first person and involves real life celebrities such as Richard Gere, Renée Zellweger, and Mike Nichols; however, it is fiction. On the jacket <b>of Make Love! The Bruce Campbell Way</b> the <b>author</b> states, quote:', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'From a violent fistfight with a Buddhist to a life-altering stint in federal prison, this novel has it all. And if the 72,444 words are too time-consuming, there are lots and lots of cool graphics. &quot;It&#39;s a great, goofy what-if.&quot; &quot;Ultimately, <b>Make</b> <b>Love</b> is a <b>Bruce</b> <b>Campbell</b> novel, starring <b>Bruce</b> <b>Campbell</b>, written for <b>Bruce</b> <b>Campbell</b> fans for whom ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Make Love the Bruce Campbell Way</b>: A Novel. <b>Make Love the Bruce Campbell Way</b>. : <b>Bruce</b> <b>Campbell</b>. Macmillan, Oct 1, 2015 - Fiction - 320 pages. What you&#39;re reading right now is known as the &quot;cover copy,&quot; or “flap copy.”. This is where the 84,951 words of my latest book are cooked down to 350 words or less to capture your imagination/download.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Rabat, city and <b>capital</b> of Morocco. One of the country’s four imperial cities, it is located on the Atlantic coast at the mouth of the Wadi Bou Regreg, opposite the city of Salé. The history of Rabat is closely connected to that of Salé, the site of which was first occupied by the Roman settlement of <b>Sala</b> (Shella).', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'List of world capitals As <b>the capital</b> cities of their countries, these 197 towns differ in terms of safety, prices, health care, pollutions level, and other conditions, these all are called the quality of life. What is the best place to live? The world&#39;s number one place for living is Australian <b>capital</b> Canberra, followed by the Canadian Ottawa.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Rabat ( / rəˈbɑːt /, also UK: / rəˈbæt /, US: / rɑːˈbɑːt /; [3] [4] [5] Arabic: الرباط, romanized : ar-Ribāṭ) is <b>the capital</b> city of Morocco and the country&#39;s seventh-largest city with an urban population of approximately 580,000 (2014) [2] and a metropolitan population of over 1.2 million. It is also <b>the capital</b> city of the Rabat-Salé-Kénitra administrative region. [6 ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>county</b> covers an area of 779.9 square kilometres (301.1 sq mi). Its administrative seat is the town <b>of Polkowice</b> , and it also contains the towns of Chocianów and Przemków . As of 2019 the total population of the <b>county</b> is 62,948, out of which the population <b>of Polkowice</b> is 22,480, that of Chocianów is 7,892, that of Przemków is 6,107, and the rural population is 26,469.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Polkowice</b> is located in historic Lower Silesia, about 15 km (9 mi) northwest of Lubin.The nearest airport is Wrocław Airport, located 72 km (45 mi) from <b>Polkowice</b>.. Situated in a traditional mining region, the town is part of the largest industrial copper-extraction area in Poland, with a copper-processing plant operating nearby.Nearby <b>Polkowice</b> Dolne is the site of a former State ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Gmina <b>Polkowice</b> is an urban-rural gmina (administrative district) in <b>Polkowice</b> <b>County</b>, Lower Silesian Voivodeship, in south-western Poland.Its seat is the town <b>of Polkowice</b>, which lies approximately 80 kilometres (50 mi) north-west of the regional <b>capital</b> Wrocław.. The gmina covers an area of 158.77 square kilometres (61.3 sq mi), and as of 2019 its total population is 27,676.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Paul</b> Revere <b>Braniff</b> was <b>born</b> in Kansas <b>City</b>, Kansas. He was the younger brother of Thomas Elmer <b>Braniff</b>. He grew up during the early era of aviation, and, as a youngster, became fascinated with the new way of transport. His family moved to Oklahoma <b>City</b>, Oklahoma, in 1900. Marriage. <b>Braniff</b> married Marie Agnes Maney on April 29, 1920.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Paul</b> Revere <b>Braniff</b> (August 30, 1897 – June 15, 1954) was an airline entrepreneur. <b>Paul</b>, along with his brother Thomas Elmer <b>Braniff</b>, was one of the original founders of <b>Braniff</b> Airways, Inc. d/b/a <b>Braniff</b> International Airways (after 1948). <b>Paul</b> Revere <b>Braniff</b> was <b>born</b> in Kansas <b>City</b>, Kansas. He grew up during the early era of aviation, and, as a youngster, became fascinated with the new ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Paul</b> <b>Braniff</b>, who was <b>born</b> on October 29, 1927, and died in Oklahoma <b>City</b> on February 1, 2013 at the age of 85. Marie was graduated from St. Agnes College in Baltimore, Md., and throughout her married life was involved in the aviation affairs of her husband that at times included scouting for new routes.', 'score': 'N/A'}], [], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Rectified linear units improve restricted boltzmann machines. V Nair, GE <b>Hinton</b>. Proceedings of the 27th international conference on machine learning (ICML …. , 2010. 25342. 2010. Reducing the dimensionality of data with neural networks. GE <b>Hinton</b>, RR Salakhutdinov. Science 313 (5786), 504-507.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The following articles are merged in <b>Scholar</b>. Their combined <b>citations</b> are counted only for the first article. ... <b>Geoffrey</b> <b>Hinton</b>. Unknown affiliation. No verified email. Articles Cited by. Title. Sort. ... GF <b>Hinton</b>, F Cambridge. 182: 1981:', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Geoffrey</b> E. <b>Hinton</b>&#39;s 389 research works with 467,418 <b>citations</b> and 350,337 reads, including: Robust and data-efficient generalization of self-supervised machine learning for diagnostic imaging', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Delaware</b> single dad a contestant on &#39;<b>Worst Cooks in America</b>&#39; <b>on the Food</b> <b>Network</b>. There&#39;s no shame in this game: One of the &quot;<b>Worst Cooks in America</b>&quot; lives in <b>Delaware</b>. Avi Boodram who lives in ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Avi Boodram who lives in Christiana, will display his apparent lack of culinary abilities <b>on the Food</b> <b>Network</b> <b>program</b> that begins <b>airing</b> at 8 p.m. <b>Sunday</b>, <b>Jan</b>. <b>7</b>. Boodram is one of 16 contestant\\xads competing in ‘<b>Worst Cooks in America</b>: Spoiled Rotten,” a seven-episode series that follows men and women as they go through a culinary “boot camp” led by <b>Food</b> <b>Network</b> chefs Anne Burrell and ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'New York City, New York. The <b>Food</b> <b>Network</b>’s studio is located at Chelsea Market in New York’s Manhattan. Along with many other shows of the <b>network</b>, parts of ‘<b>Worst Cooks In America</b>’ are filmed in this studio. Chelsea Market seems to be the finest location possible for a cooking show. The market itself is a hub for foodies, with many ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Maud Ventura, Emma Ramadan (Translator) In this suspenseful and darkly funny debut novel, a sophisticated French woman spends her life obsessing over her perfect <b>husband</b>--but can their marriage survive her passionate love? At forty years old, she has an enviable life: a successful career, stunning looks, a beautiful house in the suburbs, two ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>My</b> <b>Husband</b>, her first novel, was a bestseller in France and winner of the Prix du Premier Roman. Emma Ramadan (translator) is the recipient of the PEN Translation Prize, the Albertine Prize, an NEA Fellowship, and a Fulbright Scholarship. She lives in Brooklyn.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>My</b> <b>Husband</b> is the novel for any woman with a <b>husband</b>. Or perhaps for any man with a wife. Merveilleux, Mademoiselle Ventura!” — A. J. Finn, #1 NYT bestselling author of The Woman in the Window “While I tend to be more obsessed with <b>my</b> <b>husband</b>’s exes than <b>my</b> <b>husband</b> himself, this book had me reeling, laughing and re-examining <b>my</b> life ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Alexei</b> Mikhailovich <b>Ugarov</b> (Russian: Алексей Михайлович Угаров; born November 2, 1985) is a Belarusian professional ice hockey forward.He is currently an unrestricted free agent who most recently played for Severstal Cherepovets of the Kontinental Hockey League (KHL). He previously played three seasons for HC Nizhnekamsk Neftekhimik in the Russian Super League.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Hockey player <b>Ugarov</b> <b>Alexei</b>: up-to-date statistics, KHL matches, latest news', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Visit Aleksei <b>UGAROV</b> profile and read the full biography, watch videos and read all the latest news. Click here for more. IOC; Paris 2024; Milano Cortina 2026; LA 2028; Brisbane 2032; Museum; Shop; Olympic Refuge Foundation; English. Olympic Games; Athletes; <b>Sports</b>; News; Olympic Channel; Let&#39;s Move; Aleksei <b>UGAROV</b>. Team Belarus. Ice Hockey ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Joshua <b>Da Silva</b> (born 19 June 1998) is a Trinidadian cricketer. He made his domestic debut in 2018 for Trinidad and Tobago, and his international debut for the West Indies cricket team in December 2020. Personal life. <b>Da</b> <b>Silva</b> is of Portuguese descent, with his ancestors hailing from Madeira. Both ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'JOSHUA <b>DA</b> <b>SILVA’S</b> TWITTER: @joshuadasilva08: JOSHUA <b>DA</b> <b>SILVA</b> BIOGRAPHY. JOSHUA <b>DA</b> <b>SILVA’S</b> NEWS. News. Watch – West Indies’ Shamar Joseph smashes long six that breaks tiles on Trent Bridge roof. by Staff Writer July 20, 2024 July 20, 2024. News. <b>Da</b> <b>Silva</b> and Hodge lead West Indies to 266-8 against Australia in day-night test.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'West Indies wicketkeeper Joshua <b>da</b> <b>Silva</b> had a different definition of friendly to his Sunday league opponents. In 2017, <b>Da</b> <b>Silva</b> was 18 and spending his summer with Old Wimbledonians Cricket Club ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Every four years on the first Tuesday following the first Monday of November, voters head to the polls to elect the president of the United States. The <b>votes</b> of the public determine electors, who formally choose the president through the <b>electoral</b> <b>college</b>. The number of electors a state receives is', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Ohio has 17 <b>electoral</b> <b>votes</b>. Georgia has 16 <b>electoral</b> <b>votes</b>. North Carolina has 16 <b>electoral</b> <b>votes</b>. Michigan has 15 <b>electoral</b> <b>votes</b>. New Jersey has 14 <b>electoral</b> <b>votes</b>. Virginia has 13 <b>electoral</b> <b>votes</b>. <b>Washington</b> has 12 <b>electoral</b> <b>votes</b>. Arizona has 11 <b>electoral</b> <b>votes</b>. Indiana has 11 <b>electoral</b> <b>votes</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Forty-eight states, including <b>Washington</b>, use a “winner-take-all” system; the presidential ticket that receives the most <b>votes</b> in our state is entitled to all of <b>Washington</b>’s <b>electoral</b> <b>votes</b>. Legislation passed in 2019 requires each elector to <b>vote</b> for the party and presidential ticket for which they are an elector.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The British <b>Empire</b> ran for centuries and covered vast swathes of the world. It is, as Sanghera reveals, fundamental to understanding Britain. However, even among those who celebrate the <b>empire</b> there seems to be a desire not to look at it too closely – not to include the subject in our school history <b>books</b>, not to emphasize it too much in our favourite museums.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Home - Sathnam Sanghera. Sathnam Sanghera is the Sunday Times bestselling <b>author</b> of Empireland: How Modern Britain is Shaped by its Imperial Past , memoir The Boy With The Topknot, and novel Marriage Material. The highly-anticipated, acclaimed sequel to Empireland, Empireworld: How British Imperialism Has Shaped the Globe is available now.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Synopsis. <b>Author</b>. An urgent, incisive account of how the depredations of its imperial past dog Britain’s view of itself today, Empireland is provocative, meticulously researched writing of the highest order. Winner of The British Book Awards 2022 Non-Fiction Narrative Book of the Year. Longlisted for the Baillie Gifford Prize for Non-Fiction ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The Kermadec <b>Ocean</b> <b>Sanctuary</b> will not go ahead, with Cabinet deciding to stop work on the proposed reserve and remove the Bill that would have established it from Parliament’s order paper. “The Kermadec <b>Ocean</b> <b>Sanctuary</b> Bill would have created <b>a 620,000</b> <b>sq</b> <b>km</b> economic no-go zone,” Oceans and Fisheries Minister Shane Jones says. “The ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Prime Minister John Key has announced the creation of <b>a 620,000</b> km2 <b>Ocean</b> <b>Sanctuary</b> in the Kermadec region, one of the most pristine and unique environments on Earth. “The Kermadec <b>Ocean</b> <b>Sanctuary</b> will be one of the world’s largest and most significant fully-protected areas, preserving important habitats for seabirds, whales and dolphins, endangered marine turtles and thousands of species ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>Government</b>’s announcement that it will scrap plans for a vast marine <b>sanctuary</b> around the Kermadec <b>Islands</b> is ‘shameful’ and will make it impossible for Aotearoa <b>New</b> <b>Zealand</b> to meet its international commitments, says the World Wide Fund for Nature (WWF) <b>New</b> <b>Zealand</b>.. Plans had been underway for <b>a 620,000</b> square kilometre <b>ocean</b> <b>sanctuary</b> around the Kermadec <b>Islands</b>/Rangitāhua to ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The Fr. is for Robert Friedberg. His book Paper Money of the United States <b>has</b> a list of all circulating <b>American</b> <b>bank</b> <b>notes</b>, assigning them each a Friedberg Number. As we mentioned before, the 1995 $20 was signed by Withrow and Rubin. On 25 th August 2023, a special Fr. 2081-D 1995 $20 sold for $1,920.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'On 25 th August 2023, an Atlanta Fr. 2079-F 1993 $20 sold for $2,400 because of Solid #5 Serial Numbers. A Boston was cheaper. One month earlier on 27 th July 2023, a Boston Fr. 2079-A 1993 $20 was $2,220 with Solid #6 Serial Numbers. Both <b>notes</b> were graded 66 EPQ Gem Uncirculated.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The $20 <b>note</b> includes an embedded security thread that glows green when illuminated by UV light. When held to light, a portrait watermark of President Jackson is visible from both sides of the <b>note</b>.The <b>note</b> includes a color-shifting numeral 20 in the lower right corner of the <b>note</b>. Watch Video.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Nora Roberts. If there’s one <b>author</b> on this list who’s a recognized household name, it’s Nora Roberts. Since 1980, Roberts has written and published an astounding number of romances — her website claims the number stands at over 215! But this incredibly prolific production has not come at the cost of quality.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'With 26 <b>books</b>, there&#39;s a ton of Colleen Hoover romances to read. Here&#39;s how to read <b>all</b> of Colleen Hoover <b>books</b> in order, so you never miss a word.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Colleen Hoover (born December 11, 1979, Sulphur Springs, Texas, U.S.) is an American <b>author</b> who became a publishing phenomenon in the early 21st century and is known for hugely popular <b>books</b> that typically feature romance and dramatic plot twists.Forgoing more traditional marketing plans, Hoover self-published many of her <b>books</b>, which became sensations on social media, especially TikTok.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>2001–02</b> <b>Serie</b> <b>B</b> is the 70th season since its establishment in 1929. It is the second highest football league in Italy. Teams. Modena, Palermo, Como and Messina had been promoted from <b>Serie</b> C, while Reggina, Vicenza, Napoli and Bari had been relegated from <b>Serie</b> A. Personnel and sponsoring. Team', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>2001</b>-2002 <b>Serie</b> <b>B</b> Stats. Governing <b>Country</b>: Italy it. Level: 2nd Tier ( See League Structure) Gender: Male. Champion: Como. Most Goals: Luís Oliveira (Como) - 23. Most Assists: Mark Bresciano (Empoli) - 12. Most Clean Sheets: Marco Ballotta (Modena) - 20. Become a Stathead &amp; surf this site ad-free.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>Serie B</b> (Italian pronunciation: [ˈsɛːrje ˈbi]), officially known as <b>Serie</b> BKT for sponsorship reasons, is the second-highest division in the Italian football league system after the <b>Serie</b> A.It has been operating for over ninety years since the 1929–30 season.It had been organized by Lega Calcio until 2010 and the Lega <b>Serie B</b> ever since. Common nicknames for the league are campionato ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Angel</b> is an American supernatural television series, a spinoff of Buffy the Vampire Slayer.The series was created by Buffy &#39; s creator, writer and director Joss Whedon, in collaboration with David Greenwalt.It aired on The WB from October 5, 1999, to May 19, 2004, consisting of five seasons and 110 episodes. Like Buffy, it was produced by Whedon&#39;s production company, Mutant Enemy.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Angel</b>: Created by David Greenwalt, Joss Whedon. With David Boreanaz, Alexis Denisof, J. August Richards, Charisma Carpenter. The vampire <b>Angel</b>, cursed with a soul, moves to Los Angeles and aids people with supernatural-related problems while questing for his own redemption.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Angel</b> fiction is a sub-<b>genre</b> of literature that revolves around the concept of <b>angels</b>, their interactions with humans, and their conflicts with other supernatural beings. These stories often explore themes of love, redemption, and morality, while reflecting on the nature of good and evil.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Yinka Ayenuwa</b> (<b>born</b> 2 May 1986 in Warri) is a Nigerian weightlifter. He competed in the men&#39;s 69 kg event at the 2014 Commonwealth Games [2] where he won a silver medal. [3] In 2015, he won 3 silver medals at the African Games .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Yinka</b> <b>Ayenuwa</b> (<b>born</b> 2 May 1986 in Warri) is a Nigerian weightlifter. He competed in the men&#39;s 69 kg event at the 2014 Commonwealth Games where he won a silver medal. In 2015, he won 3 silver medals at the African Games. Celebs Wiki. <b>Yinka</b> <b>Ayenuwa</b> fans also viewed: John Davis (weightlifter)', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>ayenuwa</b>.<b>yinka</b>.3. Show Famous Birthdays Today, Nigeria. ... About <b>Yinka</b> <b>Ayenuwa</b>. <b>Yinka</b> <b>Ayenuwa</b> (<b>born</b> 2 May 1986 in Warri) is a Nigerian weightlifter. He competed in the men&#39;s 69 kg event at the 2014 Commonwealth Games where he won a silver medal. In 2015, he won 3 silver medals at the African Games. Read more at Wikipedia. See Also.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'All the results of the games played <b>in the 1966</b> <b>World</b> <b>Cup</b> with goals scored, extratime and penalties information. The Soccer <b>World</b> Cups.com. Champions, stats, national teams and players from each <b>World</b> <b>Cup</b>. ... <b>1966</b> Soccer <b>World</b> <b>Cup</b> <b>Scores</b> ... <b>Final</b> Game. England. 4 - 2. West Germany ( 2 - 0 ) on extra <b>time</b>. H2H. Share: Follow us on: ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>1966</b> <b>FIFA</b> <b>World</b> <b>Cup</b> <b>final</b> was a football match played at Wembley Stadium in London on 30 July <b>1966</b> to determine the winner of the <b>1966</b> <b>FIFA</b> <b>World</b> <b>Cup</b>, the eighth <b>FIFA</b> <b>World</b> <b>Cup</b>. The match was contested by England and West Germany, with England winning 4–2 after extra <b>time</b> to claim the Jules Rimet Trophy.It was the first – and to date only – occasion that England has hosted or won the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>FIFA</b> <b>World</b> <b>Cup</b> (<b>Final</b>) England vs. West Germany Historical Head-to-Head . Attendance: 96,924. ... <b>Half Time</b>. 78&amp;rsquor; 2:1. Martin Peters. 89&amp;rsquor; 2:2. Wolfgang Weber ... This includes the entire history of the <b>FIFA</b> Women&#39;s <b>World</b> <b>Cup</b> as well as recent domestic league seasons from nine countries, ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Tikhvinsky District</b> (Russian: Ти́хвинский райо́н) is an administrative and municipal <b>district</b> (), one of the seventeen in Leningrad Oblast, Russia.It is located in the southeast of the oblast and borders with Lodeynopolsky <b>District</b> in the north, Podporozhsky <b>District</b> in the northeast, Babayevsky <b>District</b> of Vologda Oblast in the east, Boksitogorsky <b>District</b> in the southeast ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Tikhvin (Russian: Ти́хвин; Veps: Tihvin) is a town and the administrative center <b>of Tikhvinsky</b> <b>District</b> in Leningrad Oblast, Russia, located on both banks of the Tikhvinka River in the east of the oblast, 200 kilometers (120 mi) east of St. Petersburg.Tikhvin is also an industrial and cultural center of the <b>district</b>, as well as its transportation hub.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Tikhvinsky</b> (masculine), Tikhvinskaya (feminine), or Tikhvinskoye (neuter) may refer to: . <b>Tikhvinsky</b> <b>District</b>, a <b>district</b> of Leningrad Oblast, Russia; Tikhvinskoye Urban Settlement, a municipal formation corresponding to Tikhvinskoye Settlement Municipal Formation, an administrative division <b>of Tikhvinsky</b> <b>District</b> of Leningrad Oblast, Russia; Tikhvinskoye (rural locality), a rural locality (a ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Conan, Lord of the Black River</b> is a fantasy novel by American writer Leonard Carpenter, featuring Robert E. Howard&#39;s sword and sorcery hero <b>Conan</b> the Barbarian. It was first published in paperback by Tor <b>Books</b> in April 1996. Plot After successfully fulfilling his commission to overthrow a tyrannical baron in Koth, <b>Conan</b> travels into Baalur, a ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '3.28. 80 ratings8 reviews. <b>Conan</b> the Cimmerian must venture into the nightmare world of the dead to retrieve the Silver Lotus, a powerful weapon that can undo the dreaded incantation that holds the city of Queen Rufia under the spell of the undead witch Zeriti. Genres Fantasy Sword and Sorcery. 288 pages, Paperback. First published April 15, 1996.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Leonard Paul Carpenter (born in 1948) is a technical writer and <b>author</b> of fantasy and science fiction. Among Carpenter&#39;s works are eleven <b>Conan</b> novels published by Tor <b>Books</b>, which he claims &quot;make him the most prolific contributor, living or dead, to the <b>Conan</b> literary saga of the late Robert E. Howard.&quot; He has also written the science fiction novel Fatal Strain, and a number of short stories ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Hunt</b>: Directed by Craig Zobel. With Betty Gilpin, Hilary Swank, Ike Barinholtz, Wayne Duvall. Twelve strangers wake up in a clearing. They don&#39;t know where they are, or how they got there. They don&#39;t know they&#39;ve been chosen - for a very specific purpose - The <b>Hunt</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Hunt</b> is a 2020 American action horror film directed by Craig Zobel and written by Nick Cuse and Damon Lindelof.The film stars Betty Gilpin, Hilary Swank, Ike Barinholtz, and Emma Roberts. Jason Blum was a <b>producer</b> under his Blumhouse Productions banner, along with Lindelof. Zobel and Lindelof have said that the film is intended as a satire on the profound political divide between the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>Hunt</b> (2020) cast and crew credits, including actors, actresses, directors, writers and more. Menu. Movies. Release Calendar Top 250 Movies Most Popular Movies Browse Movies by Genre Top Box Office Showtimes &amp; Tickets Movie News India Movie Spotlight. TV Shows.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Saraburi</b> City (thesaban mueang) is the provincial <b>capital</b> <b>of Saraburi</b> Province in central Thailand. [1] [2] In 2020, it had a population of 60,809 people, and covers the complete tambon Pak Phriao of the Mueang <b>Saraburi</b> district .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Saraburi</b> is on the east side of the Chao Phraya River valley. The eastern part of the province is covered by high plains and plateaus, while the western part is mostly low flat plains. [citation needed] <b>Saraburi</b> province has 848 km 2 (327 sq mi) of forest or 24.2 percent of provincial area. [1] The town, as a gateway to the northeastern region ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Saraburi</b> has about 67,800 residents. <b>Mapcarta</b>, the open map. SE Asia. Thailand. <b>Saraburi</b>. Mueang <b>Saraburi</b>. <b>Saraburi</b> <b>Saraburi</b> is a city in the Chao Phraya Basin region of Thailand. It is home to one of the most beautiful religious sites in Thailand, a large botanical garden, and many other natural and historical sights.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Charlton Heston. Select from the options above. Q4. <b>The first US National Champions of which sport were Team Roslindale when the first sanctioned Official Nationals were played in Leominster Ma in 1974</b>? A. Ice skating. B.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>The first US National Champions of which sport were Team Roslindale when the first sanctioned Official Nationals were played in Leominster Ma in 1974</b>? A . Ice skating B . Roller Blading C . Rollerball D . Street hockey ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The Americans <b>were</b> shut out in that game (31-0), but it didn’t matter because more importantly it rekindled interest in what up until then had largely been an unknown <b>sport</b> in the <b>US</b>. It would be another 33 years, however, before an American <b>national</b> <b>team</b> <b>played</b> its <b>first</b> <b>official</b> international match against Canada in Pittsburgh where the <b>US</b> lost 23-10.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Strand</b>’s guitarist Scott Shelly was a founding member, along with Jeff Porcaro, of the Grant High School-based band Rural Still Life. This outfit featured a number of musicians who would later establish themselves on the West Coast AOR scene, like Carlos Vega, Michael Landau or Steve Lukather and David Paich.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'John <b>Strand</b>, who was released early from prison last week, said federal prison officials in Miami locked him in solitary confinement for four months of his term over accusations ranging from ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Bricks and bottles were thrown at police officers as violence broke out on The <b>Strand</b>. Protests were organised in the city centre on Saturday, including one called &#39;Save Our Kids&#39; near the Liver ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>St</b>. <b>Albert</b>&#39;s <b>Church</b> (Latvian: Svētā Alberta Romas katoļu baznīca) is a Roman Catholic <b>church</b> in <b>Riga</b>, ... <b>St</b>. <b>Albert</b> <b>Church</b>, <b>Riga</b>, Pipe Organ <b>St</b>. <b>Albert</b> <b>Church</b>, <b>Riga</b>, inside view, the main door with the Choir balcony. The 30 stops pipe organ of the <b>church</b> was built in 1912., by Emil Martin &amp; Co., Opus 316. Emil Martin was the son of, also ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Riga</b> <b>St</b>. <b>Albert</b> Roman Catholic <b>Church</b> In Latvia It is one of the best churches in Latvia which you must visit. History: The Roman Catholic <b>Church</b> <b>of St</b>. <b>Albert</b> was founded in 1778 with the intention of providing a place of worship for the Catholic residents of <b>Riga</b>. The original <b>church</b> building was destroyed during the WWII bombings. The present-day <b>church</b> was built in 1957.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>Riga</b> <b>St</b>. <b>Albert</b> Roman Catholic <b>Church</b> is a large and magnificent <b>church</b> in the Latvian capital, which is popular for its services and events. The <b>church</b> was built in 1901 and features beautiful architecture and lavish decoration. It is home to a number of prayer and evangelization groups as well as a Caritas group that works to help the needy.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Here, he’s in the role of Tim Jenkin, a real-life political prisoner during apartheid-era South Africa, who made a daring <b>escape</b> from his cell at Pretoria Local Prison, the whites-only ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>ESCAPE</b> FROM PRETORIA <b>director</b> Francis Annan has revealed exclusively to <b>Express.co.uk</b> how historically accurate the Daniel Radcliffe prison break movie is, plus some real-life Easter Eggs.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The Great <b>Escape</b> is a 1963 American epic war suspense adventure film [2] starring Steve McQueen, James Garner and Richard Attenborough and featuring James Donald, Charles Bronson, Donald Pleasence, James Coburn, Hannes Messemer, David McCallum, Gordon Jackson, John Leyton and Angus Lennie.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Inspector</b> Jacques <b>Clouseau</b> (French: [ʒɑk kluzo]), later granted the rank of Chief <b>Inspector</b>, is a fictional character in Blake Edwards&#39; farcical The Pink Panther series. He is portrayed by Peter Sellers in the original series, and also by Alan Arkin in the 1968 film <b>Inspector Clouseau</b> and, in a cameo, by Roger Moore (credited as Turk Thrust II) in the 1983 film Curse of the Pink Panther.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Films: 7 Cato Fong is <b>Clouseau&#39;s</b> Chinese <b>manservant</b>, trained to attack him regularly to keep him alert and skilled in martial arts. Cato and <b>Clouseau</b> have a love-hate relationship, with their fights being long and vicious, as well as destructive to the furniture, and always interrupted by the telephone ringing, at which point they will become civil again. Cato puts a lot of effort into taking ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'He was best known for playing Cato Fong, <b>Inspector</b> <b>Clouseau&#39;s</b> <b>manservant</b>, in the Pink Panther film series. The character was first introduced in A Shot in the Dark (1964), the second film in the series, and was a role that Kwouk would reprise on another six occasions until the 2006 series reboot.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Twenty-second Amendment, amendment (<b>1951</b>) to the Constitution of the United States effectively limiting to two the <b>number</b> of <b>terms</b> a <b>president</b> of the United States may serve. It was <b>one</b> of 273 recommendations to the <b>U.S</b>. Congress by the Hoover Commission, created by Pres. Harry S. Truman, to reorganize and reform the federal government.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The Twenty-second Amendment ( Amendment XXII) <b>to the</b> United States Constitution limits the <b>number</b> of times a person can be elected to the office of <b>President</b> of the United States to two <b>terms</b>, and sets additional eligibility conditions for presidents who succeed to the unexpired <b>terms</b> of their predecessors. [1] Congress approved the Twenty-second Amendment on March 21, 1947, and submitted it ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'SECTION. 1. No person shall be elected to the office of the <b>President</b> more than twice, and no person who has held the office of <b>President</b>, or acted as <b>President</b>, for more than two years of a term to which some other person was elected <b>President</b> shall be elected to the office of <b>President</b> more than once.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Operation Chastise, commonly known as the <b>Dambusters</b> <b>Raid</b>, was an attack on German dams carried out on the night of 16/17 May 1943 by 617 Squadron RAF Bomber Command, later called the <b>Dam Busters</b>, using special &quot;bouncing bombs&quot; developed by Barnes Wallis.The Möhne and Edersee dams were breached, causing catastrophic flooding of the Ruhr valley and of villages in the Eder valley; the Sorpe Dam ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Dambusters</b> <b>Raid</b>. On the night of 16-17 May 1943, Wing Commander Guy Gibson led 617 Squadron of the Royal Air Force on an audacious bombing <b>raid</b> to destroy three dams in the Ruhr valley, the industrial heartland of Germany. The mission was codenamed Operation &#39;Chastise&#39;. The dams were fiercely protected.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '15 May 2013. Seventy <b>years</b> ago an RAF bomber <b>raid</b> destroyed important German dams. At the time many argued it was only a propaganda victory. It was much more than that, writes historian Dan Snow ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'You can get a <b>free</b> <b>TV</b> <b>licence</b> if you&#39;re 75 or over and you get Pension Credit, or a discount if you&#39;re blind or in residential care - who qualifies, how to apply', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>free</b> <b>TV</b> <b>Licence</b> for people aged 75 or over applies to those in receipt of either part of Pension Credit – Guarantee Credit or Savings Credit (or both). It’s easy to check if you can get Pension Credit.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Apply for a <b>free</b> <b>TV</b> <b>Licence</b>. How <b>do</b> you qualify for a reduced fee <b>licence</b>? If you live in a residential care home, supported housing or sheltered accommodation you may be <b>entitled</b> to a reduced fee <b>TV</b> <b>Licence</b>. If you are blind (severely sight impaired) and can provide the appropriate evidence, you are eligible to apply for a 50% concession.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>William Zeckendorf Jr</b>. (October 31, 1929 – February 12, 2014) was an American real estate developer. Son <b>of William Zeckendorf</b> Sr., he was the second of three generations of one of New York&#39;s great real estate dynasties. [1] While keeping a lower profile than his famously flamboyant <b>father</b>, <b>Zeckendorf</b> <b>Jr</b>. was highly successful in his own right.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'But while <b>Zeckendorf</b> <b>Jr</b>. may lack his <b>father</b>’s flair, he was a historic heavyweight — in 1986, the New York Times named him the city’s “most active real estate developer,” citing the 20 ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>William</b> <b>Zeckendorf</b> <b>Jr</b>. was born on Oct. 31, 1929, in Manhattan. He graduated from the Lawrenceville School in Lawrenceville, N.J., near Princeton, and attended the University of Arizona for two ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Results, prize money and highest break for the 2024 World Snooker Championship in Sheffield.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Wilson 9-5 O&#39;<b>Connor</b> Joe O&#39;<b>Connor</b> is going for his shots, trying to get something started but he&#39;s struggling to stay in position. A couple of nice reds go in but not much comes of either.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Watch live BBC coverage of the World Snooker Championship 2024 at the Crucible Theatre in Sheffield.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Luis Fernando Tena</b> Garduño (<b>born</b> 20 January 1958) is a Mexican professional football manager and former player who is the head coach of the Guatemala national team .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Age, Biography and Wiki. <b>Luis Fernando Tena</b> was <b>born</b> on 20 January, 1958 in Mexico <b>City</b>, Mexico. Discover <b>Luis Fernando Tena</b>&#39;s Biography, Age, Height, Physical Stats, Dating/Affairs, Family and career updates.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Luis</b> <b>Fernando</b> <b>Tena</b>. Self: Chivas: El Rebaño Sagrado. <b>Born</b> on January 20, 1958 in Mexico <b>City</b>, <b>Luis</b> <b>Fernando</b> <b>Tena</b> Garduño is a Mexican former professional football player and former head coach of Club Deportivo Guadalajara, one of Mexico&#39;s most legendary teams. <b>Luis</b> <b>Fernando</b> played professional football for 12 years, starting in 1976 with Atlético Español (now Club Necaxa) and ending his ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Country</b> Argentina: Province: Catamarca Province: Time zone: UTC−3 : <b>Colonia Nueva Coneta</b> is a village and municipality in Catamarca Province in northwestern Argentina. Location. It is located 12 km from the city of San Fernando del Valle de Catamarca, ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Hoy 9 de agosto arribamos al 50 aniversario de la <b>Colonia</b> <b>Nueva</b> <b>Coneta</b>, fue la primera <b>colonia</b> del valle central de Catamarca, concebida según las premisas de la ciencia agraria. Lee además ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'El área proyectada para la ejecución de la colonización era de una superficie de 12 mil hectáreas dividida en un principio en cinco colonias”, apunta en su libro “<b>Nueva</b> <b>Coneta</b>, un pueblo ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Canton</b> of <b>Geneva</b>, officially the Republic and <b>Canton</b> of <b>Geneva</b>, is one of the 26 cantons of the Swiss Confederation.It is composed of forty-five municipalities, and the seat of the government and parliament is in the city of <b>Geneva</b>.. <b>Geneva</b> is the French-speaking westernmost <b>canton</b> of Switzerland.It lies at the western end of Lake <b>Geneva</b> and on both sides of the Rhone, its main river.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Geneva</b> (/ dʒ ə ˈ n iː v ə / jə-NEE-və, Arpitan: [dzəˈnɛva] ⓘ; French: Genève ⓘ) is the second-most populous city in Switzerland (after Zürich) and the most populous of the French-speaking Romandy.Situated in the southwest of the country, where the Rhône exits Lake <b>Geneva</b>, it is the <b>capital</b> of the Republic and <b>Canton</b> of <b>Geneva</b>, and a centre for international diplomacy.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Geneva</b>, city, <b>capital</b> of Genève <b>canton</b>, in the far southwestern corner of Switzerland that juts into France.One of Europe’s most cosmopolitan cities, <b>Geneva</b> has served as a model for republican government and owes its preeminence to the triumph of human, rather than geographic, factors. It developed its unique character from the 16th century, when, as the centre of the Calvinist Reformation ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Harvard University</b> is a private Ivy League research <b>university</b> in Cambridge, Massachusetts.Founded in 1636 as <b>Harvard</b> College and named for its first benefactor, Puritan clergyman John <b>Harvard</b>, it is the oldest institution of higher learning in the <b>United States</b>.Its influence, wealth, and rankings have made it one of the most prestigious universities in the world.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Harvard</b> <b>University</b> holds 5,083 acres of real estate. The main campus occupies several locations in Cambridge including the historic and famous <b>Harvard</b> Yard. Athletic facilities and the <b>Harvard</b> Business School are located across the Charles River in Allstom, Massachusetts. The <b>Harvard</b> Medical School and School of Dental Medicine are located in ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Harvard</b> <b>University</b> is located in the <b>state</b> of Massachusetts. Specifically, it&#39;s situated in the city of Cambridge, which is just across the Charles River from Boston. This area is known for its concentration of prestigious academic institutions and vibrant student life. If you end up applying to <b>Harvard</b> and want tips on how to maximize your chances of acceptance, check out this blog post from ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Plesetsky District</b> (Russian: Плесе́цкий райо́н) is an administrative <b>district</b> one of the twenty-one in Arkhangelsk Oblast, Russia. As a municipal division, it is incorporated as <b>Plesetsky</b> Municipal <b>District</b>. It is located in the west of the oblast and borders with Primorsky <b>District</b> in the north, Kholmogorsky <b>District</b> in the northeast, the territories of the town of oblast ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'A youth team in the Bandy Championship of Arkhangelsk Oblast. Plesetsk (Russian: Плесе́цк) is an urban locality (a work settlement) and the administrative center <b>of Plesetsky</b> <b>District</b>, Arkhangelsk Oblast, Russia, situated about 800 kilometers (500 mi) northeast of Moscow and 180 kilometers (110 mi) south of Arkhangelsk. Municipally, it is the administrative center of Plesetskoye Urban ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'In 1930, the okrug was abolished, and the <b>district</b> became subordinate to the central administration of Northern Krai. In 1936, the krai itself was transformed into Northern Oblast. In 1937, Northern Oblast was split into Arkhangelsk Oblast and Vologda Oblast, and Plesetsk remains the center <b>of Plesetsky</b> <b>District</b> of Arkhangelsk Oblast.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'You can use our free music <b>genre</b> finder and analyzer to quickly find the <b>genre</b> and more interesting information (such as the song’s key, BPM, popularity, etc.) about any music you love or to find an artist’s <b>genre</b>. Just enter the song title or artist name and leave the rest to our <b>genre</b> checker tool. “You are what you listen to.”.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Movie <b>genres</b> are stylistic categories where a particular movie can be placed based on the setting, characters, plot, mood, tone, and theme. A film&#39;s main <b>genre</b> category will be based on where the majority of the content lands. ... This is where shapes are <b>cut</b> out and placed on top of one another to make figures and settings, all used to tell a ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Ballroom dance music: pasodoble, cha cha cha and others. Vogue (dance) Children&#39;s music. Dance music. Slow dance. Drug use in music. Incidental music or music for stage and screen: music written for the score of a film, play, musicals, or other spheres, such as filmi, video game music, music hall songs and showtunes and others.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Randy Goffe, known professionally as <b>HOME</b>, is a Synthwave/Chillwave music <b>producer</b> from Punta Gorda, Florida, USA who now lives in Chicago.[1] <b>HOME</b>&#39;s music is influenced by 70&#39;s and 80&#39;s music, layered with a bit of Chiptune and Chillsynth. He is often regarded as the creator of chillsynth. He is primarily known for composing the 2014 song Resonance for his 2014 album Odyssey. He started ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Home</b>: Directed by Tim Johnson. With Jim Parsons, Rihanna, Steve Martin, Jennifer Lopez. Oh, a lovable misfit alien, runs away from his planet and takes shelter on Earth, where he befriends Tip, an adventurous young girl who is on a quest to find her displaced mother Lucy.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>Producers</b> is a 1967 American satirical black comedy film. It was written and directed by Mel Brooks, and stars Zero Mostel, Gene Wilder, Dick Shawn, and Kenneth Mars.The film is about a con artist theater <b>producer</b> and his accountant who scheme to get rich by fraudulently overselling interests in a stage musical purposely designed to fail. Searching for the worst script imaginable, they ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '100 <b>years</b> of supporting the Armed Forces community. In our centenary <b>year</b>, we are firmly focused on our future. By building on a century of work we’ll make sure we are a charity fit for the next 100. RBL was formed on 15 May 1921, bringing together four organisations of ex-servicemen that had established themselves after the First World War.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Royal British Legion</b> Women&#39;s Section (RBLWS) was <b>founded</b> in 1921 and operated independently for some 96 <b>years</b>, with its own branches, standards and standard bearers, county branches, income and expenditure, national central committee, and annual conference.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'In the early <b>years</b> of the newly formed <b>British</b> <b>Legion</b>, founder and President Earl Haig worked tirelessly championing the needs of the Armed Forces, launching the Poppy Day Appeal in 1921 and helping to shape modern Remembrance. He also worked hard at grass-roots level, touring the country with Lady Haig, making speeches, visiting branches ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Browse today’s rankings of the <b>wealthiest</b> people and families globally. Discover the net worth, age, and other information about the <b>richest</b> people in the world.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Key Takeaways. Elon Musk, CEO of Tesla, is the <b>richest</b> person and the <b>richest</b> <b>man</b> in the world with a net worth of $252 billion. After Musk is Jeff Bezos, founder of Amazon. Other billionaires ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Here are the 10 <b>richest</b> people <b>on Earth</b> as of July 1, ... Bezos was the world’s <b>richest</b> person on ... <b>Who is</b> the <b>richest</b> <b>man</b> in the world? As of July 1, 2024, the <b>richest</b> person in the world is Tesla and SpaceX CEO Elon Musk. He’s worth $221.4 billion. He moved into the number one spot in late May 2024, overtaking Bernard Arnault of France.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Below follows a list of Olympic <b>countries</b> that have participated in the Olympic Games since 1896. The list is ordered alphabetically by the three letter <b>country</b> codes. AFG. Afghanistan. LBA. Libya. ALB. Albania. LIE.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'U.S. Federal Information Processing Standard No. 10. An even more detailed version of these <b>country</b> codes can also be obtained as a CSV file in the download area. A list of all international calling codes is also available separately. <b>Country</b>. ISO 3166-1. alpha2. ISO 3166-1. alpha3. ISO 3166-1.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>MON</b> (1968 W) MHL ... Used as the <b>country</b> code for Athletes from Kuwait, when the Kuwait Olympic Committee was suspended the first time, at the 2010 Summer Youth Olympics, the 2010 Asian Games and the 2011 Asian Winter Games;', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Appointment with Fear was a horror drama series originally <b>broadcast</b> on BBC <b>Radio</b> in the 1940s and 1950s, and revived on a number of occasions since. The format comprised a dramatised horror story of approximately half an hour in length, introduced by a character <b>known</b> <b>as the Man</b> in <b>Black</b>. The plays themselves were a mixture of classic horror ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Mark Austin – former BBC journalist and sports correspondent for BBC News, from 1982 to 1986. He joined ITN in 1986 as sports correspondent. Now works for Sky News. Khalid Aziz – main presenter and reporter on Look North in Leeds during the 1970s and early 1980s. He left to join TVS on its inception in 1982.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Unfortunately very few shows of Appointment With Fear have survived. Only four shows are <b>known</b> to exist, namely 1/2, 3/1, 3/6 and 6/4. No shows from The <b>Man</b> In <b>Black</b> series have survived. However all of Fear On Four still exists. All of the stories from the first two series of Fear On Four have been published in: The <b>Man</b> In <b>Black</b>, BBC Books, 1990', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Honda <b>has</b> enjoyed four distinct eras of F1 involvement, and the current <b>one</b> <b>has</b> proved to be the second most successful. The first chapter was a works <b>team</b> from Japan that competed from 1964 to ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Sky Sports</b>&#39; Craig Slater explains. Honda have <b>announced</b> <b>they</b> will <b>withdraw</b> <b>from Formula</b> 1 <b>at the end</b> of the 2021 season. The Japanese manufacturer provide engines to both Red Bull and AlphaTauri ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Ferrari will continue to supply three teams, and Renault just <b>one</b>. The FIA regulations have a provision designed to help teams that are left without an engine partner by obliging the manufacturer with the smallest number of partners to step in, which puts the spotlight on Renault. Red Bull used Renault power for 12 seasons from 2007 and 2018 ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Albert Arnold <b>Gore</b> Sr. (December 26, 1907 – December 5, 1998) was an American politician who served as a United States Senator from Tennessee from 1953 to 1971. A member of the Democratic Party, he previously served as a U.S. Representative from the state&#39;s 4th congressional district from 1939 to 1953. He was the <b>father</b> <b>of Al</b> <b>Gore</b>, who served as the 45th vice president of the United States ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Al Gore</b> would participate in one vice-presidential debate against Vice President Dan Quayle, and Admiral James ... &quot;In all fairness, it&#39;s something <b>Gore</b> had worked on a long time. <b>Gore</b> is not the <b>Father</b> of the Internet, but in all fairness, <b>Gore</b> is the person who, in the Congress, most systematically worked to make sure that we got to an ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Al</b> <b>Gore</b> served as the 45th vice president of the United States from 1993 to 2001. ... 1948, in Washington, D.C., where his <b>father</b>, Albert <b>Gore</b> Sr., was serving as a Democrat in the U.S. House of ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Kurfürstendamm. The best known and most popular <b>shopping</b> <b>street</b> in Berlin runs from Breitscheidplatz to Halensee and is home to multiple department stores and shops for major chains. As you move west towards Halensee, the boutiques become classier and the window displays more luxurious. Stroll past the showcases of international fashion ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>street</b> is long ,it&#39;s the equivalent of the Champs Elysees in Paris with an amazing amount of <b>shopping</b> outlets from high end shops such as Gucci ,Luise Vuitton and Hermes to more conventional boutique shops plus you have huge <b>shopping</b> centres such as Europa Centre and the slighly more upmarket and famous KaDeWe department store but there is so much more to this <b>street</b> than shops.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Restaurants at Kurfürstendamm View over Kurfürstendamm. The Kurfürstendamm (German pronunciation: [ˌkuːɐ̯fʏʁstn̩ˈdam] ⓘ; colloquially Ku&#39;damm, ⓘ; English: Prince Elector Embankment) is one of the most famous avenues in Berlin.The <b>street</b> takes its name from the former Kurfürsten (prince-electors) of Brandenburg.The broad, long boulevard can be considered the Champs-Élysées of ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Dusty had no emotional integrity… if a contestant fell flat on their face, Dusty would smile, if they won the booby-<b>prize</b> of a brand new <b>dustbin</b>, ... The <b>quiz</b> <b>show</b> ran from 1978 – 1988 and was shown on ITV. The <b>show</b> was based on the spanish gameshow called Un, dos, tres … responda, otra vez. 3-2-1 would regularly pull-in 17 million ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>3–2</b>–1 was a British <b>game</b> <b>show</b> that was made by Yorkshire <b>Television</b> for ITV.It ran for ten years, from 29 July 1978 to 24 December 1988, with Ted Rogers as the host.. It was based on a Spanish gameshow called Un, dos, tres... responda otra vez and was a trio of three <b>shows</b> in one: a <b>quiz</b>, variety and a <b>game</b> <b>show</b>.. The <b>show</b> was a huge success, consistently pulling in large ratings.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Synopsis Three for the price of one . Once entertaining (but in retrospect, truly cringeworthy) prime-time <b>show</b>, which was Yorkshire <b>TV</b>&#39;s greatest contribution to ITV <b>game</b> <b>shows</b> before Countdown hit the airwaves. Divided into three key parts - first, the rather jolly <b>quiz</b> where each of the three couples were given two rounds of 10 questions, their money being equal to 10 x first round score x ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Walker</b> was <b>born</b> on September 12, 1973, in Glendale, California. His mother, Cheryl (née Crabtree), was a fashion model, and his father, <b>Paul</b> William <b>Walker</b> III, was a sewer contractor and former amateur boxer who was a two-time Golden Gloves champion.<b>Walker</b>&#39;s paternal grandfather, William, had a short-lived boxing career as &quot;Irish&quot; Billy <b>Walker</b>, while another raced factory cars for Ford in ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Birth date: September 12, 1973. Birth State: California. Birth <b>City</b>: Glendale. Birth Country: United States. Gender: Male. Best Known For: <b>Paul</b> <b>Walker</b> was an American actor who came to fame in ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Paul</b> <b>Walker</b>. Actor: The Fast and the Furious. <b>Paul</b> William <b>Walker</b> IV was <b>born</b> in Glendale, California. He grew up together with his brothers, Caleb and Cody, and sisters, Ashlie and Amie. Their parents, <b>Paul</b> William <b>Walker</b> III, a sewer contractor, and Cheryl (Crabtree) <b>Walker</b>, a model, separated around September 2004. His grandfather, William <b>Walker</b>, was a Pearl Harbor survivor and a Navy ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Into the Woods</b> is a 1986 musical with music and lyrics by Stephen Sondheim and book by James Lapine.. The musical intertwines the plots of several Brothers Grimm fairy tales, exploring the consequences of the characters&#39; wishes and quests.The main characters are taken from &quot;Little Red Riding Hood&quot; (spelled &quot;Ridinghood&quot; in the published vocal score), &quot;Jack and the Beanstalk&quot;, &quot;Rapunzel ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Into the Woods</b>. STEPHEN SONDHEIM AND JAMES LAPINE 1986. INTRODUCTION <b>AUTHOR</b> BIOGRAPHY PLOT SUMMARY CHARACTERS THEMES STYLE HISTORICAL CONTEXT CRITICAL OVERVIEW CRITICISM SOURCES FURTHER READING INTRODUCTION. <b>Into the Woods</b>, published in 1986, is a collaborative work by Stephen Sondheim (music and lyrics) and James Lapine (story).It was the product of a workshop at Playwrights Horizon in New ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>author</b> <b>of Into the Woods</b> – the best selling book on how and why we tell stories - John was for many years Visiting Professor of English Language and Literature at the University of Newcastle-upon-Tyne. Customer reviews. 4.6 out of 5 stars. 4.6 out of 5. 1,544 global ratings ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '“This is my first <b>time</b> here and, honestly, it’s super-surreal, but I’m having the shittiest day,” <b>Phoebe</b> Bridgers confides in the John Peel Stage crowd after ‘Scott Street’ comes to a ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Mon 7 March <b>2022</b> 20:<b>01</b>, UK. <b>Phoebe</b> Bridgers has just announced a string of live shows across the UK and Europe as part of her Reunion Tour. These new dates mark the singer-songwriter’s first UK/EU dates science the release of her 2020 album Punisher. The news comes after Bridgers was confirmed for the <b>2022</b> Glastonbury <b>Festival</b> line-up.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'A jet-black guitar and a band adorned in skeleton attire as a lovely homage to the I Know the End track marks a resoundingly stylish set for <b>Phoebe</b> Bridgers’ debut Glastonbury set. She takes her Punisher -heavy set to the legendary John Peel stage with a grand deal of success. Bridgers’ clear lyrical qualities are perfect for the ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Lewis Carroll (born January 27, 1832, Daresbury, Cheshire, England—died January 14, 1898, Guildford, Surrey) was an English logician, mathematician, photographer, and novelist, especially remembered for <b>Alice</b>’s Adventures in Wonderland (1865) and its sequel, Through the Looking-Glass (1871). His poem The Hunting of the Snark (1876) is ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Alice</b>&#39;s Adventures in Wonderland (also known as <b>Alice</b> in Wonderland) is an 1865 English children&#39;s novel by Lewis Carroll, a mathematics don at the University of Oxford.It details the story of a girl named <b>Alice</b> who falls through a rabbit hole into a fantasy world of anthropomorphic creatures. It is seen as an example of the literary nonsense genre. The artist John Tenniel provided 42 wood ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Franziska Kohlt is a researcher in 19th-century history of science and literature. She is the <b>author</b> of numerous articles on Lewis Carroll, Victorian culture and science, and the forthcoming <b>Alice</b> Through the Wonderglass: The unexpected histories of a children’s classic (Reaktion 2024), and editor of The Lewis Carroll Review, and the Through ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The former <b>director</b> of a Norfolk crane hire company is on the <b>run</b> after fleeing the country weeks before he was found guilty of multiple child sex offences, it can be revealed. Oliver Arnold, 49 ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Nuns on the <b>Run (1990</b>) cast and crew credits, including actors, actresses, directors, writers and more. Menu. Movies. ... Second Unit <b>Director</b> or Assistant <b>Director</b> . David Brown ... first assistant <b>director</b> Neil Calder ... third assistant <b>director</b> Gus Maclean ... second assistant <b>director</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Nuns on the <b>Run</b> is a 1990 British comedy film starring Eric Idle and Robbie Coltrane, also featuring Camille Coduri and Janet Suzman.The film was written and directed by Jonathan Lynn and produced by HandMade Films.Many of the outdoor scenes were shot in Chiswick, White City and Kings Cross.The soundtrack was composed and performed by Yello and also features George Harrison&#39;s song &quot;Blow Away ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The North Atlantic Treaty <b>Organization</b> (NATO) said it is investigating claims that data was stolen from <b>unclassified</b> <b>websites</b> under the military alliance’s control. A hacking group named SiegedSec — which has been at the center of several <b>recent</b> hacks involving U.S. municipalities over the last year — claimed to have stolen 9 GB of data.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'NATO says it is “actively addressing incidents” <b>affecting</b> <b>its</b> <b>unclassified</b> <b>websites</b> after a hacking group claimed to have stolen numerous strategic planning and research documents from the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'NATO said it was investigating the alleged breach in what appears to have been a minor incident involving <b>unclassified</b> documents. “NATO cyber experts are actively addressing incidents <b>affecting</b> some <b>unclassified</b> NATO <b>websites</b>,” a NATO official told CyberScoop. “Additional cybersecurity measures have been put in place.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'See full company information. <b>Opening</b> $1,694,535. 399 theaters. Release Date Nov 9, 2023. MPAA PG-13. Running Time 1 hr 45 min. Genres Action Adventure Fantasy. In Release 240 days/34 weeks ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The total <b>box</b> <b>office</b> was down 2.55% to $11.09 billion, which is the third-biggest yearly <b>box</b> <b>office</b> total of all time, behind $11.12 billion earned in 2015 and $11.37 billion earned in 2016. Meanwhile, ticket prices rose 4% to $8.97. This means total attendance was 1.236 billion, the lowest since 1995.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>The Marvels</b> made $47 million domestically and $63 million internationally on its <b>opening</b> <b>weekend</b>.Making a total of $110 million on its first <b>weekend</b> in theaters.This makes it the lowest <b>opening</b> ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'By the time of his death in 1993, Federico Fellini had won four best foreign language film Oscars, tying him with his countryman Vittorio De Sica for the most wins by any director. But 25 years ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Following neo-realism, <b>Italian</b> filmmakers branched out in more ambitious directions. Some directors ventured into the extensively formalist <b>style</b> of filmmaking with big plots, big productions and epic runtimes. Others became earned their fame through their experimental, and often controversial, films.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '7. Bernardo Bertolucci Writer Director <b>Producer</b> The Conformist (1970) Bernardo Bertolucci, the <b>Italian</b> director whose films were known for their colorful visual <b>style</b>, was born in Parma, Italy. He attended Rome University and became famous as a poet.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Karl Bartholomaeus Heller</b> (20 November 1824 – 14 December 1880) was an Austrian botanist and naturalist who explored Mexico in 1845–48 and published his memoir. ... <b>Born</b> in Moravia, <b>Heller</b> was a professor at the Theresianum in Vienna. Among <b>Heller</b>&#39;s later works is his defense of Darwinism, ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Karl</b> <b>Bartholomaeus</b> In the latter year Johann Jakob Heckel published the livebearing freshwater Green swordtail , since the early 20th century a common aquarium fish, from specimens <b>Heller</b> deposited in Vienna. <b>Born</b> in Moravia, <b>Heller</b> was a professor at the Theresianum in Vienna. Among <b>Heller</b>&#39;s later works is his defense of Darwinism, Darwin und der Darwinismus, 1869.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Karl</b> <b>Bartholomaeus</b> <b>Heller</b> was an Austrian botanist and naturalist who explored Mexico in 1845-1848 and published his memoir. Career In the latter year Johann Jakob Heckel published the livebearing freshwater Green swordtail (Xiphophorus helleri), since the early 20th century a common aquarium fish, from specimens <b>Heller</b> deposited in Vienna.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Frequency Finder - England, Scotland, Wales and Ireland. Welcome to Frequency Finder, a website providing details of all <b>radio</b> <b>stations</b> in England, Scotland, Wales and Ireland with features on <b>radio</b> transmission and history. All listings include frequencies, coverage areas, format and ownership. Transmission details, historical notes and maps ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'FM and <b>AM</b> <b>Radio</b> Frequencies Below is a list of all the BBC&#39;s <b>radio</b> services that are available on FM and <b>AM</b>. We have listed the frequency ranges, if <b>you</b> want to know the exact frequency for <b>your</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Welcome to <b>Radio</b>-Locator.com, the most trusted <b>AM</b> and FM <b>radio station</b> search engine on the internet. We have links to over 17,200 <b>radio</b> <b>stations</b>&#39; web pages and over 12,900 <b>stations</b>&#39; audio streams from <b>radio</b> <b>stations</b> in the U.S. and around the world. <b>find</b> U.S. <b>radio</b> by <b>your</b> location. <b>find</b> U.S. <b>radio</b> by city or zip.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'You can use our free music <b>genre</b> finder and analyzer to quickly find the <b>genre</b> and more interesting information (such as the song’s key, BPM, popularity, etc.) about any music you love or to find an artist’s <b>genre</b>. Just enter the song title or artist name and leave the rest to our <b>genre</b> checker tool. “You are what you listen to.”.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Free <b>Fire</b> is a free-to-play battle royale game developed and published by Garena for Android and iOS. It was released on 8 December 2017. It became the most downloaded mobile game globally in 2019 and has over 1 billion downloads on Google Play Store.In the first quarter of 2021 it was the highest grossing mobile game in the US. In November 2019, it surpassed $1 billion in lifetime revenue.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The song was first released by neo-rockabilly singer Robert Gordon, who had met Springsteen through E Street Band bass player Garry Tallent.They remained on friendly terms before Springsteen gave Gordon the song &quot;<b>Fire</b>&quot; after seeing a live gig by Gordon and Link Wray.According to Gordon, &quot;it was a choice between &#39;<b>Fire</b>&#39; and another new song but [Springsteen] decided to keep the other one for ...', 'score': 'N/A'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 126/126 [00:11<00:00, 10.87it/s, Generation Speed: 65.21 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This question cannot be answered based on the given documents.', 'United States', 'Jesse Lasky.', 'Born', 'Arthur Crudup', 'Lesley Garrett', 'Emile Mosseri', 'No information about a gym workout on 2022/03/15 is provided in any of the given documents.', 'Fantasy MUD', 'December 27, 1943', 'Patience Agbabi', 'None', 'Football', '305,000', '3', 'Jockey', 'Malaysia', 'No information is provided about Faith attending a book club meeting on 2022/03/26. The given documents do not mention Faith or a specific date.', 'Steve Cram', 'Ocean Beach in San Francisco.', 'Alfred Bingham', 'Mark Bomback', 'Halik Kochanski', 'The question assumes that Adrian attended the Manchester Food and Drink Festival, which is mentioned in the documents. However, there is no mention of a specific date September ', 'Sunshine Desserts', 'Finisterre', '2010', 'tbd\\nAsia Minor', 'Football', 'Ski jumping', 'State elections in 1898.', 'South America', 'Chopin', 'Elden Ring', 'football', 'Krapf', 'Joan Crawford', 'Football', 'John George Santos', 'Canada', 'Not mentioned.', 'Football', 'Erin Hunter is the team of writers behind Warrior.', 'Director \\n\\n(Note: There are multiple occurrences of \"director\" in the documents, and I assume you are asking about the instances where the context is \"mates\")', 'Jim Abrahams and brothers David and Jerry Zucker.', 'Not provided in the documents.', 'Francesco Guardi', 'Town of Belorechensk', 'George Raymond Richard Martin', 'Catholic', 'Rasskazovo', 'Vienna', 'Netherlands', 'Kerry Irving', 'Germano Almeida.', 'There is no information about Georgina attending an Indie film festival on 2022/04/30.', 'Los Angeles.', 'India', 'Lake Placid', 'Edward', 'General Motors', 'State of Rio de Janeiro', 'Soccer', \"I couldn't find the name of the director of Homecoming from the given documents.\", 'There is no mention of the capital of Medzilaborce District in the given documents. The provided information does not mention the capital.', 'Coal Westwood', 'Catholic', 'Bruce Campbell', 'Salé', 'The capital of Polkowice County is the town of Polkowice.', 'Kansas City, Kansas.', 'Vincent van Gogh', '467,418', 'Avi Boodram', 'Suspenseful and darkly funny debut novel.', 'Ice Hockey', 'Cricketer', '12', 'Sathnam Sanghera', 'Kermadec Islands', '20', 'Colleen Hoover', 'Italy', 'Supernatural', 'Warri.', '78:2:1', 'Tikhvin', 'Leonard Carpenter', 'Jason Blum', 'Saraburi', 'B) Roller Skating', 'Not mentioned in the documents.', 'Roman Catholic', 'Francis Annan.', 'Cato Fong', '2', '1943', '75', 'William Zeckendorf Sr.', 'Snooker', 'Mexico City', 'Argentina', 'Geneva', 'Massachusetts.', 'Plesetsk', 'Genre: none (not mentioned)', 'HOME (Randy Goffe)', '1921', 'Elon Musk', 'MHL', 'BBC Radio', 'Honda', 'Albert Gore Sr.', 'Berlin', '3-2-1', 'Glendale', 'Stephen Sondheim and James Lapine.', '20:01', 'Lewis Carroll', 'Jonathan Lynn', 'NATO', '110 million', 'There is no given producer of \"Italian Style\" in the provided documents. However, Bernardo Bertolucci was a producer, and he is described as being', 'Moravia', '1090 AM', 'song']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 74/74 [00:01<00:00, 57.76it/s, Generation Speed: 234.16 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'em': 0.435, 'f1': 0.4870818713450291, 'sub_em': 0.515, 'precision': 0.4936527777777778, 'recall': 0.4975}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<flashrag.dataset.dataset.Dataset at 0x7f4bc0066bb0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flashrag.dataset.utils import split_dataset, merge_dataset\n",
    "from flashrag.pipeline import SequentialPipeline\n",
    "from flashrag.utils import get_dataset\n",
    "# from flashrag.utils import get_retriever, get_generator, get_refiner, get_judger\n",
    "from flashrag.prompt import PromptTemplate\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def run_judger_gru(classifiers, dataset, split, do_eval=True, pred_process_fun=None):\n",
    "    # judge_result: list of bool element, representing whether to use retrieval\n",
    "    judge_result = judger_gru(dataset, classifiers)\n",
    "    print()\n",
    "    print('Judge result: ',judge_result)\n",
    "    print()\n",
    "    config= set_config(dataset_name=dataset, split = split)\n",
    "    template= PromptTemplate(\n",
    "        config = config,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "\n",
    "    all_split = get_dataset(config)\n",
    "    dataset = all_split[split]\n",
    "    dataset.update_output('judge_result', judge_result)\n",
    "\n",
    "    # split dataset based on judge_result\n",
    "    pos_dataset, neg_dataset = split_dataset(dataset, judge_result)\n",
    "    pipeline = SequentialPipeline(config)\n",
    "\n",
    "    print()\n",
    "    print('Questions that NEED retrieval',len(pos_dataset))\n",
    "    print('Questions that does NOT need retrieval',len(neg_dataset))\n",
    "    print()\n",
    "\n",
    "    pos_dataset = pipeline.run_internet_retrieval(pos_dataset, do_eval=False)\n",
    "    pipeline.prompt_template = template\n",
    "    neg_dataset = pipeline.naive_run(neg_dataset, do_eval=False)\n",
    "\n",
    "    # merge datasets into original format\n",
    "    dataset = merge_dataset(pos_dataset, neg_dataset, judge_result)\n",
    "\n",
    "    dataset = pipeline.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "run_judger_gru(['intent_aware', 'time_aware', 'knowledge_aware'], dataset ='retrievalqa_200', split = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_judger_gru(['intent_aware', 'time_aware', 'knowledge_aware'], dataset ='retrievalqa_200', split = 'train')\n",
    "\n",
    "- with retrievalqa_200 (length: 200)\n",
    "\n",
    "{'em': 0.435, 'f1': 0.4870818713450291, 'sub_em': 0.515, 'precision': 0.4936527777777778, 'recall': 0.4975}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline - TriGate Standard RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length small_dataset:  200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_301467/3661175900.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  hidden_states_tensor= torch.tensor(hidden_states, dtype=torch.float32)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Judge result:  [False False False False False False False  True  True False False  True\n",
      " False  True  True False False False False  True False  True  True False\n",
      " False  True  True False  True False False False  True False  True  True\n",
      " False False  True False False False  True False False False  True False\n",
      "  True False  True False False False  True False  True False False False\n",
      "  True  True False  True False False False  True False False  True False\n",
      " False False False False  True  True False  True False False  True False\n",
      " False False False False False False False False False  True False  True\n",
      " False  True False  True  True  True False False False  True  True False\n",
      "  True  True False False False False False False False  True  True  True\n",
      " False False  True  True  True False  True  True False  True False False\n",
      " False False  True  True False  True  True False False False False False\n",
      "  True False False  True  True False False  True False False  True False\n",
      "  True  True False False False False False False  True False  True False\n",
      " False  True  True False False  True  True False False False False False\n",
      "  True  True  True False  True False False False False  True False False\n",
      " False  True False  True False  True False False]\n",
      "\n",
      "Find `question` in template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `reference` in template\n",
      "Find `question` in template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 11:08:48,198\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 11:08:48 config.py:1130] Casting torch.float32 to torch.float16.\n",
      "INFO 08-08 11:08:48 config.py:1151] Downcasting torch.float32 to torch.float16.\n",
      "INFO 08-08 11:08:48 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/cs/student/projects2/dsml/cdiezmar/models/llama3-8B-instruct', speculative_config=None, tokenizer='/cs/student/projects2/dsml/cdiezmar/models/llama3-8B-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/cs/student/projects2/dsml/cdiezmar/models/llama3-8B-instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 11:08:51 model_runner.py:146] Loading model weights took 14.9634 GB\n",
      "INFO 08-08 11:08:55 gpu_executor.py:83] # GPU blocks: 1656, # CPU blocks: 2048\n",
      "INFO 08-08 11:08:56 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-08 11:08:56 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-08 11:09:00 model_runner.py:924] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions that NEED retrieval 126\n",
      "Questions that does NOT need retrieval 74\n",
      "[[{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Benjamin</b> Franklin <b>Movie</b> (<b>2022</b>) with release date, trailer, cast and songs. ... <b>21</b> 20. T The <b>Night</b> Watch. Oregon Renaissance Band. 1: 11 <b>21</b>. H Honey ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '75 %. <b>2022</b> • 2 Episodes. Season 1 of <b>Benjamin</b> Franklin premiered on April 4, <b>2022</b>. An American (1775-1790) (1x2, April 5, <b>2022</b>) Season Finale. View All Seasons.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Benjamin</b> Franklin: With Peter Coyote, Mandy Patinkin, Josh Lucas, Liam Neeson. Exploring the life and work of writer and publisher, scientist and inventor, diplomat and signer of both the Declaration of Independence and the United States Constitution: <b>Benjamin</b> Franklin.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Budapest</b> is <b>the capital</b> and most populous city of Hungary.It is the ninth-largest city in the European Union by population within city limits and the second largest city on the Danube river. The city has an estimated population of 1,752,286 over a land area of about 525 square kilometres (203 square miles). <b>Budapest</b>, which is both a city and municipality, forms the centre of the <b>Budapest</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Budapest</b>, city, <b>capital</b> of Hungary, and seat of Pest megye (county). The city is the political, administrative, industrial, and commercial centre of Hungary. The site has been continuously settled since prehistoric times and is now the home of about one-fifth of the country’s population. Area city, 203 square miles (525 square km).', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Budapest</b>. <b>Budapest</b> is <b>the capital</b> of Hungary, a small, landlocked country in Central Europe.It is made up of two historic parts, Buda and Pest, which were once separate cities. In the 19 th century, however, the two cities were merged, creating the city now known as <b>Budapest</b>. The city became <b>the capital</b> of Hungary after being the seat of power for several kingdoms, including the Roman province ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Giant</b> logo used before re-branding in 2020. Some stores still use this logo as of 2024. The <b>Giant</b> Company (formerly known as <b>Giant</b> <b>Food</b> Stores) is an American regional supermarket chain that operates in Pennsylvania, Maryland, Virginia, and West Virginia under the <b>Giant</b> and Martin&#39;s brands. It is a subsidiary of Ahold Delhaize, and headquartered in Carlisle, Pennsylvania.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'What <b>Country</b> Owns <b>Giant</b> <b>Food</b>? <b>Giant</b> <b>Food</b> is a major grocery store chain that operates throughout the Mid-Atlantic region of the United States. The company was founded in 1936 by David Javitch and Nathan Hochman and has been headquartered in Landover, Maryland since its inception. It is currently owned by Dutch retail <b>giant</b> Ahold Delhaize, which ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Giant</b> <b>Food</b> continues to grow and innovate, with same-day speed available in 100% of its market area. <b>Giant</b> <b>Food</b>. Hannaford. Established: 1883. 187 stores. ... In addition to Delhaize – the <b>country</b>’s leading supermarket chain – our brands in Belgium include the supermarket Albert Heijn, and online retailer bol.com. In Luxembourg we also ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Thing</b> <b>We</b> <b>Love</b> is a 1918 American silent drama film starring Wallace Reid, Kathlyn Williams, and Tully Marshall, produced by Jesse Lasky, distributed by Paramount Pictures, and directed by Lou Tellegen.This marked Tellegen&#39;s second foray into directing as he usually was a leading man in front of the camera like Reid.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Thing We Love</b>: Directed by Lou Tellegen. With Wallace Reid, Kathlyn Williams, Tully Marshall, Mayme Kelso. Just prior to America&#39;s declaration of war, Margaret Kenwood of the Kenwood Manufacturing Company determines that the plant should produce munitions to support the Allies. Rodney Sheridan, her sweetheart and a vice president of the company, remains unimpressed with Margaret&#39;s ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>Thing</b> <b>We</b> <b>Love</b> (1918) cast and crew credits, including actors, actresses, directors, writers and more. Menu. Movies. Release Calendar Top 250 Movies Most Popular Movies Browse Movies by Genre Top Box Office Showtimes &amp; Tickets Movie News India Movie Spotlight. TV Shows.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'For all Fixture / Results for the 2022-2023 for <b>Grantham</b> <b>Town</b> <b>FC</b>. <b>Grantham</b> <b>Town</b> <b>FC</b> compete in the Northern Premier League - East. top of page. s THE GINGERBREADS <b>GRANTHAM</b> <b>TOWN</b> FOOTBALL CLUB. Home. News. First Team. Fixtures; Results 2023/24; Table; Hospitality &amp; Events. Sponsorship. Kit Sponsorship;', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '2023–24. Northern Premier League Division One East, 17th of 20 (transferred) Website. Club website. Home colours. Away colours. <b>Grantham Town</b> Football Club is a football club based in <b>Grantham</b>, Lincolnshire, England. They are currently members of the Northern Premier League Division One Midlands and <b>play</b> at the South Kesteven <b>Sports</b> Stadium.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 's THE GINGERBREADS <b>GRANTHAM</b> <b>TOWN</b> FOOTBALL CLUB. Home. News. First Team. Fixtures; Results 2023/24; Table; Hospitality &amp; Events. Sponsorship. Kit Sponsorship; Player &amp; Management Sponsorship; ... Club Name: <b>Grantham</b> <b>Town</b> <b>FC</b> Ltd. Company Number: 294783 (Limited by Shares) Primary Shareholders: <b>Grantham</b> <b>Town</b> <b>FC</b> Ltd.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The main difference between <b>home country</b> and host <b>country</b> is that the <b>home</b> <b>country</b> refers to the <b>country</b> where a person was born while the host <b>country</b> refers to the <b>country</b> where a person resides.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'A <b>home</b> <b>country</b> is generally associated with the nation where an individual is born, raised, or has long-term residency. Conversely, a host <b>country</b> refers to a nation where an individual temporarily resides, often due to work, study, or travel. Companies might be headquartered in their <b>home</b> <b>country</b> but can have branches or operations in ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>country</b> a person comes from.... Click for English pronunciations, examples sentences, video.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '“<b>That’s</b> All <b>Right</b>” was in its way a faithful homage to the 1946 Arthur “Big Boy” Crudup blues from which it derived, an homage intended neither to imitate nor to displace the original. And yet it was in its own way utterly, strikingly different. Probably what made it most different was its youthful purity, its unchecked sense of ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '‘<b>That’s</b> All <b>Right</b>’ began to gain traction around Memphis and soon migrated down south to the Louisiana Hayride, a country music radio programme that was open to playing blues and R&amp;B. The house drummer at the Hayride was D.J. Fontana, who provided Presley with his first backup of percussion. The pieces of Presley’s initial rock and roll ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The meaning behind “<b>That’s</b> All <b>Right</b>” goes beyond its catchy melody and toe-tapping rhythm. At its core, the song speaks to the acceptance of a failed relationship and the determination to move on. It portrays the resilience and optimism of a person who understands that sometimes, ending a relationship is necessary for personal growth and ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'She has also sung <b>opera</b> and pop classics with Bryan Ferry, ... <b>In 2002</b> Garrett was appointed <b>a CBE</b> for her services to music. She was also <b>awarded</b> with a BASCA Gold Badge <b>Award</b> in October 2010, ... The <b>Singer</b> (<b>2002</b>) So Deep is the Night (2003) When I Fall in Love (2007) – UK No. 11;', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Lesley Garrett <b>CBE</b> is Britain’s best known soprano, regularly appearing in <b>opera</b>, music theatre, concert, on television and CD. ... Recent albums are Travelling Light, The <b>Singer</b>, So Deep is the Night and Amazing Grace. ... Lesley <b>was awarded</b> <b>a CBE</b> in the <b>2002</b> New Year’s Honours List for Services to Music and is a Fellow of the Royal ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>opera</b> <b>singer</b>, one of The Three Tenors, received honorary knighthood from Queen Elizabeth II on Oct. 14, <b>2002</b>. His knighthood is honorary because he’s a Spanish citizen. Mick Jagger (GBE)', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'When Emile Mosseri stepped in as the <b>composer</b> <b>of Homecoming</b>, he had gigantic shoes to fill.The first season of the Amazon Prime series, based on the popular Gimlet podcast, was “scored” by the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'omecoming&quot; by Bruce DaweDawe here dramatises the <b>homecoming</b> of Australian veter. ns&#39; bodies from Vietnam. This is clearly an anti-war poem, reproducing in the seventies the sentiments of t. e First World War poets.In 25 lines of broken verse presented in one demanding stanza, Dawe recounts how &quot;they are bringing&quot; home the bodies &quot;in deep freeze ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Hugh Hagood Hardy, CM (February 26, 1937 – January 1, 1997) was a Canadian <b>composer</b>, pianist, and vibraphonist.He played mainly jazz and easy listening music. He is best known for the 1975 single, &quot;The <b>Homecoming</b>&quot; from his album of the same name, and for his soundtrack to the Anne of Green Gables and Anne of Avonlea films.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'We reveal the quietest and busiest times at the <b>gym</b> later in the report. 2023/24 <b>GYM</b> USAGE. Key Findings. 16% of people in the UK are currently a member of a <b>gym</b> (up <b>2</b>% year-on-year) A further 16% are planning to join a <b>gym</b> in the next calendar year, which is down <b>3</b>% compared to last year.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The goal is to subtract the starting time from the ending time under the correct conditions. If the times are not already in 24-hour time, convert them to 24-hour time. AM hours are the same in both 12-hour and 24-hour time. For <b>PM</b> hours, add 12 to the number to convert it to 24-hour time. For example, 1:<b>00</b> <b>PM</b> would be 13:<b>00</b> in 24-hour time.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Studies show us that 54% of <b>gym</b> memberships and 76% of group <b>fitness</b> class attendees in the UK are female. This research also reports that the most popular group <b>fitness</b> classes amongst women are Spinning, Aerobics, and Yoga. The figures below – published in October 2020 by Sport England – show general <b>fitness</b> participation statistics for ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'In years when January 1st (New Year&#39;s Day) is on a Saturday, the <b>holiday</b> is observed on the preceding day (December 31st). As a result, December 2021 will have two <b>federal holidays</b>. List of 2024 and 2025 <b>Federal Holidays</b>. There are eleven <b>federal holidays</b> recognized by the <b>US</b> government. View all <b>holidays</b> and download printable <b>holidays</b> table.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'USA <b>Federal</b> <b>Holidays</b> for Year 2024. Date Day of Week <b>Holiday</b> Name; January 1 Monday: New Year’s Day: January 15 Monday', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>United States</b> recognizes 12 <b>federal</b> <b>holidays</b>. Learn about <b>federal</b>, state, and cultural <b>holidays</b> celebrated in the <b>U.S</b>. <b>Federal</b> <b>holidays</b> Many government offices and some private businesses close on annual <b>federal</b> <b>holidays</b>. If the <b>holiday</b> falls during the weekend, the government may observe it on a different day.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Genre</b>(s) Fantasy MUD: Mode(s) Multiplayer: A screenshot from MUD1. Multi-User Dungeon, or MUD (referred to as MUD1, to distinguish it from its successor, MUD2, and the MUD <b>genre</b> in general), is the first MUD. History. MUD was created in 1978 by Roy Trubshaw and Richard Bartle at the University of Essex on a DEC PDP-10.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Welcome, Visitor! You have arrived at the home of the game Multi-User Dungeon, or MUD, also known to former players on CompuServe as British Legends. MUD is the world&#39;s oldest virtual world. It is a text-based game played using a TELNET program. Text-based and old? No, this is not a retro-gaming site. Just like good books didn&#39;t go out of ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'MUD1 <b>(1978</b>) 5 comments. Multi-User Dungeon, or MUD (referred to as MUD1, to distinguish it from its successor, MUD2, and the MUD <b>genre</b> in general) is the first MUD and the oldest virtual world in existence. It was created in 1978 by Roy Trubshaw at Essex University on a DEC PDP-10 in the UK, using the MACRO-10 assembly language.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>John</b> Edward <b>Robinson</b> (<b>born</b> December 27, 1943) is an American convicted serial killer, kidnapper, rapist, and forger.He was found guilty and received the death penalty in 2003 for three murders committed in Kansas.Two years later, as part of a plea deal, he admitted responsibility in five other murders committed in Missouri, for which he received multiple life sentences without possibility of ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>John</b> Robert Campbell <b>Robinson</b> (<b>born</b> 29 August 1971) is a former professional footballer who played as a midfielder. He made over 400 appearances during his professional career with Brighton &amp; Hove Albion , Charlton Athletic , Cardiff <b>City</b> and Gillingham and also won 30 caps for Wales .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>John</b> <b>Robinson</b> was <b>born</b> in Cicero, Illinois, and grew up in a working-class family. As a child, he showed early signs of sociopathy: deceit, manipulation, and kleptomania. He stole from his parents and school friends and often lied about his class achievements. <b>Robinson</b> eventually married Nancy Jo Lynch and the couple had four children.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Few people know who said &quot;<b>Patience</b> is a virtue&quot; but the first publishing appears to come from the poem, &quot;Piers Plowman&quot; in the 14th century.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Patience</b> Strong <b>Patience</b> Strong was a prolific English writer best known for her inspirational poems and prose. She published under the pen name <b>Patience</b> Strong, choosing anonymity to allow readers to focus on the message of her writing rather than the <b>author</b>&#39;s personal life.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Patience</b> Worth. <b>Patience</b> Worth was allegedly a spirit contacted by Pearl Lenore Curran (February 15, 1883 – December 2, 1937). This symbiotic relationship produced several novels, poetry and prose which Pearl Curran claimed were delivered to her through channelling the spirit <b>of Patience</b> Worth.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Alain Laurier</b> (French pronunciation: [alɛ̃ loʁje]) (12 September 1944 – 25 December 2023) was a French football manager and player. Career. <b>Laurier</b> was born in Créteil (Val-de-Marne). He made his debut for Cœuilly, which became Reims. At the Champigny club, he played alongside big names such as Raymond Kopa and Lucien Muller. He played ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Alain</b>•<b>Laurier</b>. Used name. <b>Alain</b>•<b>Laurier</b>. Born. 12 September 1944 in Créteil, Val-de-Marne (FRA) Died. December 2023. Measurements. 176 cm / 70 kg.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Access all <b>Alain</b> <b>Laurier</b>&#39;s information, news, matches and stats. Search our website and discover everything about your favourite player', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Washington CNN —. An estimated 305,000 people initially <b>received</b> federal <b>student</b> <b>loan</b> <b>bills</b> with the wrong amount – <b>many</b> with charges higher than they should be – when payments resumed this ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'More than 28 million federal <b>student</b> <b>loan</b> <b>borrowers</b> returned to repayment this month after a pandemic-related relief program put their monthly <b>bills</b> on pause for nearly four years. David Degner ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'An internal memo to high-level officials at the Department of Education also reported that an additional 21,000 people have <b>received</b> incorrect <b>student</b> <b>loan</b> <b>bills</b> since the restart began, a ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The third <b>Republican</b> <b>presidential</b> primary <b>debate</b> of the 2024 campaign cycle has begun in Miami, Florida. The <b>debate</b>, hosted by NBC News, marks a new phase in the GOP race, as the stage narrows to ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'CHRIS CHRISTIE. <b>Republican</b> U.S. <b>presidential</b> <b>candidate</b> and former New Jersey Gov. Chris Christie addresses The Faith and Freedom Coalition&#39;s 2023 &quot;Road to Majority&quot; conference in Washington, U.S ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'By Maggie Astor , Andrew Fischer , Eleanor Lutz and Elena Shao Updated Nov. 6, 2023. Five <b>candidates</b> have qualified for the third <b>Republican</b> <b>presidential</b> <b>debate</b> in Miami on Nov. 8, as the field ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Dominick Bellizzi</b> ( c. 1912 – 17 May 1934) was an American jockey who died at age 21 as a result of a horse racing accident. He was known as &quot;The Duke&quot;. [2] <b>Bellizzi</b> was born in New York to Albanian immigrants Samuel and Teresa <b>Bellizzi</b>. An up-and-coming young jockey in Thoroughbred racing, during 1933 <b>Bellizzi</b> rode to victory in the Futurity ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Dominick</b> <b>Bellizzi</b> was an American jockey who died at age 21 as a result of a horse racing accident. He was known as &quot;The Duke&quot;.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Dominick</b> <b>Bellizzi</b> joined ClassDojo in 2016. <b>Dominick</b> <b>Bellizzi</b> is currently Chief Technology Officer at ClassDojo - View - ClassDojo org chart. Create your alert to follow the career of <b>Dominick</b> <b>Bellizzi</b>.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Kuala Lumpur</b> remained <b>the capital</b> after the formation of Malaysia on 16 September 1963. The Malaysian Houses of Parliament were completed at the edge of the Lake Gardens in 1963. The Majestic Theatre on Pudu Road was an early pioneer in <b>Kuala Lumpur</b>&#39;s cinema scene. It was converted into an amusement park in the 1990s and demolished in 2009.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Kuala Lumpur</b>, Malaysia, at dusk. <b>Kuala Lumpur</b>, <b>capital</b> of Malaysia. The city is located in west-central Peninsular (West) Malaysia, midway along the west coast tin and rubber belt and about 25 miles (40 km) east of its ocean port, Port Kelang, on the Strait of Malacca. It is the country’s largest urban area and its cultural, commercial, and ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Kuala</b> <b>Lumpur</b>, <b>the capital</b> of Malaysia, is located in west-central Peninsular Malaysia; its ocean port, Port Kelang, sits about 25 miles (40 km) to the west on the Strait of Malacca. <b>Kuala</b> <b>Lumpur</b>, which lies astride the confluence of the Kelang and Gombak rivers, is Malaysia&#39;s largest urban area as well as its cultural, commercial, and transportation centre.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Find a <b>Book</b> <b>Club</b>. Welcome to the best place on the internet to find a <b>book</b> <b>club</b>. Here you&#39;ll find listings of all the <b>book</b> clubs near you, as well as of nearby individuals who are looking to join one. You can contact any group or individual, and you can also list yourself and/or your group so that other people can find you.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The first rule of <b>book</b> <b>club</b>: Don’t talk about <b>book</b> <b>club</b>. We’re just kidding, but there are definitely rules to keep in mind when trying to be a respectful and engaged <b>book</b> <b>club</b> member.Here, we’ve rounded up the dos and don’ts for having a successful <b>book</b> <b>club</b> <b>meeting</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Are you looking for the best way to schedule and organize regular <b>book</b> <b>club</b> <b>meetings</b>? You’re in luck! This article will help you set expectations, choose a <b>book</b>, find a venue, pick a date, and keep track of attendance. You’ll be well on your way to running successful <b>book</b> <b>club</b> <b>meetings</b> in no <b>time</b>.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Stephen Cram, CBE (born 14 October 1960) is a British retired track and field <b>athlete</b>. Along with fellow Britons Sebastian Coe and Steve Ovett, he was one of the world&#39;s dominant middle distance runners during the 1980s. <b>Nicknamed</b> &quot;<b>The Jarrow</b> <b>Arrow</b>&quot;, after his home town, Cram set world records in the 1,500 m, 2,000 m, and the mile during a 19-day period in the summer of 1985.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'First he ran 3:29:67 in the 1500m in Nice, thereby becoming the first <b>athlete</b> to crack three minutes and 30 seconds. This was a near-mythical landmark in the sport; much as the four-minute mile ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The “perfect” race, forgetting to meet his family and a memorable journey through a wartime tunnel. <b>The Jarrow</b> <b>Arrow</b> recalls the experience of becoming the first men’s 1500m world champion ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Sea foam</b>. <b>Sea foam</b>, ocean <b>foam</b>, beach <b>foam</b>, or spume is a type of <b>foam</b> created by the agitation of seawater, particularly when it contains higher concentrations of dissolved organic matter (including proteins, lignins, and lipids) derived from sources such as the offshore breakdown of algal blooms. [1] These compounds can act as surfactants or ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'When we speak of <b>sea</b> or ocean <b>foam</b>, we refer to the accumulation of bubbles and froth that forms on the ocean’s surface. Ocean <b>foam</b> can be found in a variety of shapes, sizes, and colors. For example, depending on its composition, the <b>foam</b> can be colored from white to dark brown. Seafoam can be found in small patches only a few inches thick ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Sea</b> <b>foam</b> forms when dissolved organic matter in the ocean is churned up. <b>Sea</b> <b>Foam</b> at Ocean Beach in San Francisco. If you scoop up some water from the ocean in a clear glass and look at it closely, you&#39;ll see that it&#39;s chock full of tiny particles. Seawater contains dissolved salts, proteins, fats, dead algae, detergents and other pollutants ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>The Techniques of Democracy</b>. <b>The Techniques of Democracy</b> is a book written by Alfred Bingham. It was published in 1943 by New York City publishers Duell, Sloan and Pearce. In this book, Bingham argues against both dogmatic individualism and dogmatic socialism. [1] Categories: 1942 non-fiction <b>books</b>. Duell, Sloan and Pearce <b>books</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Tytler was born in the Old Town of Edinburgh, the eldest son of Ann Craig of Costerton (1722–1783) and her husband William Tytler of Woodhouselee (<b>author</b> of Inquiry into the Evidence against Mary Queen of Scots). He was educated at Edinburgh High School and Kensington Academy in London (1763/64), and then studied law at the University of Edinburgh, qualifying as an advocate in 1770.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'In the year of elections, read Margaret Atwood, Mary Beard, Lea Ypi, Elif Shafak and more on what <b>democracy</b> means - and why it matters. In 2024, nearly half the world will take part in a national election, with billions heading to the polls. It&#39;s a thrilling, unprecedented opportunity for change - yet <b>democracy</b> is also under threat.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Joseph L. Mankiewicz was a two-<b>time</b> Academy Award-winning <b>screenwriter</b> and director whose work left an indelible mark on 20th-century cinema. He penned elegant, clever screenplays for classics like All About Eve, A Letter to Three Wives, and Cleopatra, earning him a place among Hollywood&#39;s elite filmmakers.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '2. Woody Allen. Woody Allen is largely a persona non grata in the film industry, but his success as a <b>screenwriter</b> lands him on the list of greatest screenwriters of all <b>time</b>. His best work is arguably Annie Hall (1977), Manhattan (1979), and Hannah and Her Sister s (1986).', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Screenwriter</b>, novelist, playwright, non-fiction <b>author</b>. Born in Highland Park, Illinois, USA, began his career as a novelist in 1957. Started writing screenplays in 1965 with &quot;Masquerade&quot;. A two-<b>time</b> Academy Award Winner, he is one of the most successful screenwriters and script doctors in Hollywood.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '10. “Don’t Be Cruel” (Elvis Presley) Length: 2 minutes, 4 seconds. Finally, we <b>have</b> <b>one</b> last <b>song</b> from the legendary Elvis Presley. “Don’t Be Cruel” <b>was released</b> in 1959 and was the first <b>song</b> Presley’s publishers brought him to record. The tune took the <b>number</b> 1 spot on all three main <b>charts</b> for pop, country, and R&amp;B.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'On this page you can see the list of every <b>UK</b> <b>Number</b> 1 <b>song</b> in ... 08/03/<b>1975</b>: IF: <b>TELLY</b> <b>SAVALAS</b>: 2: 22/03/<b>1975</b> ... As of 12th July 2024 the most recent <b>UK</b> artists <b>to have</b> a <b>number</b> <b>one</b> single in ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>UK</b> singles chart was first compiled in 1969. However, the records and statistics listed here date back to 1952 because the Official <b>Charts</b> Company counts a selected period of the New Musical Express chart (only from 1952 to 1960) and the Record Retailer chart from 1960 to 1969 as predecessors for the period prior to 11 February 1969, where multiples of competing <b>charts</b> coexisted side by side.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Introduction. &quot;<b>Resistance</b>&quot; by Owen Sheers is a captivating novel set in an alternate history during World War II. It tells the story of a group of women left behind in a Welsh valley as the men go off to fight in the war. The women are tasked with maintaining the farms and their community, but they soon encounter a mysterious and unexpected enemy.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Resistance</b> (2018) is an award-winning book written by Jennifer Nielsen.The story is about Chaya Lindner, a 16-year-old Jewish girl working as a courier in Nazi-occupied Poland, who eventually takes part in the Warsaw Ghetto Uprising of 1943.Jennifer Nielsen is an American <b>author</b> who mostly writes children’s and YA fiction. Her works include The Ascendance (2012-2014) series, as well as other ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Resistance</b> is an extraordinarily powerful, humane and haunting account of how and why all across Nazi-occupied Europe some people decided to resist the Third Reich. This could range from open partisan warfare in the occupied Soviet Union to dangerous acts of insurrection in the Netherlands or Norway. Some of these <b>resistance</b> movements were ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'September 14, <b>2022</b>. Le <b>Festival</b> de la Gastronomie of Saint-Martin, a celebration of the exceptional cuisine of the Caribbean ’s <b>culinary</b> capital – French St. Martin, will <b>take</b> <b>place</b> for the second year in a row from will <b>take</b> <b>place</b> from November 11th to the 22nd of <b>2022</b>. Epicurean enthusiasts from around the world will once again discover ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The Manchester Food and Drink <b>Festival</b> is a well established, nationally acclaimed event. Conceived and developed by Phil Jones in 1998, the event originated as a means of showing the rest of the nation that there was more to Manchester than meat pies and gravy! In the 24 festivals that have taken <b>place</b> over that time, Manchester’s dining ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The event will now <b>take</b> <b>place</b> between Thursday 22nd – Sunday 25th September and then again from Thursday 29th September – Sunday 2nd October. If that wasn’t all, the <b>festival</b> will be bringing tons of events, offers and more to local restaurants, too, with a special focus on some of the chefs and restaurants that have been a part of MFDF ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'February 4th <b>2002</b> marks the end of an era for the <b>Shipping</b> Forecast. Sea <b>area</b> Finisterre changes its <b>name</b> to <b>FitzRoy</b> and boundaries between neighbouring areas are to be realigned.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'For decades, his region was <b>known</b> as Finisterre, after a peninsula on the west coast of Galicia, Spain. But in <b>2002</b> the Met Office decided to rename it, to prevent confusion with an <b>area</b> of that <b>name</b> in the French/Spanish <b>shipping</b> forecast: today it’s <b>Fitzroy</b>. 13.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Robert <b>FitzRoy</b> circa 1850. The <b>Shipping</b> Forecast was established by Vice-Admiral Robert <b>FitzRoy</b>, the first professional weather forecaster, captain of HMS Beagle and founder of the Met Office. In October 1859, the steam clipper Royal Charter was wrecked in a strong storm off Anglesey; 450 people lost their lives.In response to this loss, <b>FitzRoy</b> introduced a warning service for <b>shipping</b> in ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Korea</b>’s <b>first</b> ever <b>grand</b> <b>prix</b> was always going to have a place in the history books. But taking nearly three hours to complete, it also became the longest world championship <b>race</b> since 1960. Read on for more stats and facts from the Korean <b>Grand</b> <b>Prix</b>. Fernando Alonso’s Korean <b>Grand</b> <b>Prix</b> win has the hallmarks of … Continue reading <b>Korea</b>’s <b>first</b> <b>Grand Prix</b> was the longest for 50 <b>years</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'After the 2012 Australian <b>Grand</b> <b>Prix</b>, organisers of the <b>race</b> in <b>Korea</b> announced that they had reached a new deal with Formula One Management that would save $20.5 million (₩23 billion) in costs. Kang Hyo-seok, director of <b>race</b> organisation for the Korean <b>Grand Prix</b>, admitted that the <b>race</b> was still &#39;too expensive&#39; for <b>Korea</b>, anticipating an estimated loss of $26 million (₩29 billion) in 2012.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Korea</b> International Circuit History. The Korean circuit was scheduled to be ready in July 2010, but several issues delayed the inauguration. Heavy rainfalls postponed soil improvement and lack of funding from the Korean Government made things more difficult. Hence, the Koran <b>Grand</b> <b>Prix</b> saw some unfinished facilities in 2010.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Paul</b> (Koinē Greek: Παῦλος, romanized: Paûlos), also named Saul of Tarsus (Aramaic: ܫܐܘܠ, romanized: Šāʾūl), commonly known as <b>Paul</b> the Apostle and <b>Saint Paul</b>, was a Christian apostle (c. 5 – c. 64/65 AD) who spread the teachings of Jesus in the first-century world. For his contributions towards the New Testament, he is generally regarded as one of the most important figures ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Life. <b>Paul</b> was a Greek -speaking Jew from Asia Minor. His birthplace, Tarsus, was a major city in eastern Cilicia, a region that had been made part of the Roman province of Syria by the time of <b>Paul</b>’s adulthood. Two of the main cities of Syria, Damascus and Antioch, played a prominent part in his life and letters.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Introduction <b>Saint Paul</b> ©. <b>Saint Paul</b> is undoubtedly one of the most important figures in the history of the Western world. Just a quick look at the headlines of his life are enough to understand ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Frank Cornan</b>. Francis <b>Cornan</b> (5 May 1880 – 31 May 1971) was an English professional footballer born in Sunderland, who played as an inside left or left half. He made 165 appearances in the Football League playing for Barnsley (in three separate spells), Birmingham and Aston Villa. He died in Halifax, West Yorkshire, aged 91.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Francis <b>Cornan</b> was a Football player born on 1880-05-05, in Sunderland, England. Played as Midfielder.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'A biographical history of <b>Frank</b> <b>Cornan</b>, Aston Villa Midfielder, 1908-09', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Radik Zhaparov</b> (born February 29, 1984) is a Kazakh ski jumper who has competed since 2003. At the 2006 Winter Olympics in Turin, he finished 11th in the team large hill and 26th in the individual normal hill events.At the FIS Nordic World Ski Championships, <b>Zhaparov</b> has finished 11th in team events three times (2005: large, normal; 2007: large) and 24th in the individual normal hill (2007 ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Radik ZHAPAROV</b>. Team Kazakhstan. Ski Jumping. Games Participations 1. First Olympic Games Turin 2006. Year of Birth 1984. Olympic Results.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Radik</b> <b>ZHAPAROV</b>. Shvsm Dinamo. KAZ Kazakhstan. FIS Code 3787; Birthdate 1984; Age 40; Status Not active; Gender Male; Marital Status – ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Wilmington</b> insurrection of 1898, also known as the <b>Wilmington</b> massacre of 1898 or the <b>Wilmington</b> coup of 1898, [6] was a coup d&#39;état and a massacre which was carried out by white supremacists <b>in Wilmington</b>, North Carolina, United States, on Thursday, November 10, 1898. [7] The white press <b>in Wilmington</b> originally described the event as a ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'This illustration of the 1898 <b>Wilmington</b> massacre typifies how publications of the time promoted misleading characterizations of the <b>incident</b> as a &#39;race riot&#39; or a Black insurrection.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'A violent mob, whipped into a frenzy by politicians, tearing apart a town to overthrow the elected government. Following state elections in 1898, white supremacists moved into the US port of ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The Matterhorn ( German: [ˈmatɐˌhɔʁn] ⓘ, Swiss Standard German: [ˈmatərˌhɔrn]; Italian: <b>Cervino</b> [tʃerˈviːno]; French: Cervin [sɛʁvɛ̃]; Romansh: Mont (e) Cervin (u)) [note 3] is a mountain of the Alps, straddling the main watershed and border between <b>Italy</b> and Switzerland. It is a large, near-symmetric pyramidal <b>peak</b> in the extended <b>Monte</b> Rosa area of the Pennine Alps, whose ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Matterhorn, one of the best-<b>known</b> mountains in the Alps, straddling the frontier between Switzerland and <b>Italy</b>, 6 miles (10 km) southwest of the village of Zermatt, Switzerland. It stands 14,692 feet (4,478 meters). The name Matterhorn means roughly ‘the <b>peak</b> in the meadows.’', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The Matterhorn, also <b>known</b> <b>as Monte</b> <b>Cervino</b> <b>in Italy</b>, is a 4000 meter <b>peak</b> in the Alps and perhaps the most iconic mountain in the world (peakery features the Matterhorn in its logo!).', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Pope <b>Francis</b> ( Latin: Franciscus; Italian: Francesco; Spanish: Francisco; born Jorge Mario Bergoglio; [b] 17 December 1936) is head of the Catholic Church and sovereign of the Vatican City State. He is the first pope to be a member of the Society of Jesus (Jesuits), the first from the Americas and the Southern Hemisphere, and the first born or ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Francis</b> ushered in a new era of leadership of the Roman Catholic Church when he was elected pope in 2013. As the first pope from the Western Hemisphere, the first from South America, and the first from the Jesuit order, <b>Francis</b> has brought many reforms to the church and a reputation for humility. His significant achievements include the papal encyclical Laudato si’ (2015), which addressed ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Interesting Facts. Pope <b>Francis</b>, who was born in Argentina, is the first pope to have come from the Americas. Pope <b>Francis</b> was nominated for the 2014 Nobel Peace Prize. In addition to his native ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The effect can be comic. “She allows a day to pass,” the narrator tells us, “while she ruminates on for you .”. “The <b>Pole</b>” is Coetzee’s reworking of “Vita Nuova,” Dante Alighieri ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Composer</b> as <b>Pole Seeker:</b> Reading Vaughan Williams&#39;s Sinfonia antartica. Beckerman, Michael. It is a commonplace of history that we do not encounter events from the past, but rather descriptions of these events. To be more contemporary, and perhaps more accurate, we encounter &quot;spins&quot; on the events. While a kind of precise objectivity based ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>Pole</b> (TV Series 2021) cast and crew credits, including actors, actresses, directors, writers and more. Menu. Movies. Release Calendar Top 250 Movies Most Popular Movies Browse Movies by Genre Top Box Office Showtimes &amp; Tickets Movie News India Movie Spotlight. TV Shows.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Game</b> <b>of the Year</b>. Recognizing a <b>game</b> that delivers the absolute best experience across all creative and technical fields. Alan Wake 2. Baldur&#39;s Gate 3 - WINNER. Marvel&#39;s Spider-Man 2. Resident ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Cameron Monaghan, Star Wars Jedi: Survivor. Idris Elba, Cyberpunk 2077: Phantom Liberty. Melanie Liburd, Alan Wake 2. Neil Newbon, Baldur&#39;s Gate 3. Yuri Lowenthal, Marvel&#39;s Spider-Man 2.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The Walking Dead. 9.1 / 10. Telltale <b>Games</b>. Telltale <b>Games</b>. <b>Award</b> totals: 74 (100%) Here you can see the <b>awards</b> for the best <b>game</b> <b>of the year</b> according to different media.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Sofia Anker-Kofoed</b> (born 28 November 1994) is a Swedish footballer. She last played as an attacker for FC Rosengård in the Damallsvenskan .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'National; FIFA World Cup; Olympics; UEFA European Championship; CONMEBOL Copa America', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Latest on <b>Sofia</b> <b>Anker-Kofoed</b> including news, stats, videos, highlights and more on ESPN', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Naaktgeboren <b>became</b> just the third Linn-Mar wrestler to win multiple <b>state</b> titles, joining four-timer Jay Borschel and <b>three-time</b> titlist Matt McDonough. It is the <b>school</b>’s 15th overall <b>state</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>All</b> of the wrestlers who won the IHSAA <b>state</b> tournament <b>three</b> times.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Krapf was a 3-<b>time</b> <b>state</b> <b>wrestling</b> <b>champion</b> <b>for Tatnall</b>, winning at 180 pounds in 1966 and 1967 and heavyweight in 1968. He was also a 4-<b>time</b> national prep <b>school</b> champ. Ninety-four of Krapf’s ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Luan Viana</b> <b>Patrocínio</b> (born 14 January 1996), sometimes known simply as <b>Luan</b>, is a Brazilian professional footballer who plays as a forward for Mixto. Club career [ edit ] Born in São Paulo , <b>Luan Viana</b> graduated from Portuguesa &#39;s youth setup, and made his first-team debut on 13 February 2013, coming on as a late substitute in a 2–0 win at São José , for the Campeonato Paulista Série ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Full name: <b>Luan</b> <b>Viana</b> <b>Patrocinio</b> Date of birth/Age: Jan 14, 1996 (28) Place of birth: São Paulo Height: 1,86 m Citizenship: Brazil Position: Attack - Centre-Forward Foot: right Current club: Porto Velho EC Joined: Feb 16, 2024 Contract expires:- Stats of <b>Luan</b> <b>Viana</b> . View full stats. National team career ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Luan</b> Last name <b>Viana</b> <b>Patrocínio</b> Nationality Brazil Date of birth 14 January 1996 Age 28 Country of birth Brazil Place of birth São Paulo Position Attacker Height 184 cm Weight 79 kg Foot Right. Career Domestic Leagues; Domestic Cups; ... Data provided by Opta <b>Sports</b>. Articles provided by OMNISPORT.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Federal prosecutors on Tuesday <b>hit</b> <b>scandal-plagued</b> Rep. George Santos (R-N.Y.) <b>with 10</b> <b>new</b> <b>criminal</b> <b>charges</b>, <b>including</b> for charging at least $44,800 to an unaware campaign donor who had texted him ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Those initial <b>charges</b> included seven counts of <b>wire</b> <b>fraud</b>, three counts of money laundering, one count of <b>theft</b> of public funds and two counts of making materially false statements to the <b>US</b> House ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>new</b> <b>charges</b> in the so-called superseding indictment are: one count of conspiracy to commit offenses against the United States, two counts of <b>wire</b> <b>fraud</b>, two counts of making materially false ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Kuujjuarapik</b> (also spelled Kuujjuaraapik; Inuktitut: ᑰᔾᔪᐊᕌᐱᒃ little great river [5]) is the southernmost northern village ( Inuit community) at the mouth of the Great Whale River ( French: Grande Rivière de la Baleine) on the coast of Hudson Bay in Nunavik, Quebec, Canada. Almost 1,000 people, mostly Cree, live in the adjacent ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The land Kuujjuaraapik . KUUJJUARAAPIK means “small river [].”This was the first village that missionaries visited to spread the word of God. One missionary in particular, Reverend W. G. Walton [], saw numerous Inuit each year to baptize and name the infants.Myself, I was born January 1st 1914 and he baptized me in 1917; I still have my baptismal certificate, although I’m sixty-seven now.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Kuujjuarapik</b> (Inuktitut: ᑰᔾᔪᐊᕌᐱᒃ) and neighbouring Whapmagoostui are twin villages with a total of about 1800 people (2021) in Nunavik in the far north of Quebec. <b>Kuujjuarapik</b> sits at the mouth of the Grande-Baleine River on the coast of Hudson Bay. ... Locally-run general store that sells everything from coats to <b>country</b> food ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Globle will test your knowledge of geography. The goal of the game is to find the mystery <b>country</b> on the world map. After each guess, you will see on the map the <b>country</b> you have chosen and the hotter the color, the closer you are to the hidden <b>country</b>. You have an unlimited number of guesses, so use the color hints and find the target <b>country</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Every day, there is a new Mystery <b>Country</b>. Your goal is to guess which <b>country</b> it is using the fewest number of guesses. Each incorrect guess will appear on the <b>globe</b> with a colour indicating how close it is to the Mystery <b>Country</b>. The hotter the colour, the closer you are to the answer.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Pale color means that the target <b>country</b> is far from your choice. 3. Click, drag and move the <b>globe</b> to get a closer view of the map. 4. When you select the correct <b>country</b>, it will be colored in dark red. 5. In this game, your number of guesses is unlimited, so play with pleasure and improve your geographical skills!', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Alexis</b> <b>Carra</b> is a 34-year old football player aus France, (* Jan 1, 1990 in Villefranche, France). <b>Carra</b> is Jul 1, 2012 without a club since. He plays in the position Right Winger. His market value is -.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Check out the latest domestic and international stats, match logs, goals, height, weight and more for <b>Alexis</b> <b>Carra</b> playing for AS Cittadella and Vicenza Calcio in the Serie B', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Alexis Carra</b> (born 27 April 1990) is a French footballer who plays as a striker. [1] [2]', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Warriors</b> (also known as <b>Warrior</b> Cats) is a series of novels based on the adventures and drama of multiple Clans of feral cats.The series is primarily set in fictional forests. Published by HarperCollins, the series is written by authors Kate Cary and Cherith Baldry, as well as others, under the collective pseudonym Erin Hunter.The concept and plot of the pilot series were developed by series ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Erin Hunter is a collective pseudonym used by the authors Victoria Holmes, Kate Cary, Cherith Baldry, Clarissa Hutton, Inbali Iserles, Tui T. Sutherland, and Rosie Best in the writing of several children&#39;s fantasy novel series which focus on animals and their adventures. Notable works include the <b>Warriors</b>, Seekers, Survivors, Bravelands, and Bamboo Kingdom book series.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Erin Hunter is the team of writers behind <b>Warrior</b> Cats. Find out more about the authors and their stories.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Azza &amp; Zoc travel to the island of Bali to learn about jamu, an ancient medicinal drink that is a cornerstone of Indonesian culture. They&#39;ll race buffalos, h...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Mates</b> in Mind has announced that Sam Downie has joined the UK charity as its new Managing <b>Director</b>. Sam takes over from Sarah Meek who has led the charity since 2021. Sam started her career as a research psychologist and went on to set up and lead mental health services at local, regional and national level.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Meet the people who make up <b>Mates</b> in Mind - from our Board of Trustees and Managing <b>Director</b> through to our Support Managers and office team, get to know the people who make our charity what it is today. Get to know our team. Our Reports. ... <b>Mates</b> in Mind has an ambitious goal – to reach 100,000 workers in the first year, and by 2025, we aim ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'In The <b>Director</b>’s Disorder: <b>Pilot</b> Episode, you’ll play the part of Coal Westwood, retired actor, as you attempt to survive the whims of a deadly killer. To live through the night, Coal will need to avoid antagonizing the <b>Director</b> by following the script or risk his wrath. This is the role of a lifetime. Features: - Fully Voiced Dialogue', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Director</b>&#39;s Disorder: <b>Pilot</b> Episode - Psychological indie horror game | Full Gameplay / Longplay Walkthrough | 4K60fps | No CommentaryWelcome to i Scream ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Find the game and support the creator here: https://store.steampowered.com/app/2073640/The_Directors_Disorder_<b>Pilot</b>_Episode/Links:https://www.buymeacoffee.co...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Gap</b> Band was an American R&amp;B and funk band that rose to fame during the 1970s and 1980s. The band consisted of three brothers: Charlie, Ronnie, and Robert Wilson, along with other members; it was named after streets (Greenwood, Archer, and Pine) [1] [2] in the historic Greenwood neighborhood in the brothers&#39; hometown of Tulsa, Oklahoma.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Gap</b>: With Freen Sarocha Chankimha, Rebecca Patricia &#39;Becky&#39; Armstrong, Tassawan Seneewongse, Ratchanon Kanpiang. Mon is an idol of Sam, and when they meet again at the office, she is surprised by her icy exterior. They are different in class and age, with a <b>gap</b> of eight years between them.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Gap</b>: The Series ( Thai: ทฤษฎีสีชมพู; RTGS : Thritsadi Si Chomphu; lit. Pink Theory) is a Thai romantic comedy television series that premiered on Channel 3 and the IdolFactory channel on YouTube on November 19, 2022, and ran until February 11, 2023.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Giacomo Guardi</b> (13 April 1764 - 3 November 1835) was an Italian painter from Venice. The son of famous Veduta painter Francesco <b>Guardi</b>, he continued his <b>father</b>&#39;s line of work, though without the same level of renown. The majority of his works are quite small views of only minor artistic interest, more akin to postcards than to his <b>father</b>&#39;s ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The son of the famous Venetian vedutista Francesco <b>Guardi</b>, <b>Giacomo Guardi</b> followed in his <b>father</b>’s footsteps in his preference for atmospherically rendered topographical views, although unlike the former he primarily concentrated on small formats. In the nineteenth century, his countless impressive views of Venice attracted the interest of collectors and museums.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'in subject and style by his <b>father</b>. his paintings capture the picturesque beauty and atmospheric drama of Venice in an imaginative and distinctive fashion. collectively, the <b>Guardi</b> family are often said to be the last true <b>Giacomo</b> <b>Guardi</b> and francesco <b>Guardi</b>, A View of the Venetian Lagoon painters of the Venetian school in its classical form.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Belorechensky District</b> ( Russian: Белоре́ченский райо́н) is an administrative <b>district</b> ( raion ), one of the thirty-eight in Krasnodar Krai, Russia. [1] As a municipal division, it is incorporated as <b>Belorechensky</b> Municipal <b>District</b>. [5] It is located in the southern central part of the krai, but is bordered for the main ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '03608101001. Website. www .gorodbelorechensk .ru. Belorechensk ( Russian: Белоре́ченск) is a town in Krasnodar Krai, Russia, located on the Belaya River, from which it takes its name. It forms the municipal formation Belorechenskoye urban settlement, as the only locality in its composition. Population: 51,590 (2020), 53,892 ( 2010 ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Belorechensk Geography. Geographic Information regarding City of Belorechensk. Belorechensk Geographical coordinates. Latitude: 44.7667, Longitude: 39.8667. 44° 46′ 0″ North, 39° 52′ 0″ East. Belorechensk Area. 5,600 hectares. 56.00 km² (21.62 sq mi) Belorechensk Altitude.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'George Raymond Richard <b>Martin</b> (born George Raymond <b>Martin</b>; September 20, 1948), also known by the initials G.R.R.M., is an American <b>author</b>, television writer, and television producer. He is best known as the <b>author</b> of the series of epic fantasy novels A Song of Ice and Fire, which were adapted into the Primetime Emmy Award–winning television series Game of Thrones (2011–2019) and its ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'George R. R. <b>Martin</b> writes fantasy novels and for television. His first novel, Dying of the Light, debuted in 1977, and by the mid-1980s, he was also writing for TV. In 1996, <b>Martin</b> published his ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The World of Ice &amp; Fire: The Untold History of Westeros and the Game of Thrones. by. George R.R. <b>Martin</b>, Elio M. García Jr., Linda Antonsson (<b>Goodreads</b> <b>Author</b>) 4.26 avg rating — 36,796 ratings — published 2014 — 60 editions.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'A half-set being <b>played</b>. The uilleann pipes (/ ˈ ɪ l ə n / ⓘ IL-ən or / ˈ ɪ l j ə n / IL-yən, <b>Irish</b>: [ˈɪl̠ʲən̪ˠ]), also known as Union pipes and sometimes called <b>Irish</b> pipes, are the characteristic national bagpipe of Ireland. Their current <b>name</b> is a partial translation <b>of the Irish</b> language terms píobaí uilleann (literally, &quot;pipes of the <b>elbow</b>&quot;), from their method of inflation.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'These pipes are now most commonly known as Uilleann pipes (pronounced ill-yin, from <b>Irish</b> uille, <b>elbow</b>). This <b>name</b> was first applied to the instrument as last as the beginning of the 20th century when it was foisted on the public in 1903 by Grattan Flood who then proceeded to equate it with the ‘woollen’ pipes of Shakespeare, thus providing for the instrument a spurious origin in the 16th ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Uilleann pipes are the earliest recorded type of <b>Irish</b> <b>bagpipes</b> and date back to the fifth century, when they were an instrument popular in the peasant community. Fast-forward to the seventeenth century, and this type of bellows pipes started to become the fashion among the lower and upper classes alike, most notably in Ireland and France.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>François</b> <b>Gayot</b> (July 17, 1927 – December 16, 2010) was the Catholic archbishop of the Roman Catholic Archdiocese of Cap-Haïtien. Haiti. Ordained to the priesthood in 1954, <b>Gayot</b> was named bishop of the then Cap-Haïtien Diocese. In 1988 the Diocese was elevated to an archdiocese. Archbishop <b>Gayot</b> retired in 2003 and died in 2010.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Francois</b> <b>Gayot</b> was ordained a priest in the Society of Missionaries of Mary or Monfortain, on February 7, 1954. On November 22, 1974, Pope Paul VI appointed Bishop of Cap-Haitien to replace Bishop ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '&quot;<b>François</b> <b>Gayot</b> de Pitaval&quot; published on by null. (Lyons, 1673–1743, Lyons),a French legal writer, compiled 20 vols. of Causes célèbres et intéressantes (1734, ff.), which were translated into German between 1747 and 1768 Schiller wrote an introduction ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Maksim</b> <b>Andreyevich</b> <b>Fyodorov</b> ( Russian: Максим Андреевич Фёдоров; <b>born</b> 5 April 1989) is a Russian former professional footballer .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Maksim Fyodorov</b> may refer to: <b>Maksim Fyodorov</b> (footballer, <b>born</b> 1986), Russian footballer (striker) <b>Maksim Fyodorov</b> (footballer, <b>born</b> 1989), Russian footballer (midfielder) Category: Human name disambiguation pages.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Russian footballer – <b>Maksim</b> <b>Andreyevich</b> <b>Fyodorov</b> was <b>born</b> in Rasskazovo (Town in Tambov Oblast, Russia) on April 5th, 1989 and is 35 years old today.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Nina Dittrich</b> (<b>born</b> 20 November 1990 in Vienna) is an Austrian swimmer, who specialized in freestyle and butterfly events. She is a multiple-time Austrian champion, a five-time national record holder, and also, a current member of Simmering Swimming Club (German: Schwimmverein Schwechat Simmering) in Schwechat. <b>Dittrich</b> is also the daughter of Ulrike Bauer, an Austrian record holder in both ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Host <b>city</b> selection; Statistics Medals by country; Medals by athlete; ... Biographical information Roles: Competed in Olympic Games: Sex: Female: Full name: <b>Nina•Dittrich</b>: Used name: <b>Nina•Dittrich</b>: <b>Born</b>: 20 November 1990 in Wien (Vienna), Wien (AUT) Measurements: 174 cm / 58 kg: Affiliations: SV Schwechat, Schwechat (AUT) ... <b>Nina Dittrich</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'We recommend you to check the complete list of Famous People <b>born</b> on 20 November. She is a member of famous Swimmer with the age 33 years old group. <b>Nina</b> <b>Dittrich</b> Height, Weight &amp; Measurements. At 33 years old, <b>Nina</b> <b>Dittrich</b> height is 1.74m and Weight 58 kg.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Netherlands Antilles</b> (<b>Dutch</b>: Nederlandse Antillen, pronounced [ˈneːdərlɑntsə ʔɑnˈtɪlə(n)] ⓘ; Papiamento: Antia Hulandes) was a constituent <b>country</b> of the Kingdom of the <b>Netherlands</b>.The <b>country</b> consisted of several island territories located in the Caribbean Sea.The islands were also informally known as the <b>Dutch</b> <b>Antilles</b>. The <b>country</b> came into being in 1954 as the autonomous ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'In 2006 the <b>Dutch</b> government and the remaining five islands agreed to dissolve the <b>Netherlands Antilles</b> within the following several years. The event took place on October 10, 2010. None of the islands chose full independence. Curaçao and Sint Maarten became autonomous <b>countries</b> within the kingdom, a status similar to that of Aruba.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'In 1954, the various <b>Dutch</b> island colonies were united under a single <b>country</b> and were named “Netherland <b>Antilles</b>.” On December 15, 1954, with the proclamation of the “Charter for the Kingdom of <b>Netherlands</b>,” the <b>Netherlands Antilles</b> became an autonomous part of the Kingdom of <b>Netherlands</b>.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The heartwarming new memoir from the <b>author</b> of the bestselling <b>Max</b> the Miracle Dog. About the <b>Author</b>. Kerry Irving lives in Keswick, where he runs the Paw Store. His first book, <b>Max</b> the Miracle Dog, was a Sunday Times bestseller. A keen amateur photographer, Kerry enjoys daily walks around the Lake District with his three spaniels, Paddy, Harry ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Buy <b>Max the</b> Miracle <b>Dog: The</b> Heart-warming Tale of a Life-saving Friendship First Edition by Irving, Kerry (ISBN: 9780008353490) from Amazon&#39;s Book Store. Everyday low prices and free delivery on eligible orders.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Max</b> is a highly trained military canine who has always protected his fellow soldiers. But when he loses his handler and best friend, Kyle, <b>Max</b> is traumatized and unable to remain in the service. ... Not in the category of top 100 <b>books</b> on dogs ever written, but lots of fun. It felt like a made for movie book that could be accomplished within ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>O</b> <b>dia</b> <b>das</b> <b>calças</b> <b>roladas</b> is a Capeverdean novel published in 1982 by Germano Almeida.. The book was first published on Ilhéu Editora. The story is about an account of a strike that happened on the island of Santo Antão.. External links. <b>O</b> <b>dia</b> <b>das</b> <b>calcas</b> <b>roladas</b> at Editorial Caminho (in Portuguese); <b>O</b> <b>dia</b> <b>das</b> <b>calças</b> <b>roladas</b> at livrodatero.blogspot.com (in Portuguese)', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'In 1989 he founded the Ilhéu Editora publishing house and has since published 16 <b>books</b> (nine novels). Published works. His first work was <b>O</b> <b>dia</b> <b>das</b> <b>calças</b> <b>roladas</b> which was about an account of a strike on the island of Santo Antão, it was first written in 1982 and was published in 1983', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>O Dia das Calças Roladas</b>. A sua opinião. Limpar Enviar. Obrigado por partilhar connosco a sua opinião. <b>O</b> seu comentário só ficará visível após validação. Nota: Comentários com linguagem ofensiva ou provocadora, ou que não expressem uma opinião sobre <b>o</b> livro ou sobre <b>o</b> seu autor, não serão publicados.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Inside <b>Georgina</b> Chapman’s Whirlwind Trip to Venice <b>Film</b> <b>Festival</b>. By Elise Taylor. September 16, <b>2022</b> ... In her spare <b>time</b>, she&#39;s either checking out the latest New York hotspot until 2 a.m. or ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Georgina</b> Ashton is on edge. It’s the last ball of the London season and she has no-one to <b>go</b> with: her husband Robert has suddenly left town and a friend – Fanny Beaumont – has mysteriously withdrawn an invitation to <b>attend</b> the ball with her.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Los Angeles International Short <b>Film</b> <b>Festival</b>. IndieX <b>Film</b> Fest. Home; About Us; In Competition; Monthly Nominations &amp; Award Winners. 2024; 2023; <b>2022</b>; 2021; 2020; 2019; Annual Awards. 2024 Annual Awards: After Show; 2024 Annual Awards: Awards Show; 2024 Annual Awards: Red Carpet Photocall ... Reading <b>time</b> less than 1 minute. In Competition ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Bert</b> Meyers was <b>born</b> in Los Angeles in 1928. The son of Romanian Jewish immigrants, and a high school drop out, Meyers worked at manual labor jobs until finally becoming a master picture framer and gilder. When that work negatively impacted his health, he applied and was admitted to the Claremont Graduate School on the basis of his poetic ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Bert</b> Meyers was <b>born</b> in Los Angeles on March 20, 1928. The son of Romanian Jewish immigrants, he maintained strong lifelong ties to his Jewish cultural heritage without being religious. Always rebellious and a questioner of authority, Meyers decided to drop out of high school and become a poet. For many years, he worked manual labor jobs ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Bert</b> <b>Myers</b>. James Albert <b>Myers</b> (April 8, 1874 – October 12, 1915) was an American professional baseball player who played in parts of three seasons for the St. Louis Browns, Washington Senators and Philadelphia Phillies . He was <b>born</b> in Frederick, Maryland, and died in Washington, D.C., at the age of 41.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Kondh</b> is a village in Dhrangadhra Taluka in <b>Surendranagar</b> district of Gujarat State, India. Nearby villages are Rajcharadi, Hampar, Navalgadh, Bhechada, Gajanvav, Ratanpar, Rampara, Raygadh, Ravaliyavadar and Narichana . <b>Kondh</b>&#39;s main crops are magafali ( peanuts) and kappas ( cotton ). <b>Kondh</b>&#39;s Postal Index Number code is 363310 and the postal ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Kondh</b> village is located in Dhrangadhra taluka of <b>Surendranagar</b> district in Gujarat, India. It is situated 20km away from sub-district headquarter Dhrangadhra (tehsildar office) and 30km away from district headquarter <b>Surendranagar</b>. As per 2009 stats, Kalyanpur is the gram panchayat of <b>Kondh</b> village. The total geographical area of village is ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Kondh</b> is a Village in Dhrangadhra Taluka in <b>Surendranagar</b> District of Gujarat State, India. It is located 44 KM towards west from District head quarters <b>Surendranagar</b>. 25 KM from . 167 KM from State capital Gandhinagar. <b>Kondh</b> Pin code is 363310 and postal head office is Dhrangadhra . Dhavana ( 6 KM ) , Jiva ( 8 KM ) , Sapkada ( 8 KM ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'By 2034, eleven cities will <b>have</b> <b>hosted</b> the Olympic Games more than <b>once</b>: Athens ( 1896 and 2004 Summer <b>Olympics</b> ), Paris ( 1900, 1924 and 2024 Summer <b>Olympics</b> ), London ( 1908, 1948 and 2012 Summer <b>Olympics</b> ), St. Moritz ( 1928 and 1948 <b>Winter</b> <b>Olympics</b> ), Lake Placid ( 1932 and 1980 <b>Winter</b> <b>Olympics</b> ), Los Angeles ( 1932, 1984 and 2028 Summer ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Only</b> <b>Europe</b>, North America, Asia, South America, and Oceania <b>have hosted the</b> Olympic Games with <b>Europe</b> hosting 36 editions. In 2016, Brazil became the first South American <b>country</b> to <b>host</b> the <b>Olympics</b>. Africa has yet to <b>host</b> the games. In 2022, China’s Beijing City will be the first city to <b>host</b> both <b>the Winter</b> and Summer Games.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Fireworks in the shape of the Olympic rings light up the sky above the National Stadium during the opening ceremony of the 2022 <b>Winter</b> Olympic Games, Beijing, February 4, 2022. (more) What do Paris, Helsinki, Atlanta, Beijing, and Tokyo <b>have</b> in common? They all <b>have</b> <b>hosted</b> the <b>Olympics</b>. As the foremost international sporting event, the Olympic ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Cracker</b> is a British crime drama <b>series</b> produced by Granada <b>Television</b> for ITV, created and principally written by Jimmy McGovern.Set in Manchester, the <b>series</b> follows a criminal psychologist (or &quot;<b>cracker</b>&quot;), Dr Edward &quot;<b>Fitz</b>&quot; Fitzgerald, played by Robbie Coltrane, who works with the Greater Manchester Police (GMP) to help them solve crimes.. The show consists of three <b>series</b>, originally ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Cracker</b> (<b>TV</b> <b>Series</b> 1993–1996) cast and crew credits, including actors, actresses, directors, writers and more. Menu. Movies. ... <b>Fitz&#39;s</b> Mum 1 episode, 1993 Philippa Howell ... Dr Turner 1 episode, 1993 Barbara Young ... Helen McIlvanney 1 episode, 1995 ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Barbara Flynn. <b>Cracker</b>, Open All Hours, The Vanishing Man. Barbara Flynn (born Barbara Joy McMurray; 5 August 1948) is an English actress. She first came to prominence playing Freda Ashton in the ITV drama <b>series</b> A Family at War (1970–72). She went on to play the milk woman in the BBC comedy Open All Hours (1981–85), Jill Swinburne in The ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'GMC <b>Jimmy</b>. The GMC <b>Jimmy</b> was an SUV marketed by General Motors that spanned four generations and two distinct vehicles: The K5 <b>Jimmy</b> – a mid-size SUV built from 1970 to 1999 and based on the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The Best Year for GMC <b>Jimmy</b>. The year 1999 stands out as the best year for the GMC <b>Jimmy</b>. This particular <b>model</b> had a perfect balance of modern features and timeless design. In 1999, GMC introduced several updates to <b>the Jimmy</b>, which improved its overall performance, handling, and comfort. One of the most notable enhancements was the addition ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Research the GMC <b>Jimmy</b> and learn about its generations, redesigns and notable features from each individual <b>model</b> year. Opens website in a new tab. ... The GMC <b>Jimmy</b> was not <b>produced</b> from 2002–2004.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Rio de Janeiro</b> (Portuguese: [ˈʁi.u d(ʒi) ʒɐˈne(j)ɾu] ⓘ), or simply <b>Rio</b>, is the <b>capital</b> of the state of <b>Rio de Janeiro</b>.It is the second-most-populous city in Brazil (after São Paulo) and the sixth-most-populous city in the Americas.. Founded in 1565 by the Portuguese, the city was initially the seat of the Captaincy of <b>Rio de Janeiro</b>, a domain of the Portuguese Empire.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Rio</b> <b>de</b> <b>Janeiro</b> (<b>Rio</b>) is Brazil’s second-most populated city after Sao Paulo. Its population is 6.5 million, with 12.5 million in the urban area. <b>Rio</b> is located in the State of <b>Rio</b> <b>de</b> <b>Janeiro</b>, on the Atlantic coast in southeast Brazil. <b>Rio</b> was the <b>capital</b> of Brazil under Portuguese colonial rule.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Rio de Janeiro</b>, city and port, <b>capital</b> of the estado (state) of <b>Rio de Janeiro</b>, Brazil.It is located on the Atlantic Ocean, in the southeastern part of the tropical zone of South America, and is widely recognized as one of the world’s most beautiful and interesting urban centres.Although <b>Rio de Janeiro</b> continues to be the preeminent icon of Brazil in the eyes of many in the world, in reality ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>2002 Euro Beach Soccer Cup</b> was the fourth <b>Euro</b> <b>Beach</b> <b>Soccer</b> <b>Cup</b>, one of Europe&#39;s two major <b>beach</b> <b>soccer</b> championships at the time, held in February <b>2002</b>, in Barcelona, Catalonia, Spain. Portugal won the championship, claiming their second successive title and third overall, with hosts Spain finishing second. France beat Italy in the third place playoff to finish third and fourth respectively.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Euro Beach Soccer Cup</b> (EBSC), originally known as the <b>European</b> Pro <b>Beach</b> <b>Soccer</b> Championships until 2004, was a biennial (previously annual) <b>beach</b> <b>soccer</b> competition contested between <b>European</b> men&#39;s national teams, organised by <b>Beach</b> <b>Soccer</b> Worldwide (BSWW). Having started in 1998, the tournament&#39;s prestige has held in being one of the oldest and longest running <b>beach</b> <b>soccer</b> competitions ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>2002</b> <b>Euro</b> <b>Beach</b> <b>Soccer</b> <b>Cup</b> was the fourth <b>Euro</b> <b>Beach</b> <b>Soccer</b> <b>Cup</b>, one of Europe&#39;s two major <b>beach</b> <b>soccer</b> championships at the time, held in February <b>2002</b>, in Barcelona, Catalonia, Spain. Portugal won the championship, claiming their second successive title and third overall, with hosts Spain finishing second. France beat Italy in the third place playoff to finish third and fourth respectively.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Homecoming</b> is an American psychological thriller television series based on the Gimlet Media podcast of the same name. Created by Eli Horowitz and Micah Bloomberg, the series premiered November 2, 2018, on Amazon Prime Video. Horowitz and Bloomberg also serve as writers and executive producers alongside Sam Esmail, Chad Hamilton, Julia Roberts ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Spider-<b>Man: Homecoming</b>: Directed by Jon Watts. With Tom Holland, Michael Keaton, Robert Downey Jr., Marisa Tomei. Peter Parker tries to stop Adrian &#39;The Vulture&#39; Toomes from selling weapons made with advanced Chitauri technology while trying to balance his life as an ordinary high school student.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Spider-<b>Man: Homecoming</b> is a 2017 American superhero film based on the Marvel Comics character Spider-Man, co-produced by Columbia Pictures and Marvel Studios, and distributed by Sony Pictures Releasing. It is the second Spider-Man film reboot and the 16th film in the Marvel Cinematic Universe (MCU). The film was directed by Jon Watts from a ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Medzilaborce</b> ( Rusyn: Міджілабірцї, Midzhilabirtsyi; Ukrainian: Міжлабірці, Mizhlabirtsi; Hungarian: Mezőlaborc) is a town in northeastern Slovakia close to the border with Poland, located near the towns of Sanok and Bukowsko (in southeastern Małopolska ).', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Medzilaborce District</b> ( okres <b>Medzilaborce</b>) is a <b>district</b> in the Prešov <b>Region</b> of northeastern Slovakia. It is the least populated of Slovakia&#39;s 79 districts. Until 1918, the <b>district</b> was part of the county of Kingdom of Hungary of Zemplín .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Medzilaborce</b> <b>District</b> Type: <b>district</b> of Slovakia with 12,000 residents Description: <b>district</b> of Slovakia Location: Prešov <b>Region</b>, Slovakia, Central Europe, Europe View on Open\\xadStreet\\xadMap Latitude 49.2706° or 49° 16&#39; 14&quot; north Longitude 21.902° or 21° 54&#39; 7&quot; east Population 12,000 Elevation 323 metres (1,060 feet) Abbreviation ML Open ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Pilot</b> was also available in select cinemas, with screenings in Australia and New Zealand on the 16th and 17th, and in the United States on the 17th and 19th. The American presentations were accompanied by For Tonight We Might Die , the first episode of the spin-off series Class , which had originally aired the previous October and included an appearance by Capaldi as the Doctor.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Pilot</b> is the first full episode of Doctor Who helmed by Lawrence Gough who has revealed he wanted to be a <b>director</b> from the age of nine! His debut feature film was the 2009 movie, Salvage ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'It is one of its 86-year-old <b>director</b> Clint Eastwood’s finest and most unshowy features, just over 90 minutes long and as efficient in its exposition as Captain “Sully” Sullenberger (Tom ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Marian</b> <b>University</b> celebrates the most significant events of the academic and <b>religious</b> calendar with special all campus liturgies. Institutional Witness The Catholic Franciscan tradition animates <b>Marian</b> <b>University</b> and forms the intellectual and moral foundations for all <b>university</b> policies, including: student services, human resources, and financial practices.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Marian</b> <b>University</b> 3200 Cold Spring Road Indianapolis, IN 46222-1997 (317) 955-6000 admissions@<b>marian</b>.edu COMadmissions@<b>marian</b>.edu Need More Information ?', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Marian</b> <b>University</b>, a Catholic liberal arts <b>university</b> grounded in the charism of the Congregation of Sisters of St. Agnes, is a vibrant and values-centered teaching and learning community. Providing a higher education experience for undergraduate, graduate, and professional students, <b>Marian</b> <b>University</b> nurtures the capacity to thrive in all areas of life: spirituality, education, career, and ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'PS3603.A465 M35 2005. <b>Make Love! The Bruce Campbell Way</b> is a comedy novel written by actor <b>Bruce</b> <b>Campbell</b>. [1] [2] The novel is written in the first person and involves real life celebrities such as Richard Gere, Renée Zellweger, and Mike Nichols; however, it is fiction. On the jacket <b>of Make Love! The Bruce Campbell Way</b> the <b>author</b> states, quote:', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'From a violent fistfight with a Buddhist to a life-altering stint in federal prison, this novel has it all. And if the 72,444 words are too time-consuming, there are lots and lots of cool graphics. &quot;It&#39;s a great, goofy what-if.&quot; &quot;Ultimately, <b>Make</b> <b>Love</b> is a <b>Bruce</b> <b>Campbell</b> novel, starring <b>Bruce</b> <b>Campbell</b>, written for <b>Bruce</b> <b>Campbell</b> fans for whom ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'From a violent fistfight with a Buddhist to a life-altering stint in federal prison, this novel has it all. And if the 72,444 words are too time-consuming, there are lots and lots of cool graphics. Praise for <b>Make</b> <b>Love</b> <b>the Bruce</b> <b>Campbell</b> <b>Way</b>. &quot;It&#39;s a great, goofy what-if.&quot; &quot;Ultimately, <b>Make</b> <b>Love</b> is a <b>Bruce</b> <b>Campbell</b> novel, starring <b>Bruce</b> ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The salsa <b>capital</b> of the world has a number of festivals that take place throughout the year, including some of the biggest in the world. Feria de Cali is a fair held every December and people from all over the world come to the party. This carnival and parade-style fair celebrates Cali’s salsa history with traditional salsa and live music.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'As the undisputed salsa <b>capital</b> of the world, Caleños (people from Cali) eat, sleep and breathe these uptempo beats.But how exactly did the seductive Latin dance become such a big deal in the city? The answer involves the U.S. Navy, a guild of crafty club promoters and an illicit underworld empire whose cocaine trade transformed the face of the city forever.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Rabat, city and <b>capital</b> of Morocco. One of the country’s four imperial cities, it is located on the Atlantic coast at the mouth of the Wadi Bou Regreg, opposite the city of Salé. The history of Rabat is closely connected to that of Salé, the site of which was first occupied by the Roman settlement of <b>Sala</b> (Shella).', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>county</b> covers an area of 779.9 square kilometres (301.1 sq mi). Its administrative seat is the town <b>of Polkowice</b> , and it also contains the towns of Chocianów and Przemków . As of 2019 the total population of the <b>county</b> is 62,948, out of which the population <b>of Polkowice</b> is 22,480, that of Chocianów is 7,892, that of Przemków is 6,107, and the rural population is 26,469.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Polkowice</b> is located in historic Lower Silesia, about 15 km (9 mi) northwest of Lubin.The nearest airport is Wrocław Airport, located 72 km (45 mi) from <b>Polkowice</b>.. Situated in a traditional mining region, the town is part of the largest industrial copper-extraction area in Poland, with a copper-processing plant operating nearby.Nearby <b>Polkowice</b> Dolne is the site of a former State ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Gmina <b>Polkowice</b> is an urban-rural gmina (administrative district) in <b>Polkowice</b> <b>County</b>, Lower Silesian Voivodeship, in south-western Poland.Its seat is the town <b>of Polkowice</b>, which lies approximately 80 kilometres (50 mi) north-west of the regional <b>capital</b> Wrocław.. The gmina covers an area of 158.77 square kilometres (61.3 sq mi), and as of 2019 its total population is 27,676.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Paul</b> Revere <b>Braniff</b> was <b>born</b> in Kansas <b>City</b>, Kansas. He was the younger brother of Thomas Elmer <b>Braniff</b>. He grew up during the early era of aviation, and, as a youngster, became fascinated with the new way of transport. His family moved to Oklahoma <b>City</b>, Oklahoma, in 1900. Marriage. <b>Braniff</b> married Marie Agnes Maney on April 29, 1920.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Paul</b> <b>Braniff</b>, who was <b>born</b> on October 29, 1927, and died in Oklahoma <b>City</b> on February 1, 2013 at the age of 85. Marie was graduated from St. Agnes College in Baltimore, Md., and throughout her married life was involved in the aviation affairs of her husband that at times included scouting for new routes.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Paul</b> Revere <b>Braniff</b> (August 30, 1897 – June 15, 1954) was an airline entrepreneur. <b>Paul</b>, along with his brother Thomas Elmer <b>Braniff</b>, was one of the original founders of <b>Braniff</b> Airways, Inc. d/b/a <b>Braniff</b> International Airways (after 1948). <b>Paul</b> Revere <b>Braniff</b> was <b>born</b> in Kansas <b>City</b>, Kansas. He grew up during the early era of aviation, and, as a youngster, became fascinated with the new ...', 'score': 'N/A'}], [], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Rectified linear units improve restricted boltzmann machines. V Nair, GE <b>Hinton</b>. Proceedings of the 27th international conference on machine learning (ICML …. , 2010. 25342. 2010. Reducing the dimensionality of data with neural networks. GE <b>Hinton</b>, RR Salakhutdinov. Science 313 (5786), 504-507.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The following articles are merged in <b>Scholar</b>. Their combined <b>citations</b> are counted only for the first article. ... <b>Geoffrey</b> <b>Hinton</b>. Unknown affiliation. No verified email. Articles Cited by. Title. Sort. ... GF <b>Hinton</b>, F Cambridge. 182: 1981:', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Geoffrey</b> E. <b>Hinton</b>&#39;s 389 research works with 467,418 <b>citations</b> and 350,337 reads, including: Robust and data-efficient generalization of self-supervised machine learning for diagnostic imaging', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Avi Boodram who lives in Christiana, will display his apparent lack of culinary abilities <b>on the Food</b> <b>Network</b> <b>program</b> that begins <b>airing</b> at 8 p.m. <b>Sunday</b>, <b>Jan</b>. <b>7</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'There’s no shame in this game: One of the “<b>Worst Cooks in America</b>” lives in <b>Delaware</b>. Avi Boodram who lives in Christiana, will display his apparent lack of culinary abilities <b>on the Food</b> <b>Network</b> <b>program</b> that begins <b>airing</b> at 8 p.m. <b>Sunday</b>, <b>Jan</b>. <b>7</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>Food</b> <b>Network</b> series ‘ <b>Worst Cooks In America</b> ‘ is a show that strictly focuses on some of the worst cooks the country has ever <b>seen</b>. The series first <b>began</b> <b>airing</b> in January 2010, and since then has been going on rather successfully', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Alexei</b> Mikhailovich <b>Ugarov</b> (Russian: Алексей Михайлович Угаров; born November 2, 1985) is a Belarusian professional ice hockey forward.He is currently an unrestricted free agent who most recently played for Severstal Cherepovets of the Kontinental Hockey League (KHL). He previously played three seasons for HC Nizhnekamsk Neftekhimik in the Russian Super League.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Hockey player <b>Ugarov</b> <b>Alexei</b>: up-to-date statistics, KHL matches, latest news', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Visit Aleksei <b>UGAROV</b> profile and read the full biography, watch videos and read all the latest news. Click here for more. ... <b>Sports</b>; News; Olympic Channel; Let&#39;s Move; Aleksei <b>UGAROV</b>. Team Belarus. Ice Hockey. Games Participations 1. First Olympic Games Vancouver 2010. Year of Birth 1985. Olympic Results.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Joshua <b>Da Silva</b> (born 19 June 1998) is a Trinidadian cricketer. He made his domestic debut in 2018 for Trinidad and Tobago, and his international debut for the West Indies cricket team in December 2020. Personal life. <b>Da</b> <b>Silva</b> is of Portuguese descent, with his ancestors hailing from Madeira. Both ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'JOSHUA <b>DA</b> <b>SILVA’S</b> TWITTER: @joshuadasilva08: JOSHUA <b>DA</b> <b>SILVA</b> BIOGRAPHY. JOSHUA <b>DA</b> <b>SILVA’S</b> NEWS. News. Watch – West Indies’ Shamar Joseph smashes long six that breaks tiles on Trent Bridge roof. by Staff Writer July 20, 2024 July 20, 2024. News. <b>Da</b> <b>Silva</b> and Hodge lead West Indies to 266-8 against Australia in day-night test.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'A wicketkeeper/batsman from Trinidad and Tobago, Joshua <b>Da</b> <b>Silva</b> was selected among the reserves for West Indies&#39; tour of England in 2020 after an impressive domestic season in 2019-20. <b>Da</b> <b>Silva</b> ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>votes</b> of the public determine electors, who formally choose the president through the <b>electoral</b> <b>college</b>. The number of electors a state receives is Every four years on the first Tuesday following the first Monday of November, voters head to the polls to elect the president of the United States. ... The <b>table</b> provides a list of U.S ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Every state will convene its meeting of electors that same day and send their <b>electoral</b> <b>votes</b> to the President of the Senate and the National Archives. Watch the live broadcast on TVW. Counting <b>Electoral</b> <b>Votes</b> – January 6, 2025. Congress meets in a joint session to count the <b>electoral</b> <b>votes</b> and announce the results of the <b>Electoral</b> <b>College</b> <b>vote</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Select a date to see. The <b>Electoral</b> <b>College</b> outcome. <b>Electoral</b> <b>College</b> <b>votes</b> by State. The candidates. Election notes*. * Election notes include specific election information, such as third party candidates, faithless electors, challenges during the counting of the <b>electoral</b> <b>votes</b> in Congress, and other interesting facts.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Madeleine Bunting, in her selection of Top 10 <b>books</b> about the aftermath <b>of empire</b>, for The Guardian Empireland is an utterly fascinating journey.. . For many British Asians who feel a strong connection to their heritage, Empireland exposes things you will wish you had learnt at school.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Jon Wilson, a professor of the Department of History at King&#39;s College London, is the <b>author</b> of India Conquered, a 2016 book intended to rebut Ferguson&#39;s arguments in <b>Empire</b>: How Britain Made the Modern World, who catalogues the negative elements of the British Raj, and describes the <b>Empire</b> TV program (2003) as &quot;false and dangerous&quot;.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Synopsis. <b>Author</b>. An urgent, incisive account of how the depredations of its imperial past dog Britain’s view of itself today, Empireland is provocative, meticulously researched writing of the highest order. Winner of The British Book Awards 2022 Non-Fiction Narrative Book of the Year. Longlisted for the Baillie Gifford Prize for Non-Fiction ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Prime Minister John Key has announced the creation of <b>a 620,000</b> km2 <b>Ocean</b> <b>Sanctuary</b> in the Kermadec region, one of the most pristine and unique environments on Earth. “The Kermadec <b>Ocean</b> <b>Sanctuary</b> will be one of the world’s largest and most significant fully-protected areas, preserving important habitats for seabirds, whales and dolphins, endangered marine turtles and thousands of species ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Government</b>’s announcement that it will scrap plans for a vast marine <b>sanctuary</b> around the Kermadec <b>Islands</b> is ‘shameful’ and will make it impossible for Aotearoa <b>New</b> <b>Zealand</b> to meet its international commitments, says the World Wide Fund for Nature (WWF) <b>New</b> <b>Zealand</b>.. Plans had been underway for <b>a 620,000</b> square kilometre <b>ocean</b> <b>sanctuary</b> around the Kermadec <b>Islands</b>/Rangitāhua to ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The Prime Minister’s announcement today at the United Nations in <b>New</b> York of a <b>new</b> Kermadec <b>Ocean</b> <b>Sanctuary</b> is a global contribution by <b>New</b> <b>Zealand</b> towards better protection of the world’s oceans, Environment Minister Dr Nick Smith says. “Oceans are <b>the new</b> frontier for environmental protection. They make up 72 per cent of the globe and ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'On 25 th August 2023, an Atlanta Fr. 2079-F 1993 $20 sold for $2,400 because of Solid #5 Serial Numbers. A Boston was cheaper. One month earlier on 27 th July 2023, a Boston Fr. 2079-A 1993 $20 was $2,220 with Solid #6 Serial Numbers. Both <b>notes</b> were graded 66 EPQ Gem Uncirculated.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The Fr. is for Robert Friedberg. His book Paper Money of the United States <b>has</b> a list of all circulating <b>American</b> <b>bank</b> <b>notes</b>, assigning them each a Friedberg Number. As we mentioned before, the 1995 $20 was signed by Withrow and Rubin. On 25 th August 2023, a special Fr. 2081-D 1995 $20 sold for $1,920.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The $20 <b>note</b> includes an embedded security thread that glows green when illuminated by UV light. When held to light, a portrait watermark of President Jackson is visible from both sides of the <b>note</b>.The <b>note</b> includes a color-shifting numeral 20 in the lower right corner of the <b>note</b>. Watch Video.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Nora Roberts. If there’s one <b>author</b> on this list who’s a recognized household name, it’s Nora Roberts. Since 1980, Roberts has written and published an astounding number of romances — her website claims the number stands at over 215! But this incredibly prolific production has not come at the cost of quality.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Ali Hazelwood is the #1 New York Times bestselling <b>author</b> <b>of Love</b>, Theoretically and The <b>Love</b> Hypothesis, as well as a writer of peer-reviewed articles about brain science, in which no one makes out and the ever after is not always happy. Originally from Italy, she lived in Germany and Japan before moving to the US to pursue a PhD in neuroscience.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'From the bestselling <b>author</b> of the Cazalet Chronicles comes Elizabeth Jane Howard&#39;s <b>Love</b> <b>All</b>. The late 1960s. For Persephone Plover, the daughter of distant and neglectful parents, the innocent, isolated days of childhood are long past. Now she must deal with the emotions of an adult world . . .', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>2001–02</b> <b>Serie</b> <b>B</b> is the 70th season since its establishment in 1929. It is the second highest football league in Italy. Teams. Modena, Palermo, Como and Messina had been promoted from <b>Serie</b> C, while Reggina, Vicenza, Napoli and Bari had been relegated from <b>Serie</b> A. Personnel and sponsoring. Team', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Serie B</b> (Italian pronunciation: [ˈsɛːrje ˈbi]), officially known as <b>Serie</b> BKT for sponsorship reasons, is the second-highest division in the Italian football league system after the <b>Serie</b> A.It has been operating for over ninety years since the 1929–30 season.It had been organized by Lega Calcio until 2010 and the Lega <b>Serie B</b> ever since. Common nicknames for the league are campionato ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>2001–02</b> <b>Serie</b> <b>B</b> is the 70th season since its establishment in 1929. It is the second highest football league in Italy. WikiMili. <b>2001–02</b> <b>Serie</b> <b>B</b> Last updated April 14, 2024. <b>Serie</b> <b>B</b> TIM; Season: <b>2001–02</b>: Promoted: Como (3rd title) Modena Reggina Empoli: Relegated: Crotone ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Yinka Ayenuwa</b> (<b>born</b> 2 May 1986 in Warri) is a Nigerian weightlifter. He competed in the men&#39;s 69 kg event at the 2014 Commonwealth Games [2] where he won a silver medal. [3] In 2015, he won 3 silver medals at the African Games .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Yinka</b> <b>Ayenuwa</b> (<b>born</b> 2 May 1986 in Warri) is a Nigerian weightlifter. He competed in the men&#39;s 69 kg event at the 2014 Commonwealth Games where he won a silver medal. In 2015, he won 3 silver medals at the African Games. Celebs Wiki. <b>Yinka</b> <b>Ayenuwa</b> fans also viewed: John Davis (weightlifter)', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>ayenuwa</b>.<b>yinka</b>.3. Show Famous Birthdays Today, Nigeria. ... About <b>Yinka</b> <b>Ayenuwa</b>. <b>Yinka</b> <b>Ayenuwa</b> (<b>born</b> 2 May 1986 in Warri) is a Nigerian weightlifter. He competed in the men&#39;s 69 kg event at the 2014 Commonwealth Games where he won a silver medal. In 2015, he won 3 silver medals at the African Games. Read more at Wikipedia. See Also.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Tikhvinsky District</b> (Russian: Ти́хвинский райо́н) is an administrative and municipal <b>district</b> (), one of the seventeen in Leningrad Oblast, Russia.It is located in the southeast of the oblast and borders with Lodeynopolsky <b>District</b> in the north, Podporozhsky <b>District</b> in the northeast, Babayevsky <b>District</b> of Vologda Oblast in the east, Boksitogorsky <b>District</b> in the southeast ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Tikhvin (Russian: Ти́хвин; Veps: Tihvin) is a town and the administrative center <b>of Tikhvinsky</b> <b>District</b> in Leningrad Oblast, Russia, located on both banks of the Tikhvinka River in the east of the oblast, 200 kilometers (120 mi) east of St. Petersburg.Tikhvin is also an industrial and cultural center of the <b>district</b>, as well as its transportation hub.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Tikhvin is a town and the administrative center <b>of Tikhvinsky</b> <b>District</b> in Leningrad Oblast, Russia, located on both banks of the Tikhvinka River in the east of the oblast, 200 kilometers (120 mi) east of St. Petersburg. Tikhvin is also an industrial and cultural center of the <b>district</b>, as well as its transportation hub. Population: 58,459 (2010 Russian census); 63,338 (2002 Census); 71,352 ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Conan, Lord of the Black River</b> is a fantasy novel by American writer Leonard Carpenter, featuring Robert E. Howard&#39;s sword and sorcery hero <b>Conan</b> the Barbarian. It was first published in paperback by Tor <b>Books</b> in April 1996. Plot After successfully fulfilling his commission to overthrow a tyrannical baron in Koth, <b>Conan</b> travels into Baalur, a ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '3.28. 80 ratings8 reviews. <b>Conan</b> the Cimmerian must venture into the nightmare world of the dead to retrieve the Silver Lotus, a powerful weapon that can undo the dreaded incantation that holds the city of Queen Rufia under the spell of the undead witch Zeriti. Genres Fantasy Sword and Sorcery. 288 pages, Paperback. First published April 15, 1996.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Leonard Paul Carpenter (born in 1948) is a technical writer and <b>author</b> of fantasy and science fiction. Among Carpenter&#39;s works are eleven <b>Conan</b> novels published by Tor <b>Books</b>, which he claims &quot;make him the most prolific contributor, living or dead, to the <b>Conan</b> literary saga of the late Robert E. Howard.&quot; He has also written the science fiction novel Fatal Strain, and a number of short stories ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Hunt</b> is a 2020 American action horror film directed by Craig Zobel and written by Nick Cuse and Damon Lindelof.The film stars Betty Gilpin, Hilary Swank, Ike Barinholtz, and Emma Roberts. Jason Blum was a <b>producer</b> under his Blumhouse Productions banner, along with Lindelof. Zobel and Lindelof have said that the film is intended as a satire on the profound political divide between the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Hunt</b> (2020) cast and crew credits, including actors, actresses, directors, writers and more. Menu. Movies. Release Calendar Top 250 Movies Most Popular Movies Browse Movies by Genre Top Box Office Showtimes &amp; Tickets Movie News India Movie Spotlight. TV Shows.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>Hunt</b>: Directed by Craig Zobel. With Betty Gilpin, Hilary Swank, Ike Barinholtz, Wayne Duvall. Twelve strangers wake up in a clearing. They don&#39;t know where they are, or how they got there. They don&#39;t know they&#39;ve been chosen - for a very specific purpose - The <b>Hunt</b>.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Saraburi</b> City (thesaban mueang) is the provincial <b>capital</b> <b>of Saraburi</b> Province in central Thailand. [1] [2] In 2020, it had a population of 60,809 people, and covers the complete tambon Pak Phriao of the Mueang <b>Saraburi</b> district .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Saraburi</b> is on the east side of the Chao Phraya River valley. The eastern part of the province is covered by high plains and plateaus, while the western part is mostly low flat plains. [citation needed] <b>Saraburi</b> province has 848 km 2 (327 sq mi) of forest or 24.2 percent of provincial area. [1] The town, as a gateway to the northeastern region ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Saraburi</b> has about 67,800 residents. <b>Mapcarta</b>, the open map. SE Asia. Thailand. <b>Saraburi</b>. Mueang <b>Saraburi</b>. <b>Saraburi</b> <b>Saraburi</b> is a city in the Chao Phraya Basin region of Thailand. It is home to one of the most beautiful religious sites in Thailand, a large botanical garden, and many other natural and historical sights.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Charlton Heston. Select from the options above. Q4. <b>The first US National Champions of which sport were Team Roslindale when the first sanctioned Official Nationals were played in Leominster Ma in 1974</b>? A. Ice skating. B.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>The first US National Champions of which sport were Team Roslindale when the first sanctioned Official Nationals were played in Leominster Ma in 1974</b>? A . Ice skating B . Roller Blading C . Rollerball D . Street hockey ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'United States at <b>the1896</b> Summer Olympics. Fourteen competitors from the United States competed in three sports at <b>the</b> 1896 Summer Olympics in Athens, Greece. The Americans <b>were</b> the most successful athletes in terms of gold medals, beating host nation Greece, 11 to 10, despite fielding only 14 competitors compared to an estimated 169 Greek entrants.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Strand</b>’s guitarist Scott Shelly was a founding member, along with Jeff Porcaro, of the Grant High School-based band Rural Still Life. This outfit featured a number of musicians who would later establish themselves on the West Coast AOR scene, like Carlos Vega, Michael Landau or Steve Lukather and David Paich.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Strand</b> self titled album was released in 1980 and produced by Jeffrey Porcaro, who also played the drums on the track Can’t Look Back (uncredited).The <b>Strand</b> later did some opening acts for Toto (Hydra Tour). Kelly Shanahan commented on Facebook: Seems like yesterday, we were at Sunset Sound in Studio B working on basic tracks for The <b>Strand</b> album and Toto was in Studio A mixing Hydra ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Bricks and bottles were thrown at police officers as violence broke out on The <b>Strand</b>. Protests were organised in the city centre on Saturday, including one called &#39;Save Our Kids&#39; near the Liver ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>St</b>. <b>Albert</b>&#39;s <b>Church</b> (Latvian: Svētā Alberta Romas katoļu baznīca) is a Roman Catholic <b>church</b> in <b>Riga</b>, ... <b>St</b>. <b>Albert</b> <b>Church</b>, <b>Riga</b>, Pipe Organ <b>St</b>. <b>Albert</b> <b>Church</b>, <b>Riga</b>, inside view, the main door with the Choir balcony. The 30 stops pipe organ of the <b>church</b> was built in 1912., by Emil Martin &amp; Co., Opus 316. Emil Martin was the son of, also ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Riga</b> <b>St</b>. <b>Albert</b> Roman Catholic <b>Church</b> In Latvia It is one of the best churches in Latvia which you must visit. History: The Roman Catholic <b>Church</b> <b>of St</b>. <b>Albert</b> was founded in 1778 with the intention of providing a place of worship for the Catholic residents of <b>Riga</b>. The original <b>church</b> building was destroyed during the WWII bombings. The present-day <b>church</b> was built in 1957.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Riga</b> <b>St</b> Jacob&#39;s Cathedral. The <b>Riga</b> Archdiocesan Cathedral is the main Catholic <b>church</b> in Latvia. Its slender tower has preserved better than any other the pyramidal shape characteristic of <b>Riga</b>&#39;s medieval <b>church</b> towers. Historically, it is also the first Latvian parish <b>church</b> in <b>Riga</b> (1523). See more here.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Escape</b>: Directed by Lee Jong-pil. With Lee Je-hoon, Koo Kyo-hwan, Hong Xa-bin, Shin Dong-hyeon. Follows the struggles of a North Korean sergeant who is chased by a ruthless major after he defects.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The Great <b>Escape</b> is a 1963 American epic war suspense adventure film starring Steve McQueen, James Garner and Richard Attenborough and featuring James Donald, Charles Bronson, Donald Pleasence, James Coburn, Hannes Messemer, David McCallum, Gordon Jackson, John Leyton and Angus Lennie.It was filmed in Panavision, and its musical score was composed by Elmer Bernstein.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Those inciting riots online will not <b>escape</b> the law, top prosecutor warns <b>Director</b> of Public Prosecutions Stephen Parkinson said he was ‘absolutely’ seeking to prosecute people for online ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Inspector</b> Jacques <b>Clouseau</b> ( French: [ʒɑk kluzo] ), later granted the rank of Chief <b>Inspector</b>, is a fictional character in Blake Edwards &#39; farcical The Pink Panther series. He is portrayed by Peter Sellers in the original series, and also by Alan Arkin in the 1968 film <b>Inspector Clouseau</b> and, in a cameo, by Roger Moore (credited as Turk ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Films: 7 Cato Fong is <b>Clouseau&#39;s</b> Chinese <b>manservant</b>, trained to attack him regularly to keep him alert and skilled in martial arts. Cato and <b>Clouseau</b> have a love-hate relationship, with their fights being long and vicious, as well as destructive to the furniture, and always interrupted by the telephone ringing, at which point they will become civil again. Cato puts a lot of effort into taking ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'He was best known for playing Cato Fong, <b>Inspector</b> <b>Clouseau &#39;s</b> <b>manservant</b>, in the Pink Panther film series. The character was first introduced in A Shot in the Dark (1964), the second film in the series, and was a role that Kwouk would reprise on another six occasions until the 2006 series reboot.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'SECTION. 1. No person shall be elected to the office of the <b>President</b> more than twice, and no person who has held the office of <b>President</b>, or acted as <b>President</b>, for more than two years of a term to which some other person was elected <b>President</b> shall be elected to the office of <b>President</b> more than once.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The Twenty-second Amendment ( Amendment XXII) <b>to the</b> United States Constitution limits the <b>number</b> of times a person can be elected to the office of <b>President</b> of the United States to two <b>terms</b>, and sets additional eligibility conditions for presidents who succeed to the unexpired <b>terms</b> of their predecessors. [1] Congress approved the Twenty-second Amendment on March 21, 1947, and submitted it ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Twenty-second Amendment, amendment (<b>1951</b>) to the Constitution of the United States effectively limiting to two the <b>number</b> of <b>terms</b> a <b>president</b> of the United States may serve. It was <b>one</b> of 273 recommendations to the <b>U.S</b>. Congress by the Hoover Commission, created by Pres. Harry S. Truman, to reorganize and reform the federal government.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Operation Chastise, commonly known as the <b>Dambusters</b> <b>Raid</b>, was an attack on German dams carried out on the night of 16/17 May 1943 by 617 Squadron RAF Bomber Command, later called the <b>Dam Busters</b>, using special &quot;bouncing bombs&quot; developed by Barnes Wallis.The Möhne and Edersee dams were breached, causing catastrophic flooding of the Ruhr valley and of villages in the Eder valley; the Sorpe Dam ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Dambusters</b> <b>Raid</b>. On the night of 16-17 May 1943, Wing Commander Guy Gibson led 617 Squadron of the Royal Air Force on an audacious bombing <b>raid</b> to destroy three dams in the Ruhr valley, the industrial heartland of Germany. The mission was codenamed Operation &#39;Chastise&#39;. The dams were fiercely protected.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '15 May 2013. Seventy <b>years</b> ago an RAF bomber <b>raid</b> destroyed important German dams. At the time many argued it was only a propaganda victory. It was much more than that, writes historian Dan Snow ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'You can get a <b>free</b> <b>TV</b> <b>licence</b> if you&#39;re 75 or over and you get Pension Credit, or a discount if you&#39;re blind or in residential care - who qualifies, how to apply', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'If you are 74 you can already apply for a <b>free</b> <b>TV</b> <b>Licence</b>. Find out more about how you can apply for a <b>free</b> <b>TV</b> <b>Licence</b> here.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'If you&#39;re aged 75 and over and claim Pension Credit, you&#39;re <b>entitled</b> <b>to a free</b> <b>TV</b> <b>licence</b>. Pension Credit is a benefit that supplements your income if you&#39;re over the state pension <b>age</b> and on a ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Michael Emons BBC <b>Sport</b> at the Crucible Theatre. Scotland&#39;s <b>Stephen</b> Maguire, semi-finalist in 2007 and 2012, trailed 5-4 after the first session against 2008 and 2012 runner-up Ali Carter, but ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The winner receives £500,000. For the runner-up, there is the consolation of £200,000. Semi-finalists earn £100,000 and quarter-finalists £50,000. Players knocked out in the first round ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Michael Emons BBC <b>Sport</b> at the Crucible Theatre. Seven-time champion Ronnie O&#39;Sullivan only needs three more frames to beat Wales&#39; Ryan Day and move into the quarter-finals of the World Snooker ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Luis Fernando Tena</b> Garduño (<b>born</b> 20 January 1958) is a Mexican professional football manager and former player who is the head coach of the Guatemala national team .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Age, Biography and Wiki. <b>Luis Fernando Tena</b> was <b>born</b> on 20 January, 1958 in Mexico <b>City</b>, Mexico. Discover <b>Luis Fernando Tena</b>&#39;s Biography, Age, Height, Physical Stats, Dating/Affairs, Family and career updates.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Luis Fernando Tena</b>. Self: Chivas: El Rebaño Sagrado. <b>Born</b> on January 20, 1958 in Mexico <b>City</b>, <b>Luis Fernando Tena</b> Garduño is a Mexican former professional football player and former head coach of Club Deportivo Guadalajara, one of Mexico&#39;s most legendary teams.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Country</b> Argentina: Province: Catamarca Province: Time zone: UTC−3 : <b>Colonia Nueva Coneta</b> is a village and municipality in Catamarca Province in northwestern Argentina. Location. It is located 12 km from the city of San Fernando del Valle de Catamarca, ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Colonia</b> <b>Nueva</b> <b>Coneta</b> <b>Colonia</b> <b>Nueva</b> <b>Coneta</b> is a locality in Catamarca Province, Argentina. <b>Colonia</b> <b>Nueva</b> <b>Coneta</b> is situated nearby to the hamlet El Bañado and the locality La Aguada. Overview: Map: Directions: Satellite: Photo Map: Overview: Map: Directions: Satellite: Photo Map: Tap on the map to travel:', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'El área proyectada para la ejecución de la colonización era de una superficie de 12 mil hectáreas dividida en un principio en cinco colonias”, apunta en su libro “<b>Nueva</b> <b>Coneta</b>, un pueblo ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Canton</b> of <b>Geneva</b>, officially the Republic and <b>Canton</b> of <b>Geneva</b>, is one of the 26 cantons of the Swiss Confederation.It is composed of forty-five municipalities, and the seat of the government and parliament is in the city of <b>Geneva</b>.. <b>Geneva</b> is the French-speaking westernmost <b>canton</b> of Switzerland.It lies at the western end of Lake <b>Geneva</b> and on both sides of the Rhone, its main river.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Geneva</b> (/ dʒ ə ˈ n iː v ə / jə-NEE-və, Arpitan: [dzəˈnɛva] ⓘ; French: Genève ⓘ) is the second-most populous city in Switzerland (after Zürich) and the most populous of the French-speaking Romandy.Situated in the southwest of the country, where the Rhône exits Lake <b>Geneva</b>, it is the <b>capital</b> of the Republic and <b>Canton</b> of <b>Geneva</b>, and a centre for international diplomacy.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Geneva</b>, city, <b>capital</b> of Genève <b>canton</b>, in the far southwestern corner of Switzerland that juts into France.One of Europe’s most cosmopolitan cities, <b>Geneva</b> has served as a model for republican government and owes its preeminence to the triumph of human, rather than geographic, factors. It developed its unique character from the 16th century, when, as the centre of the Calvinist Reformation ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Harvard University</b> is a private Ivy League research <b>university</b> in Cambridge, Massachusetts.Founded in 1636 as <b>Harvard</b> College and named for its first benefactor, Puritan clergyman John <b>Harvard</b>, it is the oldest institution of higher learning in the <b>United States</b>.Its influence, wealth, and rankings have made it one of the most prestigious universities in the world.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Harvard</b> <b>University</b> holds 5,083 acres of real estate. The main campus occupies several locations in Cambridge including the historic and famous <b>Harvard</b> Yard. Athletic facilities and the <b>Harvard</b> Business School are located across the Charles River in Allstom, Massachusetts. The <b>Harvard</b> Medical School and School of Dental Medicine are located in ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Harvard University</b>, oldest institution of higher learning in the <b>United States</b> (founded 1636) and one of the nation’s most prestigious. The main <b>university</b> campus lies along the Charles River in Cambridge, Massachusetts, a few miles west of downtown Boston. Learn more about <b>Harvard</b> in this article.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Plesetsky District</b> ( Russian: Плесе́цкий райо́н) is an administrative <b>district</b> ( raion) one of the twenty-one in Arkhangelsk Oblast, Russia. [1] As a municipal division, it is incorporated as <b>Plesetsky</b> Municipal <b>District</b>. [7] It is located in the west of the oblast and borders with Primorsky <b>District</b> in the north, Kholmogorsky ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Plesetsk ( Russian: Плесе́цк) is an urban locality (a work settlement) and the administrative center <b>of Plesetsky</b> <b>District</b>, Arkhangelsk Oblast, Russia, situated about 800 kilometers (500 mi) northeast of Moscow and 180 kilometers (110 mi) south of Arkhangelsk. Municipally, it is the administrative center of Plesetskoye Urban Settlement, one of eight urban settlements in the <b>district</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Plesetsk (Russian: Плесе́цк) is an urban locality (a work settlement) and the administrative center <b>of Plesetsky</b> <b>District</b>, Arkhangelsk Oblast, Russia, situated about 800 kilometers (500 mi) northeast of Moscow and 180 kilometers (110 mi) south of Arkhangelsk.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'You can use our free music <b>genre</b> finder and analyzer to quickly find the <b>genre</b> and more interesting information (such as the song’s key, BPM, popularity, etc.) about any music you love or to find an artist’s <b>genre</b>. Just enter the song title or artist name and leave the rest to our <b>genre</b> checker tool. “You are what you listen to.”.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Movie <b>genres</b> are stylistic categories where a particular movie can be placed based on the setting, characters, plot, mood, tone, and theme. A film&#39;s main <b>genre</b> category will be based on where the majority of the content lands. A sub-<b>genre</b> is a smaller category that fits inside a particular <b>genre</b>. Often this is a mixture of two separate <b>genres</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Ballroom dance music: pasodoble, cha cha cha and others. Vogue (dance) Children&#39;s music. Dance music. Slow dance. Drug use in music. Incidental music or music for stage and screen: music written for the score of a film, play, musicals, or other spheres, such as filmi, video game music, music hall songs and showtunes and others.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Home</b>: Directed by Tim Johnson. With Jim Parsons, Rihanna, Steve Martin, Jennifer Lopez. Oh, a lovable misfit alien, runs away from his planet and takes shelter on Earth, where he befriends Tip, an adventurous young girl who is on a quest to find her displaced mother Lucy.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Home</b> is a 2015 American animated science fiction comedy film produced by DreamWorks Animation and distributed by 20th Century Fox. Loosely based on Adam Rex &#39;s 2007 children&#39;s book The True Meaning of Smekday , the film was directed by Tim Johnson from a screenplay by Tom J. Astle and Matt Ember, and stars the voices of Jim Parsons , Rihanna , Steve Martin , Jennifer Lopez , and Matt Jones .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Max Bialystock is a washed up Broadway <b>producer</b> when timid accountant Leo Bloom inadvertently reveals that, under the right circumstances, a <b>producer</b> could make more money with a flop than a hit. Together they come up with a sure-fire disaster waiting to happen – “Springtime for Hitler”. Unfortunately, everybody loves it.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '100 <b>years</b> of supporting the Armed Forces community. In our centenary <b>year</b>, we are firmly focused on our future. By building on a century of work we’ll make sure we are a charity fit for the next 100. RBL was formed on 15 May 1921, bringing together four organisations of ex-servicemen that had established themselves after the First World War.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Royal British Legion</b> Women&#39;s Section (RBLWS) was <b>founded</b> in 1921 and operated independently for some 96 <b>years</b>, with its own branches, standards and standard bearers, county branches, income and expenditure, national central committee, and annual conference.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'In the early <b>years</b> of the newly formed <b>British</b> <b>Legion</b>, founder and President Earl Haig worked tirelessly championing the needs of the Armed Forces, launching the Poppy Day Appeal in 1921 and helping to shape modern Remembrance. He also worked hard at grass-roots level, touring the country with Lady Haig, making speeches, visiting branches ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Browse today’s rankings of the <b>wealthiest</b> people and families globally. Discover the net worth, age, and other information about the <b>richest</b> people in the world.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Key Takeaways. Elon Musk, CEO of Tesla, is the <b>richest</b> person and the <b>richest</b> <b>man</b> in the world with a net worth of $252 billion. After Musk is Jeff Bezos, founder of Amazon. Other billionaires ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Here are the 10 <b>richest</b> people <b>on Earth</b> as of July 1, ... Bezos was the world’s <b>richest</b> person on ... <b>Who is</b> the <b>richest</b> <b>man</b> in the world? As of July 1, 2024, the <b>richest</b> person in the world is Tesla and SpaceX CEO Elon Musk. He’s worth $221.4 billion. He moved into the number one spot in late May 2024, overtaking Bernard Arnault of France.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Cocktail</b> is a 1988 American romantic comedy-drama film directed by Roger Donaldson from a screenplay by Heywood Gould, and based on Gould&#39;s book of the same name.It stars Tom Cruise, Bryan Brown and Elisabeth Shue.It tells the story of a business student, who takes up bartending in order to make ends meet.. Released on July 29, 1988, by Buena Vista Pictures (under its adult film label ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Cocktail</b> (1988) cast and crew credits, including actors, actresses, directors, writers and more. Menu. Movies. ... second assistant <b>director</b>: New York (as Christopher Gerrity) Alex Kramarchuk ... second second assistant <b>director</b>: New York Michael London ... third assistant <b>director</b>: Jamaica ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Cocktail</b>: Directed by Roger Donaldson. With Tom Cruise, Bryan Brown, Elisabeth Shue, Lisa Banes. A talented New York City bartender takes a job at a bar in Jamaica and falls in love.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Appointment with Fear was a horror drama series originally <b>broadcast</b> on BBC <b>Radio</b> in the 1940s and 1950s, and revived on a number of occasions since. The format comprised a dramatised horror story of approximately half an hour in length, introduced by a character <b>known</b> <b>as the Man</b> in <b>Black</b>. The plays themselves were a mixture of classic horror ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Classic <b>radio</b> horror, The <b>Man</b> In <b>Black</b>, has a legacy dating back to the 1940s when Valentine Dyall established the role of the disconcerting teller-of-tales. He was replaced by Edward de Souza in ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Unfortunately very few shows of Appointment With Fear have survived. Only four shows are <b>known</b> to exist, namely 1/2, 3/1, 3/6 and 6/4. No shows from The <b>Man</b> In <b>Black</b> series have survived. However all of Fear On Four still exists. All of the stories from the first two series of Fear On Four have been published in: The <b>Man</b> In <b>Black</b>, BBC Books, 1990', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Honda <b>has</b> enjoyed four distinct eras of F1 involvement, and the current <b>one</b> <b>has</b> proved to be the second most successful. The first chapter was a works <b>team</b> from Japan that competed from 1964 to ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Sky Sports</b>&#39; Craig Slater explains. Honda have <b>announced</b> <b>they</b> will <b>withdraw</b> <b>from Formula</b> 1 <b>at the end</b> of the 2021 season. The Japanese manufacturer provide engines to both Red Bull and AlphaTauri ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Honda <b>has</b> enjoyed four distinct eras of F1 involvement, and the current <b>one</b> <b>has</b> proved to be the second most successful. The first chapter was a works <b>team</b> from Japan that competed from 1964 to ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Al Gore</b> would participate in one vice-presidential debate against Vice President Dan Quayle, and Admiral James ... &quot;In all fairness, it&#39;s something <b>Gore</b> had worked on a long time. <b>Gore</b> is not the <b>Father</b> of the Internet, but in all fairness, <b>Gore</b> is the person who, in the Congress, most systematically worked to make sure that we got to an ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Albert Arnold <b>Gore</b> Sr. (December 26, 1907 – December 5, 1998) was an American politician who served as a United States Senator from Tennessee from 1953 to 1971. A member of the Democratic Party, he previously served as a U.S. Representative from the state&#39;s 4th congressional district from 1939 to 1953. He was the <b>father</b> <b>of Al</b> <b>Gore</b>, who served as the 45th vice president of the United States ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Albert Arnold <b>Gore</b> Jr. was born on March 31, 1948, in Washington, D.C. His <b>father</b>, Albert <b>Gore</b> Sr., served in the House of Representatives and the Senate for more than two decades.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Kurfürstendamm. The best known and most popular <b>shopping</b> <b>street</b> in Berlin runs from Breitscheidplatz to Halensee and is home to multiple department stores and shops for major chains. As you move west towards Halensee, the boutiques become classier and the window displays more luxurious. Stroll past the showcases of international fashion ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '6. Wilmersdorfer Straße. Wilmersdorfer Straße is located in Charlottenburg, Berlin and is Berlin&#39;s oldest pedestrian zone. It can be reached by the U7 U-Bahn line, at the Wilmersdorfer Straße stop. In addition to well-known department store chains, numerous smaller stores invite you to shop, eat, and stroll.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>street</b> is long ,it&#39;s the equivalent of the Champs Elysees in Paris with an amazing amount of <b>shopping</b> outlets from high end shops such as Gucci ,Luise Vuitton and Hermes to more conventional boutique shops plus you have huge <b>shopping</b> centres such as Europa Centre and the slighly more upmarket and famous KaDeWe department store but there is so much more to this <b>street</b> than shops.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Wilson <b>died</b> of a heart attack in <b>Manchester</b>&#39;s Christie Hospital on 10 <b>August 2007</b> <b>aged</b> <b>57</b>. Following the news of <b>his</b> death, the Union Flag on <b>Manchester</b> Town Hall was lowered to half mast as a mark of respect. Probate documents reveal <b>his</b> estate was valued at £484,747 after tax. That figure includes the value of <b>his</b> city centre flat on Little ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Tony Wilson, the presenter, journalist and music mogul, has <b>died</b> <b>aged</b> <b>57</b>. The Salford-born <b>entrepreneur</b> founded Factory <b>Records</b> in 1978 along with Alan Erasmus, <b>producer</b> Martin Hannet and renowned ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'This is the tale of a man who, in <b>his</b> passport, simply had the word ‘<b>entrepreneur</b>’. Who was born in Salford and educated at Cambridge but who was firmly rooted in <b>Manchester</b> when the ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Dusty had no emotional integrity… if a contestant fell flat on their face, Dusty would smile, if they won the booby-<b>prize</b> of a brand new <b>dustbin</b>, ... The <b>quiz</b> <b>show</b> ran from 1978 – 1988 and was shown on ITV. The <b>show</b> was based on the spanish gameshow called Un, dos, tres … responda, otra vez. 3-2-1 would regularly pull-in 17 million ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>3–2</b>–1 was a British <b>game</b> <b>show</b> that was made by Yorkshire <b>Television</b> for ITV.It ran for ten years, from 29 July 1978 to 24 December 1988, with Ted Rogers as the host.. It was based on a Spanish gameshow called Un, dos, tres... responda otra vez and was a trio of three <b>shows</b> in one: a <b>quiz</b>, variety and a <b>game</b> <b>show</b>.. The <b>show</b> was a huge success, consistently pulling in large ratings.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Synopsis Three for the price of one . Once entertaining (but in retrospect, truly cringeworthy) prime-time <b>show</b>, which was Yorkshire <b>TV</b>&#39;s greatest contribution to ITV <b>game</b> <b>shows</b> before Countdown hit the airwaves. Divided into three key parts - first, the rather jolly <b>quiz</b> where each of the three couples were given two rounds of 10 questions, their money being equal to 10 x first round score x ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Walker</b> was <b>born</b> on September 12, 1973, in Glendale, California. His mother, Cheryl (née Crabtree), was a fashion model, and his father, <b>Paul</b> William <b>Walker</b> III, was a sewer contractor and former amateur boxer who was a two-time Golden Gloves champion.<b>Walker</b>&#39;s paternal grandfather, William, had a short-lived boxing career as &quot;Irish&quot; Billy <b>Walker</b>, while another raced factory cars for Ford in ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Birth date: September 12, 1973. Birth State: California. Birth <b>City</b>: Glendale. Birth Country: United States. Gender: Male. Best Known For: <b>Paul</b> <b>Walker</b> was an American actor who came to fame in ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Paul</b> <b>Walker</b> was <b>born</b> on September 12, 1973, in Glendale, California. <b>Paul</b> William <b>Walker</b> IV, the multi-talented actor and philanthropist, entered this world in the picturesque <b>city</b> of Glendale, California. He started his acting career at a young age.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Into the Woods</b> is a 1986 musical with music and lyrics by Stephen Sondheim and book by James Lapine.. The musical intertwines the plots of several Brothers Grimm fairy tales, exploring the consequences of the characters&#39; wishes and quests.The main characters are taken from &quot;Little Red Riding Hood&quot; (spelled &quot;Ridinghood&quot; in the published vocal score), &quot;Jack and the Beanstalk&quot;, &quot;Rapunzel ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Into the Woods</b>. STEPHEN SONDHEIM AND JAMES LAPINE 1986. INTRODUCTION <b>AUTHOR</b> BIOGRAPHY PLOT SUMMARY CHARACTERS THEMES STYLE HISTORICAL CONTEXT CRITICAL OVERVIEW CRITICISM SOURCES FURTHER READING INTRODUCTION. <b>Into the Woods</b>, published in 1986, is a collaborative work by Stephen Sondheim (music and lyrics) and James Lapine (story).It was the product of a workshop at Playwrights Horizon in New ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Into</b> <b>the Woods</b> is a witty and whimsical stage musical by composer and lyricist Stephen Sondheim and playwright-director James Lapine. A dark comedy, the story <b>of Into</b> <b>the Woods</b> takes inspiration from the more melancholy and frightful elements of original folk and fairy tales—including the deaths of major characters—and features plot elements and figures from the stories of Cinderella, Jack ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Actor <b>Phoebe</b> Waller-Bridge has hailed a “new dawn” for the Edinburgh <b>Festival</b> Fringe, as the biggest ever shake-up of the event aims to crack down on exploitation, reduce its impact on the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Phoebe</b> Waller-Bridge says the <b>festival</b> wants to maintain its creative freedom. <b>Phoebe</b> Waller-Bridge says Edinburgh&#39;s Fringe <b>festival</b> is entering a &quot;new dawn&quot; as it marks its 75th anniversary. The ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Support trulyindependent journalism. <b>Phoebe</b> Waller-Bridge has said debuting her hit show Fleabag at the Edinburgh Fringe <b>Festival</b> changed her life and career in a new documentary which explores ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Lewis Carroll (born January 27, 1832, Daresbury, Cheshire, England—died January 14, 1898, Guildford, Surrey) was an English logician, mathematician, photographer, and novelist, especially remembered for <b>Alice</b>’s Adventures in Wonderland (1865) and its sequel, Through the Looking-Glass (1871). His poem The Hunting of the Snark (1876) is ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Alice</b>&#39;s Adventures in Wonderland (also known as <b>Alice</b> in Wonderland) is an 1865 English children&#39;s novel by Lewis Carroll, a mathematics don at the University of Oxford.It details the story of a girl named <b>Alice</b> who falls through a rabbit hole into a fantasy world of anthropomorphic creatures. It is seen as an example of the literary nonsense genre. The artist John Tenniel provided 42 wood ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Franziska Kohlt is a researcher in 19th-century history of science and literature. She is the <b>author</b> of numerous articles on Lewis Carroll, Victorian culture and science, and the forthcoming <b>Alice</b> Through the Wonderglass: The unexpected histories of a children’s classic (Reaktion 2024), and editor of The Lewis Carroll Review, and the Through ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The Answer <b>Run</b> (25x45’) was commissioned by Rob Unsworth, Head of BBC Daytime and Early Peak commissioning, the Commissioning Editor is Alex McLeod. This is a BBC Studios Entertainment ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The former <b>director</b> of a Norfolk crane hire company is on the <b>run</b> after fleeing the country weeks before he was found guilty of multiple child sex offences, it can be revealed. Oliver Arnold, 49 ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Nuns on the <b>Run (1990</b>) cast and crew credits, including actors, actresses, directors, writers and more. Menu. Movies. ... Second Unit <b>Director</b> or Assistant <b>Director</b> . David Brown ... first assistant <b>director</b> Neil Calder ... third assistant <b>director</b> Gus Maclean ... second assistant <b>director</b> ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'NATO &#39;actively addressing&#39; alleged <b>cyberattack</b> <b>affecting</b> some <b>websites</b> The North Atlantic Treaty <b>Organization</b> (NATO) said it is investigating claims that data was stolen from <b>unclassified</b> <b>websites</b> under the military alliance’s control.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'NATO says it is “actively addressing incidents” <b>affecting</b> <b>its</b> <b>unclassified</b> <b>websites</b> after a hacking group claimed to have stolen numerous strategic planning and research documents from the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Senior government officials are racing to limit the <b>impact</b> of what&#39;s believed to be a global <b>cyberattack</b> <b>affecting</b> U.S. federal agencies and allies, including NATO member countries.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The total <b>box</b> <b>office</b> was down 2.55% to $11.09 billion, which is the third-biggest yearly <b>box</b> <b>office</b> total of all time, behind $11.12 billion earned in 2015 and $11.37 billion earned in 2016. Meanwhile, ticket prices rose 4% to $8.97. This means total attendance was 1.236 billion, the lowest since 1995.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'While the <b>opening</b> <b>weekend</b> for the <b>The Marvels</b> release is only slightly lower than that of the franchise&#39;s second-lowest <b>opening</b>, this is still a bad sign for the new MCU movie. When adjusted for inflation, The Incredible Hulk&#39;s domestic <b>opening</b> <b>weekend</b> actually lands at around $80 million, widening the gulf between the two movies even more.That $110 million worldwide <b>opening</b> <b>weekend</b> doesn&#39;t ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Well, looking at this week&#39;s <b>box</b> <b>office</b> numbers, it might seem so. &quot;<b>The Marvels</b>,&quot; the 33rd installment in the Marvel Cinematic Universe and the sequel to the hugely successful 2019 <b>film</b> &quot;Captain ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Martin Scorsese, for one, recently admitted that he re-watches Fellini’s 1963 masterpiece 8 1/2 every year. “8 1/2 has always been a touchstone for me, in so many ways,” he said. “The ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Five years later Germi created his most successful film, Divorce, <b>Italian</b> <b>Style</b> starring Marcello Mastroianni, which won an Academy Award and, unlike Germi’s previous films, was a hilariously dark comedy. Mastroianni plays an unhappily married man who develops an obsession over his much younger cousin, but in order to pursue her in a socially ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The Tree of Wooden Clogs (1978) Ermanno Olmi was born on 24 July 1931 in Bergamo, Lombardy, Italy. He was a director and writer, known for The Tree of Wooden Clogs (1978), The Legend of the Holy Drinker (1988) and Il posto (1961). He was married to Loredana Detto. He died on 5 May 2018 in Asiago, Veneto, Italy.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Karl Bartholomaeus Heller</b> (20 November 1824 – 14 December 1880) was an Austrian botanist and naturalist who explored Mexico in 1845–48 and published his memoir. ... <b>Born</b> in Moravia, <b>Heller</b> was a professor at the Theresianum in Vienna. Among <b>Heller</b>&#39;s later works is his defense of Darwinism, ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Karl</b> <b>Bartholomaeus</b> In the latter year Johann Jakob Heckel published the livebearing freshwater Green swordtail , since the early 20th century a common aquarium fish, from specimens <b>Heller</b> deposited in Vienna. <b>Born</b> in Moravia, <b>Heller</b> was a professor at the Theresianum in Vienna. Among <b>Heller</b>&#39;s later works is his defense of Darwinism, Darwin und der Darwinismus, 1869.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Karl</b> <b>Bartholomaeus</b> <b>Heller</b> (20 November 1824 – 14 December 1880) was an Austrian botanist and naturalist who explored Mexico in 1845–48 and published his memoir. In the latter year Johann Jakob Heckel published the livebearing freshwater Green swordtail (Xiphophorus helleri), since the early 20th century a common aquarium fish, from specimens <b>Heller</b> deposited in Vienna.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Am</b> is rather neglected now, but there&#39;s still some good <b>stations</b> to listen to on it. I receive these quite well in the car.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Frequency Finder - England, Scotland, Wales and Ireland. Welcome to Frequency Finder, a website providing details of all <b>radio</b> <b>stations</b> in England, Scotland, Wales and Ireland with features on <b>radio</b> transmission and history. All listings include frequencies, coverage areas, format and ownership. Transmission details, historical notes and maps ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'FM and <b>AM</b> <b>Radio</b> Frequencies Below is a list of all the BBC&#39;s <b>radio</b> services that are available on FM and <b>AM</b>. We have listed the frequency ranges, if <b>you</b> want to know the exact frequency for <b>your</b> ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Every <b>genre</b> you care to name has a classic that references <b>fire</b> in some way: Soul, rock ‘n’ roll, hip-hop, electronic music, country, and many more besides.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '11. “Setting The World On <b>Fire</b>” By Kenny Chesney. Recorded by country music artist Kenny Chesney as a duet with pop singer Pink, “Setting the World on <b>Fire</b>” describes the story of a developing relationship and the feelings and emotions that specific time carries. It’s about two people on the brink of falling in love.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Fire</b>-Themed Songs Across Music <b>Genres</b>. <b>Fire</b> is a prevalent theme in music across different <b>genres</b>, from rock to pop, country, blues, and more. Here are some examples of <b>fire</b>-themed songs and how different <b>genres</b> approach the topic of <b>fire</b>. Rock:', 'score': 'N/A'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 126/126 [00:11<00:00, 10.96it/s, Generation Speed: 61.87 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['21:11', 'Hungary', 'United States', 'Jesse Lasky', 'Football.', 'born', 'Arthur \"Big Boy\" Crudup.', 'Lesley Garrett', 'Emile Mosseri', 'No answer can be provided based on the given documents as there is no information regarding specific dates or times of people attending the gym. The documents only provide general statistics', 'January 1', 'Fantasy', 'Not mentioned.', 'Patience Strong', 'Football', '305,000 + 21,000 = 326,000', '5', 'American jockey', 'Malaysia', 'There is no information about Faith attending a Book club meeting on 2022/03/26 in the given documents.', 'Stephen Cram', 'Ocean Beach in San Francisco.', 'Alfred Bingham', 'Joseph L. Mankiewicz', 'TELLY', 'Owen Sheers', 'There is no mention of Adrian attending a culinary festival in the provided documents.', 'Finisterre', '2010', 'Asia Minor', 'Football', 'Ski jumping.', 'State elections in 1898.', 'Monte Cervino', 'Argentina', 'The composer of Pole is Dante Alighieri.', \"Baldur's Gate 3\", 'footballer', 'Krapf', 'Football', 'Rep. George Santos', 'Canada', 'The answer is not present in the given documents.', 'Football', 'Erin Hunter.', 'Sarah Meek', 'Not specified.', 'R&B and funk', 'Francesco Guardi', 'Belorechensk', 'George R.R. Martin', 'píobaí uilleann', 'Catholic', 'Rasskazovo', 'Vienna', 'The Netherlands.', 'Kerry Irving', 'Germano Almeida.', 'There is no information in the given documents that suggests Georgina attended the Indie film festival on 2022/04/30.', 'Los Angeles', 'India', 'Lake Placid', 'Edward', 'General Motors', 'Rio de Janeiro', 'Beach Soccer', 'Jon Watts', 'Medzilaborce', 'Lawrence Gough', 'Catholic', 'Bruce Campbell', 'Morocco', 'the town of Polkowice', 'Kansas City, Kansas.', 'Jean-Pierre Rampal', '467,418', 'Avi Boodram', 'Ice hockey.', 'cricketer', 'The given documents do not mention Washington.', 'Niall Ferguson', 'Kermadec islands', '$20', 'Elizabeth Jane Howard', 'Italy', 'Warri', 'Tikhvin', 'Leonard Carpenter', 'Jason Blum.', 'Saraburi', 'A. Ice Skating', 'Jeffrey Porcaro.', 'Roman Catholic', 'Lee Jong-pil', 'Cato Fong.', '2', '1943', '74', 'There is no Stephen Connor in the provided documents. The name mentioned is Stephen Maguire, who plays snooker.', 'Mexico City', 'Argentina', 'Geneva', 'Massachusetts', 'Plesetsk', 'There is no genre mentioned in the given documents labeled as \"Cut\".', 'Not mentioned in the documents.', '1921', 'Elon Musk.', 'Roger Donaldson', 'Valentine Dyall', 'Honda', 'Albert Gore Sr.', 'Berlin', 'Tony Wilson', '3-2-1', 'Glendale', 'Stephen Sondheim and James Lapine', 'There is no information provided about Phoebe attending a \"Food festival\" on 2022/01/10 in any of the given documents. The documents', 'Lewis Carroll', 'From Doc 1: Alex McLeod', 'NATO', '$110 million', 'The producer of Italian Style is not mentioned in the given documents.', 'Moravia', '13', 'Fire']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 74/74 [00:01<00:00, 57.66it/s, Generation Speed: 218.98 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'em': 0.435, 'f1': 0.4799999999999999, 'sub_em': 0.525, 'precision': 0.4829166666666667, 'recall': 0.48750000000000004}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<flashrag.dataset.dataset.Dataset at 0x7f003c1661c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flashrag.dataset.utils import split_dataset, merge_dataset\n",
    "from flashrag.pipeline import SequentialPipeline\n",
    "from flashrag.utils import get_dataset\n",
    "# from flashrag.utils import get_retriever, get_generator, get_refiner, get_judger\n",
    "from flashrag.prompt import PromptTemplate\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def run_judger_rnn(classifiers, dataset, split, do_eval=True, pred_process_fun=None):\n",
    "    # judge_result: list of bool element, representing whether to use retrieval\n",
    "    judge_result = judger_rnn(dataset, classifiers)\n",
    "    print()\n",
    "    print('Judge result: ',judge_result)\n",
    "    print()\n",
    "    config= set_config(dataset_name=dataset, split = split)\n",
    "\n",
    "    template= PromptTemplate(\n",
    "        config = config,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "\n",
    "    all_split = get_dataset(config)\n",
    "    dataset = all_split[split]\n",
    "    dataset.update_output('judge_result', judge_result)\n",
    "\n",
    "    # split dataset based on judge_result\n",
    "    pos_dataset, neg_dataset = split_dataset(dataset, judge_result)\n",
    "    pipeline = SequentialPipeline(config)\n",
    "\n",
    "    print()\n",
    "    print('Questions that NEED retrieval',len(pos_dataset))\n",
    "    print('Questions that does NOT need retrieval',len(neg_dataset))\n",
    "    print()\n",
    "\n",
    "    pos_dataset = pipeline.run_internet_retrieval(pos_dataset, do_eval=False)\n",
    "    pipeline.prompt_template = template\n",
    "    neg_dataset = pipeline.naive_run(neg_dataset, do_eval=False)\n",
    "\n",
    "    # merge datasets into original format\n",
    "    dataset = merge_dataset(pos_dataset, neg_dataset, judge_result)\n",
    "\n",
    "    dataset = pipeline.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "run_judger_rnn(['intent_aware', 'time_aware', 'knowledge_aware'], dataset ='retrievalqa_200', split = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_judger_rnn(classifiers, dataset, split, do_eval=True, pred_process_fun=None)\n",
    "\n",
    "- with retrievalqa_200 (length: 200)\n",
    "\n",
    "{'em': 0.435, 'f1': 0.4799999999999999, 'sub_em': 0.525, 'precision': 0.4829166666666667, 'recall': 0.48750000000000004}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline - TriGate Bidir-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length small_dataset:  200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_305021/692185233.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  hidden_states_tensor= torch.tensor(hidden_states, dtype=torch.float32)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Judge result:  [False False False False  True False False  True  True False False  True\n",
      " False  True  True False False False False False False  True  True False\n",
      " False  True False False  True False False False  True False  True  True\n",
      " False False False False False False False False False False  True False\n",
      "  True False False False False False False False  True False False False\n",
      "  True  True False  True False False  True  True False False  True False\n",
      "  True False False False False  True False  True False False  True False\n",
      " False False False False False False False  True False  True False  True\n",
      " False  True False False  True  True False False False  True  True False\n",
      " False  True False False False False False False False  True  True  True\n",
      " False False False  True  True False  True  True False  True False False\n",
      " False False  True  True False False  True False False False False  True\n",
      "  True False False  True False False False  True False False  True False\n",
      "  True False False False  True False False  True  True False  True False\n",
      " False  True  True False  True False  True False False False False False\n",
      "  True  True  True False False False False False False  True False False\n",
      " False  True False  True False False False False]\n",
      "\n",
      "Find `question` in template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find `reference` in template\n",
      "Find `question` in template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 11:47:27,655\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 11:47:27 config.py:1130] Casting torch.float32 to torch.float16.\n",
      "INFO 08-08 11:47:27 config.py:1151] Downcasting torch.float32 to torch.float16.\n",
      "INFO 08-08 11:47:27 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/cs/student/projects2/dsml/cdiezmar/models/llama3-8B-instruct', speculative_config=None, tokenizer='/cs/student/projects2/dsml/cdiezmar/models/llama3-8B-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/cs/student/projects2/dsml/cdiezmar/models/llama3-8B-instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 11:47:31 model_runner.py:146] Loading model weights took 14.9634 GB\n",
      "INFO 08-08 11:47:34 gpu_executor.py:83] # GPU blocks: 1656, # CPU blocks: 2048\n",
      "INFO 08-08 11:47:35 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-08 11:47:35 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-08 11:47:40 model_runner.py:924] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions that NEED retrieval 134\n",
      "Questions that does NOT need retrieval 66\n",
      "[[{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Natalie cut in: &#39;Oh no, no. They&#39;ll know, because I&#39;m going to make it like a whole thing. I&#39;m not just going to pop up one day. We&#39;re going to do a &quot;Get ready with me to get my boobs done.&quot;&#39;', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Benjamin</b> Franklin: With Peter Coyote, Mandy Patinkin, Josh Lucas, Liam Neeson. Exploring the life and work of writer and publisher, scientist and inventor, diplomat and signer of both the Declaration of Independence and the United States Constitution: <b>Benjamin</b> Franklin.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Franklin episode 1, &quot;Sauce for Prayers&quot; &quot;As the American Revolution hangs in the balance, <b>Benjamin</b> Franklin and his grandson navigate a treacherous web of secrets and lies to secure the nation&#39;s freedom.&quot; Read WTW&#39;s Franklin episode 1 recap. Franklin episode 2, &quot;Welcome, Mischief&quot; &quot;An arms operation is threatened by a ruthless opponent.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Budapest</b> is <b>the capital</b> and most populous city of Hungary.It is the ninth-largest city in the European Union by population within city limits and the second largest city on the Danube river. The city has an estimated population of 1,752,286 over a land area of about 525 square kilometres (203 square miles). <b>Budapest</b>, which is both a city and municipality, forms the centre of the <b>Budapest</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Budapest</b>, city, <b>capital</b> of Hungary, and seat of Pest megye (county). The city is the political, administrative, industrial, and commercial centre of Hungary. The site has been continuously settled since prehistoric times and is now the home of about one-fifth of the country’s population. Area city, 203 square miles (525 square km).', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Budapest</b>. <b>Budapest</b> is <b>the capital</b> of Hungary, a small, landlocked country in Central Europe.It is made up of two historic parts, Buda and Pest, which were once separate cities. In the 19 th century, however, the two cities were merged, creating the city now known as <b>Budapest</b>. The city became <b>the capital</b> of Hungary after being the seat of power for several kingdoms, including the Roman province ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Giant</b> logo used before re-branding in 2020. Some stores still use this logo as of 2024. The <b>Giant</b> Company (formerly known as <b>Giant</b> <b>Food</b> Stores) is an American regional supermarket chain that operates in Pennsylvania, Maryland, Virginia, and West Virginia under the <b>Giant</b> and Martin&#39;s brands. It is a subsidiary of Ahold Delhaize, and headquartered in Carlisle, Pennsylvania.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Giant</b> <b>Food</b>. Netherlands. <b>Giant</b> <b>Food</b> Company Stats. As of April 2024. Industry Grocery <b>Country</b>/Territory Netherlands. Forbes Lists #231. Best Brands For Social Impact (2024) Grocery. Related People ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The world&#39;s largest artichoke and other <b>giant food</b>. Bigger is better. The road trip is an American rite of passage. Developing throughout the 20th century, the national highway system now forms a ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Thing We Love</b>: Directed by Lou Tellegen. With Wallace Reid, Kathlyn Williams, Tully Marshall, Mayme Kelso. Just prior to America&#39;s declaration of war, Margaret Kenwood of the Kenwood Manufacturing Company determines that the plant should produce munitions to support the Allies. Rodney Sheridan, her sweetheart and a vice president of the company, remains unimpressed with Margaret&#39;s ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Thing</b> <b>We</b> <b>Love</b>. The <b>Thing</b> <b>We</b> <b>Love</b> is a 1918 American silent drama film starring Wallace Reid, Kathlyn Williams, and Tully Marshall, produced by Jesse Lasky, distributed by Paramount Pictures, and directed by Lou Tellegen. This marked Tellegen&#39;s second foray into directing as he usually was a leading man in front of the camera like Reid.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>Thing</b> <b>We</b> <b>Love</b> (1918) cast and crew credits, including actors, actresses, directors, writers and more. Menu. Movies. Release Calendar Top 250 Movies Most Popular Movies Browse Movies by Genre Top Box Office Showtimes &amp; Tickets Movie News India Movie Spotlight. TV Shows.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'June 30, 2018. by Upen. 5 min read. The main difference between <b>home country</b> and host <b>country</b> is that the <b>home</b> <b>country</b> refers to the <b>country</b> where a person was born while the host <b>country</b> refers to the <b>country</b> where a person resides. Migration has urged former identities and concepts to be redefined. When a person moves to a different <b>country</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'After Ms Hasina fled the <b>country</b>, social media was flooded with reports of Hindu properties and temples being attacked. India&#39;s Foreign Minister S Jaishankar told parliament on Tuesday: &quot;What was ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'A <b>home</b> <b>country</b> is generally associated with the nation where an individual is born, raised, or has long-term residency. Conversely, a host <b>country</b> refers to a nation where an individual temporarily resides, often due to work, study, or travel. Companies might be headquartered in their <b>home</b> <b>country</b> but can have branches or operations in ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The meaning behind “<b>That’s</b> All <b>Right</b>” goes beyond its catchy melody and toe-tapping rhythm. At its core, the song speaks to the acceptance of a failed relationship and the determination to move on. It portrays the resilience and optimism of a person who understands that sometimes, ending a relationship is necessary for personal growth and ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '‘<b>That’s</b> All <b>Right</b>’ began to gain traction around Memphis and soon migrated down south to the Louisiana Hayride, a country music radio programme that was open to playing blues and R&amp;B. The house drummer at the Hayride was D.J. Fontana, who provided Presley with his first backup of percussion. The pieces of Presley’s initial rock and roll ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Hildur Guðnadóttir. Born to a family of musicians, the Icelandic <b>composer</b>, cellist, and vocalist Hildur Guðnadóttir is best known for her award-winning film and TV scores. In 2019, her score for the film Joker, starring Joaquin Phoenix and Robert de Niro, won both the Golden Globe Award and Oscar for Best Original Score, as well as a BAFTA for Best Original Music.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Lesley Garrett <b>CBE</b> is Britain’s best known soprano, regularly appearing in <b>opera</b>, music theatre, concert, on television and CD. ... Recent albums are Travelling Light, The <b>Singer</b>, So Deep is the Night and Amazing Grace. ... Lesley <b>was awarded</b> <b>a CBE</b> in the <b>2002</b> New Year’s Honours List for Services to Music and is a Fellow of the Royal ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'In a wide-ranging career, Lesley Garrett has achieved international recognition as an <b>opera</b> <b>singer</b>, recording artist, musical theatre performer and television personality. ... She <b>was awarded</b> <b>a CBE</b> in the <b>2002</b> New Year’s Honours List for Services to Music and is a Fellow of the Royal Academy of Music and an RAM Board Member. She holds ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The 64-year-old has travelled around the world singing <b>opera</b> and pop classics, eventually being <b>awarded</b> <b>a CBE</b> for her services to music <b>in 2002</b>. ... But why does the successful <b>singer</b> and TV star ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'When Emile Mosseri stepped in as the <b>composer</b> <b>of Homecoming</b>, he had gigantic shoes to fill.The first season of the Amazon Prime series, based on the popular Gimlet podcast, was “scored” by the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'omecoming&quot; by Bruce DaweDawe here dramatises the <b>homecoming</b> of Australian veter. ns&#39; bodies from Vietnam. This is clearly an anti-war poem, reproducing in the seventies the sentiments of t. e First World War poets.In 25 lines of broken verse presented in one demanding stanza, Dawe recounts how &quot;they are bringing&quot; home the bodies &quot;in deep freeze ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Hagood Hardy. Hugh Hagood Hardy, CM, <b>composer</b>, arranger, vibraphonist, pianist, percussionist (born 26 February 1937 in Angola, Indiana; died 1 January 1997 in Hamilton, Ontario). A major figure in the Canadian recording industry, Hagood Hardy had a successful jazz career, <b>composed</b> extensively for film and television, and had gold and platinum ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'What is the state of <b>fitness</b> in the UK and how does <b>gym</b> use differ from region to region? Find this and more out in our new report here.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'With women making up a large percentage of <b>fitness</b> classes, <b>gym</b> memberships and more, it’s vital that we consider the trends and statistics of women as a <b>fitness</b> demographic. Our expert guide compiles the key women’s <b>fitness</b> statistics that could impact <b>2022</b> and beyond.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Free <b>calculator</b> to get the number of hours, minutes, and seconds <b>between</b> two times. Also, a full version to calculate the time duration <b>between</b> two dates.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'In years when January 1st (New Year&#39;s Day) is on a Saturday, the <b>holiday</b> is observed on the preceding day (December 31st). As a result, December 2021 will have two <b>federal holidays</b>. List of 2024 and 2025 <b>Federal Holidays</b>. There are eleven <b>federal holidays</b> recognized by the <b>US</b> government. View all <b>holidays</b> and download printable <b>holidays</b> table.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Federal</b> law (5 <b>U.S</b>.C. 6103) establishes the public <b>holidays</b> listed in these pages for <b>Federal</b> employees. Please note that most <b>Federal</b> employees work on a Monday through Friday schedule. For these employees, when a <b>holiday</b> falls on a nonworkday -- Saturday or Sunday -- the <b>holiday</b> usually is observed on Monday (if the <b>holiday</b> falls on Sunday ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Federal</b> <b>Holidays</b> of the USA by Year Copyright © 2017 Lylesoft LLC | Contact <b>Us</b> | About List of USA USA <b>Federal Holidays</b> 2024', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Genre</b>(s) Fantasy MUD: Mode(s) Multiplayer: A screenshot from MUD1. Multi-User Dungeon, or MUD (referred to as MUD1, to distinguish it from its successor, MUD2, and the MUD <b>genre</b> in general), is the first MUD. History. MUD was created in 1978 by Roy Trubshaw and Richard Bartle at the University of Essex on a DEC PDP-10.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'MUD1 <b>(1978</b>) 5 comments. Multi-User Dungeon, or MUD (referred to as MUD1, to distinguish it from its successor, MUD2, and the MUD <b>genre</b> in general) is the first MUD and the oldest virtual world in existence. It was created in 1978 by Roy Trubshaw at Essex University on a DEC PDP-10 in the UK, using the MACRO-10 assembly language.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'To truly understand MUD1 (the game which spawned the entire <b>genre</b> of Multi-User Dungeons), we need to go back to 1976 and the release of the first text adventure game: “Colossal Cave Adventure” (or simply ADVENT as it was known), developed by William Crowther.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>John</b> Edward <b>Robinson</b> (<b>born</b> December 27, 1943) is an American convicted serial killer, kidnapper, rapist, and forger. He was found guilty and received the death penalty in 2003 for three murders committed in Kansas.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'One of the earliest cases of this was a man named <b>John</b> Edward <b>Robinson</b>, whose devious but prolific use of chat rooms, digital fraud, and catfishing earned him the label of the internet&#39;s first serial killer. <b>Robinson</b>&#39;s rampant theft and murder spree spanned several decades and the exact number of his victims remains unknown.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>John</b> Robert Campbell <b>Robinson</b> (<b>born</b> 29 August 1971) is a former professional footballer who played as a midfielder. He made over 400 appearances during his professional career with Brighton &amp; Hove Albion, Charlton Athletic, Cardiff <b>City</b> and Gillingham and also won 30 caps for Wales.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Patience</b> Agbabi was born in London to Nigerian parents. From a young age, she was privately fostered by a white English family and moved at the age of 12 from Sussex to North Wales, where she was then raised in Colwyn Bay. She studied English language and literature at Pembroke College, Oxford.. She earned an MA in Creative Writing, the Arts and Education from the University of Sussex in 2002 ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Few people know who said &quot;<b>Patience</b> is a virtue&quot; but the first publishing appears to come from the poem, &quot;Piers Plowman&quot; in the 14th century. ... Like many of the famous sayings we recite today, the original <b>author</b> <b>of &quot;Patience</b> is a virtue&quot; is hard to pin down. Some date it back to Cato to the Elder in the third or fourth century.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Pearl Curran, about 1926. <b>Patience</b> Worth was allegedly a spirit contacted by Pearl Lenore Curran (February 15, 1883 – December 2, 1937). This symbiotic relationship produced several novels, poetry and prose which Pearl Curran claimed were delivered to her through channelling the spirit <b>of Patience</b> Worth.. Psychologists and skeptics who have studied Curran&#39;s writings are in agreement that ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Their images are now available to the public in the US, after <b>Disney&#39;s</b> <b>copyright</b> <b>expired</b>. It means creatives like cartoonists can now rework and use the earliest versions <b>of Mickey</b> <b>and Minnie</b>. In ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The original story of <b>Winnie</b>-<b>the-Pooh</b>, created by English author A. A. Milne and illustrator E. H. Shepard, entered the public domain in 2022. The move effectively ended <b>Disney&#39;s</b> exclusive use of the character and led to a low-budget horror movie <b>Winnie</b> <b>the Pooh</b>: Blood and Honey.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Winnie</b> <b>the Pooh</b>, which became another beloved <b>Disney</b> character, actually entered the public domain in 2022 after the <b>copyright</b> on A.A. Milne’s original stories about the bear <b>expired</b>. Last year ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Alain Laurier</b> (French pronunciation: [alɛ̃ loʁje]) (12 September 1944 – 25 December 2023) was a French football manager and player. Career. <b>Laurier</b> was born in Créteil (Val-de-Marne). He made his debut for Cœuilly, which became Reims. At the Champigny club, he played alongside big names such as Raymond Kopa and Lucien Muller. He played ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Alain LAURIER</b>. Team France. Games Participations 1. First Olympic Games Mexico City 1968. Year of Birth 1944. Olympic Results.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Alain</b>•<b>Laurier</b>. Used name. <b>Alain</b>•<b>Laurier</b>. Born. 12 September 1944 in Créteil, Val-de-Marne (FRA) Died. December 2023. Measurements. 176 cm / 70 kg.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Many</b> of the <b>borrowers</b> <b>received</b> <b>bills</b> with charges higher than they should be. The Department of Education identified the errors and directed <b>student</b> <b>loan</b> servicing companies to place the impacted ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'More than 28 million federal <b>student</b> <b>loan</b> <b>borrowers</b> returned to repayment this month after a pandemic-related relief program put their monthly <b>bills</b> on pause for nearly four years.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The Education Department said less than 1% of <b>student</b>-<b>loan</b> <b>borrowers</b> were affected by payment issues as <b>bills</b> started becoming due.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Republican</b> National Committee invited eight <b>candidates</b> to the first <b>debate</b> in August. <b>Candidates</b> qualified by getting 40,000 donors (with 200 unique donors in 20 different states) and by ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The third <b>Republican</b> <b>presidential</b> primary <b>debate</b> of the 2024 campaign cycle has begun in Miami, Florida. The <b>debate</b>, hosted by NBC News, marks a new phase in the GOP race, as the stage narrows to ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'CHRIS CHRISTIE. <b>Republican</b> U.S. <b>presidential</b> <b>candidate</b> and former New Jersey Gov. Chris Christie addresses The Faith and Freedom Coalition&#39;s 2023 &quot;Road to Majority&quot; conference in Washington, U.S ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Alphonse Gabriel <b>Capone</b> (/ k ə ˈ p oʊ n /; January 17, 1899 – January 25, 1947), sometimes known by the nickname &quot;Scarface&quot;, was an <b>American</b> <b>gangster</b> and businessman who attained notoriety during the Prohibition era as the co-founder and boss of the Chicago Outfit from 1925 to 1931. His seven-<b>year</b> reign as a crime boss ended when he went to prison at the age of 33.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Al <b>Capone</b> (born January 17, 1899, Brooklyn, New York, U.S.—<b>died</b> January 25, 1947, Palm Island, Miami Beach, Florida) was an <b>American</b> Prohibition-era <b>gangster</b>, who dominated organized crime in Chicago from 1925 to 1931 and became perhaps the most famous <b>gangster</b> in the United States.. <b>Capone</b>’s parents immigrated to the United States from Naples in 1893.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'When you think <b>gangster</b>, you think Al <b>Capone</b>. The man is widely recognised as one of the most notorious crime bosses that has ever existed throughout history. <b>Capone</b>’s rise to power and infamy came, mainly, as a direct result of Prohibition in the United States – a nationwide constitutional ban on the production, importation, transportation, and sale of alcoholic beverages from 1920 to 1933.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Dominick Bellizzi</b> ( c. 1912 – 17 May 1934) was an American jockey who died at age 21 as a result of a horse racing accident. He was known as &quot;The Duke&quot;. [2] <b>Bellizzi</b> was born in New York to Albanian immigrants Samuel and Teresa <b>Bellizzi</b>. An up-and-coming young jockey in Thoroughbred racing, during 1933 <b>Bellizzi</b> rode to victory in the Futurity ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Dominick</b> <b>Bellizzi</b> was an American jockey who died at age 21 as a result of a horse racing accident. He was known as &quot;The Duke&quot;.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Dominick</b> <b>Bellizzi</b> joined ClassDojo in 2016. <b>Dominick</b> <b>Bellizzi</b> is currently Chief Technology Officer at ClassDojo - View - ClassDojo org chart. Create your alert to follow the career of <b>Dominick</b> <b>Bellizzi</b>.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Kuala Lumpur</b> was the founding <b>capital</b> of the Federation of Malaya and its successor, Malaysia. The city remained the seat of the executive and judicial branches of the Malaysian federal government until these were relocated to Putrajaya in early 1999. [10] However, some sections of the political bodies still remain in <b>Kuala Lumpur</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Kuala Lumpur</b>, <b>capital</b> of Malaysia. The city is located in west-central Peninsular (West) Malaysia, midway along the west coast tin and rubber belt and about 25 miles (40 km) east of its ocean port, Port Kelang, on the Strait of Malacca. It is the country’s largest urban area and its cultural, commercial, and transportation centre.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'History of <b>Kuala Lumpur</b> <b>Kuala</b> <b>Lumpur</b> in 1971. ... <b>Kuala</b> <b>Lumpur</b> is the largest city in Malaysia; it is also the nation&#39;s <b>capital</b>. The history of <b>Kuala Lumpur</b> began in the middle of the 19th century with the rise of the tin mining industry, and boomed in the early 20th century with the development of rubber plantations in Selangor.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Double platinum and BRIT award-winning artist Paloma <b>Faith</b> will embark on a huge UK summer tour in <b>2022</b>! She&#39;ll be headlining after evening racing at Newmarket Racecourses on Friday 17th June, at Haydock Park Racecourse on Friday 1st July and at Sandown Park Racecourse on Wednesday 20th July <b>2022</b>. The extensive 23 date tour will run throughout ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'You can choose to <b>attend</b> the weekly <b>book</b> <b>club</b> <b>meeting</b> either in person at the Edinburgh Jesuit Centre at Sacred Heart Church at 11.15am (in the Ogilvie Room) or online using the Zoom app at 7.00pm - 8.00pm each week: Zoom Login Information. Join us on Zoom by following the link below. The link will be the same each week!', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Winner of the Commonwealth Writers’ Prize and shortlisted for the Costa First Novel Award, A Golden Age is a story of passion, revolution, hope, <b>faith</b> and unexpected heroism in the middle of chaos.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Stephen Cram, CBE (born 14 October 1960) is a British retired track and field <b>athlete</b>. Along with fellow Britons Sebastian Coe and Steve Ovett, he was one of the world&#39;s dominant middle distance runners during the 1980s. <b>Nicknamed</b> &quot;<b>The Jarrow</b> <b>Arrow</b>&quot;, after his home town, Cram set world records in the 1,500 m, 2,000 m, and the mile during a 19-day period in the summer of 1985.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The Crossword Solver found 30 answers to &quot;<b>athlete</b> <b>nicknamed</b> <b>the jarrow</b> <b>arrow</b>, steve (4)&quot;, 4 letters crossword clue. The Crossword Solver finds answers to classic crosswords and cryptic crossword puzzles. Enter the length or pattern for better results. Click the answer to find similar crossword clues . Enter a Crossword Clue.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'First he ran 3:29:67 in the 1500m in Nice, thereby becoming the first <b>athlete</b> to crack three minutes and 30 seconds. This was a near-mythical landmark in the sport; much as the four-minute mile ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Sea foam</b>. <b>Sea foam</b>, ocean <b>foam</b>, beach <b>foam</b>, or spume is a type of <b>foam</b> created by the agitation of seawater, particularly when it contains higher concentrations of dissolved organic matter (including proteins, lignins, and lipids) derived from sources such as the offshore breakdown of algal blooms. [1] These compounds can act as surfactants or ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'A <b>foaming</b> ocean is a fascinating phenomenon. <b>Sea</b> <b>foam</b> is an important ecological contributor, but that doesn&#39;t mean its always safe.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Sea</b> <b>foam</b> forms when dissolved organic matter in the ocean is churned up. <b>Sea</b> <b>Foam</b> at Ocean Beach in San Francisco. If you scoop up some water from the ocean in a clear glass and look at it closely, you&#39;ll see that it&#39;s chock full of tiny particles. Seawater contains dissolved salts, proteins, fats, dead algae, detergents and other pollutants ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>The Techniques of Democracy</b> is a book written by Alfred Bingham. It was published in 1943 by New York City publishers Duell, Sloan and Pearce. In this book, Bingham argues against both dogmatic individualism and dogmatic socialism. [1] Categories: 1942 non-fiction <b>books</b>. Duell, Sloan and Pearce <b>books</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Tytler was born in the Old Town of Edinburgh, the eldest son of Ann Craig of Costerton (1722–1783) and her husband William Tytler of Woodhouselee (<b>author</b> of Inquiry into the Evidence against Mary Queen of Scots). He was educated at Edinburgh High School and Kensington Academy in London (1763/64), and then studied law at the University of Edinburgh, qualifying as an advocate in 1770.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'In the year of elections, read Margaret Atwood, Mary Beard, Lea Ypi, Elif Shafak and more on what <b>democracy</b> means - and why it matters. In 2024, nearly half the world will take part in a national election, with billions heading to the polls. It&#39;s a thrilling, unprecedented opportunity for change - yet <b>democracy</b> is also under threat.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Screenwriter</b>, novelist, playwright, non-fiction <b>author</b>. Born in Highland Park, Illinois, USA, began his career as a novelist in 1957. Started writing screenplays in 1965 with &quot;Masquerade&quot;. A two-<b>time</b> Academy Award Winner, he is one of the most successful screenwriters and script doctors in Hollywood.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The term “<b>screenwriter</b>” has expanded in recent years to include writers for television, gaming, and all screen-based media emerging in the 21st century, such as Extended Reality or XR. Traditionally, “<b>screenwriter</b>” meant “<b>writer</b> of movies.” Today, I am focusing on the 50 greatest screenwriters in film.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Joseph L. Mankiewicz was a two-<b>time</b> Academy Award-winning <b>screenwriter</b> and director whose work left an indelible mark on 20th-century cinema. He penned elegant, clever screenplays for classics like All About Eve, A Letter to Three Wives, and Cleopatra, earning him a place among Hollywood&#39;s elite filmmakers.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Dedication, <b>Bach</b>&#39;s manuscript. It is uncertain when most of the material for the <b>Brandenburg Concertos</b> was written. It is clear that the first movement of <b>Concerto</b> No. 1 (BWV 1046) was based on an introduction to <b>Bach</b>&#39;s 1713 cantata Was mir behagt, and the second and last may have been as well. It also seems likely that <b>Concerto</b> No. 5 was the last to be written; it features a prominent ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Brandenburg Concertos</b>, six concerti grossi by Johann Sebastian <b>Bach</b>, considered masterful examples of balance between assorted groups of soloists and a small orchestra. The collection was composed circa 1711–20 and dedicated in 1721 to Christian Ludwig, the margrave ( marquess) of <b>Brandenburg</b> and the younger brother of King Frederick I of ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Many</b> Baroque composers <b>wrote</b> dozens or even hundreds of <b>concertos</b>, ... <b>Bach</b>’s <b>Brandenburg</b> <b>Concertos</b> 1 – 6 performed by Claudio Abbado and Orchestra Mozart', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'On this page you can see the list of every <b>UK</b> <b>Number</b> 1 <b>song</b> in ... 08/03/<b>1975</b>: IF: <b>TELLY</b> <b>SAVALAS</b>: 2: 22/03/<b>1975</b> ... As of 12th July 2024 the most recent <b>UK</b> artists <b>to have</b> a <b>number</b> <b>one</b> single in ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '10. “Don’t Be Cruel” (Elvis Presley) Length: 2 minutes, 4 seconds. Finally, we <b>have</b> <b>one</b> last <b>song</b> from the legendary Elvis Presley. “Don’t Be Cruel” <b>was released</b> in 1959 and was the first <b>song</b> Presley’s publishers brought him to record. The tune took the <b>number</b> 1 spot on all three main <b>charts</b> for pop, country, and R&amp;B.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>UK</b> singles chart was first compiled in 1969. However, the records and statistics listed here date back to 1952 because the Official <b>Charts</b> Company counts a selected period of the New Musical Express chart (only from 1952 to 1960) and the Record Retailer chart from 1960 to 1969 as predecessors for the period prior to 11 February 1969, where multiples of competing <b>charts</b> coexisted side by side.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '100 Best <b>Books</b> of the 21st Century: As voted on by 503 novelists, nonfiction writers, poets, critics and other book lovers — with a little help from the staff of <b>The New York Times</b> Book Review.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Resistance</b> is an extraordinarily powerful, humane and haunting account of how and why all across Nazi-occupied Europe some people decided to resist the Third Reich. This could range from open partisan warfare in the occupied Soviet Union to dangerous acts of insurrection in the Netherlands or Norway. Some of these <b>resistance</b> movements were ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Resistance</b> is an extraordinarily powerful, humane and haunting account of how and why all across Nazi-occupied Europe some people decided to resist the Third Reich. This could range from open partisan warfare in the occupied Soviet Union to dangerous acts of insurrection in the Netherlands or Norway. Some of these <b>resistance</b> movements were ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'September 14, <b>2022</b>. Le <b>Festival</b> de la Gastronomie of Saint-Martin, a celebration of the exceptional cuisine of the Caribbean ’s <b>culinary</b> capital – French St. Martin, will <b>take</b> <b>place</b> for the second year in a row from will <b>take</b> <b>place</b> from November 11th to the 22nd of <b>2022</b>. Epicurean enthusiasts from around the world will once again discover ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The Manchester Food and Drink <b>Festival</b> is a well established, nationally acclaimed event. Conceived and developed by Phil Jones in 1998, the event originated as a means of showing the rest of the nation that there was more to Manchester than meat pies and gravy! In the 24 festivals that have taken <b>place</b> over that time, Manchester’s dining ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The event will now <b>take</b> <b>place</b> between Thursday 22nd – Sunday 25th September and then again from Thursday 29th September – Sunday 2nd October. If that wasn’t all, the <b>festival</b> will be bringing tons of events, offers and more to local restaurants, too, with a special focus on some of the chefs and restaurants that have been a part of MFDF ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Fall and Rise of Reginald Perrin</b> is a British sitcom starring Leonard Rossiter in the title role. Three series were produced from 1976 to 1979, based on a series of novels written by David Nobbs. Nobbs adapted the screenplay for the first series from the first novel. Some of its subplots were considered too dark or risqué for television ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Fall And Rise of Reginald Perrin</b> ... <b>Reginald</b> Iolanthe <b>Perrin</b> had <b>worked</b> in the same boring job with Sunshine Desserts for 20 years. Every day he left his boring Norbiton home, took the same boring train journey, arrived at his boring office, and was greeted by his boring secretary Joan (Sue Nicholls) – a middle-aged bundle of simmering ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '9. <b>What was the name</b> of the cat who chased Pixie and Dixie? 10. What starts with ‘T’, ends with ‘T’ and has ‘T’ in it? ANSWERS: 1. Sunshine Desserts; 2. Brian Talbot (Ipswich and ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Robert <b>FitzRoy</b> circa 1850. The <b>Shipping</b> Forecast was established by Vice-Admiral Robert <b>FitzRoy</b>, the first professional weather forecaster, captain of HMS Beagle and founder of the Met Office. In October 1859, the steam clipper Royal Charter was wrecked in a strong storm off Anglesey; 450 people lost their lives.In response to this loss, <b>FitzRoy</b> introduced a warning service for <b>shipping</b> in ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Finisterre Sinks Beneath The Waves. February 4th <b>2002</b> marks the end of an era for the <b>Shipping</b> Forecast. Sea <b>area</b> Finisterre changes its <b>name</b> to <b>FitzRoy</b> and boundaries between neighbouring areas ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'For decades, his region was <b>known</b> as Finisterre, after a peninsula on the west coast of Galicia, Spain. But in <b>2002</b> the Met Office decided to rename it, to prevent confusion with an <b>area</b> of that <b>name</b> in the French/Spanish <b>shipping</b> forecast: today it’s <b>Fitzroy</b>. 13.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Korea</b>’s <b>first</b> <b>Grand Prix</b> was the longest for 50 <b>years (Korean</b> GP facts and stats) 25th October 2010, 7:00 | Written by Keith Collantine <b>Korea</b>’s <b>first</b> ever <b>grand</b> <b>prix</b> was always going to have a place in the history books. But taking nearly three hours to complete, it also became the longest world championship <b>race</b> since 1960.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The Korean <b>Grand Prix</b> ( Korean: 코리아 그랑프리) was a Formula One <b>race</b> held in <b>South</b> <b>Korea</b>, from 2010 until 2013, when it was dropped from the Formula One calendar.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'F1 <b>Korea</b> <b>Grand</b> <b>Prix</b> <b>Korea</b> International Circuit was designed by circuit architect Hermann Tilke and is located about 400 kilometers <b>south</b> of Seoul in Yeongam, <b>South</b> Jeolla. Due to construction difficulties and financial issues, the Korean <b>Grand</b> <b>Prix</b> has only been held on the circuit four times: from 2010 to 2013. The track itself is probably most famous for <b>its</b> daunting pit lane: the entry and ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Paul</b> (Koinē Greek: Παῦλος, romanized: Paûlos), also named Saul of Tarsus (Aramaic: ܫܐܘܠ, romanized: Šāʾūl), commonly known as <b>Paul</b> the Apostle and <b>Saint Paul</b>, was a Christian apostle (c. 5 – c. 64/65 AD) who spread the teachings of Jesus in the first-century world. For his contributions towards the New Testament, he is generally regarded as one of the most important figures ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Life. <b>Paul</b> was a Greek -speaking Jew from Asia Minor. His birthplace, Tarsus, was a major city in eastern Cilicia, a region that had been made part of the Roman province of Syria by the time of <b>Paul</b>’s adulthood. Two of the main cities of Syria, Damascus and Antioch, played a prominent part in his life and letters.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Introduction <b>Saint Paul</b> ©. <b>Saint Paul</b> is undoubtedly one of the most important figures in the history of the Western world. Just a quick look at the headlines of his life are enough to understand ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Frank Cornan</b>. Francis <b>Cornan</b> (5 May 1880 – 31 May 1971) was an English professional footballer born in Sunderland, who played as an inside left or left half. He made 165 appearances in the Football League playing for Barnsley (in three separate spells), Birmingham and Aston Villa. He died in Halifax, West Yorkshire, aged 91.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'A biographical history of <b>Frank</b> <b>Cornan</b>, Aston Villa Midfielder, 1908-09', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Francis <b>Cornan</b> was a Football player born on 1880-05-05, in Sunderland, England. Played as Midfielder.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Radik Zhaparov</b> (born February 29, 1984) is a Kazakh ski jumper who has competed since 2003. At the 2006 Winter Olympics in Turin, he finished 11th in the team large hill and 26th in the individual normal hill events.At the FIS Nordic World Ski Championships, <b>Zhaparov</b> has finished 11th in team events three times (2005: large, normal; 2007: large) and 24th in the individual normal hill (2007 ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Radik ZHAPAROV</b>. Team Kazakhstan. Ski Jumping. Games Participations 1. First Olympic Games Turin 2006. Year of Birth 1984. Olympic Results.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Radik</b> <b>ZHAPAROV</b>. Shvsm Dinamo. KAZ Kazakhstan. FIS Code 3787; Birthdate 1984; Age 40; Status Not active; Gender Male; Marital Status – ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Like most desert dwellers, the <b>fennec</b> fox <b>has</b> developed the ability to go for long periods without water. These foxes are cream-colored with black-tipped tails. Their adorable appearance makes ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>fennec</b> fox (Vulpes zerda) is a small crepuscular fox native to the deserts of North Africa, ranging from Western Sahara and Mauritania to the Sinai Peninsula. Its most distinctive feature is its unusually large ears, which serve to dissipate heat and listen for underground prey. The <b>fennec</b> is the smallest fox <b>species</b>. Its coat, ears, and kidney functions have adapted to the desert ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Bat-eared</b> Fox; Physical Description and Appearance. Size: Foxes are typically shorter than its Canidae cousins, jackals, and wolves, but are larger than the Raccoon dogs. The red fox, the largest fox <b>species</b>, measures 35-50 cm at the shoulders while the smallest <b>fennec</b> fox stands at 20 cm. Weight: Their weight varies depending on <b>the species</b> ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Wilmington</b> insurrection of 1898, also known as the <b>Wilmington</b> massacre of 1898 or the <b>Wilmington</b> coup of 1898, [6] was a coup d&#39;état and a massacre which was carried out by white supremacists <b>in Wilmington</b>, North Carolina, United States, on Thursday, November 10, 1898. [7] The white press <b>in Wilmington</b> originally described the event as a ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'A violent mob, whipped into a frenzy by politicians, tearing apart a town to overthrow the elected government. Following state elections in 1898, white supremacists moved into the US port of ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'This illustration of the 1898 <b>Wilmington</b> massacre typifies how publications of the time promoted misleading characterizations of the <b>incident</b> as a &#39;race riot&#39; or a Black insurrection.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The Matterhorn ( German: [ˈmatɐˌhɔʁn] ⓘ, Swiss Standard German: [ˈmatərˌhɔrn]; Italian: <b>Cervino</b> [tʃerˈviːno]; French: Cervin [sɛʁvɛ̃]; Romansh: Mont (e) Cervin (u)) [note 3] is a mountain of the Alps, straddling the main watershed and border between <b>Italy</b> and Switzerland. It is a large, near-symmetric pyramidal <b>peak</b> in the extended <b>Monte</b> Rosa area of the Pennine Alps, whose ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Matterhorn, one of the best-<b>known</b> mountains in the Alps, straddling the frontier between Switzerland and <b>Italy</b>, 6 miles (10 km) southwest of the village of Zermatt, Switzerland. It stands 14,692 feet (4,478 meters). The name Matterhorn means roughly ‘the <b>peak</b> in the meadows.’', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The Matterhorn, also <b>known</b> <b>as Monte</b> <b>Cervino</b> <b>in Italy</b>, is a 4000 meter <b>peak</b> in the Alps and perhaps the most iconic mountain in the world (peakery features the Matterhorn in its logo!).', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Francis</b> ushered in a new era of leadership of the Roman Catholic Church when he was elected pope in 2013. As the first pope from the Western Hemisphere, the first from South America, and the first from the Jesuit order, <b>Francis</b> has brought many reforms to the church and a reputation for humility. His significant achievements include the papal encyclical Laudato si’ (2015), which addressed ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Interesting Facts. Pope <b>Francis</b>, who was born in Argentina, is the first pope to have come from the Americas. Pope <b>Francis</b> was nominated for the 2014 Nobel Peace Prize. In addition to his native ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Pope <b>Francis</b> ( Latin: Franciscus; Italian: Francesco; Spanish: Francisco; born Jorge Mario Bergoglio; [b] 17 December 1936) is head of the Catholic Church and sovereign of the Vatican City State. He is the first pope to be a member of the Society of Jesus (Jesuits), the first from the Americas and the Southern Hemisphere, and the first born or ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>St</b>. <b>Ignatius of Loyola</b> (born 1491, <b>Loyola</b>, Castile [Spain]—died July 31, 1556, Rome [Italy]; canonized March 12, 1622; feast day July 31) was a Spanish theologian and mystic, one of the most influential figures in the Roman Catholic Counter-Reformation in the 16th century, and <b>founder</b> of the Society of Jesus (<b>Jesuits</b>) in Paris in 1534.. Early <b>life</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>The Life of St. Ignatius of Loyola</b>. July 31 is the feast day <b>of St</b>. <b>Ignatius</b> <b>Loyola</b>, the <b>founder</b> of the Society of Jesus who passed away on this day in the <b>year</b> 1556. We invite you to learn more about his <b>life</b> below and to use the following Ignatian resources and reflections to aid you in your contemplation: Artwork: Icon of the First ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Iñigo López de <b>Loyola</b> — better known as <b>St</b>. <b>Ignatius of Loyola</b> — was born in 1491 in the Basque region of Spain to minor nobility. The youngest of 13 children, <b>Ignatius</b> spent his formative years away from <b>Loyola</b> at court in the kingdom of Castile serving as page to the treasurer. He spent his days wooing women, fighting and gambling.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'In The <b>Pole</b>, the main character, Beatrice, has asked the pianist to give a concert in Barcelona. Later, as a courtesy – and without much enthusiasm – she takes him out to dinner. ... [Chopin] <b>composed</b> most of his preludes.” He is referring to 1838 and 1839, when the lovers spent a few months on the island. This episode is described in ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The effect can be comic. “She allows a day to pass,” the narrator tells us, “while she ruminates on for you .”. “The <b>Pole</b>” is Coetzee’s reworking of “Vita Nuova,” Dante Alighieri ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Mike Morasky (born June 14, 1964) is an American <b>composer</b>, visual effects artist, director and programmer. He <b>composed</b> the scores for the Valve games Team Fortress 2, the Left 4 Dead series, Portal 2, Counter-Strike: Global Offensive, Half-Life: Alyx and Counter-Strike 2.He worked on visual effects for the Lord of the Rings and Matrix films, and founded the underground art bands Steel <b>Pole</b> ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The Arcade <b>Awards</b>, also known as the Arkie <b>Awards</b>, was one of the first <b>video</b> <b>game</b> <b>awards</b>, dating back to the golden age of arcade <b>video</b> <b>games</b> and lasting up until the <b>video</b> <b>game</b> crash of 1983. It was held since 1980 (<b>for games</b> released in 1979 and earlier) and were announced annually by Electronic <b>Games</b> magazine since 1981, covering several platform categories. [68]', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Game</b> <b>of the Year</b>. Recognizing a <b>game</b> that delivers the absolute best experience across all creative and technical fields. Alan Wake 2. Baldur&#39;s Gate 3 - WINNER. Marvel&#39;s Spider-Man 2. Resident ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>Game</b> <b>Awards</b> 2022 are a wrap, and Elden Ring took home the biggest <b>award</b> of the night: <b>Game</b> <b>of the Year</b>. God of War Ragnarok also did pretty well for itself, taking home a handful of <b>awards</b>.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Sofia Anker-Kofoed</b> (born 28 November 1994) is a Swedish footballer. She last played as an attacker for FC Rosengård in the Damallsvenskan .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'National; FIFA World Cup; Olympics; UEFA European Championship; CONMEBOL Copa America', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Latest on <b>Sofia</b> <b>Anker-Kofoed</b> including news, stats, videos, highlights and more on ESPN', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Krapf was a 3-<b>time</b> <b>state</b> <b>wrestling</b> <b>champion</b> <b>for Tatnall</b>, winning at 180 pounds in 1966 and 1967 and heavyweight in 1968. He was also a 4-<b>time</b> national prep <b>school</b> champ.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>All</b> of the wrestlers who won the IHSAA <b>state</b> tournament <b>three</b> times.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Resume: New, on top in the photo above, was a <b>three-time</b> Section III <b>Champion</b>, a <b>three-time</b> <b>state</b> place finisher and the 1995 <b>state</b> champ at 126. He compiled a high <b>school</b> career record of 183 ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Luan</b> Last name <b>Viana</b> <b>Patrocínio</b> Nationality Brazil Date of birth 14 January 1996 Age 28 Country of birth Brazil Place of birth São Paulo Position Attacker Height 184 cm Weight 79 kg Foot Right. Career Domestic Leagues; Domestic Cups; ... Data provided by Opta <b>Sports</b>. Articles provided by OMNISPORT.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Luan Viana</b> <b>Patrocínio</b> (born 14 January 1996), sometimes known simply as <b>Luan</b>, is a Brazilian professional footballer who plays as a forward for Mixto. Club career [ edit ] Born in São Paulo , <b>Luan Viana</b> graduated from Portuguesa &#39;s youth setup, and made his first-team debut on 13 February 2013, coming on as a late substitute in a 2–0 win at São José , for the Campeonato Paulista Série ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Full name: <b>Luan</b> <b>Viana</b> <b>Patrocinio</b> Date of birth/Age: Jan 14, 1996 (28) Place of birth: São Paulo Height: 1,86 m Citizenship: Brazil Position: Attack - Centre-Forward Foot: right Current club: Porto Velho EC Joined: Feb 16, 2024 Contract expires:- Stats of <b>Luan</b> <b>Viana</b> . View full stats. National team career ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Federal prosecutors on Tuesday <b>hit</b> <b>scandal-plagued</b> Rep. George Santos (R-N.Y.) <b>with 10</b> <b>new</b> <b>criminal</b> <b>charges</b>, <b>including</b> for charging at least $44,800 to an unaware campaign donor who had texted him ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Those initial <b>charges</b> included seven counts of <b>wire</b> <b>fraud</b>, three counts of money laundering, one count of <b>theft</b> of public funds and two counts of making materially false statements to the <b>US</b> House ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>new</b> <b>charges</b> in the so-called superseding indictment are: one count of conspiracy to commit offenses against the United States, two counts of <b>wire</b> <b>fraud</b>, two counts of making materially false ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Kuujjuarapik</b> (also spelled Kuujjuaraapik; Inuktitut: ᑰᔾᔪᐊᕌᐱᒃ little great river) is the southernmost northern village (Inuit community) at the mouth of the Great Whale River (French: Grande Rivière de la Baleine) on the coast of Hudson Bay in Nunavik, Quebec, Canada.Almost 1,000 people, mostly Cree, live in the adjacent village of Whapmagoostui.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Kuujjuarapik</b> (Inuktitut: ᑰᔾᔪᐊᕌᐱᒃ) and neighbouring Whapmagoostui are twin villages with a total of about 1800 people (2021) in Nunavik in the far north of Quebec. <b>Kuujjuarapik</b> sits at the mouth of the Grande-Baleine River on the coast of Hudson Bay. ... Locally-run general store that sells everything from coats to <b>country</b> food ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The land Kuujjuaraapik . KUUJJUARAAPIK means “small river [].”This was the first village that missionaries visited to spread the word of God. One missionary in particular, Reverend W. G. Walton [], saw numerous Inuit each year to baptize and name the infants.Myself, I was born January 1st 1914 and he baptized me in 1917; I still have my baptismal certificate, although I’m sixty-seven now.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Check out the latest domestic and international stats, match logs, goals, height, weight and more for <b>Alexis</b> <b>Carra</b> playing for AS Cittadella and Vicenza Calcio in the Serie B', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Alexis Carra</b> is a 34-year old football player aus France, (* Jan 1, 1990 in Villefranche, France). <b>Carra</b> is Jul 1, 2012 without a club since. He plays in the position Right Winger. His market value is -.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Alexis</b> <b>Carra</b> is a 34-year-old Football ex-player. Born in France on 1990-01-01, he played as Forward. Weights 76 kg and is 180 cm tall.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Warriors</b> (also known as <b>Warrior</b> Cats) is a series of novels based on the adventures and drama of multiple Clans of feral cats.The series is primarily set in fictional forests. Published by HarperCollins, the series is written by authors Kate Cary and Cherith Baldry, as well as others, under the collective pseudonym Erin Hunter.The concept and plot of the pilot series were developed by series ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Erin Hunter is a collective pseudonym used by the authors Victoria Holmes, Kate Cary, Cherith Baldry, Clarissa Hutton, Inbali Iserles, Tui T. Sutherland, and Rosie Best in the writing of several children&#39;s fantasy novel series which focus on animals and their adventures. Notable works include the <b>Warriors</b>, Seekers, Survivors, Bravelands, and Bamboo Kingdom book series.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Erin Hunter is the team of writers behind <b>Warrior</b> Cats. Find out more about the authors and their stories.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Mates</b> in Mind has announced that Sam Downie has joined the UK charity as its new Managing <b>Director</b>. Sam takes over from Sarah Meek who has led the charity since 2021. Sam started her career as a research psychologist and went on to set up and lead mental health services at local, regional and national level.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Meet the people who make up <b>Mates</b> in Mind - from our Board of Trustees and Managing <b>Director</b> through to our Support Managers and office team, get to know the people who make our charity what it is today. Get to know our team. Our Reports. ... <b>Mates</b> in Mind has an ambitious goal – to reach 100,000 workers in the first year, and by 2025, we aim ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Prior to leading <b>Mates</b> in Mind, Sam was the CEO of WPF Therapy. Sam has a Masters in Leading in Organisations: Psychodynamic and Systemic Approaches. Patron - Michelle Wiles . ... Steve is a <b>Director</b> of Business Services &amp; HSW for Tideway and has more than 20 years&#39; experience managing health and safety in several sectors including engineering ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Gap</b> Band was an American R&amp;B and funk band that rose to fame during the 1970s and 1980s. The band consisted of three brothers: Charlie, Ronnie, and Robert Wilson, along with other members; it was named after streets (Greenwood, Archer, and Pine) [1] [2] in the historic Greenwood neighborhood in the brothers&#39; hometown of Tulsa, Oklahoma.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Gap</b>: With Freen Sarocha Chankimha, Rebecca Patricia &#39;Becky&#39; Armstrong, Tassawan Seneewongse, Ratchanon Kanpiang. Mon is an idol of Sam, and when they meet again at the office, she is surprised by her icy exterior. They are different in class and age, with a <b>gap</b> of eight years between them.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Gap</b>: The Series ( Thai: ทฤษฎีสีชมพู; RTGS : Thritsadi Si Chomphu; lit. Pink Theory) is a Thai romantic comedy television series that premiered on Channel 3 and the IdolFactory channel on YouTube on November 19, 2022, and ran until February 11, 2023.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Giacomo Guardi</b> (13 April 1764 - 3 November 1835) was an Italian painter from Venice. The son of famous Veduta painter Francesco <b>Guardi</b>, he continued his <b>father</b>&#39;s line of work, though without the same level of renown. The majority of his works are quite small views of only minor artistic interest, more akin to postcards than to his <b>father</b>&#39;s ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The son of the famous Venetian vedutista Francesco <b>Guardi</b>, <b>Giacomo Guardi</b> followed in his <b>father</b>’s footsteps in his preference for atmospherically rendered topographical views, although unlike the former he primarily concentrated on small formats. In the nineteenth century, his countless impressive views of Venice attracted the interest of collectors and museums.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'in subject and style by his <b>father</b>. his paintings capture the picturesque beauty and atmospheric drama of Venice in an imaginative and distinctive fashion. collectively, the <b>Guardi</b> family are often said to be the last true <b>Giacomo</b> <b>Guardi</b> and francesco <b>Guardi</b>, A View of the Venetian Lagoon painters of the Venetian school in its classical form.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Belorechensky District</b> ( Russian: Белоре́ченский райо́н) is an administrative <b>district</b> ( raion ), one of the thirty-eight in Krasnodar Krai, Russia. [1] As a municipal division, it is incorporated as <b>Belorechensky</b> Municipal <b>District</b>. [5] It is located in the southern central part of the krai, but is bordered for the main ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '03608101001. Website. www .gorodbelorechensk .ru. Belorechensk ( Russian: Белоре́ченск) is a town in Krasnodar Krai, Russia, located on the Belaya River, from which it takes its name. It forms the municipal formation Belorechenskoye urban settlement, as the only locality in its composition. Population: 51,590 (2020), 53,892 ( 2010 ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Belorechensk Geography. Geographic Information regarding City of Belorechensk. Belorechensk Geographical coordinates. Latitude: 44.7667, Longitude: 39.8667. 44° 46′ 0″ North, 39° 52′ 0″ East. Belorechensk Area. 5,600 hectares. 56.00 km² (21.62 sq mi) Belorechensk Altitude.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>50p</b> <b>coin</b> first appeared in our change on 14 October 1969. Our history guide follows the <b>coin</b> through the decades, looking at memorable <b>50p</b> issues along the way. The seven-sided <b>50p</b> <b>coin</b> first came <b>into</b> <b>circulation</b> on 14 October 1969, to replace the ten shilling note, ahead of <b>Britain</b>’s move to decimalisation in 1971.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'First <b>introduced</b> to United Kingdom coinage in 1969, the <b>50p</b> <b>coin</b> has become an iconic piece of British history. Today, the <b>50p</b> is used to commemorate historical occasions and has inspired many to start collecting, with the <b>coin</b> often credited as the gateway <b>coin</b> to starting a collection. The third <b>coin</b> issued as part of the decimalisation ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The British decimal fifty pence <b>coin</b> (often shortened to <b>50p</b> in writing and speech) is a denomination of sterling coinage worth 1 ⁄ 2 of one pound.Its obverse has featured the profile of the current Monarch since the <b>coin&#39;s</b> introduction in 1969. As of October 2022, five different royal portraits have been used.. As of March 2013 there were an estimated 920 million <b>50p</b> <b>coins</b> in <b>circulation</b>.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'George Raymond Richard <b>Martin</b> (born George Raymond <b>Martin</b>; September 20, 1948), also known by the initials G.R.R.M., is an American <b>author</b>, television writer, and television producer. He is best known as the <b>author</b> of the series of epic fantasy novels A Song of Ice and Fire, which were adapted into the Primetime Emmy Award–winning television series Game of Thrones (2011–2019) and its ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'George R. R. <b>Martin</b> writes fantasy novels and for television. His first novel, Dying of the Light, debuted in 1977, and by the mid-1980s, he was also writing for TV. In 1996, <b>Martin</b> published his ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The World of Ice &amp; Fire: The Untold History of Westeros and the Game of Thrones. by. George R.R. <b>Martin</b>, Elio M. García Jr., Linda Antonsson (<b>Goodreads</b> <b>Author</b>) 4.26 avg rating — 36,796 ratings — published 2014 — 60 editions.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'A half-set being <b>played</b>. The uilleann pipes (/ ˈ ɪ l ə n / ⓘ IL-ən or / ˈ ɪ l j ə n / IL-yən, <b>Irish</b>: [ˈɪl̠ʲən̪ˠ]), also known as Union pipes and sometimes called <b>Irish</b> pipes, are the characteristic national bagpipe of Ireland. Their current <b>name</b> is a partial translation <b>of the Irish</b> language terms píobaí uilleann (literally, &quot;pipes of the <b>elbow</b>&quot;), from their method of inflation.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'These pipes are now most commonly known as Uilleann pipes (pronounced ill-yin, from <b>Irish</b> uille, <b>elbow</b>). This <b>name</b> was first applied to the instrument as last as the beginning of the 20th century when it was foisted on the public in 1903 by Grattan Flood who then proceeded to equate it with the ‘woollen’ pipes of Shakespeare, thus providing for the instrument a spurious origin in the 16th ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Uilleann pipes are the earliest recorded type of <b>Irish</b> <b>bagpipes</b> and date back to the fifth century, when they were an instrument popular in the peasant community. Fast-forward to the seventeenth century, and this type of bellows pipes started to become the fashion among the lower and upper classes alike, most notably in Ireland and France.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>François</b> <b>Gayot</b> (July 17, 1927 – December 16, 2010) was the Catholic archbishop of the Roman Catholic Archdiocese of Cap-Haïtien. Haiti . Ordained to the priesthood in 1954, <b>Gayot</b> was named bishop of the then Cap-Haïtien Diocese. In 1988 the Diocese was elevated to an archdiocese.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Archbishop <b>François</b> <b>Gayot</b>, who went to Rome, representing the Catholic Church of Haiti at the occasion of the International Commission on Migration (CIM), had been hospitalized urgently to ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>François</b> <b>Gayot</b> de Pitaval <b>François</b> <b>Gayot</b> de Pitaval (1673–1743) was a French advocate . He compiled a famous collection of causes célèbres. [1] Later the literary genre of true crime collections became known as Pitaval .', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Maksim</b> <b>Andreyevich</b> <b>Fyodorov</b> ( Russian: Максим Андреевич Фёдоров; <b>born</b> 5 April 1989) is a Russian former professional footballer .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Maksim Fyodorov</b> may refer to: <b>Maksim Fyodorov</b> (footballer, <b>born</b> 1986), Russian footballer (striker) <b>Maksim Fyodorov</b> (footballer, <b>born</b> 1989), Russian footballer (midfielder) Category: Human name disambiguation pages.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Russian footballer – <b>Maksim</b> <b>Andreyevich</b> <b>Fyodorov</b> was <b>born</b> in Rasskazovo (Town in Tambov Oblast, Russia) on April 5th, 1989 and is 35 years old today.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Nina Dittrich</b> (<b>born</b> 20 November 1990 in Vienna) is an Austrian swimmer, who specialized in freestyle and butterfly events. She is a multiple-time Austrian champion, a five-time national record holder, and also, a current member of Simmering Swimming Club (German: Schwimmverein Schwechat Simmering) in Schwechat. <b>Dittrich</b> is also the daughter of Ulrike Bauer, an Austrian record holder in both ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Host <b>city</b> selection; Statistics Medals by country; Medals by athlete; ... Biographical information Roles: Competed in Olympic Games: Sex: Female: Full name: <b>Nina•Dittrich</b>: Used name: <b>Nina•Dittrich</b>: <b>Born</b>: 20 November 1990 in Wien (Vienna), Wien (AUT) Measurements: 174 cm / 58 kg: Affiliations: SV Schwechat, Schwechat (AUT) ... <b>Nina Dittrich</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'We recommend you to check the complete list of Famous People <b>born</b> on 20 November. She is a member of famous Swimmer with the age 33 years old group. <b>Nina</b> <b>Dittrich</b> Height, Weight &amp; Measurements. At 33 years old, <b>Nina</b> <b>Dittrich</b> height is 1.74m and Weight 58 kg.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Netherlands Antilles</b> ( <b>Dutch</b>: Nederlandse Antillen, pronounced [ˈneːdərlɑntsə ʔɑnˈtɪlə (n)] ⓘ; Papiamento: Antia Hulandes) [2] was a constituent <b>country</b> of the Kingdom of the <b>Netherlands</b>. The <b>country</b> consisted of several island territories located in the Caribbean Sea. The islands were also informally known as the <b>Dutch</b> <b>Antilles</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'In 2006 the <b>Dutch</b> government and the remaining five islands agreed to dissolve the <b>Netherlands Antilles</b> within the following several years. The event took place on October 10, 2010. None of the islands chose full independence. Curaçao and Sint Maarten became autonomous <b>countries</b> within the kingdom, a status similar to that of Aruba.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'In 1954, the various <b>Dutch</b> island colonies were united under a single <b>country</b> and were named “Netherland <b>Antilles</b>.” On December 15, 1954, with the proclamation of the “Charter for the Kingdom of <b>Netherlands</b>,” the <b>Netherlands Antilles</b> became an autonomous part of the Kingdom of <b>Netherlands</b>.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Buy Forever <b>Max</b>: The heartwarming new memoir from the <b>author</b> of the bestselling <b>Max</b> the Miracle Dog by Irving, Kerry (ISBN: 9780008645045) from Amazon&#39;s Book Store. Everyday low prices and free delivery on eligible orders.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Buy <b>Max the</b> Miracle <b>Dog: The</b> Heart-warming Tale of a Life-saving Friendship First Edition by Irving, Kerry (ISBN: 9780008353490) from Amazon&#39;s Book Store. Everyday low prices and free delivery on eligible orders.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Max:</b> Best Friend,<b></b> Hero,<b></b> Marine. Paperback – June 9, 2015. The New York Times bestselling movie novelization—a powerful story about a special dog and what it means to be a hero that more and more readers have discovered. When Justin&#39;s older brother, Kyle, is killed in Afghanistan, Justin can&#39;t believe that his brother is really gone.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>O</b> <b>dia</b> <b>das</b> <b>calças</b> <b>roladas</b> is a Capeverdean novel published in 1982 by Germano Almeida.. The book was first published on Ilhéu Editora. The story is about an account of a strike that happened on the island of Santo Antão.. External links. <b>O</b> <b>dia</b> <b>das</b> <b>calcas</b> <b>roladas</b> at Editorial Caminho (in Portuguese); <b>O</b> <b>dia</b> <b>das</b> <b>calças</b> <b>roladas</b> at livrodatero.blogspot.com (in Portuguese)', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'In 1989 he founded the Ilhéu Editora publishing house and has since published 16 <b>books</b> (nine novels). Published works. His first work was <b>O</b> <b>dia</b> <b>das</b> <b>calças</b> <b>roladas</b> which was about an account of a strike on the island of Santo Antão, it was first written in 1982 and was published in 1983', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>O Dia das Calças Roladas</b>. A sua opinião. Limpar Enviar. Obrigado por partilhar connosco a sua opinião. <b>O</b> seu comentário só ficará visível após validação. Nota: Comentários com linguagem ofensiva ou provocadora, ou que não expressem uma opinião sobre <b>o</b> livro ou sobre <b>o</b> seu autor, não serão publicados.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The 2023 Venice International <b>Film</b> <b>Festival</b> is currently underway and celebrities from all over the world are turning up in their fashionable best. Cristiano Ronaldo&#39;s girlfriend <b>Georgina</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Inside <b>Georgina</b> Chapman’s Whirlwind Trip to Venice <b>Film</b> <b>Festival</b>. By Elise Taylor. September 16, <b>2022</b> ... In her spare <b>time</b>, she&#39;s either checking out the latest New York hotspot until 2 a.m. or ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'GEORGINAGIOGEORGINA RODRIGUEZ ARRIVES AT CANNES <b>FILM</b> <b>FESTIVAL</b> 2022Cristiano Ronaldo&#39;s FIANCÉ <b>Georgina</b> Rodriguez became the cynosure the star of the day of al...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Bert</b> Meyers was <b>born</b> in Los Angeles on March 20, 1928. The son of Romanian Jewish immigrants, he maintained strong lifelong ties to his Jewish cultural heritage without being religious. Always rebellious and a questioner of authority, Meyers decided to drop out of high school and become a poet. For many years, he worked manual labor jobs including janitor, farm worker, house painter, and ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Bert</b> Meyers was <b>born</b> in Los Angeles in 1928. The son of Romanian Jewish immigrants, and a high school drop out, Meyers worked at manual labor jobs until finally becoming a master picture framer and gilder. When that work negatively impacted his health, he applied and was admitted to the Claremont Graduate School on the basis of his poetic achievements.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Bert</b> Meyers was <b>born</b> Bertram Ivan Meyers in Los Angeles on March 20, 1928. The son of Romanian and Polish Jewish immigrants, he maintained strong lifelong ties to his Jewish cultural heritage without being religious. Always rebellious and a questioner of authority, he decided to drop out of high school and become a poet.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Kondh</b> is a village in Dhrangadhra Taluka in <b>Surendranagar</b> district of Gujarat State, India. Nearby villages are Rajcharadi, Hampar, Navalgadh, Bhechada, Gajanvav, Ratanpar, Rampara, Raygadh, Ravaliyavadar and Narichana . <b>Kondh</b>&#39;s main crops are magafali ( peanuts) and kappas ( cotton ). <b>Kondh</b>&#39;s Postal Index Number code is 363310 and the postal ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Kondh</b> village is located in Dhrangadhra taluka of <b>Surendranagar</b> district in Gujarat, India. It is situated 20km away from sub-district headquarter Dhrangadhra (tehsildar office) and 30km away from district headquarter <b>Surendranagar</b>. As per 2009 stats, Kalyanpur is the gram panchayat of <b>Kondh</b> village. The total geographical area of village is ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'About <b>Kondh</b>. <b>Kondh</b> is a Village in Dhrangadhra Taluka in <b>Surendranagar</b> District of Gujarat State, India. It is located 44 KM towards west from District head quarters <b>Surendranagar</b>. 25 KM from . 167 KM from State capital Gandhinagar. <b>Kondh</b> Pin code is 363310 and postal head office is Dhrangadhra . Dhavana ( 6 KM ) , Jiva ( 8 KM ) , Sapkada ( 8 ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Cracker</b> is a British crime drama <b>series</b> produced by Granada <b>Television</b> for ITV, created and principally written by Jimmy McGovern.Set in Manchester, the <b>series</b> follows a criminal psychologist (or &quot;<b>cracker</b>&quot;), Dr Edward &quot;<b>Fitz</b>&quot; Fitzgerald, played by Robbie Coltrane, who works with the Greater Manchester Police (GMP) to help them solve crimes.. The show consists of three <b>series</b>, originally ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Cracker</b> (<b>TV</b> <b>Series</b> 1993–1996) cast and crew credits, including actors, actresses, directors, writers and more. Menu. Movies. ... <b>Fitz&#39;s</b> Mum 1 episode, 1993 Philippa Howell ... Dr Turner 1 episode, 1993 Barbara Young ... Helen McIlvanney 1 episode, 1995 ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Barbara Flynn. <b>Cracker</b>, Open All Hours, The Vanishing Man. Barbara Flynn (born Barbara Joy McMurray; 5 August 1948) is an English actress. She first came to prominence playing Freda Ashton in the ITV drama <b>series</b> A Family at War (1970–72). She went on to play the milk woman in the BBC comedy Open All Hours (1981–85), Jill Swinburne in The ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'GMC <b>Jimmy</b>. The GMC <b>Jimmy</b> was an SUV marketed by General Motors that spanned four generations and two distinct vehicles: The K5 <b>Jimmy</b> – a mid-size SUV built from 1970 to 1999 and based on the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Research the GMC <b>Jimmy</b> and learn about its generations, redesigns and notable features from each individual <b>model</b> year. Opens website in a new tab. ... The GMC <b>Jimmy</b> was not <b>produced</b> from 2002–2004.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Motor vehicles <b>produced</b> by country in 2013. This is a list of <b>manufacturers</b> by motor vehicle production, by year, based on Organisation Internationale des Constructeurs d&#39;Automobiles (OICA).. Figures include passenger <b>cars</b>, light commercial vehicles, minibuses, trucks, buses and coaches.OICA defines these entries as follows: Passenger <b>cars</b> are motor vehicles with at least four wheels, used for ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Rio de Janeiro</b> (Portuguese: [ˈʁi.u d(ʒi) ʒɐˈne(j)ɾu] ⓘ), or simply <b>Rio</b>, is the <b>capital</b> of the state of <b>Rio de Janeiro</b>.It is the second-most-populous city in Brazil (after São Paulo) and the sixth-most-populous city in the Americas.. Founded in 1565 by the Portuguese, the city was initially the seat of the Captaincy of <b>Rio de Janeiro</b>, a domain of the Portuguese Empire.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Rio</b> <b>de</b> <b>Janeiro</b> (<b>Rio</b>) is Brazil’s second-most populated city after Sao Paulo. Its population is 6.5 million, with 12.5 million in the urban area. <b>Rio</b> is located in the State of <b>Rio</b> <b>de</b> <b>Janeiro</b>, on the Atlantic coast in southeast Brazil. <b>Rio</b> was the <b>capital</b> of Brazil under Portuguese colonial rule.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Rio de Janeiro</b>, city and port, <b>capital</b> of the estado (state) of <b>Rio de Janeiro</b>, Brazil.It is located on the Atlantic Ocean, in the southeastern part of the tropical zone of South America, and is widely recognized as one of the world’s most beautiful and interesting urban centres.Although <b>Rio de Janeiro</b> continues to be the preeminent icon of Brazil in the eyes of many in the world, in reality ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>2002 Euro Beach Soccer Cup</b> was the fourth <b>Euro</b> <b>Beach</b> <b>Soccer</b> <b>Cup</b>, one of Europe&#39;s two major <b>beach</b> <b>soccer</b> championships at the time, held in February <b>2002</b>, in Barcelona, Catalonia, Spain. Portugal won the championship, claiming their second successive title and third overall, with hosts Spain finishing second. France beat Italy in the third place playoff to finish third and fourth respectively.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Euro Beach Soccer Cup</b> (EBSC), originally known as the <b>European</b> Pro <b>Beach</b> <b>Soccer</b> Championships until 2004, was a biennial (previously annual) <b>beach</b> <b>soccer</b> competition contested between <b>European</b> men&#39;s national teams, organised by <b>Beach</b> <b>Soccer</b> Worldwide (BSWW). Having started in 1998, the tournament&#39;s prestige has held in being one of the oldest and longest running <b>beach</b> <b>soccer</b> competitions ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>2002</b> <b>Euro</b> <b>Beach</b> <b>Soccer</b> <b>Cup</b> was the fourth <b>Euro</b> <b>Beach</b> <b>Soccer</b> <b>Cup</b>, one of Europe&#39;s two major <b>beach</b> <b>soccer</b> championships at the time, held in February <b>2002</b>, in Barcelona, Catalonia, Spain. Portugal won the championship, claiming their second successive title and third overall, with hosts Spain finishing second. France beat Italy in the third place playoff to finish third and fourth respectively.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>1989</b> Tiananmen <b>Square</b> protests and <b>massacre</b> Part of the Cold War, the Revolutions of <b>1989</b> and the Chinese <b>democracy</b> <b>movement</b> Protesters in Tiananmen <b>Square</b> on 2 June (top), and tanks in <b>Beijing</b> in July (bottom) Date Initial protests: 15 April – 4 June <b>1989</b> (1 month, 2 weeks and 6 days) <b>Massacre</b>: 3–4 June <b>1989</b> (1 day); 35 years ago Location <b>Beijing</b>, China and 400 cities nationwide Tiananmen ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>In 1989</b> <b>Beijing</b>&#39;s Tiananmen <b>Square</b> became the focus for large-scale protests, which <b>were</b> crushed by China&#39;s Communist rulers. The events produced one of the most iconic photos of the 20th Century ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'What really happened in Tiananmen <b>Square</b> <b>in 1989</b>? It has been over 33 years since hundreds if not thousands of unarmed peaceful pro-<b>democracy</b> protesters <b>were</b> killed in <b>Beijing</b> and the arrest of tens of thousands of demonstrators in cities across China. The protesters, based in Tiananmen <b>Square</b> in central <b>Beijing</b>, <b>were</b> peacefully calling for political and economic reform.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Homecoming</b> is an American psychological thriller television series based on the Gimlet Media podcast of the same name.Created by Eli Horowitz and Micah Bloomberg, the series premiered November 2, 2018, on Amazon Prime Video.Horowitz and Bloomberg also serve as writers and executive producers alongside Sam Esmail, Chad Hamilton, Julia Roberts, Alex Blumberg, Matt Lieber, and Chris Giliberti.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The Show is directed by Mr Robot creator Sam Esmail. Taking journalists round the elaborate set of their soon-to-launch Amazon show <b>Homecoming</b> earlier this year, series creators Eli Horowitz and ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>director</b>&#39;s assistant / assistant: micah bloomberg and eli horowitz (17 episodes, 2018-2020) Nick Bohr. ... assistant production coordinator / assistant production office coordinator (10 episodes, 2018) Carly Cohen. ... set production assistant (10 episodes, 2018) Daniel Erikson.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Medzilaborce District</b> (okres <b>Medzilaborce</b>) is a <b>district</b> in the Prešov <b>Region</b> of northeastern Slovakia. It is the least populated of Slovakia&#39;s 79 districts. Until 1918, the <b>district</b> was part of the county of Kingdom of Hungary of Zemplín. Economy and infrastructure.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Medzilaborce</b> (Rusyn: Міджілабірцї, Midzhilabirtsyi; Ukrainian: Міжлабірці, Mizhlabirtsi; Hungarian: Mezőlaborc) is a town in northeastern Slovakia close to the border with Poland, located near the towns of Sanok and Bukowsko (in southeastern Małopolska). Its population is approximately 6,500.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Prešov <b>Region</b>. <b>Medzilaborce</b> <b>District</b>. <b>Medzilaborce</b> <b>District</b> is a <b>district</b> in the Prešov <b>Region</b> of northeastern Slovakia. It is the least populated of Slovakia&#39;s 79 districts. Until 1918, the <b>district</b> was part of the county of Kingdom of Hungary of Zemplín. Map. Directions. Satellite. Photo Map.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'In The <b>Director</b>’s Disorder: <b>Pilot</b> Episode, you’ll play the part of Coal Westwood, retired actor, as you attempt to survive the whims of a deadly killer. To live through the night, Coal will need to avoid antagonizing the <b>Director</b> by following the script or risk his wrath. This is the role of a lifetime. Features: - Fully Voiced Dialogue', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The Aviator is a 2004 American epic biographical drama film directed by Martin Scorsese and written by John Logan.It stars Leonardo DiCaprio as Howard Hughes, Cate Blanchett as Katharine Hepburn, and Kate Beckinsale as Ava Gardner.The supporting cast features Ian Holm, John C. Reilly, Alec Baldwin, Jude Law, Gwen Stefani, Kelli Garner, Matt Ross, Willem Dafoe, Alan Alda, and Edward Herrmann.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Find the game and support the creator here: https://store.steampowered.com/app/2073640/The_Directors_Disorder_<b>Pilot</b>_Episode/Links:https://www.buymeacoffee.co...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Marian</b> <b>University</b> celebrates the most significant events of the academic and <b>religious</b> calendar with special all campus liturgies. Institutional Witness The Catholic Franciscan tradition animates <b>Marian</b> <b>University</b> and forms the intellectual and moral foundations for all <b>university</b> policies, including: student services, human resources, and financial practices.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Marian</b> <b>University</b> 3200 Cold Spring Road Indianapolis, IN 46222-1997 (317) 955-6000 admissions@<b>marian</b>.edu COMadmissions@<b>marian</b>.edu Need More Information ?', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'About <b>Marian</b> <b>University</b>. <b>Marian</b> <b>University</b> grew out of the dedication and vision of Sister Theresa Hackelmeier and the Sisters of Saint Francis, Oldenburg, Indiana, who established a school in Oldenburg, Indiana, in 1851. ... sex, gender, gender identity, sexual orientation, <b>religion</b>, creed, national origin, age or disabilities in the selection ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The oldest extant <b>Hydra</b> narrative appears in Hesiod&#39;s Theogony, while the oldest images of the monster are found on a pair of bronze fibulae dating to c. 700 BC. In both these sources, the main motifs of <b>the Hydra</b> myth are already present: a multi-headed serpent that is slain by Heracles and Iolaus.While these fibulae portray a six-headed <b>Hydra</b>, its number of <b>heads</b> was first fixed in writing ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '3. <b>The Hydra</b> was a multi-headed monster--according to Diodorus (first century B.C.), it had a hundred <b>heads</b>; Simonides (sixth century B.C.) said it had fifty. The most common opinion, however, seems to be that it had nine. What made <b>the Hydra</b> so difficult was the fact that, whenever one of its <b>heads</b> was chopped off, two would grow in its place.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The destruction of the Lernean <b>Hydra</b> became one of the 12 Labours of Heracles.For that and other labours, Heracles enlisted the aid of his nephew Iolaus.As Heracles severed each mortal <b>head</b>, Iolaus was set to the task of cauterizing the fresh wounds so that no new <b>heads</b> would emerge. When only the immortal <b>head</b> remained, Heracles cut it off too and buried it under a heavy rock.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'PS3603.A465 M35 2005. <b>Make Love! The Bruce Campbell Way</b> is a comedy novel written by actor <b>Bruce</b> <b>Campbell</b>. [1] [2] The novel is written in the first person and involves real life celebrities such as Richard Gere, Renée Zellweger, and Mike Nichols; however, it is fiction. On the jacket <b>of Make Love! The Bruce Campbell Way</b> the <b>author</b> states, quote:', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'From a violent fistfight with a Buddhist to a life-altering stint in federal prison, this novel has it all. And if the 72,444 words are too time-consuming, there are lots and lots of cool graphics. &quot;It&#39;s a great, goofy what-if.&quot; &quot;Ultimately, <b>Make</b> <b>Love</b> is a <b>Bruce</b> <b>Campbell</b> novel, starring <b>Bruce</b> <b>Campbell</b>, written for <b>Bruce</b> <b>Campbell</b> fans for whom ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'From a violent fistfight with a Buddhist to a life-altering stint in federal prison, this novel has it all. And if the 72,444 words are too time-consuming, there are lots and lots of cool graphics. Praise for <b>Make</b> <b>Love</b> <b>the Bruce</b> <b>Campbell</b> <b>Way</b>. &quot;It&#39;s a great, goofy what-if.&quot; &quot;Ultimately, <b>Make</b> <b>Love</b> is a <b>Bruce</b> <b>Campbell</b> novel, starring <b>Bruce</b> ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The salsa <b>capital</b> of the world has a number of festivals that take place throughout the year, including some of the biggest in the world. Feria de Cali is a fair held every December and people from all over the world come to the party. This carnival and parade-style fair celebrates Cali’s salsa history with traditional salsa and live music.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The Hoysala Kingdom was a Kannadiga power originating from the Indian subcontinent that ruled most of what is now Karnataka between the 10th and the 14th centuries. The <b>capital</b> of the Hoysalas was initially located at Belur, but was later moved to Halebidu.. The Hoysala rulers were originally from Malenadu, an elevated region in the Western Ghats.In the 12th century, taking advantage of the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'As the undisputed salsa <b>capital</b> of the world, Caleños (people from Cali) eat, sleep and breathe these uptempo beats.But how exactly did the seductive Latin dance become such a big deal in the city? The answer involves the U.S. Navy, a guild of crafty club promoters and an illicit underworld empire whose cocaine trade transformed the face of the city forever.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>county</b> covers an area of 779.9 square kilometres (301.1 sq mi). Its administrative seat is the town <b>of Polkowice</b> , and it also contains the towns of Chocianów and Przemków . As of 2019 the total population of the <b>county</b> is 62,948, out of which the population <b>of Polkowice</b> is 22,480, that of Chocianów is 7,892, that of Przemków is 6,107, and the rural population is 26,469.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Polkowice</b> is located in historic Lower Silesia, about 15 km (9 mi) northwest of Lubin.The nearest airport is Wrocław Airport, located 72 km (45 mi) from <b>Polkowice</b>.. Situated in a traditional mining region, the town is part of the largest industrial copper-extraction area in Poland, with a copper-processing plant operating nearby.Nearby <b>Polkowice</b> Dolne is the site of a former State ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Gmina <b>Polkowice</b> is an urban-rural gmina (administrative district) in <b>Polkowice</b> <b>County</b>, Lower Silesian Voivodeship, in south-western Poland.Its seat is the town <b>of Polkowice</b>, which lies approximately 80 kilometres (50 mi) north-west of the regional <b>capital</b> Wrocław.. The gmina covers an area of 158.77 square kilometres (61.3 sq mi), and as of 2019 its total population is 27,676.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Paul</b> Revere <b>Braniff</b> was <b>born</b> in Kansas <b>City</b>, Kansas. He was the younger brother of Thomas Elmer <b>Braniff</b>. He grew up during the early era of aviation, and, as a youngster, became fascinated with the new way of transport. His family moved to Oklahoma <b>City</b>, Oklahoma, in 1900. Marriage. <b>Braniff</b> married Marie Agnes Maney on April 29, 1920.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Paul</b> <b>Braniff</b>, who was <b>born</b> on October 29, 1927, and died in Oklahoma <b>City</b> on February 1, 2013 at the age of 85. Marie was graduated from St. Agnes College in Baltimore, Md., and throughout her married life was involved in the aviation affairs of her husband that at times included scouting for new routes.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Paul</b> Revere <b>Braniff</b> (August 30, 1897 – June 15, 1954) was an airline entrepreneur. <b>Paul</b>, along with his brother Thomas Elmer <b>Braniff</b>, was one of the original founders of <b>Braniff</b> Airways, Inc. d/b/a <b>Braniff</b> International Airways (after 1948). <b>Paul</b> Revere <b>Braniff</b> was <b>born</b> in Kansas <b>City</b>, Kansas. He grew up during the early era of aviation, and, as a youngster, became fascinated with the new ...', 'score': 'N/A'}], [], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Rectified linear units improve restricted boltzmann machines. V Nair, GE <b>Hinton</b>. Proceedings of the 27th international conference on machine learning (ICML …. , 2010. 25342. 2010. Reducing the dimensionality of data with neural networks. GE <b>Hinton</b>, RR Salakhutdinov. Science 313 (5786), 504-507.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The following articles are merged in <b>Scholar</b>. Their combined <b>citations</b> are counted only for the first article. ... <b>Geoffrey</b> <b>Hinton</b>. Unknown affiliation. No verified email. Articles Cited by. Title. Sort. ... GF <b>Hinton</b>, F Cambridge. 182: 1981:', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Geoffrey</b> E. <b>Hinton</b>&#39;s 389 research works with 467,418 <b>citations</b> and 350,337 reads, including: Robust and data-efficient generalization of self-supervised machine learning for diagnostic imaging', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Delaware</b> single dad a contestant on &#39;<b>Worst Cooks in America</b>&#39; <b>on the Food</b> <b>Network</b>. ... <b>Worst Cooks in America</b>&quot; lives in <b>Delaware</b>. ... <b>Food</b> <b>Network</b> <b>program</b> that begins <b>airing</b> at 8 p.m. <b>Sunday</b>, <b>Jan</b>. <b>7</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Avi Boodram who lives in Christiana, will display his apparent lack of culinary abilities <b>on the Food</b> <b>Network</b> <b>program</b> that begins <b>airing</b> at 8 p.m. <b>Sunday</b>, <b>Jan</b>. <b>7</b>. Boodram is one of 16 contestant\\xads competing in ‘<b>Worst Cooks in America</b>: Spoiled Rotten,” a seven-episode series that follows men and women as they go through a culinary “boot camp” led by <b>Food</b> <b>Network</b> chefs Anne Burrell and ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'New York City, New York. The <b>Food</b> <b>Network</b>’s studio is located at Chelsea Market in New York’s Manhattan. Along with many other shows of the <b>network</b>, parts of ‘<b>Worst Cooks In America</b>’ are filmed in this studio. Chelsea Market seems to be the finest location possible for a cooking show. The market itself is a hub for foodies, with many ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Alexei</b> Mikhailovich <b>Ugarov</b> (Russian: Алексей Михайлович Угаров; born November 2, 1985) is a Belarusian professional ice hockey forward.He is currently an unrestricted free agent who most recently played for Severstal Cherepovets of the Kontinental Hockey League (KHL). He previously played three seasons for HC Nizhnekamsk Neftekhimik in the Russian Super League.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Hockey player <b>Ugarov</b> <b>Alexei</b>: up-to-date statistics, KHL matches, latest news', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Visit Aleksei <b>UGAROV</b> profile and read the full biography, watch videos and read all the latest news. Click here for more. ... <b>Sports</b>; News; Olympic Channel; Let&#39;s Move; Aleksei <b>UGAROV</b>. Team Belarus. Ice Hockey. Games Participations 1. First Olympic Games Vancouver 2010. Year of Birth 1985. Olympic Results.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Joshua <b>Da Silva</b> (born 19 June 1998) is a Trinidadian cricketer. He made his domestic debut in 2018 for Trinidad and Tobago, and his international debut for the West Indies cricket team in December 2020. Personal life. <b>Da</b> <b>Silva</b> is of Portuguese descent, with his ancestors hailing from Madeira. Both ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'A wicketkeeper/batsman from Trinidad and Tobago, Joshua <b>Da</b> <b>Silva</b> was selected among the reserves for West Indies&#39; tour of England in 2020 after an impressive domestic season in 2019-20. <b>Da</b> <b>Silva</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Joshua <b>Da Silva&#39;s</b> maiden Test ton gives WI 93-run lead . Mar 26 2022 . Sri Lanka end frustrating stand just before Lunch on Day 5 . Nov 25 2021 . <b>Da</b> <b>Silva</b>, Joseph make Bangladesh toil .', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Table of Contents. <b>Bathsheba Everdene,</b> fictional character, <b>heroine</b> of the pastoral <b>novel</b> Far from the Madding Crowd (1874) by <b>Thomas</b> <b>Hardy</b>. <b>Bathsheba</b>, the owner of a small farm, has several suitors: the abusive ne’er-do-well Sergeant Francis Troy, whom she marries; William Boldwood, a neighbouring farmer who kills Troy; and Gabriel Oak, a ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Bathsheba</b> <b>Everdene</b> Character Analysis. Next. Gabriel Oak. <b>Bathsheba</b>, the orphaned daughter of townspeople, is raised by her aunt in the countryside. From a young age, she is used to managing things on her own: for example, her aunt has her take charge of milking cows and fetching supplies for the house. She is handsome and can be vain about her ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Far from <b>the</b> Madding Crowd (1874) is <b>Thomas</b> <b>Hardy&#39;s</b> fourth published <b>novel</b> and his first major literary success. It was published on 23 November 1874. It originally appeared anonymously as a monthly serial in Cornhill Magazine, where it gained a wide readership.. The <b>novel</b> is set in <b>Thomas</b> <b>Hardy&#39;s</b> Wessex in rural southwest England, as had been his earlier Under the Greenwood Tree.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>votes</b> of the public determine electors, who formally choose the president through the <b>electoral</b> <b>college</b>. The number of electors a state receives is Every four years on the first Tuesday following the first Monday of November, voters head to the polls to elect the president of the United States. ... The <b>table</b> provides a list of U.S ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Every state will convene its meeting of electors that same day and send their <b>electoral</b> <b>votes</b> to the President of the Senate and the National Archives. Watch the live broadcast on TVW. Counting <b>Electoral</b> <b>Votes</b> – January 6, 2025. Congress meets in a joint session to count the <b>electoral</b> <b>votes</b> and announce the results of the <b>Electoral</b> <b>College</b> <b>vote</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Select a date to see. The <b>Electoral</b> <b>College</b> outcome. <b>Electoral</b> <b>College</b> <b>votes</b> by State. The candidates. Election notes*. * Election notes include specific election information, such as third party candidates, faithless electors, challenges during the counting of the <b>electoral</b> <b>votes</b> in Congress, and other interesting facts.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Madeleine Bunting, in her selection of Top 10 <b>books</b> about the aftermath <b>of empire</b>, for The Guardian Empireland is an utterly fascinating journey.. . For many British Asians who feel a strong connection to their heritage, Empireland exposes things you will wish you had learnt at school.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Synopsis. <b>Author</b>. An urgent, incisive account of how the depredations of its imperial past dog Britain’s view of itself today, Empireland is provocative, meticulously researched writing of the highest order. Winner of The British Book Awards 2022 Non-Fiction Narrative Book of the Year. Longlisted for the Baillie Gifford Prize for Non-Fiction ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Jon Wilson, a professor of the Department of History at King&#39;s College London, is the <b>author</b> of India Conquered, a 2016 book intended to rebut Ferguson&#39;s arguments in <b>Empire</b>: How Britain Made the Modern World, who catalogues the negative elements of the British Raj, and describes the <b>Empire</b> TV program (2003) as &quot;false and dangerous&quot;.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Prime Minister John Key has announced the creation of <b>a 620,000</b> km2 <b>Ocean</b> <b>Sanctuary</b> in the Kermadec region, one of the most pristine and unique environments on Earth. “The Kermadec <b>Ocean</b> <b>Sanctuary</b> will be one of the world’s largest and most significant fully-protected areas, preserving important habitats for seabirds, whales and dolphins, endangered marine turtles and thousands of species ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Government</b>’s announcement that it will scrap plans for a vast marine <b>sanctuary</b> around the Kermadec <b>Islands</b> is ‘shameful’ and will make it impossible for Aotearoa <b>New</b> <b>Zealand</b> to meet its international commitments, says the World Wide Fund for Nature (WWF) <b>New</b> <b>Zealand</b>.. Plans had been underway for <b>a 620,000</b> square kilometre <b>ocean</b> <b>sanctuary</b> around the Kermadec <b>Islands</b>/Rangitāhua to ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The Kermadec <b>Ocean</b> <b>Sanctuary</b> will not go ahead, with Cabinet deciding to stop work on the proposed reserve and remove the Bill that would have established it from Parliament’s order paper. “The Kermadec <b>Ocean</b> <b>Sanctuary</b> Bill would have created <b>a 620,000</b> <b>sq</b> <b>km</b> economic no-go zone,” Oceans and Fisheries Minister Shane Jones says. “The ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'They switched to small size in 1929 and are the only type of currency in circulation today in the United States. They were originally printed in denominations of $5, $10, $20, $50, $100, $500, $1,000, $5,000 and $10,000. The $500, $1,000, $5,000 and $10,000 denominations were last printed in 1945 and discontinued in 1969, making the $100 bill ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The Fr. is for Robert Friedberg. His book Paper Money of the United States <b>has</b> a list of all circulating <b>American</b> <b>bank</b> <b>notes</b>, assigning them each a Friedberg Number. As we mentioned before, the 1995 $20 was signed by Withrow and Rubin. On 25 th August 2023, a special Fr. 2081-D 1995 $20 sold for $1,920.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'On 25 th August 2023, an Atlanta Fr. 2079-F 1993 $20 sold for $2,400 because of Solid #5 Serial Numbers. A Boston was cheaper. One month earlier on 27 th July 2023, a Boston Fr. 2079-A 1993 $20 was $2,220 with Solid #6 Serial Numbers. Both <b>notes</b> were graded 66 EPQ Gem Uncirculated.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Nora Roberts. If there’s one <b>author</b> on this list who’s a recognized household name, it’s Nora Roberts. Since 1980, Roberts has written and published an astounding number of romances — her website claims the number stands at over 215! But this incredibly prolific production has not come at the cost of quality.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Colleen Hoover is the reigning queen of BookTok, the <b>books</b>-obsessed side of TikTok.With 26 different romance <b>books</b> to her name, she has a legion of fans across social media and an unprecedented ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'However, among current <b>love</b> and romance <b>authors</b>, not <b>all</b> of them have the same insight at the time of writing, so there is a wide variety <b>of love</b> stories to read and enjoy. The most outstanding are: Laura Esquivel, Helen Fielding, Diana Gabaldon, the beloved and very famous Elisabet Benavent, Nicholas Sparks, Rainbow Rowell, John Green, Megan Maxwell, Anne Jacobs and Jojo Moyes to name a few.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>2001–02</b> <b>Serie</b> <b>B</b> is the 70th season since its establishment in 1929. It is the second highest football league in Italy. Teams. Modena, Palermo, Como and Messina had been promoted from <b>Serie</b> C, while Reggina, Vicenza, Napoli and Bari had been relegated from <b>Serie</b> A. Personnel and sponsoring. Team', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>2001</b>-2002 <b>Serie</b> <b>B</b> Stats. Governing <b>Country</b>: Italy it. Level: 2nd Tier ( See League Structure) Gender: Male. Champion: Como. Most Goals: Luís Oliveira (Como) - 23. Most Assists: Mark Bresciano (Empoli) - 12. Most Clean Sheets: Marco Ballotta (Modena) - 20. Become a Stathead &amp; surf this site ad-free.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>Serie B</b> (Italian pronunciation: [ˈsɛːrje ˈbi]), officially known as <b>Serie</b> BKT for sponsorship reasons, is the second-highest division in the Italian football league system after the <b>Serie</b> A.It has been operating for over ninety years since the 1929–30 season.It had been organized by Lega Calcio until 2010 and the Lega <b>Serie B</b> ever since. Common nicknames for the league are campionato ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Yinka Ayenuwa</b> (<b>born</b> 2 May 1986 in Warri) is a Nigerian weightlifter. He competed in the men&#39;s 69 kg event at the 2014 Commonwealth Games [2] where he won a silver medal. [3] In 2015, he won 3 silver medals at the African Games .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Yinka</b> <b>Ayenuwa</b> (<b>born</b> 2 May 1986 in Warri) is a Nigerian weightlifter. He competed in the men&#39;s 69 kg event at the 2014 Commonwealth Games where he won a silver medal. In 2015, he won 3 silver medals at the African Games. Celebs Wiki. <b>Yinka</b> <b>Ayenuwa</b> fans also viewed: John Davis (weightlifter)', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>ayenuwa</b>.<b>yinka</b>.3. Show Famous Birthdays Today, Nigeria. ... About <b>Yinka</b> <b>Ayenuwa</b>. <b>Yinka</b> <b>Ayenuwa</b> (<b>born</b> 2 May 1986 in Warri) is a Nigerian weightlifter. He competed in the men&#39;s 69 kg event at the 2014 Commonwealth Games where he won a silver medal. In 2015, he won 3 silver medals at the African Games. Read more at Wikipedia. See Also.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'All the results of the games played <b>in the 1966</b> <b>World</b> <b>Cup</b> with goals scored, extratime and penalties information. The Soccer <b>World</b> Cups.com. Champions, stats, national teams and players from each <b>World</b> <b>Cup</b>. ... <b>1966</b> Soccer <b>World</b> <b>Cup</b> <b>Scores</b> ... <b>Final</b> Game. England. 4 - 2. West Germany ( 2 - 0 ) on extra <b>time</b>. H2H. Share: Follow us on: ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>1966</b> <b>FIFA</b> <b>World</b> <b>Cup</b> <b>final</b> was a football match played at Wembley Stadium in London on 30 July <b>1966</b> to determine the winner of the <b>1966</b> <b>FIFA</b> <b>World</b> <b>Cup</b>, the eighth <b>FIFA</b> <b>World</b> <b>Cup</b>. The match was contested by England and West Germany, with England winning 4–2 after extra <b>time</b> to claim the Jules Rimet Trophy.It was the first – and to date only – occasion that England has hosted or won the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>FIFA</b> <b>World</b> <b>Cup</b> (<b>Final</b>) England vs. West Germany Historical Head-to-Head . Attendance: 96,924. ... <b>Half Time</b>. 78&amp;rsquor; 2:1. Martin Peters. 89&amp;rsquor; 2:2. Wolfgang Weber ... This includes the entire history of the <b>FIFA</b> Women&#39;s <b>World</b> <b>Cup</b> as well as recent domestic league seasons from nine countries, ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Tikhvinsky District</b> ( Russian: Ти́хвинский райо́н) is an administrative [1] and municipal [4] <b>district</b> ( raion ), one of the seventeen in Leningrad Oblast, Russia. It is located in the southeast of the oblast and borders with Lodeynopolsky <b>District</b> in the north, Podporozhsky <b>District</b> in the northeast, Babayevsky <b>District</b> of Vologda Oblast in the east, Boksitogorsky <b>District</b> in ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Tikhvin ( Russian: Ти́хвин; Veps: Tihvin) is a town and the administrative center <b>of Tikhvinsky</b> <b>District</b> in Leningrad Oblast, Russia, located on both banks of the Tikhvinka River in the east of the oblast, 200 kilometers (120 mi) east of St. Petersburg. Tikhvin is also an industrial and cultural center of the <b>district</b>, as well as its ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Tikhvin is a town and the administrative center <b>of Tikhvinsky</b> <b>District</b> in Leningrad Oblast, Russia, located on both banks of the Tikhvinka River in the east of the oblast, 200 kilometers (120 mi) east of St. Petersburg. Tikhvin is also an industrial and cultural center of the <b>district</b>, as well as its transportation hub. Population: 58,459 (2010 Russian census); 63,338 (2002 Census); 71,352 ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Conan, Lord of the Black River</b> is a fantasy novel by American writer Leonard Carpenter, featuring Robert E. Howard&#39;s sword and sorcery hero <b>Conan</b> the Barbarian. It was first published in paperback by Tor <b>Books</b> in April 1996. Plot After successfully fulfilling his commission to overthrow a tyrannical baron in Koth, <b>Conan</b> travels into Baalur, a ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '3.28. 80 ratings8 reviews. <b>Conan</b> the Cimmerian must venture into the nightmare world of the dead to retrieve the Silver Lotus, a powerful weapon that can undo the dreaded incantation that holds the city of Queen Rufia under the spell of the undead witch Zeriti. Genres Fantasy Sword and Sorcery. 288 pages, Paperback. First published April 15, 1996.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Leonard Paul Carpenter (born in 1948) is a technical writer and <b>author</b> of fantasy and science fiction. Among Carpenter&#39;s works are eleven <b>Conan</b> novels published by Tor <b>Books</b>, which he claims &quot;make him the most prolific contributor, living or dead, to the <b>Conan</b> literary saga of the late Robert E. Howard.&quot; He has also written the science fiction novel Fatal Strain, and a number of short stories ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Hunt</b> is a 2020 American action horror film [a] directed by Craig Zobel and written by Nick Cuse and Damon Lindelof. The film stars Betty Gilpin, Hilary Swank, Ike Barinholtz, and Emma Roberts. Jason Blum was a <b>producer</b> under his Blumhouse Productions banner, along with Lindelof. [4]', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Hunt</b> (2020) cast and crew credits, including actors, actresses, directors, writers and more.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>Hunt</b>: Directed by Craig Zobel. With Betty Gilpin, Hilary Swank, Ike Barinholtz, Wayne Duvall. Twelve strangers wake up in a clearing. They don&#39;t know where they are, or how they got there. They don&#39;t know they&#39;ve been chosen - for a very specific purpose - The <b>Hunt</b>.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Saraburi</b> City (thesaban mueang) is the provincial <b>capital</b> <b>of Saraburi</b> Province in central Thailand. [1] [2] In 2020, it had a population of 60,809 people, and covers the complete tambon Pak Phriao of the Mueang <b>Saraburi</b> district .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Saraburi</b> is on the east side of the Chao Phraya River valley. The eastern part of the province is covered by high plains and plateaus, while the western part is mostly low flat plains. [citation needed] <b>Saraburi</b> province has 848 km 2 (327 sq mi) of forest or 24.2 percent of provincial area. [1] The town, as a gateway to the northeastern region ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Province <b>capital</b>: <b>Saraburi</b> Name in Thai: สระบุรี. Location on map. Province of region Central Thailand The province <b>of Saraburi</b>, located in the heart of Thailand, is also a province that is not very frequented by foreign tourists. Close to Bangkok, it offers a nature and countryside escape to travelers looking for cultural ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Strand</b>’s guitarist Scott Shelly was a founding member, along with Jeff Porcaro, of the Grant High School-based band Rural Still Life. This outfit featured a number of musicians who would later establish themselves on the West Coast AOR scene, like Carlos Vega, Michael Landau or Steve Lukather and David Paich.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Strand</b> self titled album was released in 1980 and produced by Jeffrey Porcaro, who also played the drums on the track Can’t Look Back (uncredited). The <b>Strand</b> later did some opening acts for Toto (Hydra Tour). ... <b>Producer</b> – Jeffrey Porcaro Engineer (Remix) – Chris Blackwell Mixed By – Kent Nebergall Recorded By – Kent Nebergall.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Bricks and bottles were thrown at police officers as violence broke out on The <b>Strand</b>. Protests were organised in the city centre on Saturday, including one called &#39;Save Our Kids&#39; near the Liver ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>St</b>. <b>Albert</b>&#39;s <b>Church</b> (Latvian: Svētā Alberta Romas katoļu baznīca) is a Roman Catholic <b>church</b> in <b>Riga</b>, ... <b>St</b>. <b>Albert</b> <b>Church</b>, <b>Riga</b>, Pipe Organ <b>St</b>. <b>Albert</b> <b>Church</b>, <b>Riga</b>, inside view, the main door with the Choir balcony. The 30 stops pipe organ of the <b>church</b> was built in 1912., by Emil Martin &amp; Co., Opus 316. Emil Martin was the son of, also ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Riga</b> <b>St</b>. <b>Albert</b> Roman Catholic <b>Church</b> In Latvia It is one of the best churches in Latvia which you must visit. History: The Roman Catholic <b>Church</b> <b>of St</b>. <b>Albert</b> was founded in 1778 with the intention of providing a place of worship for the Catholic residents of <b>Riga</b>. The original <b>church</b> building was destroyed during the WWII bombings. The present-day <b>church</b> was built in 1957.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Riga</b> Cathedral (Rīgas Doms) in the capital <b>Riga</b> was originally built in 1211.. The main <b>religion</b> traditionally practiced in Latvia is Christianity.As of 2019, it is the largest <b>religion</b> (68.84%), though only about 7% of the population attends <b>religious</b> services regularly.. Lutheranism is the main Christian denomination among ethnic Latvians due to strong historical links with the Nordic ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Buenos Aires</b> is the national capital of Argentina and one of Latin America&#39;s most important ports and populous cities. It is situated on the shore of the Río de la Plata, 150 miles from the Atlantic Ocean, and has a distinctive European character and a vibrant cultural scene.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Learn about the history, demographics, and culture of<b> Buenos Aires,</b> the largest city and capital of Argentina. Find out how this city became the center of independence from Spain and a multicultural hub in South America.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Buenos Aires</b> is the capital and primate city of Argentina, located on the western shore of the Río de la Plata. It is an autonomous district with a rich cultural and multicultural heritage, and a global city with a high quality of life.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The Great <b>Escape</b> is a 1963 American epic war suspense adventure film starring Steve McQueen, James Garner and Richard Attenborough and featuring James Donald, Charles Bronson, Donald Pleasence, James Coburn, Hannes Messemer, David McCallum, Gordon Jackson, John Leyton and Angus Lennie.It was filmed in Panavision, and its musical score was composed by Elmer Bernstein.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The Great <b>Escape</b> (1963) cast and crew credits, including actors, actresses, directors, writers and more. Menu. ... Second Unit <b>Director</b> or Assistant <b>Director</b> . Jack N. Reddish ... assistant <b>director</b> John Flynn ... assistant <b>director</b> (uncredited) Robert E. Relyea ... second unit <b>director</b> (uncredited) ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The Great <b>Escape</b>: Directed by John Sturges. With Steve McQueen, James Garner, Richard Attenborough, James Donald. Allied prisoners of war plan for several hundred of their men to <b>escape</b> from a German camp during World War II.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Inspector</b> Jacques <b>Clouseau</b> (French: [ʒɑk kluzo]), later granted the rank of Chief <b>Inspector</b>, is a fictional character in Blake Edwards&#39; farcical The Pink Panther series. He is portrayed by Peter Sellers in the original series, and also by Alan Arkin in the 1968 film <b>Inspector Clouseau</b> and, in a cameo, by Roger Moore (credited as Turk Thrust II) in the 1983 film Curse of the Pink Panther.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Films: 7 Cato Fong is <b>Clouseau&#39;s</b> Chinese <b>manservant</b>, trained to attack him regularly to keep him alert and skilled in martial arts. Cato and <b>Clouseau</b> have a love-hate relationship, with their fights being long and vicious, as well as destructive to the furniture, and always interrupted by the telephone ringing, at which point they will become civil again. Cato puts a lot of effort into taking ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'He was best known for playing Cato Fong, <b>Inspector</b> <b>Clouseau&#39;s</b> <b>manservant</b>, in the Pink Panther film series. The character was first introduced in A Shot in the Dark (1964), the second film in the series, and was a role that Kwouk would reprise on another six occasions until the 2006 series reboot.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Twenty-second Amendment, amendment (<b>1951</b>) to the Constitution of the United States effectively limiting to two the <b>number</b> of <b>terms</b> a <b>president</b> of the United States may serve. It was <b>one</b> of 273 recommendations to the <b>U.S</b>. Congress by the Hoover Commission, created by Pres. Harry S. Truman, to reorganize and reform the federal government.It was formally proposed by the <b>U.S</b>. Congress on March 24 ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The Twenty-second Amendment (Amendment XXII) <b>to the</b> United States Constitution limits the <b>number</b> of times a person can be elected to the office of <b>President</b> of the United States to two <b>terms</b>, and sets additional eligibility conditions for presidents who succeed to the unexpired <b>terms</b> of their predecessors. Congress approved the Twenty-second Amendment on March 21, 1947, and submitted it to the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Section 1. No person shall be elected to the office of the <b>President</b> more than twice, and no person who has held the office of <b>President</b>, or acted as <b>President</b>, for more than two years of a term to which some other person was elected <b>President</b> shall be elected to the office of the <b>President</b> more than once. But this Article shall not apply to ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Operation Chastise, commonly known as the <b>Dambusters</b> <b>Raid</b>, was an attack on German dams carried out on the night of 16/17 May 1943 by 617 Squadron RAF Bomber Command, later called the <b>Dam Busters</b>, using special &quot;bouncing bombs&quot; developed by Barnes Wallis.The Möhne and Edersee dams were breached, causing catastrophic flooding of the Ruhr valley and of villages in the Eder valley; the Sorpe Dam ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Dambusters</b> <b>Raid</b>. On the night of 16-17 May 1943, Wing Commander Guy Gibson led 617 Squadron of the Royal Air Force on an audacious bombing <b>raid</b> to destroy three dams in the Ruhr valley, the industrial heartland of Germany. The mission was codenamed Operation &#39;Chastise&#39;. The dams were fiercely protected.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The <b>Raid</b>. On the night of 16-17 May 1943, the audacious <b>raid</b>, using purpose-built “bouncing bombs”, successfully destroyed the Möhne and Edersee Dams. Successful detonation required great technical skill from the pilots; they needed to be dropped from a height of 60 feet, at a ground speed of 232 mph, in extremely challenging conditions.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'You can get a <b>free</b> <b>TV</b> <b>licence</b> if you’re 75 or older and you either: The <b>licence</b> covers everyone living at your address. You can apply when you’re 74 if you already get Pension Credit. You’ll ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>free</b> <b>TV</b> <b>Licence</b> for people aged 75 or over applies to those in receipt of either part of Pension Credit – Guarantee Credit or Savings Credit (or both). It’s easy to check if you can get Pension Credit. Just call the Department for Work and Pensions on 0800 99 1234 (opening hours 8.00am - 6.00pm) or visit gov.uk/pension-credit.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The annual <b>TV</b> <b>licence</b> fee is currently pegged at £169.50, but it&#39;s not a universal requirement for everyone to pay up. A <b>TV</b> <b>licence</b> is necessary if you watch or record live <b>TV</b> on any channel ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>William Zeckendorf Jr</b>. (October 31, 1929 – February 12, 2014) was an American real estate developer. Son <b>of William Zeckendorf</b> Sr., he was the second of three generations of one of New York&#39;s great real estate dynasties. [1] While keeping a lower profile than his famously flamboyant <b>father</b>, <b>Zeckendorf</b> <b>Jr</b>. was highly successful in his own right.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'But while <b>Zeckendorf</b> <b>Jr</b>. may lack his <b>father</b>’s flair, he was a historic heavyweight — in 1986, the New York Times named him the city’s “most active real estate developer,” citing the 20 ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>William</b> <b>Zeckendorf</b> <b>Jr</b>. was born on Oct. 31, 1929, in Manhattan. He graduated from the Lawrenceville School in Lawrenceville, N.J., near Princeton, and attended the University of Arizona for two ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Michael Emons BBC <b>Sport</b> at the Crucible Theatre. Scotland&#39;s <b>Stephen</b> Maguire, semi-finalist in 2007 and 2012, trailed 5-4 after the first session against 2008 and 2012 runner-up Ali Carter, but ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The winner receives £500,000. For the runner-up, there is the consolation of £200,000. Semi-finalists earn £100,000 and quarter-finalists £50,000. Players knocked out in the first round ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Stephen</b> Hendry Seven-time world champion on BBC Four It was as if he&#39;s completely relaxed and felt he was at the point of no return, forget the situation and just <b>play</b>. 20', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Luis Fernando Tena</b> Garduño (<b>born</b> 20 January 1958) is a Mexican professional football manager and former player who is the head coach of the Guatemala national team .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Luis</b> <b>Fernando</b> <b>Tena</b>. Self: Chivas: El Rebaño Sagrado. <b>Born</b> on January 20, 1958 in Mexico <b>City</b>, <b>Luis</b> <b>Fernando</b> <b>Tena</b> Garduño is a Mexican former professional football player and former head coach of Club Deportivo Guadalajara, one of Mexico&#39;s most legendary teams. <b>Luis</b> <b>Fernando</b> played professional football for 12 years, starting in 1976 with Atlético Español (now Club Necaxa) and ending his ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Luis</b> <b>Fernando</b> <b>Tena</b> 1 2 2 1 Guatemala Manager Appointed: Nov 5, 2021 Contract until: Dec 31, 2025 www.cruzazulfc.com + Date of birth/Age: Jan 20, 1958 (66) Place of birth: Mexico <b>City</b> Citizenship: Mexico Avg. term as coach : 1.01 Years Preferred formation : 4-2-3-1', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The <b>Canton</b> of <b>Geneva</b>, officially the Republic and <b>Canton</b> of <b>Geneva</b>, [4] [5] is one of the 26 cantons of the Swiss Confederation. It is composed of forty-five municipalities, and the seat of the government and parliament is in the city of <b>Geneva</b> . <b>Geneva</b> is the French-speaking westernmost <b>canton</b> of Switzerland. It lies at the western end of Lake ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Geneva</b> ( / dʒəˈniːvə / jə-NEE-və, [5] Arpitan: [dzəˈnɛva] ⓘ; French: Genève [ʒənɛv] ⓘ) [note 1] is the second-most populous city in Switzerland (after Zürich) and the most populous of the French-speaking Romandy. Situated in the southwest of the country, where the Rhône exits Lake <b>Geneva</b>, it is the <b>capital</b> of the Republic and <b>Canton</b> of <b>Geneva</b>, and a centre for international ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Geneva</b>, city, <b>capital</b> of Genève <b>canton</b>, in the far southwestern corner of Switzerland that juts into France. One of Europe’s most cosmopolitan cities, <b>Geneva</b> has served as a model for republican government and owes its preeminence to the triumph of human, rather than geographic, factors. It developed its unique character from the 16th ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Harvard University</b> is a private Ivy League research <b>university</b> in Cambridge, Massachusetts.Founded in 1636 as <b>Harvard</b> College and named for its first benefactor, Puritan clergyman John <b>Harvard</b>, it is the oldest institution of higher learning in the <b>United States</b>.Its influence, wealth, and rankings have made it one of the most prestigious universities in the world.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Harvard</b> <b>University</b> holds 5,083 acres of real estate. The main campus occupies several locations in Cambridge including the historic and famous <b>Harvard</b> Yard. Athletic facilities and the <b>Harvard</b> Business School are located across the Charles River in Allstom, Massachusetts. The <b>Harvard</b> Medical School and School of Dental Medicine are located in ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Harvard University</b>, oldest institution of higher learning in the <b>United States</b> (founded 1636) and one of the nation’s most prestigious. The main <b>university</b> campus lies along the Charles River in Cambridge, Massachusetts, a few miles west of downtown Boston. Learn more about <b>Harvard</b> in this article.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'What music <b>genre</b> an artist or song belong to? What key or tempo is this song? You can use our free music <b>genre</b> finder and analyzer to quickly find the <b>genre</b> and more interesting information (such as the song’s key, BPM, popularity, etc.) about any music you love or to find an artist’s <b>genre</b>. Just enter the song title or artist name and leave the rest to our <b>genre</b> checker tool.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Complete list of movie <b>genres</b> and sub-<b>genres</b> with examples including action, comedy, drama, fantasy, horror, sci-fi, thriller, and western.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'This is a list of music <b>genres</b> and styles. Music can be described in terms of many <b>genres</b> and styles. Classifications are often arbitrary, and may be disputed and closely related forms often overlap. Larger <b>genres</b> and styles comprise more specific sub-categories.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Home</b> is a 2015 American animated science fiction comedy film produced by DreamWorks Animation and distributed by 20th Century Fox. Loosely based on Adam Rex &#39;s 2007 children&#39;s book The True Meaning of Smekday , the film was directed by Tim Johnson from a screenplay by Tom J. Astle and Matt Ember, and stars the voices of Jim Parsons , Rihanna , Steve Martin , Jennifer Lopez , and Matt Jones .', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Home</b>: Directed by Tim Johnson. With Jim Parsons, Rihanna, Steve Martin, Jennifer Lopez. Oh, a lovable misfit alien, runs away from his planet and takes shelter on Earth, where he befriends Tip, an adventurous young girl who is on a quest to find her displaced mother Lucy.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Anna Margaret <b>Home</b>. ( 1938-01-13) 13 January 1938 (age 86) Education. University of Oxford. Occupation (s) Television executive and <b>producer</b>. Anna Margaret <b>Home</b> OBE ( / ˈhjuːm / HYOOM; born 13 January 1938) is an English television <b>producer</b> and executive who worked for most of her career at the BBC .', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '100 <b>years</b> of supporting the Armed Forces community. In our centenary <b>year</b>, we are firmly focused on our future. By building on a century of work we’ll make sure we are a charity fit for the next 100. RBL was formed on 15 May 1921, bringing together four organisations of ex-servicemen that had established themselves after the First World War.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>Royal British Legion</b> Women&#39;s Section (RBLWS) was <b>founded</b> in 1921 and operated independently for some 96 <b>years</b>, with its own branches, standards and standard bearers, county branches, income and expenditure, national central committee, and annual conference.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'In the early <b>years</b> of the newly formed <b>British</b> <b>Legion</b>, founder and President Earl Haig worked tirelessly championing the needs of the Armed Forces, launching the Poppy Day Appeal in 1921 and helping to shape modern Remembrance. He also worked hard at grass-roots level, touring the country with Lady Haig, making speeches, visiting branches ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Browse today’s rankings of the <b>wealthiest</b> people and families globally. Discover the net worth, age, and other information about the <b>richest</b> people in the world.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Elon Musk, CEO of Tesla, is the <b>richest</b> person and the <b>richest</b> <b>man</b> in the world with a net worth of $252 billion. After Musk is Jeff Bezos, founder of Amazon. Other billionaires with some of the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Forbes presents the 2024 World&#39;s Billionaires List. View the <b>richest</b> people in the world including the youngest billionaires, female billionaires and newest billionaires.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Offers a list of all olympic <b>countries</b> that has participated in the olympics since 1896. The list functions as an overview of the 3 letter <b>country</b> codes abbreviations.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Each <b>country</b> has its three-letter abbreviation or code that is used during The Olympic Games to represent that <b>country</b>. The following is a list of the 204 &quot;<b>countries</b>&quot; that are recognized by the IOC (International Olympic Committee) as National Olympic Committees. An asterisk (*) indicates a territory and not an independent <b>country</b>; a listing of the independent <b>countries</b> of the world is available.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'International <b>Country</b> Codes (ISO, OIC, Fips, ...) It is not only the US Americans who are known for their many abbreviations. Statisticians, too, can often achieve more with just a few letters. And sometimes you do need them: The international <b>country</b> codes of all states according to several different standards. ISO: International Organization for Standardization IOC: International Olympic ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Appointment with Fear was a horror drama series originally <b>broadcast</b> on BBC <b>Radio</b> in the 1940s and 1950s, and revived on a number of occasions since. The format comprised a dramatised horror story of approximately half an hour in length, introduced by a character <b>known</b> <b>as the Man</b> in <b>Black</b>. The plays themselves were a mixture of classic horror ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Unfortunately very few shows of Appointment With Fear have survived. Only four shows are <b>known</b> to exist, namely 1/2, 3/1, 3/6 and 6/4. No shows from The <b>Man</b> In <b>Black</b> series have survived. However all of Fear On Four still exists. All of the stories from the first two series of Fear On Four have been published in: The <b>Man</b> In <b>Black</b>, BBC Books, 1990', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '4. Reunion by Janice Okoh. 4/5 A desperate love quest sparks a chilling reunion in Janice Okoh&#39;s cautionary tale. 3. The Beaten Track by Dawn King. 3/5 Dawn King&#39;s tale of a young couple ignoring ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Ferrari will continue to supply three teams, and Renault just <b>one</b>. The FIA regulations have a provision designed to help teams that are left without an engine partner by obliging the manufacturer with the smallest number of partners to step in, which puts the spotlight on Renault. Red Bull used Renault power for 12 seasons from 2007 and 2018 ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'On Friday Honda surprised the <b>motorsport</b> world by announcing that it will &quot;conclude&quot; its <b>Formula</b> 1 involvement <b>at the end</b> of the 2021 season. ... <b>announced</b> <b>that they</b> would indeed <b>team</b> up ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'After pulling its works <b>team</b> out of the championship <b>at the end</b> of 2008, Honda returned to F1 in 2015 as an engine builder, the second <b>year</b> of the current hybrid V6 regulations.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Gore</b> was the Democratic nominee for president of the United States in the 2000 presidential election, which he lost to George W. Bush. [a] The son of politician Albert <b>Gore</b> Sr., <b>Gore</b> was an elected official for 24 years. He was a U.S. representative from Tennessee (1977–1985) and from 1985 to 1993 served as a U.S. senator from that state.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Al</b> <b>Gore</b> served as the 45th vice president of the United States from 1993 to 2001. He is also known for his work regarding environmental issues.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Early Life Born Albert Arnold <b>Gore</b>, Jr., on March 31, 1948 in Washington, D.C., where his <b>father</b>, Albert <b>Gore</b>, Sr., was serving as a Democrat in the U.S. House from Tennessee. His <b>father</b> also ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Kurfürstendamm. The best known and most popular <b>shopping</b> <b>street</b> in Berlin runs from Breitscheidplatz to Halensee and is home to multiple department stores and shops for major chains. As you move west towards Halensee, the boutiques become classier and the window displays more luxurious. Stroll past the showcases of international fashion ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'The <b>street</b> is long ,it&#39;s the equivalent of the Champs Elysees in Paris with an amazing amount of <b>shopping</b> outlets from high end shops such as Gucci ,Luise Vuitton and Hermes to more conventional boutique shops plus you have huge <b>shopping</b> centres such as Europa Centre and the slighly more upmarket and famous KaDeWe department store but there is so much more to this <b>street</b> than shops.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Kurfürstendamm. The Kurfürstendamm ( German pronunciation: [ˌkuːɐ̯fʏʁstn̩ˈdam] ⓘ; colloquially Ku&#39;damm, [ˈkuːdam] ⓘ; [1] English: Prince Elector Embankment) is one of the most famous avenues in Berlin. The <b>street</b> takes its name from the former Kurfürsten ( prince-electors) of Brandenburg.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Wilson <b>died</b> of a heart attack in <b>Manchester</b>&#39;s Christie Hospital on 10 <b>August 2007</b> <b>aged</b> <b>57</b>. Following the news of <b>his</b> death, the Union Flag on <b>Manchester</b> Town Hall was lowered to half mast as a mark of respect. Probate documents reveal <b>his</b> estate was valued at £484,747 after tax. That figure includes the value of <b>his</b> city centre flat on Little ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'This is the tale of a man who, in <b>his</b> passport, simply had the word ‘<b>entrepreneur</b>’. Who was born in Salford and educated at Cambridge but who was firmly rooted in <b>Manchester</b> when the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Wilson was married twice and was with <b>his</b> long-term partner, former Miss England Yvette Livesey, for 17 years until <b>his</b> death from cancer <b>aged</b> <b>57</b> in 2007. Tony Wilson and <b>his</b> partner of 17 years ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The overall objective of the <b>game</b> was to survive elimination through to part three of the <b>show</b>, and try to unravel a series of cryptic clues in order to win the star <b>prize</b>. However, one of the clues referred to &quot;Dusty Bin,&quot; the <b>show&#39;s</b> booby <b>prize</b>; any contestants who wound up with Dusty at the end of the <b>show</b> received only a new <b>dustbin</b>.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Gloriously crap <b>consolation</b> <b>prizes</b>; Questions where multiple choice is the exception rather than the rule. For the purpose of this month’s Not So Perfect Ten, I shall be focusing on the second point: Gloriously crap <b>consolation</b> <b>prizes</b>. By this, trophies not worth the balsa wood they were made from; miniature follies which serve little or no ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Nostalgic <b>quiz</b> <b>show</b>: 3-2-1 featuring Ted Rogers &amp; Dusty Bin WILL they go home with a <b>prize</b> to be proud of, or will they hang their head in shame and disbelief when they walk away with <b>a dustbin</b>? The <b>game</b> was all about solving the cryptic impossible clues and eliminating the one that leads to Dusty Bin.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'His handbook was published the following <b>year</b>, and boys formed &#39;<b>Scout</b> Patrols&#39; to try out the book&#39;s ideas. The <b>scout</b> <b>movement</b> grew all over the world and <b>Baden-Powell</b> was declared &quot;Chief <b>Scout</b> of ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Lieutenant-General <b>Robert</b> Stephenson Smyth <b>Baden-Powell</b>, 1st Baron <b>Baden-Powell</b>, OM, GCMG, GCVO, KCB, KStJ, DL ( / ˈbeɪdən ˈpoʊəl / BAY-dən POH-əl; [4] 22 February 1857 – 8 January 1941) was a British Army officer, writer, founder and first Chief <b>Scout</b> of the world-wide <b>Scout</b> <b>Movement</b>, and founder, with his sister Agnes, of the world-wide Girl Guide/Girl <b>Scout</b> <b>Movement</b>. <b>Baden-Powell</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Andrea A. Dancer. <b>Robert</b> Stephenson Smyth <b>Baden-Powell</b> (1857–1941), founder of the <b>Boy</b> <b>Scout</b> <b>Movement</b> in 1907, was a British military hero during the Boer War. <b>Within</b> an ethos and era of empire-building, athleticism, soldier-heroes and the pursuit of “manliness,” <b>Baden-Powell</b> valued the arts and adapted his artistic skill to his wartime ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Paul Walker</b>. <b>Paul</b> William <b>Walker</b> IV [1] (September 12, 1973 [2] – November 30, 2013) was an American actor. He was best known for his role as Brian O&#39;Conner in the Fast &amp; Furious franchise. <b>Paul Walker</b> began his career as a child actor in the 1980s, gaining recognition in the 1990s after appearing in the television soap opera The Young and ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Paul</b> <b>Walker</b> was an American actor who came to fame in movies such as &#39;Varsity Blues&#39; and became well-known for his starring role in &#39;The Fast and the Furious&#39; franchise.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Paul</b> <b>Walker</b>. Actor: The Fast and the Furious. <b>Paul</b> William <b>Walker</b> IV was <b>born</b> in Glendale, California. He grew up together with his brothers, Caleb and Cody, and sisters, Ashlie and Amie. Their parents, <b>Paul</b> William <b>Walker</b> III, a sewer contractor, and Cheryl (Crabtree) <b>Walker</b>, a model, separated around September 2004. His grandfather, William <b>Walker</b>, was a Pearl Harbor survivor and a Navy ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Into the Woods</b> is a 1986 musical with music and lyrics by Stephen Sondheim and book by James Lapine.. The musical intertwines the plots of several Brothers Grimm fairy tales, exploring the consequences of the characters&#39; wishes and quests.The main characters are taken from &quot;Little Red Riding Hood&quot; (spelled &quot;Ridinghood&quot; in the published vocal score), &quot;Jack and the Beanstalk&quot;, &quot;Rapunzel ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Into the Woods</b>. STEPHEN SONDHEIM AND JAMES LAPINE 1986. INTRODUCTION <b>AUTHOR</b> BIOGRAPHY PLOT SUMMARY CHARACTERS THEMES STYLE HISTORICAL CONTEXT CRITICAL OVERVIEW CRITICISM SOURCES FURTHER READING INTRODUCTION. <b>Into the Woods</b>, published in 1986, is a collaborative work by Stephen Sondheim (music and lyrics) and James Lapine (story).It was the product of a workshop at Playwrights Horizon in New ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'In <b>Into</b> <b>the Woods</b>, John analyses how and most innovatively why this blueprint works, based on his decades of experience not only studying and teaching five-act story structure, ... Of all the <b>books</b> I’ve read about story construction and the art of fiction, this one is the most comprehensive and concise.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '“This is my first <b>time</b> here and, honestly, it’s super-surreal, but I’m having the shittiest day,” <b>Phoebe</b> Bridgers confides in the John Peel Stage crowd after ‘Scott Street’ comes to a ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Phoebe</b> playing Latitude <b>festival</b> (UK) means the O2 Priority dates for other UK shows are probably accurate (same <b>time</b> frame) This thread is archived New comments cannot be posted and votes cannot be cast', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Tickets for the extra date <b>go</b> on sale on this Friday (March 18) at 10am local <b>time</b> – get them here. 🙏 Wow, due to overwhelming demand, @<b>phoebe</b>_bridgers has announced a fourth night here at O2 ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Lewis Carroll (born January 27, 1832, Daresbury, Cheshire, England—died January 14, 1898, Guildford, Surrey) was an English logician, mathematician, photographer, and novelist, especially remembered for <b>Alice</b>’s Adventures in Wonderland (1865) and its sequel, Through the Looking-Glass (1871). His poem The Hunting of the Snark (1876) is ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Alice</b>&#39;s Adventures in Wonderland (also known as <b>Alice</b> in Wonderland) is an 1865 English children&#39;s novel by Lewis Carroll, a mathematics don at the University of Oxford.It details the story of a girl named <b>Alice</b> who falls through a rabbit hole into a fantasy world of anthropomorphic creatures. It is seen as an example of the literary nonsense genre. The artist John Tenniel provided 42 wood ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Franziska Kohlt is a researcher in 19th-century history of science and literature. She is the <b>author</b> of numerous articles on Lewis Carroll, Victorian culture and science, and the forthcoming <b>Alice</b> Through the Wonderglass: The unexpected histories of a children’s classic (Reaktion 2024), and editor of The Lewis Carroll Review, and the Through ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Band on the <b>Run</b>, the third studio album by Paul McCartney and Wings, remains the frontman’s most successful and critically acclaimed effort of his post-Beatles years. Released in December 1973, the LP features classics such as the title track and ‘Jet’. It is also notable for being mostly recorded at record label EMI’s studio in Lagos, Nigeria, as McCartney wanted to make an album in a ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'On The <b>Run</b> - Full Cast &amp; Crew. 2022; ... A tormented Emma, runs on her treadmill while doing online counseling. <b>Director</b> 1 Credit. Claudia Gerini. Screenwriter 1 Credit. Antonio Baiocco. Actor ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The Answer <b>Run</b> (25x45’) was commissioned by Rob Unsworth, Head of BBC Daytime and Early Peak commissioning, the Commissioning Editor is Alex McLeod. This is a BBC Studios Entertainment ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'The North Atlantic Treaty <b>Organization</b> (NATO) said it is investigating claims that data was stolen from <b>unclassified</b> <b>websites</b> under the military alliance’s control. A hacking group named SiegedSec — which has been at the center of several <b>recent</b> hacks involving U.S. municipalities over the last year — claimed to have stolen 9 GB of data.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'NATO says it is “actively addressing incidents” <b>affecting</b> <b>its</b> <b>unclassified</b> <b>websites</b> after a hacking group claimed to have stolen numerous strategic planning and research documents from the ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'Officials blame Russia-based criminal gang for large-scale <b>cyberattack</b> 02:43. Senior government officials are racing to limit the <b>impact</b> of what&#39;s believed to be a global <b>cyberattack</b> <b>affecting</b> U.S ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Distributor Walt Disney Studios Motion Pictures. See full company information. <b>Opening</b> $46,110,859. 4,030 theaters. Release Date Nov 10, 2023. MPAA PG-13. Running Time 1 hr 45 min. Genres Action ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>The Marvels</b> made $47 million domestically and $63 million internationally on its <b>opening</b> <b>weekend</b>.Making a total of $110 million on its first <b>weekend</b> in theaters.This makes it the lowest <b>opening</b> ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'While the <b>opening</b> <b>weekend</b> for the <b>The Marvels</b> release is only slightly lower than that of the franchise&#39;s second-lowest <b>opening</b>, this is still a bad sign for the new MCU movie. When adjusted for inflation, The Incredible Hulk&#39;s domestic <b>opening</b> <b>weekend</b> actually lands at around $80 million, widening the gulf between the two movies even more.That $110 million worldwide <b>opening</b> <b>weekend</b> doesn&#39;t ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Martin Scorsese, for one, recently admitted that he re-watches Fellini’s 1963 masterpiece 8 1/2 every year. “8 1/2 has always been a touchstone for me, in so many ways,” he said. “The ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Five years later Germi created his most successful film, Divorce, <b>Italian</b> <b>Style</b> starring Marcello Mastroianni, which won an Academy Award and, unlike Germi’s previous films, was a hilariously dark comedy. Mastroianni plays an unhappily married man who develops an obsession over his much younger cousin, but in order to pursue her in a socially ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The Tree of Wooden Clogs (1978) Ermanno Olmi was born on 24 July 1931 in Bergamo, Lombardy, Italy. He was a director and writer, known for The Tree of Wooden Clogs (1978), The Legend of the Holy Drinker (1988) and Il posto (1961). He was married to Loredana Detto. He died on 5 May 2018 in Asiago, Veneto, Italy.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': '<b>Karl Bartholomaeus Heller</b> (20 November 1824 – 14 December 1880) was an Austrian botanist and naturalist who explored Mexico in 1845–48 and published his memoir. ... <b>Born</b> in Moravia, <b>Heller</b> was a professor at the Theresianum in Vienna. Among <b>Heller</b>&#39;s later works is his defense of Darwinism, ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Karl</b> Bartholomäus <b>Heller</b> (* 20. November 1824 in Mislibořitz in Mähren; † 14. Dezember 1880 in Wien) war ein österreichischer Lehrer und Naturforscher. Leben. <b>Heller</b>, dessen Vater den Wiener Gartenbauverein leitete, bekundete frühzeitig Interesse für die Naturwissenschaften, deren Studien er sich auch bald zuwandte. ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>Karl</b> <b>Bartholomaeus</b> In the latter year Johann Jakob Heckel published the livebearing freshwater Green swordtail , since the early 20th century a common aquarium fish, from specimens <b>Heller</b> deposited in Vienna. <b>Born</b> in Moravia, <b>Heller</b> was a professor at the Theresianum in Vienna. Among <b>Heller</b>&#39;s later works is his defense of Darwinism, Darwin und der Darwinismus, 1869.', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'A <b>standard</b> British <b>Monopoly</b> <b>board</b>, featuring locations in London. The locations on the <b>standard</b> British version of the <b>board</b> game <b>Monopoly</b> are set in London and were selected in 1935 by Victor Watson, managing director of John Waddington Limited.Watson became interested in the <b>board</b> game <b>after</b> his son Norman had tried the Parker Brothers original US version and recommended the company produce ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': '<b>Monopoly</b> <b>Station</b> Rules. <b>Monopoly</b> <b>Station</b> Strategies. Final word. The 4 train <b>stations</b> on <b>Monopoly</b> <b>board</b> games in the UK are Kings Cross, Marylebone, Fenchurch Street and Liverpool Street. Each <b>station</b> costs £200 to buy. You can’t build houses or hotels on <b>Monopoly</b> railway <b>stations</b>. Owning all 4 <b>Monopoly</b> <b>stations</b> is a good strategy to help ...', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The UK <b>Monopoly</b> <b>board</b> has property names based on streets and <b>stations</b> in London. Whether you’re familiar with London or not, looking at the London <b>Monopoly</b> <b>board</b> streets on a map can be fascinating. Below, you can see a map showing the locations of each of the London <b>Monopoly</b> properties. Then, read on for an insight into what each London ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'Welcome to <b>Radio</b>-Locator.com, the most trusted <b>AM</b> and FM <b>radio station</b> search engine on the internet. We have links to over 17,200 <b>radio</b> <b>stations</b>&#39; web pages and over 12,900 <b>stations</b>&#39; audio streams from <b>radio</b> <b>stations</b> in the U.S. and around the world. <b>find</b> U.S. <b>radio</b> by <b>your</b> location. <b>find</b> U.S. <b>radio</b> by city or zip.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Welcome to Frequency Finder, a website providing details of all <b>radio</b> <b>stations</b> in England, Scotland, Wales and Ireland with features on <b>radio</b> transmission and history. All listings include frequencies, coverage areas, format and ownership. Transmission details, historical notes and maps are also included.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': '<b>1090</b> The Patriot (KPTR, <b>1090</b> <b>AM</b>) is a Talk <b>radio station</b> licensed to Seattle, WA, and serves the Seattle-Tacoma <b>radio</b> market. The <b>station</b> is currently owned by iHeartMedia. Call sign: KPTR Frequency: <b>1090</b> <b>AM</b> City of license: Seattle, WA Format: Talk Owner: iHeartMedia Area Served: Seattle-Tacoma Sister <b>stations</b>: Sports <b>Radio</b> 93.3 KJR, 95.7 The Jet, 102.5 KZOK, HITS 106.1 Seattle, Sports <b>Radio</b> ...', 'score': 'N/A'}], [{'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.0', 'contents': 'You can use our free music <b>genre</b> finder and analyzer to quickly find the <b>genre</b> and more interesting information (such as the song’s key, BPM, popularity, etc.) about any music you love or to find an artist’s <b>genre</b>. Just enter the song title or artist name and leave the rest to our <b>genre</b> checker tool. “You are what you listen to.”.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.1', 'contents': 'Free <b>Fire</b> is a free-to-play battle royale game developed and published by Garena for Android and iOS. It was released on 8 December 2017. It became the most downloaded mobile game globally in 2019 and has over 1 billion downloads on Google Play Store.In the first quarter of 2021 it was the highest grossing mobile game in the US. In November 2019, it surpassed $1 billion in lifetime revenue.', 'score': 'N/A'}, {'id': 'https://api.bing.microsoft.com/api/v7/#WebPages.2', 'contents': 'The song was first released by neo-rockabilly singer Robert Gordon, who had met Springsteen through E Street Band bass player Garry Tallent.They remained on friendly terms before Springsteen gave Gordon the song &quot;<b>Fire</b>&quot; after seeing a live gig by Gordon and Link Wray.According to Gordon, &quot;it was a choice between &#39;<b>Fire</b>&#39; and another new song but [Springsteen] decided to keep the other one for ...', 'score': 'N/A'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 134/134 [00:12<00:00, 10.70it/s, Generation Speed: 74.40 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This question cannot be answered because there is no information in the given documents about Benjamin attending a movie night on 2022/10/21 or any other date', 'Hungary', 'United States and Netherlands', 'Jesse Lasky', 'The country where a person is born.', 'There is no information about the composer of \"That\\'s Right\" in the given documents.', 'Lesley Garrett', 'Emile Mosseri', 'There is no information about who attended the gym workout between 2:00 PM and 3:00 PM on 2022/03/15 in Cross', \"January 1st (New Year's Day)\", 'Fantasy', '1943', 'Patience Agbabi.', 'none', 'Football', 'There is no information in the given documents that clearly states \"roughly how many student loan borrowers recently received inaccurate bills.\" However, we can infer that the Department', '8', '1947', 'Jockey', 'Malaysia', 'The answer is not provided as the document does not mention Faith attending a book club meeting on 2022/03/26. Additionally, there is no information', 'Stephen Cram', 'Ocean Beach in San Francisco.', 'Alfred Bingham', 'There is no mention of a screenwriter for Tine in the provided documents.', '6', 'TELLY', 'Not mentioned in the given documents.', 'The documents do not mention Adrian attending a culinary festival on 2022/09/27.', 'Sunshine Desserts', 'Finisterre', '2010', 'Asia Minor', 'Footballer', 'Ski jumping', 'No animal has these varieties.', 'State elections in 1898.', 'Monte Cervino', 'Argentina', '1556', 'Coetzee.', 'Elden Ring', 'football', 'Krapf', 'Footballer.', 'Rep. George Santos (R-N.Y.)', 'Canada', 'Football', 'Erin Hunter', 'Sarah Meek', 'R&B and funk band', 'Francesco Guardi.', 'Belorechensk', '1969', 'George R.R. Martin', 'píobaí uilleann', 'Catholic', 'Rasskazovo', 'Vienna', 'The Netherlands', 'Kerry Irving', 'Germano Almeida', 'I apologize, but there is no mention of Georgina attending an indie film festival on 2022/04/30 in any of the provided documents.', 'Los Angeles', 'India', 'Edward', 'General Motors', 'The state of Rio de Janeiro.', 'Beach Soccer', 'Tiananmen Square', 'Sam Esmail', \"Not applicable, since Medzilaborce District is not a capital of any country, it's a district in Prešov Region of Slovakia.\", 'Martin Scorsese', 'Catholic', '9', 'Bruce Campbell', 'world', 'Wrocław', 'Kansas City, Kansas.', 'Maurice Legacy', '467,418', 'A single dad from Delaware.', 'Ice hockey.', 'Cricketer', 'Far from the Madding Crowd', 'The documents do not mention Washington or the number of Electoral College votes he brought to the table. The documents appear to be discussing the general process of the Electoral College', 'Ferguson', 'Kermadec Islands', 'There is no mention of the White House on the reverse of any American bank note in the given documents.', 'The answer is not provided in the given documents as there is no author named \"Love All\".', 'Italy', 'Warri', '2:1', 'Tikhvin', 'Leonard Carpenter', 'Jason Blum', 'Saraburi', 'Jeffrey Porcaro', 'Roman Catholic', 'Buenos Aires', 'John Sturges.', 'Cato Fong', '2', '1943', '74', 'William Zeckendorf Sr.', 'Not mentioned in the documents.', 'Mexico City', 'Geneva', 'Massachusetts', 'I couldn\\'t find any information about the song or artist \"Cut\" in the given documents. Therefore, I cannot determine the genre of \"Cut\" based on', 'According to Doc 1, the producer of Home was not mentioned, but according to Doc 3, Anna Margaret Home was a television producer.', '1921', 'Elon Musk', 'MYS', 'BBC Radio', 'Honda', 'Albert Gore Sr.', 'Berlin', 'Tony Wilson', '3', '1907', 'Glendale', 'Stephen Sondheim and James Lapine.', 'There is no information about Phoebe attending a food festival on 2022/01/10 in any of the given documents. The documents only mention Ph', 'Lewis Carroll', 'Claudia Gerini.', 'NATO', '110 million', 'Germi', 'Moravia', 'Kings Cross', '1090 AM: KPTR, The Patriot.', 'Fire is a game.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 66/66 [00:01<00:00, 60.42it/s, Generation Speed: 214.22 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'em': 0.405, 'f1': 0.46247076023391803, 'sub_em': 0.5, 'precision': 0.4655277777777778, 'recall': 0.475}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<flashrag.dataset.dataset.Dataset at 0x7f6e2c0a01f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flashrag.dataset.utils import split_dataset, merge_dataset\n",
    "from flashrag.pipeline import SequentialPipeline\n",
    "from flashrag.utils import get_dataset\n",
    "# from flashrag.utils import get_retriever, get_generator, get_refiner, get_judger\n",
    "from flashrag.prompt import PromptTemplate\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def run_judger_bidir_rnn(classifiers, dataset, split, do_eval=True, pred_process_fun=None):\n",
    "    # judge_result: list of bool element, representing whether to use retrieval\n",
    "    judge_result = judger_bidir_rnn(dataset, classifiers)\n",
    "    print()\n",
    "    print('Judge result: ',judge_result)\n",
    "    print()\n",
    "    config= set_config(dataset_name=dataset, split = split)\n",
    "    \n",
    "    template= PromptTemplate(\n",
    "        config = config,\n",
    "        system_prompt =  \"Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\",\n",
    "        user_prompt = \"Question: {question}\"\n",
    "    )\n",
    "\n",
    "    all_split = get_dataset(config)\n",
    "    dataset = all_split[split]\n",
    "    dataset.update_output('judge_result', judge_result)\n",
    "\n",
    "    # split dataset based on judge_result\n",
    "    pos_dataset, neg_dataset = split_dataset(dataset, judge_result)\n",
    "    pipeline = SequentialPipeline(config)\n",
    "\n",
    "    print()\n",
    "    print('Questions that NEED retrieval',len(pos_dataset))\n",
    "    print('Questions that does NOT need retrieval',len(neg_dataset))\n",
    "    print()\n",
    "\n",
    "    pos_dataset = pipeline.run_internet_retrieval(pos_dataset, do_eval=False)\n",
    "    pipeline.prompt_template = template\n",
    "    neg_dataset = pipeline.naive_run(neg_dataset, do_eval=False)\n",
    "\n",
    "    # merge datasets into original format\n",
    "    dataset = merge_dataset(pos_dataset, neg_dataset, judge_result)\n",
    "\n",
    "    dataset = pipeline.evaluate(dataset, do_eval=do_eval, pred_process_fun=pred_process_fun)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "run_judger_bidir_rnn(['intent_aware', 'time_aware', 'knowledge_aware'], dataset ='retrievalqa_200', split = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution Judger Comparison with GroundTruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length small_dataset:  2400\n",
      "Accuracy Ratio (needs retrieval): 0.74\n",
      "Accuracy Ratio (does not need retrieval): 0.55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7375, 0.5466666666666666)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "def accuracy_judger_with_gt(judge_result, dataset):\n",
    "    folder_dataset = '/cs/student/projects2/dsml/cdiezmar/dataset/'\n",
    "    extension = '/train.jsonl'\n",
    "    file_path = folder_dataset + dataset + extension\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sampled_entries = [json.loads(line) for line in f]\n",
    "\n",
    "    # Extract the 'param_knowledge_answerable' values and convert them to False/True\n",
    "    param_knowledge_answerable = [entry['param_knowledge_answerable'] for entry in sampled_entries]\n",
    "\n",
    "# Ensure lengths match\n",
    "    assert len(judge_result) == len(param_knowledge_answerable), \"Lengths of judge result and dataset do not match.\"\n",
    "\n",
    "    # Calculate the ratios\n",
    "    needs_retrieval_correct = sum(1 for jr, pk in zip(judge_result, param_knowledge_answerable) if jr == True and pk == 0)\n",
    "    needs_retrieval_total = sum(1 for pk in param_knowledge_answerable if pk == 0)\n",
    "    needs_retrieval_ratio = needs_retrieval_correct / needs_retrieval_total if needs_retrieval_total != 0 else 0\n",
    "\n",
    "    no_retrieval_correct = sum(1 for jr, pk in zip(judge_result, param_knowledge_answerable) if jr == False and pk == 1)\n",
    "    no_retrieval_total = sum(1 for pk in param_knowledge_answerable if pk == 1)\n",
    "    no_retrieval_ratio = no_retrieval_correct / no_retrieval_total if no_retrieval_total != 0 else 0\n",
    "\n",
    "    print(f\"Accuracy Ratio (needs retrieval): {needs_retrieval_ratio:.2f}\")\n",
    "    print(f\"Accuracy Ratio (does not need retrieval): {no_retrieval_ratio:.2f}\")\n",
    "    return needs_retrieval_ratio, no_retrieval_ratio\n",
    "\n",
    "dataset = 'retrievalqa_2400'\n",
    "classifiers = ['intent_aware', 'time_aware', 'knowledge_aware']\n",
    "# classifiers = []\n",
    "judge_result = judger_lstm(dataset, classifiers)\n",
    "accuracy_judger_with_gt(judge_result, dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results - RetrievalQA accuracy ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length: 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger LSTM:\n",
    "\n",
    "Length small_dataset:  200\n",
    "Accuracy Ratio (needs retrieval): 0.70\n",
    "Accuracy Ratio (does not need retrieval): 0.56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger GRU:\n",
    "\n",
    "Length small_dataset:  200\n",
    "Accuracy Ratio (needs retrieval): 0.78\n",
    "Accuracy Ratio (does not need retrieval): 0.52\n",
    "(0.78, 0.52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger RNN:\n",
    "\n",
    "Length small_dataset:  200\n",
    "Accuracy Ratio (needs retrieval): 0.77\n",
    "Accuracy Ratio (does not need retrieval): 0.51\n",
    "(0.77, 0.51)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger Bidir-RNN:\n",
    "\n",
    "Length small_dataset:  200\n",
    "Accuracy Ratio (needs retrieval): 0.75\n",
    "Accuracy Ratio (does not need retrieval): 0.41\n",
    "(0.75, 0.41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger MLP (1 layer):\n",
    "\n",
    "Length small_dataset:  200\n",
    "Accuracy Ratio (needs retrieval): 0.72\n",
    "Accuracy Ratio (does not need retrieval): 0.38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger LSTM:\n",
    "\n",
    "Length small_dataset:  1000\n",
    "Accuracy Ratio (needs retrieval): 0.77\n",
    "Accuracy Ratio (does not need retrieval): 0.56\n",
    "(0.766, 0.562)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger GRU:\n",
    "\n",
    "Length small_dataset:  1000\n",
    "Accuracy Ratio (needs retrieval): 0.81\n",
    "Accuracy Ratio (does not need retrieval): 0.52\n",
    "(0.812, 0.518)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger RNN:\n",
    "\n",
    "Length small_dataset:  1000\n",
    "Accuracy Ratio (needs retrieval): 0.79\n",
    "Accuracy Ratio (does not need retrieval): 0.53\n",
    "(0.794, 0.528)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger Bidir-RNN:\n",
    "\n",
    "Length small_dataset:  1000\n",
    "Accuracy Ratio (needs retrieval): 0.79\n",
    "Accuracy Ratio (does not need retrieval): 0.48\n",
    "(0.792, 0.482)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger MLP (1 layer):\n",
    "\n",
    "Length small_dataset:  1000\n",
    "Accuracy Ratio (needs retrieval): 0.78\n",
    "Accuracy Ratio (does not need retrieval): 0.43\n",
    "(0.778, 0.426)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length 2400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger LSTM:\n",
    "\n",
    "Length small_dataset:  2400\n",
    "Accuracy Ratio (needs retrieval): 0.74\n",
    "Accuracy Ratio (does not need retrieval): 0.55\n",
    "(0.7375, 0.5466666666666666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger GRU:\n",
    "\n",
    "Length small_dataset:  2400\n",
    "Accuracy Ratio (needs retrieval): 0.79\n",
    "Accuracy Ratio (does not need retrieval): 0.51\n",
    "(0.7941666666666667, 0.5133333333333333)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger RNN:\n",
    "\n",
    "Length small_dataset:  2400\n",
    "Accuracy Ratio (needs retrieval): 0.78\n",
    "Accuracy Ratio (does not need retrieval): 0.50\n",
    "(0.7775, 0.5016666666666667)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger Bidir-RNN:\n",
    "\n",
    "Length small_dataset:  2400\n",
    "Accuracy Ratio (needs retrieval): 0.79\n",
    "Accuracy Ratio (does not need retrieval): 0.48\n",
    "(0.7875, 0.4825)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger MLP (1 layer):\n",
    "\n",
    "Length small_dataset:  2400\n",
    "Accuracy Ratio (needs retrieval): 0.77\n",
    "Accuracy Ratio (does not need retrieval): 0.43\n",
    "(0.7658333333333334, 0.43333333333333335)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs - Accuracy Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "models = ['LSTM', 'GRU', 'RNN', 'Bidir-RNN', 'MLP (1 layer)']\n",
    "accuracy_needs_retrieval = [0.77, 0.81, 0.79, 0.79, 0.78]\n",
    "accuracy_does_not_need_retrieval = [0.56, 0.52, 0.53, 0.48, 0.43]\n",
    "\n",
    "# Plotting\n",
    "x = range(len(models))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, accuracy_needs_retrieval, marker='o', label='Accuracy (needs retrieval)')\n",
    "plt.plot(x, accuracy_does_not_need_retrieval, marker='o', label='Accuracy (does not need retrieval)')\n",
    "plt.xticks(x, models)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy Ratio')\n",
    "plt.title('Accuracy Ratios for Different Models')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "models = ['LSTM', 'GRU', 'RNN', 'Bidir-RNN', 'MLP (1 layer)', 'SKR']\n",
    "accuracy_needs_retrieval = [0.77, 0.81, 0.79, 0.79, 0.78, 0.81]\n",
    "accuracy_does_not_need_retrieval = [0.56, 0.52, 0.53, 0.48, 0.43, 0.15]\n",
    "\n",
    "# Plotting\n",
    "x = range(len(models))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x, accuracy_needs_retrieval, width=0.4, label='Accuracy (needs retrieval)', align='center')\n",
    "plt.bar([p + 0.4 for p in x], accuracy_does_not_need_retrieval, width=0.4, label='Accuracy (does not need retrieval)', align='center')\n",
    "\n",
    "plt.xticks([p + 0.2 for p in x], models)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy Ratio')\n",
    "plt.title('Accuracy Ratios for Different Models')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=2)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "models = ['Judger LSTM', 'Judger GRU', 'Judger RNN', 'Judger Bidir-RNN', 'Judger MLP (1 layer)']\n",
    "accuracy_needs_retrieval = [0.74, 0.79, 0.78, 0.79, 0.77]\n",
    "accuracy_does_not_need_retrieval = [0.55, 0.51, 0.50, 0.48, 0.43]\n",
    "\n",
    "# Plotting\n",
    "x = range(len(models))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x, accuracy_needs_retrieval, width=0.4, label='Accuracy (needs retrieval)', align='center')\n",
    "plt.bar([p + 0.4 for p in x], accuracy_does_not_need_retrieval, width=0.4, label='Accuracy (does not need retrieval)', align='center')\n",
    "\n",
    "plt.xticks([p + 0.2 for p in x], models)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy Ratio')\n",
    "plt.title('Accuracy Ratios for Different Judger Models')\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=2)  # Move legend down\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Results - Pipelines Performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger LSTM:\n",
    "\n",
    "0.435"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger GRU:\n",
    "\n",
    "0.435"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger MLP (1 layer):\n",
    "\n",
    "0.425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No RAG:\n",
    "\n",
    "0.285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always RAG:\n",
    "\n",
    "0.410"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger RNN:\n",
    "\n",
    "0.435"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judger Bidirectional RNN:\n",
    "\n",
    "0.405"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
